{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\isekaizen\\utils\\input_shapes.py",
  "imports": [
    {
      "name": "torch",
      "line": 5
    },
    {
      "name": "torch.nn",
      "line": 6
    },
    {
      "name": "logging",
      "line": 7
    },
    {
      "name": "typing.Tuple",
      "line": 8
    },
    {
      "name": "typing.List",
      "line": 8
    },
    {
      "name": "typing.Dict",
      "line": 8
    },
    {
      "name": "typing.Any",
      "line": 8
    },
    {
      "name": "typing.Optional",
      "line": 8
    },
    {
      "name": "typing.Union",
      "line": 8
    }
  ],
  "classes": {},
  "functions": {
    "infer_input_shape": {
      "start_line": 12,
      "end_line": 99,
      "parameters": [
        {
          "name": "model"
        }
      ],
      "return_type": "complex_type",
      "calls": [
        {
          "name": "logger.debug",
          "line": 25
        },
        {
          "name": "model.modules",
          "line": 28
        },
        {
          "name": "logger.warning",
          "line": 96
        },
        {
          "name": "isinstance",
          "line": 29
        },
        {
          "name": "hasattr",
          "line": 71
        },
        {
          "name": "logger.info",
          "line": 73
        },
        {
          "name": "hasattr",
          "line": 77
        },
        {
          "name": "hasattr",
          "line": 77
        },
        {
          "name": "model.forward.__annotations__.items",
          "line": 78
        },
        {
          "name": "isinstance",
          "line": 47
        },
        {
          "name": "tuple",
          "line": 72
        },
        {
          "name": "logger.info",
          "line": 35
        },
        {
          "name": "logger.info",
          "line": 50
        },
        {
          "name": "isinstance",
          "line": 52
        },
        {
          "name": "str",
          "line": 81
        },
        {
          "name": "logger.info",
          "line": 40
        },
        {
          "name": "logger.info",
          "line": 45
        },
        {
          "name": "logger.info",
          "line": 55
        },
        {
          "name": "isinstance",
          "line": 57
        },
        {
          "name": "logger.info",
          "line": 60
        },
        {
          "name": "any",
          "line": 62
        },
        {
          "name": "....strip",
          "line": 85
        },
        {
          "name": "shape_str.split",
          "line": 86
        },
        {
          "name": "logger.info",
          "line": 89
        },
        {
          "name": "tuple",
          "line": 90
        },
        {
          "name": "hasattr",
          "line": 65
        },
        {
          "name": "isinstance",
          "line": 62
        },
        {
          "name": "logger.info",
          "line": 67
        },
        {
          "name": "int",
          "line": 88
        },
        {
          "name": "type_str.split",
          "line": 85
        },
        {
          "name": "part.strip",
          "line": 88
        },
        {
          "name": "part.strip",
          "line": 88
        },
        {
          "name": "tuple",
          "line": 89
        }
      ],
      "docstring": "\n    Attempt to infer input shape from model architecture.\n    \n    This utility function analyzes model layers to determine the most likely\n    input shape for the model, based on conventions and layer expectations.\n    \n    Args:\n        model: PyTorch model to analyze\n        \n    Returns:\n        Tuple representing input shape (including batch dimension of 1)\n    ",
      "code_snippet": "logger = logging.getLogger(__name__)\n\ndef infer_input_shape(model: nn.Module) -> Tuple[int, ...]:\n    \"\"\"\n    Attempt to infer input shape from model architecture.\n    \n    This utility function analyzes model layers to determine the most likely\n    input shape for the model, based on conventions and layer expectations.\n    \n    Args:\n        model: PyTorch model to analyze\n        \n    Returns:\n        Tuple representing input shape (including batch dimension of 1)\n    \"\"\"\n    logger.debug(f\"Inferring input shape for {model.__class__.__name__}\")\n    \n    # Try to get input shape from first conv layer or linear layer\n    for module in model.modules():\n        if isinstance(module, nn.Conv2d):\n            channels = module.in_channels\n            # Estimate input size based on common datasets\n            if channels == 3:\n                # RGB image, likely CIFAR or similar\n                shape = (1, 3, 32, 32)\n                logger.info(f\"Inferred input shape: {shape} (from Conv2d with 3 channels)\")\n                return shape\n            elif channels == 1:\n                # Grayscale image, likely MNIST or similar\n                shape = (1, 1, 28, 28)\n                logger.info(f\"Inferred input shape: {shape} (from Conv2d with 1 channel)\")\n                return shape\n            else:\n                # General case with unusual channel count\n                shape = (1, channels, 32, 32)  # Use 32x32 as a safe default\n                logger.info(f\"Inferred input shape: {shape} (from Conv2d with {channels} channels)\")\n                return shape\n        elif isinstance(module, nn.Conv1d):\n            channels = module.in_channels\n            shape = (1, channels, 128)  # Default sequence length of 128\n            logger.info(f\"Inferred input shape: {shape} (from Conv1d)\")\n            return shape\n        elif isinstance(module, nn.Conv3d):\n            channels = module.in_channels\n            shape = (1, channels, 16, 32, 32)  # Default 3D shape\n            logger.info(f\"Inferred input shape: {shape} (from Conv3d)\")\n            return shape\n        elif isinstance(module, nn.Linear) and module.in_features > 0:\n            # Linear layer typically expects flattened input\n            shape = (1, module.in_features)\n            logger.info(f\"Inferred input shape: {shape} (from Linear layer)\")\n            return shape\n        elif any(isinstance(module, layer_type) for layer_type in \n                [nn.RNN, nn.LSTM, nn.GRU, nn.RNNCell, nn.LSTMCell, nn.GRUCell]):\n            # Recurrent networks typically expect (seq_len, batch, input_size) or (batch, seq_len, input_size)\n            if hasattr(module, 'input_size'):\n                shape = (1, 64, module.input_size)  # batch, seq_len, features\n                logger.info(f\"Inferred input shape: {shape} (from RNN/LSTM/GRU)\")\n                return shape\n    \n    # Check if model has specific input shape requirements in attributes or docstring\n    if hasattr(model, 'expected_input_shape') and model.expected_input_shape is not None:\n        shape = (1,) + tuple(model.expected_input_shape)\n        logger.info(f\"Using model.expected_input_shape: {shape}\")\n        return shape\n    \n    # Check if model has a forward method with type hints\n    if hasattr(model, 'forward') and hasattr(model.forward, '__annotations__'):\n        for param_name, param_type in model.forward.__annotations__.items():\n            if param_name != 'return':\n                # Try to extract shape information from type hints\n                type_str = str(param_type)\n                if 'Tensor' in type_str and 'shape' in type_str:\n                    try:\n                        # Very basic parsing of type hint like \"Tensor[shape=(B, 3, 224, 224)]\"\n                        shape_str = type_str.split('shape=')[1].strip('()[]')\n                        shape_parts = shape_str.split(',')\n                        # Replace B with 1 for batch dimension\n                        shape = [1 if part.strip() == 'B' else int(part.strip()) for part in shape_parts]\n                        logger.info(f\"Inferred input shape from type hints: {tuple(shape)}\")\n                        return tuple(shape)\n                    except (IndexError, ValueError):\n                        pass\n    \n    # Default fallback - assume CIFAR-like dataset as a safe default\n    default_shape = (1, 3, 32, 32)\n    logger.warning(f\"Could not infer input shape, using default: {default_shape}\")\n    return default_shape\n\ndef get_sample_input(model: nn.Module, device: Optional[torch.device] = None) -> torch.Tensor:\n    \"\"\"\n    Create a sample input tensor of the appropriate shape for the model."
    },
    "get_sample_input": {
      "start_line": 99,
      "end_line": 125,
      "parameters": [
        {
          "name": "model"
        },
        {
          "name": "device"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "infer_input_shape",
          "line": 118
        },
        {
          "name": "torch.zeros",
          "line": 121
        },
        {
          "name": "next",
          "line": 113
        },
        {
          "name": "torch.device",
          "line": 115
        },
        {
          "name": "model.parameters",
          "line": 113
        },
        {
          "name": "torch.cuda.is_available",
          "line": 115
        }
      ],
      "docstring": "\n    Create a sample input tensor of the appropriate shape for the model.\n    \n    Args:\n        model: PyTorch model to create input for\n        device: Device to place the tensor on (default: infer from model)\n        \n    Returns:\n        Sample input tensor with batch dimension of 1\n    ",
      "code_snippet": "    return default_shape\n\ndef get_sample_input(model: nn.Module, device: Optional[torch.device] = None) -> torch.Tensor:\n    \"\"\"\n    Create a sample input tensor of the appropriate shape for the model.\n    \n    Args:\n        model: PyTorch model to create input for\n        device: Device to place the tensor on (default: infer from model)\n        \n    Returns:\n        Sample input tensor with batch dimension of 1\n    \"\"\"\n    # Infer device if not provided\n    if device is None:\n        try:\n            device = next(model.parameters()).device\n        except (StopIteration, AttributeError):\n            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Get input shape \n    input_shape = infer_input_shape(model)\n    \n    # Create sample input\n    sample_input = torch.zeros(input_shape, device=device)\n    \n    return sample_input\n\ndef create_batch(model: nn.Module, batch_size: int, device: Optional[torch.device] = None) -> torch.Tensor:\n    \"\"\"\n    Create a batch of the appropriate shape for the model."
    },
    "create_batch": {
      "start_line": 125,
      "end_line": 155,
      "parameters": [
        {
          "name": "model"
        },
        {
          "name": "batch_size",
          "type": "int"
        },
        {
          "name": "device"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "infer_input_shape",
          "line": 138
        },
        {
          "name": "torch.zeros",
          "line": 151
        },
        {
          "name": "next",
          "line": 143
        },
        {
          "name": "torch.device",
          "line": 145
        },
        {
          "name": "model.parameters",
          "line": 143
        },
        {
          "name": "torch.cuda.is_available",
          "line": 145
        }
      ],
      "docstring": "\n    Create a batch of the appropriate shape for the model.\n    \n    Args:\n        model: PyTorch model to create batch for\n        batch_size: Desired batch size\n        device: Device to place the tensor on (default: infer from model)\n        \n    Returns:\n        Batch tensor with specified batch dimension\n    ",
      "code_snippet": "    return sample_input\n\ndef create_batch(model: nn.Module, batch_size: int, device: Optional[torch.device] = None) -> torch.Tensor:\n    \"\"\"\n    Create a batch of the appropriate shape for the model.\n    \n    Args:\n        model: PyTorch model to create batch for\n        batch_size: Desired batch size\n        device: Device to place the tensor on (default: infer from model)\n        \n    Returns:\n        Batch tensor with specified batch dimension\n    \"\"\"\n    # Get input shape \n    input_shape = infer_input_shape(model)\n    \n    # Infer device if not provided\n    if device is None:\n        try:\n            device = next(model.parameters()).device\n        except (StopIteration, AttributeError):\n            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Create shape with specified batch size\n    batch_shape = (batch_size,) + input_shape[1:]\n    \n    # Create batch\n    batch = torch.zeros(batch_shape, device=device)\n    \n    return batch"
    }
  },
  "constants": {}
}