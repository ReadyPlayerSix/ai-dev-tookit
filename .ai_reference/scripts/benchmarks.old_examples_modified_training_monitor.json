{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\benchmarks.old\\examples\\modified\\training_monitor.py",
  "imports": [
    {
      "name": "os",
      "line": 12
    },
    {
      "name": "sys",
      "line": 13
    },
    {
      "name": "time",
      "line": 14
    },
    {
      "name": "json",
      "line": 15
    },
    {
      "name": "logging",
      "line": 16
    },
    {
      "name": "torch",
      "line": 17
    },
    {
      "name": "torch.nn",
      "line": 18
    },
    {
      "name": "psutil",
      "line": 19
    },
    {
      "name": "numpy",
      "line": 20
    },
    {
      "name": "typing.Dict",
      "line": 21
    },
    {
      "name": "typing.List",
      "line": 21
    },
    {
      "name": "typing.Any",
      "line": 21
    },
    {
      "name": "typing.Optional",
      "line": 21
    },
    {
      "name": "typing.Tuple",
      "line": 21
    },
    {
      "name": "datetime.datetime",
      "line": 22
    }
  ],
  "classes": {
    "TrainingMonitor": {
      "start_line": 32,
      "end_line": 378,
      "methods": {
        "__init__": {
          "start_line": 35,
          "end_line": 81,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "model"
            },
            {
              "name": "output_dir",
              "type": "str"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "os.makedirs",
              "line": 45
            },
            {
              "name": "time.time",
              "line": 48
            },
            {
              "name": "torch.cuda.is_available",
              "line": 74
            },
            {
              "name": "os.path.join",
              "line": 44
            },
            {
              "name": "torch.cuda.device_count",
              "line": 76
            },
            {
              "name": "range",
              "line": 78
            },
            {
              "name": "torch.cuda.reset_peak_memory_stats",
              "line": 79
            }
          ],
          "docstring": "\n        Initialize the training monitor\n        \n        Args:\n            model: The model being trained\n            output_dir: Directory for saving results\n        ",
          "code_snippet": "    \"\"\"Monitors training progress and resource utilization\"\"\"\n    \n    def __init__(self, model: nn.Module, output_dir: str = None):\n        \"\"\"\n        Initialize the training monitor\n        \n        Args:\n            model: The model being trained\n            output_dir: Directory for saving results\n        \"\"\"\n        self.model = model\n        self.output_dir = output_dir or os.path.join(\"examples\", \"output\")\n        os.makedirs(self.output_dir, exist_ok=True)\n        \n        # Initialization timestamp\n        self.start_time = time.time()\n        self.last_update_time = self.start_time\n        \n        # Training metrics\n        self.metrics = {\n            \"epoch\": 0,\n            \"train_loss\": [],\n            \"train_acc\": [],\n            \"val_loss\": [],\n            \"val_acc\": [],\n            \"learning_rates\": [],\n            \"batch_sizes\": [],\n            \"time_per_epoch\": [],\n            \"memory_usage\": [],\n            \"cpu_usage\": []\n        }\n        \n        # Peak metrics\n        self.peak_metrics = {\n            \"peak_gpu_memory\": 0,\n            \"peak_cpu_util\": 0,\n            \"best_val_acc\": 0,\n            \"best_val_epoch\": 0\n        }\n        \n        # Initialize resource tracking\n        self.has_gpu = torch.cuda.is_available()\n        if self.has_gpu:\n            self.gpu_count = torch.cuda.device_count()\n            # Reset peak memory stats\n            for i in range(self.gpu_count):\n                torch.cuda.reset_peak_memory_stats(i)\n    \n    def update(self, epoch: int, metrics: Dict[str, float], batch_size: int, learning_rate: float) -> Dict[str, Any]:\n        \"\"\"\n        Update training metrics"
        },
        "update": {
          "start_line": 81,
          "end_line": 145,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "epoch",
              "type": "int"
            },
            {
              "name": "metrics"
            },
            {
              "name": "batch_size",
              "type": "int"
            },
            {
              "name": "learning_rate",
              "type": "float"
            }
          ],
          "return_type": "complex_type",
          "calls": [
            {
              "name": "....append",
              "line": 105
            },
            {
              "name": "....append",
              "line": 106
            },
            {
              "name": "time.time",
              "line": 109
            },
            {
              "name": "....append",
              "line": 111
            },
            {
              "name": "psutil.cpu_percent",
              "line": 115
            },
            {
              "name": "....append",
              "line": 116
            },
            {
              "name": "max",
              "line": 119
            },
            {
              "name": "self._get_gpu_memory_info",
              "line": 123
            },
            {
              "name": "....append",
              "line": 124
            },
            {
              "name": "range",
              "line": 128
            },
            {
              "name": "max",
              "line": 130
            },
            {
              "name": "....append",
              "line": 102
            },
            {
              "name": "max",
              "line": 129
            },
            {
              "name": "torch.cuda.max_memory_allocated",
              "line": 129
            }
          ],
          "docstring": "\n        Update training metrics\n        \n        Args:\n            epoch: Current epoch number\n            metrics: Dictionary with train_loss, train_acc, val_loss, val_acc\n            batch_size: Current batch size\n            learning_rate: Current learning rate\n            \n        Returns:\n            Dictionary with updated metrics\n        ",
          "code_snippet": "                torch.cuda.reset_peak_memory_stats(i)\n    \n    def update(self, epoch: int, metrics: Dict[str, float], batch_size: int, learning_rate: float) -> Dict[str, Any]:\n        \"\"\"\n        Update training metrics\n        \n        Args:\n            epoch: Current epoch number\n            metrics: Dictionary with train_loss, train_acc, val_loss, val_acc\n            batch_size: Current batch size\n            learning_rate: Current learning rate\n            \n        Returns:\n            Dictionary with updated metrics\n        \"\"\"\n        # Update epoch\n        self.metrics[\"epoch\"] = epoch\n        \n        # Update training metrics\n        for key in [\"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\"]:\n            if key in metrics:\n                if key not in self.metrics:\n                    self.metrics[key] = []\n                self.metrics[key].append(metrics[key])\n        \n        # Update batch size and learning rate\n        self.metrics[\"batch_sizes\"].append(batch_size)\n        self.metrics[\"learning_rates\"].append(learning_rate)\n        \n        # Update time metrics\n        current_time = time.time()\n        epoch_time = current_time - self.last_update_time\n        self.metrics[\"time_per_epoch\"].append(epoch_time)\n        self.last_update_time = current_time\n        \n        # Update resource metrics\n        cpu_percent = psutil.cpu_percent()\n        self.metrics[\"cpu_usage\"].append(cpu_percent)\n        \n        # Update peak CPU utilization\n        self.peak_metrics[\"peak_cpu_util\"] = max(self.peak_metrics[\"peak_cpu_util\"], cpu_percent)\n        \n        # Update GPU memory usage if available\n        if self.has_gpu:\n            memory_info = self._get_gpu_memory_info()\n            self.metrics[\"memory_usage\"].append(memory_info)\n            \n            # Update peak GPU memory\n            peak_memory = 0\n            for i in range(self.gpu_count):\n                peak_memory = max(peak_memory, torch.cuda.max_memory_allocated(i))\n            self.peak_metrics[\"peak_gpu_memory\"] = max(self.peak_metrics[\"peak_gpu_memory\"], peak_memory)\n        \n        # Update best validation accuracy\n        if \"val_acc\" in metrics and metrics[\"val_acc\"] > self.peak_metrics[\"best_val_acc\"]:\n            self.peak_metrics[\"best_val_acc\"] = metrics[\"val_acc\"]\n            self.peak_metrics[\"best_val_epoch\"] = epoch\n        \n        # Return current state of all metrics\n        return {\n            \"metrics\": self.metrics,\n            \"peak_metrics\": self.peak_metrics,\n            \"current_epoch\": epoch,\n            \"current_batch_size\": batch_size,\n            \"elapsed_time\": current_time - self.start_time\n        }\n    \n    def _get_gpu_memory_info(self) -> Dict[str, int]:\n        \"\"\"Get current GPU memory information\"\"\""
        },
        "_get_gpu_memory_info": {
          "start_line": 146,
          "end_line": 167,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": "complex_type",
          "calls": [
            {
              "name": "range",
              "line": 158
            },
            {
              "name": "torch.cuda.get_device_properties",
              "line": 159
            },
            {
              "name": "torch.cuda.memory_allocated",
              "line": 161
            },
            {
              "name": "torch.cuda.memory_reserved",
              "line": 162
            }
          ],
          "docstring": "Get current GPU memory information",
          "code_snippet": "        }\n    \n    def _get_gpu_memory_info(self) -> Dict[str, int]:\n        \"\"\"Get current GPU memory information\"\"\"\n        memory_info = {\n            \"total\": 0,\n            \"allocated\": 0,\n            \"reserved\": 0,\n            \"free\": 0\n        }\n        \n        if not self.has_gpu:\n            return memory_info\n        \n        for i in range(self.gpu_count):\n            device_props = torch.cuda.get_device_properties(i)\n            memory_info[\"total\"] += device_props.total_memory\n            memory_info[\"allocated\"] += torch.cuda.memory_allocated(i)\n            memory_info[\"reserved\"] += torch.cuda.memory_reserved(i)\n        \n        memory_info[\"free\"] = memory_info[\"total\"] - memory_info[\"allocated\"]\n        return memory_info\n    \n    def measure_inference_time(self, input_size: Tuple[int, ...] = None, \n                              num_samples: int = 100, \n                              warmup: int = 10) -> Dict[str, float]:"
        },
        "measure_inference_time": {
          "start_line": 167,
          "end_line": 234,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "input_size"
            },
            {
              "name": "num_samples",
              "type": "int"
            },
            {
              "name": "warmup",
              "type": "int"
            }
          ],
          "return_type": "complex_type",
          "calls": [
            {
              "name": "logger.info",
              "line": 181
            },
            {
              "name": "self.model.eval",
              "line": 184
            },
            {
              "name": "torch.randn",
              "line": 195
            },
            {
              "name": "time.time",
              "line": 207
            },
            {
              "name": "time.time",
              "line": 216
            },
            {
              "name": "logger.info",
              "line": 231
            },
            {
              "name": "next",
              "line": 187
            },
            {
              "name": "torch.no_grad",
              "line": 198
            },
            {
              "name": "range",
              "line": 199
            },
            {
              "name": "torch.cuda.synchronize",
              "line": 204
            },
            {
              "name": "torch.no_grad",
              "line": 208
            },
            {
              "name": "range",
              "line": 209
            },
            {
              "name": "torch.cuda.synchronize",
              "line": 214
            },
            {
              "name": "str",
              "line": 228
            },
            {
              "name": "self.model.parameters",
              "line": 187
            },
            {
              "name": "self.model",
              "line": 200
            },
            {
              "name": "self.model",
              "line": 210
            }
          ],
          "docstring": "\n        Measure model inference time\n        \n        Args:\n            input_size: Input size tuple (batch_size, channels, height, width) for vision models\n            num_samples: Number of samples to use for measurement\n            warmup: Number of warmup iterations\n            \n        Returns:\n            Dictionary with inference timing metrics\n        ",
          "code_snippet": "        return memory_info\n    \n    def measure_inference_time(self, input_size: Tuple[int, ...] = None, \n                              num_samples: int = 100, \n                              warmup: int = 10) -> Dict[str, float]:\n        \"\"\"\n        Measure model inference time\n        \n        Args:\n            input_size: Input size tuple (batch_size, channels, height, width) for vision models\n            num_samples: Number of samples to use for measurement\n            warmup: Number of warmup iterations\n            \n        Returns:\n            Dictionary with inference timing metrics\n        \"\"\"\n        logger.info(\"Measuring inference time...\")\n        \n        # Set model to evaluation mode\n        self.model.eval()\n        \n        # Determine device\n        device = next(self.model.parameters()).device\n        \n        # Create default input size for vision models if not specified\n        if input_size is None:\n            # Default size for vision models: (1, 3, 32, 32)\n            input_size = (1, 3, 32, 32)\n        \n        # Create random input\n        dummy_input = torch.randn(input_size, device=device)\n        \n        # Warmup\n        with torch.no_grad():\n            for _ in range(warmup):\n                _ = self.model(dummy_input)\n        \n        # Synchronize if using GPU\n        if device.type == 'cuda':\n            torch.cuda.synchronize()\n        \n        # Measure inference time\n        start_time = time.time()\n        with torch.no_grad():\n            for _ in range(num_samples):\n                _ = self.model(dummy_input)\n                \n        # Synchronize again if using GPU\n        if device.type == 'cuda':\n            torch.cuda.synchronize()\n        \n        end_time = time.time()\n        \n        # Calculate metrics\n        total_time = end_time - start_time\n        time_per_sample = total_time / num_samples\n        samples_per_second = num_samples / total_time\n        \n        inference_metrics = {\n            \"total_time\": total_time,\n            \"time_per_sample\": time_per_sample,\n            \"samples_per_second\": samples_per_second,\n            \"batch_size\": input_size[0],\n            \"device\": str(device)\n        }\n        \n        logger.info(f\"Inference time: {time_per_sample*1000:.2f} ms/sample ({samples_per_second:.2f} samples/sec)\")\n        return inference_metrics\n    \n    def get_model_size(self) -> Dict[str, Any]:\n        \"\"\"\n        Get model size information"
        },
        "get_model_size": {
          "start_line": 234,
          "end_line": 264,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": "complex_type",
          "calls": [
            {
              "name": "sum",
              "line": 242
            },
            {
              "name": "sum",
              "line": 243
            },
            {
              "name": "sum",
              "line": 246
            },
            {
              "name": "sum",
              "line": 247
            },
            {
              "name": "logger.info",
              "line": 261
            },
            {
              "name": "p.numel",
              "line": 242
            },
            {
              "name": "p.numel",
              "line": 243
            },
            {
              "name": "self.model.parameters",
              "line": 242
            },
            {
              "name": "self.model.parameters",
              "line": 243
            },
            {
              "name": "p.numel",
              "line": 246
            },
            {
              "name": "p.element_size",
              "line": 246
            },
            {
              "name": "self.model.parameters",
              "line": 246
            },
            {
              "name": "b.numel",
              "line": 247
            },
            {
              "name": "b.element_size",
              "line": 247
            },
            {
              "name": "self.model.buffers",
              "line": 247
            }
          ],
          "docstring": "\n        Get model size information\n        \n        Returns:\n            Dictionary with model size metrics\n        ",
          "code_snippet": "        return inference_metrics\n    \n    def get_model_size(self) -> Dict[str, Any]:\n        \"\"\"\n        Get model size information\n        \n        Returns:\n            Dictionary with model size metrics\n        \"\"\"\n        # Count parameters\n        num_params = sum(p.numel() for p in self.model.parameters())\n        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n        \n        # Estimate model size in memory\n        param_size = sum(p.numel() * p.element_size() for p in self.model.parameters())\n        buffer_size = sum(b.numel() * b.element_size() for b in self.model.buffers())\n        \n        # Get model architecture name if possible\n        model_name = self.model.__class__.__name__\n        \n        model_info = {\n            \"name\": model_name,\n            \"parameter_count\": num_params,\n            \"trainable_parameter_count\": trainable_params,\n            \"model_size\": param_size + buffer_size,  # Size in bytes\n            \"param_size\": param_size,\n            \"buffer_size\": buffer_size\n        }\n        \n        logger.info(f\"Model: {model_name} with {num_params:,} parameters ({(param_size + buffer_size)/1e6:.2f} MB)\")\n        return model_info\n    \n    def get_final_metrics(self) -> Dict[str, Any]:\n        \"\"\"\n        Get final training metrics"
        },
        "get_final_metrics": {
          "start_line": 264,
          "end_line": 328,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": "complex_type",
          "calls": [
            {
              "name": "self.measure_inference_time",
              "line": 301
            },
            {
              "name": "self.get_model_size",
              "line": 304
            },
            {
              "name": "time.time",
              "line": 272
            },
            {
              "name": "len",
              "line": 275
            },
            {
              "name": "np.mean",
              "line": 276
            },
            {
              "name": "np.max",
              "line": 277
            },
            {
              "name": "np.min",
              "line": 278
            },
            {
              "name": "len",
              "line": 284
            },
            {
              "name": "sum",
              "line": 287
            }
          ],
          "docstring": "\n        Get final training metrics\n        \n        Returns:\n            Dictionary with final metrics\n        ",
          "code_snippet": "        return model_info\n    \n    def get_final_metrics(self) -> Dict[str, Any]:\n        \"\"\"\n        Get final training metrics\n        \n        Returns:\n            Dictionary with final metrics\n        \"\"\"\n        # Calculate overall training time\n        total_time = time.time() - self.start_time\n        \n        # Calculate per-epoch statistics\n        if len(self.metrics[\"time_per_epoch\"]) > 0:\n            avg_epoch_time = np.mean(self.metrics[\"time_per_epoch\"])\n            max_epoch_time = np.max(self.metrics[\"time_per_epoch\"])\n            min_epoch_time = np.min(self.metrics[\"time_per_epoch\"])\n        else:\n            avg_epoch_time = max_epoch_time = min_epoch_time = 0\n        \n        # Calculate convergence metrics if we have validation data\n        convergence_metrics = {}\n        if \"val_acc\" in self.metrics and len(self.metrics[\"val_acc\"]) > 0:\n            # Time to reach best validation accuracy\n            best_epoch = self.peak_metrics[\"best_val_epoch\"]\n            time_to_best = sum(self.metrics[\"time_per_epoch\"][:best_epoch]) if best_epoch > 0 else 0\n            \n            # Calculate convergence rate (accuracy gained per epoch)\n            if best_epoch > 0 and self.metrics[\"val_acc\"][0] < self.peak_metrics[\"best_val_acc\"]:\n                convergence_rate = (self.peak_metrics[\"best_val_acc\"] - self.metrics[\"val_acc\"][0]) / best_epoch\n            else:\n                convergence_rate = 0\n                \n            convergence_metrics = {\n                \"time_to_best_accuracy\": time_to_best,\n                \"convergence_rate_per_epoch\": convergence_rate\n            }\n        \n        # Measure final inference time\n        inference_metrics = self.measure_inference_time()\n        \n        # Get model size info\n        model_info = self.get_model_size()\n        \n        # Combine all metrics\n        final_metrics = {\n            \"training_time\": total_time,\n            \"epoch_stats\": {\n                \"average_time\": avg_epoch_time,\n                \"max_time\": max_epoch_time,\n                \"min_time\": min_epoch_time,\n                \"total_epochs\": self.metrics[\"epoch\"]\n            },\n            \"best_accuracy\": self.peak_metrics[\"best_val_acc\"],\n            \"best_epoch\": self.peak_metrics[\"best_val_epoch\"],\n            \"convergence\": convergence_metrics,\n            \"inference\": inference_metrics,\n            \"model\": model_info,\n            \"resource_usage\": {\n                \"peak_gpu_memory_gb\": self.peak_metrics[\"peak_gpu_memory\"] / 1e9 if self.has_gpu else 0,\n                \"peak_cpu_util\": self.peak_metrics[\"peak_cpu_util\"]\n            }\n        }\n        \n        return final_metrics\n    \n    def save_metrics(self, filename: Optional[str] = None) -> str:\n        \"\"\"\n        Save all metrics to a JSON file"
        },
        "save_metrics": {
          "start_line": 328,
          "end_line": 378,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "filename"
            }
          ],
          "return_type": "str",
          "calls": [
            {
              "name": "os.path.join",
              "line": 342
            },
            {
              "name": "make_serializable",
              "line": 369
            },
            {
              "name": "logger.info",
              "line": 375
            },
            {
              "name": "....strftime",
              "line": 339
            },
            {
              "name": "self.get_final_metrics",
              "line": 348
            },
            {
              "name": "....isoformat",
              "line": 349
            },
            {
              "name": "isinstance",
              "line": 354
            },
            {
              "name": "open",
              "line": 372
            },
            {
              "name": "json.dump",
              "line": 373
            },
            {
              "name": "isinstance",
              "line": 356
            },
            {
              "name": "datetime.now",
              "line": 339
            },
            {
              "name": "datetime.now",
              "line": 349
            },
            {
              "name": "make_serializable",
              "line": 355
            },
            {
              "name": "isinstance",
              "line": 358
            },
            {
              "name": "make_serializable",
              "line": 357
            },
            {
              "name": "isinstance",
              "line": 360
            },
            {
              "name": "obj.items",
              "line": 357
            },
            {
              "name": "type",
              "line": 358
            },
            {
              "name": "obj.tolist",
              "line": 361
            },
            {
              "name": "isinstance",
              "line": 362
            },
            {
              "name": "int",
              "line": 363
            },
            {
              "name": "isinstance",
              "line": 364
            },
            {
              "name": "float",
              "line": 365
            },
            {
              "name": "str",
              "line": 367
            }
          ],
          "docstring": "\n        Save all metrics to a JSON file\n        \n        Args:\n            filename: Optional filename (if not provided, a timestamped name is used)\n            \n        Returns:\n            Path to the saved file\n        ",
          "code_snippet": "        return final_metrics\n    \n    def save_metrics(self, filename: Optional[str] = None) -> str:\n        \"\"\"\n        Save all metrics to a JSON file\n        \n        Args:\n            filename: Optional filename (if not provided, a timestamped name is used)\n            \n        Returns:\n            Path to the saved file\n        \"\"\"\n        if filename is None:\n            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n            filename = f\"training_metrics_{timestamp}.json\"\n        \n        filepath = os.path.join(self.output_dir, filename)\n        \n        # Get final metrics\n        all_metrics = {\n            \"training\": self.metrics,\n            \"peak\": self.peak_metrics,\n            \"final\": self.get_final_metrics(),\n            \"timestamp\": datetime.now().isoformat()\n        }\n        \n        # Convert non-serializable values\n        def make_serializable(obj):\n            if isinstance(obj, (list, tuple)):\n                return [make_serializable(item) for item in obj]\n            elif isinstance(obj, dict):\n                return {k: make_serializable(v) for k, v in obj.items()}\n            elif isinstance(obj, (int, float, str, bool, type(None))):\n                return obj\n            elif isinstance(obj, np.ndarray):\n                return obj.tolist()\n            elif isinstance(obj, np.integer):\n                return int(obj)\n            elif isinstance(obj, np.floating):\n                return float(obj)\n            else:\n                return str(obj)\n        \n        serializable_metrics = make_serializable(all_metrics)\n        \n        # Save to file\n        with open(filepath, 'w') as f:\n            json.dump(serializable_metrics, f, indent=2)\n        \n        logger.info(f\"Metrics saved to: {filepath}\")\n        return filepath\n\n\n# Example of how to use the training monitor\ndef example_usage():"
        }
      },
      "class_variables": [],
      "bases": [],
      "docstring": "Monitors training progress and resource utilization"
    }
  },
  "functions": {
    "example_usage": {
      "start_line": 380,
      "end_line": 429,
      "parameters": [],
      "return_type": null,
      "calls": [
        {
          "name": "nn.Sequential",
          "line": 383
        },
        {
          "name": "torch.device",
          "line": 395
        },
        {
          "name": "model.to",
          "line": 396
        },
        {
          "name": "TrainingMonitor",
          "line": 399
        },
        {
          "name": "range",
          "line": 402
        },
        {
          "name": "monitor.get_final_metrics",
          "line": 420
        },
        {
          "name": "print",
          "line": 421
        },
        {
          "name": "print",
          "line": 422
        },
        {
          "name": "print",
          "line": 423
        },
        {
          "name": "monitor.save_metrics",
          "line": 426
        },
        {
          "name": "print",
          "line": 427
        },
        {
          "name": "nn.Conv2d",
          "line": 384
        },
        {
          "name": "nn.ReLU",
          "line": 385
        },
        {
          "name": "nn.MaxPool2d",
          "line": 386
        },
        {
          "name": "nn.Conv2d",
          "line": 387
        },
        {
          "name": "nn.ReLU",
          "line": 388
        },
        {
          "name": "nn.AdaptiveAvgPool2d",
          "line": 389
        },
        {
          "name": "nn.Flatten",
          "line": 390
        },
        {
          "name": "nn.Linear",
          "line": 391
        },
        {
          "name": "time.sleep",
          "line": 404
        },
        {
          "name": "monitor.update",
          "line": 415
        },
        {
          "name": "print",
          "line": 417
        },
        {
          "name": "torch.cuda.is_available",
          "line": 395
        }
      ],
      "docstring": "Example usage of the TrainingMonitor class",
      "code_snippet": "\n# Example of how to use the training monitor\ndef example_usage():\n    \"\"\"Example usage of the TrainingMonitor class\"\"\"\n    # Create a simple model for demonstration\n    model = nn.Sequential(\n        nn.Conv2d(3, 64, kernel_size=3, padding=1),\n        nn.ReLU(),\n        nn.MaxPool2d(2),\n        nn.Conv2d(64, 128, kernel_size=3, padding=1),\n        nn.ReLU(),\n        nn.AdaptiveAvgPool2d((1, 1)),\n        nn.Flatten(),\n        nn.Linear(128, 10)\n    )\n    \n    # Check if CUDA is available\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    \n    # Create a training monitor\n    monitor = TrainingMonitor(model)\n    \n    # Simulate a few training epochs\n    for epoch in range(1, 6):\n        # Simulate training\n        time.sleep(1)\n        \n        # Update with fake metrics\n        fake_metrics = {\n            \"train_loss\": 1.0 - 0.1 * epoch,\n            \"train_acc\": 50 + 10 * epoch,\n            \"val_loss\": 1.2 - 0.1 * epoch,\n            \"val_acc\": 45 + 10 * epoch\n        }\n        \n        # Update monitor\n        monitor.update(epoch, fake_metrics, batch_size=64, learning_rate=0.01)\n        \n        print(f\"Epoch {epoch} completed. Validation accuracy: {fake_metrics['val_acc']}%\")\n    \n    # Get final metrics\n    final_metrics = monitor.get_final_metrics()\n    print(f\"\\nTraining completed in {final_metrics['training_time']:.2f} seconds\")\n    print(f\"Best validation accuracy: {final_metrics['best_accuracy']:.2f}% at epoch {final_metrics['best_epoch']}\")\n    print(f\"Inference time: {final_metrics['inference']['time_per_sample']*1000:.2f} ms/sample\")\n    \n    # Save metrics\n    filepath = monitor.save_metrics()\n    print(f\"Metrics saved to: {filepath}\")\n\n\nif __name__ == \"__main__\":\n    example_usage()"
    }
  },
  "constants": {}
}