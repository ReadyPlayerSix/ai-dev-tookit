{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\isekaizen\\mediators\\pattern_data_mediator\\pattern_data_mediator.py",
  "imports": [
    {
      "name": "logging",
      "line": 11
    }
  ],
  "classes": {
    "PatternDataMediator": {
      "start_line": 16,
      "end_line": 330,
      "methods": {
        "__init__": {
          "start_line": 34,
          "end_line": 51,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "pattern_map"
            }
          ],
          "return_type": null,
          "calls": [],
          "docstring": "Initialize the PatternDataMediator with empty data structures.",
          "code_snippet": "    \"\"\"\n    \n    def __init__(self, pattern_map=None):\n        \"\"\"Initialize the PatternDataMediator with empty data structures.\"\"\"\n        # Raw data storage - organized by epoch for better management\n        self.batch_data_by_epoch = {}  # {epoch: [(batch_indices, correct_mask), ...]}\n        \n        # Processed metrics cache - also organized by epoch\n        self.metrics_by_epoch = {}  # {epoch: {'accuracies': {}, 'risks': {}}}\n        \n        # Current state\n        self.current_epoch = 0\n        self.pattern_service = None\n        self.pattern_map = pattern_map\n        \n        # Pattern tracking counters\n        self.pattern_correct = {}\n        self.pattern_total = {}\n    \n    def initialize(self):\n        \"\"\"Initialize pattern tracking counters.\"\"\"\n        self.pattern_correct = {}"
        },
        "initialize": {
          "start_line": 51,
          "end_line": 64,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [],
          "docstring": "Initialize pattern tracking counters.",
          "code_snippet": "        self.pattern_total = {}\n    \n    def initialize(self):\n        \"\"\"Initialize pattern tracking counters.\"\"\"\n        self.pattern_correct = {}\n        self.pattern_total = {}\n        \n        # If we have a pattern map, initialize counters for all pattern types\n        if self.pattern_map and 'pattern_types' in self.pattern_map:\n            for pattern_type in self.pattern_map['pattern_types']:\n                self.pattern_correct[pattern_type] = 0\n                self.pattern_total[pattern_type] = 0\n                \n        return self\n    \n    def set_pattern_service(self, pattern_service):\n        \"\"\"\n        Set the pattern service reference for mapping indices to pattern types."
        },
        "set_pattern_service": {
          "start_line": 64,
          "end_line": 73,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "pattern_service"
            }
          ],
          "return_type": null,
          "calls": [],
          "docstring": "\n        Set the pattern service reference for mapping indices to pattern types.\n        \n        Args:\n            pattern_service: The pattern service object\n        ",
          "code_snippet": "        return self\n    \n    def set_pattern_service(self, pattern_service):\n        \"\"\"\n        Set the pattern service reference for mapping indices to pattern types.\n        \n        Args:\n            pattern_service: The pattern service object\n        \"\"\"\n        self.pattern_service = pattern_service\n    \n    def update_with_batch_recognition(self, batch_indices, correct_mask, epoch):\n        \"\"\"\n        Store raw batch recognition data from isekaiZen with epoch tracking."
        },
        "update_with_batch_recognition": {
          "start_line": 73,
          "end_line": 119,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "batch_indices"
            },
            {
              "name": "correct_mask"
            },
            {
              "name": "epoch"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "....append",
              "line": 94
            },
            {
              "name": "logger.debug",
              "line": 95
            },
            {
              "name": "self._cleanup_old_epochs",
              "line": 117
            },
            {
              "name": "logger.debug",
              "line": 91
            },
            {
              "name": "....format",
              "line": 95
            },
            {
              "name": "enumerate",
              "line": 99
            },
            {
              "name": "....format",
              "line": 91
            },
            {
              "name": "len",
              "line": 95
            },
            {
              "name": "self.pattern_service.get_pattern_type",
              "line": 103
            },
            {
              "name": "len",
              "line": 100
            }
          ],
          "docstring": "\n        Store raw batch recognition data from isekaiZen with epoch tracking.\n        \n        Args:\n            batch_indices (list): List of data indices in the batch\n            correct_mask (tensor): Boolean mask of correct predictions\n            epoch (int): Current training epoch\n        ",
          "code_snippet": "        self.pattern_service = pattern_service\n    \n    def update_with_batch_recognition(self, batch_indices, correct_mask, epoch):\n        \"\"\"\n        Store raw batch recognition data from isekaiZen with epoch tracking.\n        \n        Args:\n            batch_indices (list): List of data indices in the batch\n            correct_mask (tensor): Boolean mask of correct predictions\n            epoch (int): Current training epoch\n        \"\"\"\n        # Update current epoch\n        self.current_epoch = epoch\n        \n        # Initialize epoch data if needed\n        if epoch not in self.batch_data_by_epoch:\n            self.batch_data_by_epoch[epoch] = []\n            # Reset pattern counters for new epoch\n            self.pattern_correct = {}\n            self.pattern_total = {}\n            logger.debug(\"PatternDataMediator: Initialized data storage for epoch {}\".format(epoch))\n        \n        # Store the batch data\n        self.batch_data_by_epoch[epoch].append((batch_indices, correct_mask))\n        logger.debug(\"PatternDataMediator: Added batch with {} examples for epoch {}\".format(len(batch_indices), epoch))\n        \n        # Process this batch immediately to update running pattern metrics\n        if self.pattern_service:\n            for i, idx in enumerate(batch_indices):\n                if i >= len(correct_mask):\n                    break\n                    \n                pattern_type = self.pattern_service.get_pattern_type(idx)\n                \n                if pattern_type:\n                    # Initialize counters if needed\n                    if pattern_type not in self.pattern_total:\n                        self.pattern_total[pattern_type] = 0\n                        self.pattern_correct[pattern_type] = 0\n                    \n                    # Update counters\n                    self.pattern_total[pattern_type] += 1\n                    if correct_mask[i]:\n                        self.pattern_correct[pattern_type] += 1\n        \n        # Clean up old epochs - keep only current and previous\n        self._cleanup_old_epochs()\n    \n    def _process_epoch_data(self, epoch):\n        \"\"\"\n        Process data for a specific epoch."
        },
        "_process_epoch_data": {
          "start_line": 119,
          "end_line": 238,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "epoch"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logger.debug",
              "line": 190
            },
            {
              "name": "pattern_total.items",
              "line": 195
            },
            {
              "name": "logger.debug",
              "line": 219
            },
            {
              "name": "logger.isEnabledFor",
              "line": 220
            },
            {
              "name": "....format",
              "line": 226
            },
            {
              "name": "accuracies.items",
              "line": 227
            },
            {
              "name": "logger.info",
              "line": 230
            },
            {
              "name": "logger.warning",
              "line": 134
            },
            {
              "name": "logger.debug",
              "line": 140
            },
            {
              "name": "....format",
              "line": 190
            },
            {
              "name": "....format",
              "line": 219
            },
            {
              "name": "accuracies.items",
              "line": 221
            },
            {
              "name": "....format",
              "line": 228
            },
            {
              "name": "....format",
              "line": 134
            },
            {
              "name": "....format",
              "line": 140
            },
            {
              "name": "logger.debug",
              "line": 145
            },
            {
              "name": "logger.warning",
              "line": 175
            },
            {
              "name": "max",
              "line": 201
            },
            {
              "name": "logger.debug",
              "line": 222
            },
            {
              "name": "risks.get",
              "line": 229
            },
            {
              "name": "pattern_total.get",
              "line": 229
            },
            {
              "name": "....format",
              "line": 145
            },
            {
              "name": "enumerate",
              "line": 158
            },
            {
              "name": "....format",
              "line": 175
            },
            {
              "name": "logger.info",
              "line": 178
            },
            {
              "name": "min",
              "line": 201
            },
            {
              "name": "....format",
              "line": 222
            },
            {
              "name": "isinstance",
              "line": 154
            },
            {
              "name": "self.pattern_service.get_pattern_type",
              "line": 162
            },
            {
              "name": "risks.get",
              "line": 223
            },
            {
              "name": "pattern_total.get",
              "line": 223
            },
            {
              "name": "len",
              "line": 159
            }
          ],
          "docstring": "\n        Process data for a specific epoch.\n        \n        This method calculates pattern accuracies and risks based on the\n        collected batch data for the specified epoch.\n        \n        Args:\n            epoch (int): Epoch to process\n            \n        Returns:\n            bool: True if processing was successful, False otherwise\n        ",
          "code_snippet": "        self._cleanup_old_epochs()\n    \n    def _process_epoch_data(self, epoch):\n        \"\"\"\n        Process data for a specific epoch.\n        \n        This method calculates pattern accuracies and risks based on the\n        collected batch data for the specified epoch.\n        \n        Args:\n            epoch (int): Epoch to process\n            \n        Returns:\n            bool: True if processing was successful, False otherwise\n        \"\"\"\n        # First check: do we have any data to process?\n        if not self.pattern_service:\n            logger.warning(\"PatternDataMediator: Cannot process epoch {} - pattern service not available\".format(epoch))\n            return False\n            \n        # For current epoch, use the running counters that are updated with each batch\n        if epoch == self.current_epoch and self.pattern_total:\n            # Use the incrementally maintained counters\n            logger.debug(\"PatternDataMediator: Using running counters for current epoch {}\".format(epoch))\n            pattern_correct = self.pattern_correct\n            pattern_total = self.pattern_total\n        elif epoch in self.batch_data_by_epoch:\n            # Process historical data for previous epochs\n            logger.debug(\"PatternDataMediator: Processing historical data for epoch {}\".format(epoch))\n            \n            # Initialize counters\n            pattern_correct = {}\n            pattern_total = {}\n            \n            # Process all batch data for this epoch\n            for batch_indices, correct_mask in self.batch_data_by_epoch[epoch]:\n                # Skip processed markers\n                if isinstance(batch_indices, str) and batch_indices == 'processed':\n                    continue\n                    \n                # Map batch indices to pattern types\n                for i, idx in enumerate(batch_indices):\n                    if i >= len(correct_mask):\n                        continue\n                        \n                    pattern_type = self.pattern_service.get_pattern_type(idx)\n                    \n                    if pattern_type:\n                        # Initialize counters if needed\n                        if pattern_type not in pattern_total:\n                            pattern_total[pattern_type] = 0\n                            pattern_correct[pattern_type] = 0\n                        \n                        # Update counters\n                        pattern_total[pattern_type] += 1\n                        if correct_mask[i]:\n                            pattern_correct[pattern_type] += 1\n        else:\n            logger.warning(\"PatternDataMediator: No data available for epoch {}\".format(epoch))\n            # Fall back to using default values for the patterns in pattern_map\n            if self.pattern_map and 'pattern_types' in self.pattern_map:\n                logger.info(f\"[MEDIATOR FALLBACK] Using default pattern data for epoch {epoch}\")\n                \n                # Create default data for all pattern types\n                pattern_correct = {}\n                pattern_total = {}\n                \n                for pattern_type in self.pattern_map['pattern_types']:\n                    pattern_total[pattern_type] = 100  # Default sample count\n                    pattern_correct[pattern_type] = 50  # Default 50% accuracy\n            else:\n                return False\n            \n        logger.debug(\"PatternDataMediator: Processing data for epoch {}\".format(epoch))\n        \n        # Calculate accuracies and risks\n        accuracies = {}\n        risks = {}\n        for pattern_type, total in pattern_total.items():\n            if total > 0:\n                accuracy = pattern_correct[pattern_type] / total\n                accuracies[pattern_type] = accuracy\n                \n                # Calculate risk as inverse of accuracy, bounded to [0.1, 0.9]\n                risks[pattern_type] = max(0.1, min(0.9, 1.0 - accuracy))\n        \n        # If no entries yet, make sure we have something\n        if not accuracies and self.pattern_map and 'pattern_types' in self.pattern_map:\n            for p_type in self.pattern_map['pattern_types']:\n                if p_type not in accuracies:\n                    # Default values as fallback\n                    accuracies[p_type] = 0.5  # Neutral default accuracy\n                    risks[p_type] = 0.5      # Neutral default risk\n        \n        # Store the processed metrics\n        self.metrics_by_epoch[epoch] = {\n            'accuracies': accuracies,\n            'risks': risks,\n            'processed': True\n        }\n        \n        # Log the results\n        logger.debug(\"PatternDataMediator: Processed epoch {} data:\".format(epoch))\n        if logger.isEnabledFor(logging.DEBUG):\n            for pattern_type, accuracy in accuracies.items():\n                logger.debug(\"  Pattern '{}': accuracy={:.4f}, risk={:.4f}, count={}\".format(\n                    pattern_type, accuracy, risks.get(pattern_type, 0), pattern_total.get(pattern_type, 0)))\n        \n        # More detailed logging at info level for key pattern metrics\n        msg = \"Pattern metrics for epoch {}: \".format(epoch)\n        for pattern_type, accuracy in accuracies.items():\n            msg += \"'{}' (acc={:.2f}, risk={:.2f}, n={}), \".format(\n                pattern_type, accuracy, risks.get(pattern_type, 0), pattern_total.get(pattern_type, 0))\n        logger.info(msg)\n        \n        # For past epochs, clear the raw data to save memory\n        if epoch != self.current_epoch and epoch in self.batch_data_by_epoch:\n            self.batch_data_by_epoch[epoch] = [('processed', True)]\n        \n        return True\n    \n    def _cleanup_old_epochs(self):\n        \"\"\"\n        Remove data from epochs except current and previous."
        },
        "_cleanup_old_epochs": {
          "start_line": 238,
          "end_line": 258,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "list",
              "line": 249
            },
            {
              "name": "list",
              "line": 254
            },
            {
              "name": "sorted",
              "line": 246
            },
            {
              "name": "self.batch_data_by_epoch.keys",
              "line": 249
            },
            {
              "name": "self.metrics_by_epoch.keys",
              "line": 254
            },
            {
              "name": "self.batch_data_by_epoch.keys",
              "line": 246
            }
          ],
          "docstring": "\n        Remove data from epochs except current and previous.\n        \n        This method helps manage memory usage by keeping only the\n        necessary data for recent epochs.\n        ",
          "code_snippet": "        return True\n    \n    def _cleanup_old_epochs(self):\n        \"\"\"\n        Remove data from epochs except current and previous.\n        \n        This method helps manage memory usage by keeping only the\n        necessary data for recent epochs.\n        \"\"\"\n        # Keep only current and previous epoch\n        epochs_to_keep = sorted(self.batch_data_by_epoch.keys(), reverse=True)[:2]\n        \n        # Remove older epochs\n        for epoch in list(self.batch_data_by_epoch.keys()):\n            if epoch not in epochs_to_keep:\n                del self.batch_data_by_epoch[epoch]\n        \n        # Also clean up metrics\n        for epoch in list(self.metrics_by_epoch.keys()):\n            if epoch not in epochs_to_keep:\n                del self.metrics_by_epoch[epoch]\n    \n    def get_pattern_accuracies(self, epoch=None, force_recalculate=False):\n        \"\"\"\n        Get pattern accuracies for the specified epoch (defaults to current)."
        },
        "get_pattern_accuracies": {
          "start_line": 258,
          "end_line": 286,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "epoch"
            },
            {
              "name": "force_recalculate"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logger.warning",
              "line": 283
            },
            {
              "name": "self._process_epoch_data",
              "line": 274
            },
            {
              "name": "logger.debug",
              "line": 279
            },
            {
              "name": "logger.info",
              "line": 280
            }
          ],
          "docstring": "\n        Get pattern accuracies for the specified epoch (defaults to current).\n        \n        Args:\n            epoch (int, optional): Epoch for which to get accuracies. Defaults to current epoch.\n            force_recalculate (bool, optional): Whether to force recalculation. Defaults to False.\n            \n        Returns:\n            dict: Mapping from pattern types to accuracies\n        ",
          "code_snippet": "                del self.metrics_by_epoch[epoch]\n    \n    def get_pattern_accuracies(self, epoch=None, force_recalculate=False):\n        \"\"\"\n        Get pattern accuracies for the specified epoch (defaults to current).\n        \n        Args:\n            epoch (int, optional): Epoch for which to get accuracies. Defaults to current epoch.\n            force_recalculate (bool, optional): Whether to force recalculation. Defaults to False.\n            \n        Returns:\n            dict: Mapping from pattern types to accuracies\n        \"\"\"\n        epoch = self.current_epoch if epoch is None else epoch\n        \n        # Check if we need to process the data\n        if force_recalculate or epoch not in self.metrics_by_epoch:\n            # We need to process this epoch\n            self._process_epoch_data(epoch)\n        \n        # Log the metrics we're returning for debugging\n        if epoch in self.metrics_by_epoch:\n            accuracies = self.metrics_by_epoch[epoch]['accuracies']\n            logger.debug(f\"PatternDataMediator providing accuracies for epoch {epoch}: {accuracies}\")\n            logger.info(f\"[CRITICAL DATA] Providing pattern accuracies for epoch {epoch}: {accuracies}\")\n            return accuracies\n        \n        logger.warning(f\"PatternDataMediator: No accuracy data available for epoch {epoch}\")\n        return {}\n    \n    def get_pattern_risks(self, epoch=None, force_recalculate=False):\n        \"\"\"\n        Get pattern risks for the specified epoch."
        },
        "get_pattern_risks": {
          "start_line": 286,
          "end_line": 312,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "epoch"
            },
            {
              "name": "force_recalculate"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.get_pattern_accuracies",
              "line": 300
            },
            {
              "name": "logger.warning",
              "line": 309
            },
            {
              "name": "logger.debug",
              "line": 305
            },
            {
              "name": "logger.info",
              "line": 306
            }
          ],
          "docstring": "\n        Get pattern risks for the specified epoch.\n        \n        Args:\n            epoch (int, optional): Epoch for which to get risks. Defaults to current epoch.\n            force_recalculate (bool, optional): Whether to force recalculation. Defaults to False.\n            \n        Returns:\n            dict: Mapping from pattern types to risks\n        ",
          "code_snippet": "        return {}\n    \n    def get_pattern_risks(self, epoch=None, force_recalculate=False):\n        \"\"\"\n        Get pattern risks for the specified epoch.\n        \n        Args:\n            epoch (int, optional): Epoch for which to get risks. Defaults to current epoch.\n            force_recalculate (bool, optional): Whether to force recalculation. Defaults to False.\n            \n        Returns:\n            dict: Mapping from pattern types to risks\n        \"\"\"\n        epoch = self.current_epoch if epoch is None else epoch\n        \n        # Ensure metrics are calculated\n        self.get_pattern_accuracies(epoch, force_recalculate)\n        \n        # Log the metrics we're returning for debugging\n        if epoch in self.metrics_by_epoch:\n            risks = self.metrics_by_epoch[epoch]['risks']\n            logger.debug(f\"PatternDataMediator providing risks for epoch {epoch}: {risks}\")\n            logger.info(f\"[CRITICAL DATA] Providing pattern risks for epoch {epoch}: {risks}\")\n            return risks\n        \n        logger.warning(f\"PatternDataMediator: No risk data available for epoch {epoch}\")\n        return {}\n    \n    def end_epoch(self, epoch):\n        \"\"\"\n        Signal the end of an epoch to ensure all data is processed."
        },
        "end_epoch": {
          "start_line": 312,
          "end_line": 330,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "epoch"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logger.debug",
              "line": 319
            },
            {
              "name": "self._process_epoch_data",
              "line": 322
            },
            {
              "name": "self._cleanup_old_epochs",
              "line": 328
            },
            {
              "name": "....format",
              "line": 319
            }
          ],
          "docstring": "\n        Signal the end of an epoch to ensure all data is processed.\n        \n        Args:\n            epoch (int): The epoch that is ending\n        ",
          "code_snippet": "        return {}\n    \n    def end_epoch(self, epoch):\n        \"\"\"\n        Signal the end of an epoch to ensure all data is processed.\n        \n        Args:\n            epoch (int): The epoch that is ending\n        \"\"\"\n        logger.debug(\"PatternDataMediator: End of epoch {} signaled, processing remaining data\".format(epoch))\n        \n        # Process any remaining data for this epoch\n        self._process_epoch_data(epoch)\n        \n        # Move to the next epoch\n        self.current_epoch = epoch + 1\n        \n        # Clean up old epochs\n        self._cleanup_old_epochs()"
        }
      },
      "class_variables": [],
      "bases": [],
      "docstring": "\n    Mediator component that handles data transfer between isekaiZen pattern tracking \n    and EVE optimizer, with efficient caching and calculation management.\n    \n    This class acts as an intermediary between pattern recognition and optimization\n    components, handling the efficient transfer and processing of data between them.\n    It stores batch-level recognition data by epoch and provides processed metrics\n    when requested, with appropriate caching to minimize recomputation.\n    \n    Attributes:\n        batch_data_by_epoch (dict): Raw batch data organized by epoch\n        metrics_by_epoch (dict): Processed metrics organized by epoch\n        current_epoch (int): Current training epoch\n        pattern_service (object): Reference to pattern service for type mapping\n        pattern_map (dict): Pattern map with sample mappings\n    "
    }
  },
  "functions": {},
  "constants": {}
}