{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\isekaizen\\core\\optimizers.py",
  "imports": [
    {
      "name": "torch",
      "line": 7
    },
    {
      "name": "logging",
      "line": 8
    },
    {
      "name": "typing.Union",
      "line": 9
    },
    {
      "name": "typing.Optional",
      "line": 9
    },
    {
      "name": "typing.Dict",
      "line": 9
    },
    {
      "name": "typing.List",
      "line": 9
    },
    {
      "name": "typing.Tuple",
      "line": 9
    },
    {
      "name": "typing.Any",
      "line": 9
    },
    {
      "name": "typing.Protocol",
      "line": 9
    },
    {
      "name": "abc.ABC",
      "line": 10
    },
    {
      "name": "abc.abstractmethod",
      "line": 10
    }
  ],
  "classes": {
    "BatchSizeSelector": {
      "start_line": 14,
      "end_line": 76,
      "methods": {
        "__init__": {
          "start_line": 22,
          "end_line": 51,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "model"
            },
            {
              "name": "device"
            },
            {
              "name": "min_batch",
              "type": "int"
            },
            {
              "name": "max_batch",
              "type": "int"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "torch.device",
              "line": 39
            },
            {
              "name": "self.model.to",
              "line": 45
            },
            {
              "name": "next",
              "line": 44
            },
            {
              "name": "torch.cuda.is_available",
              "line": 39
            },
            {
              "name": "model.parameters",
              "line": 44
            }
          ],
          "docstring": "\n        Initialize the optimizer.\n        \n        Args:\n            model: PyTorch model to optimize\n            device: Target device for optimization (default: auto-detect)\n            min_batch: Minimum batch size to consider\n            max_batch: Maximum batch size to consider\n        ",
          "code_snippet": "    \n    @abstractmethod\n    def __init__(\n        self, \n        model: torch.nn.Module, \n        device: Optional[torch.device] = None,\n        min_batch: int = 4,\n        max_batch: int = 512,\n    ):\n        \"\"\"\n        Initialize the optimizer.\n        \n        Args:\n            model: PyTorch model to optimize\n            device: Target device for optimization (default: auto-detect)\n            min_batch: Minimum batch size to consider\n            max_batch: Maximum batch size to consider\n        \"\"\"\n        self.model = model\n        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.min_batch = min_batch\n        self.max_batch = max_batch\n        \n        # Move model to device if needed\n        if next(model.parameters()).device != self.device:\n            self.model.to(self.device)\n            \n        # Training state tracking\n        self.epoch = 0\n        self.iteration = 0\n    \n    @abstractmethod\n    def update_training_state(self, loss: float, gradient_norm: float, batch_accuracy: Optional[float] = None):\n        \"\"\""
        },
        "update_training_state": {
          "start_line": 52,
          "end_line": 62,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "loss",
              "type": "float"
            },
            {
              "name": "gradient_norm",
              "type": "float"
            },
            {
              "name": "batch_accuracy"
            }
          ],
          "return_type": null,
          "calls": [],
          "docstring": "\n        Update internal state based on current training metrics.\n        \n        Args:\n            loss: Current loss value\n            gradient_norm: Norm of current gradients\n        ",
          "code_snippet": "    \n    @abstractmethod\n    def update_training_state(self, loss: float, gradient_norm: float, batch_accuracy: Optional[float] = None):\n        \"\"\"\n        Update internal state based on current training metrics.\n        \n        Args:\n            loss: Current loss value\n            gradient_norm: Norm of current gradients\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    def get_optimal_batch_size(self) -> int:\n        \"\"\""
        },
        "get_optimal_batch_size": {
          "start_line": 63,
          "end_line": 72,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": "int",
          "calls": [],
          "docstring": "\n        Calculate the current optimal batch size.\n        \n        Returns:\n            Current optimal batch size\n        ",
          "code_snippet": "    \n    @abstractmethod\n    def get_optimal_batch_size(self) -> int:\n        \"\"\"\n        Calculate the current optimal batch size.\n        \n        Returns:\n            Current optimal batch size\n        \"\"\"\n        pass\n    \n    def increment_epoch(self):\n        \"\"\"Increment the epoch counter\"\"\"\n        self.epoch += 1"
        },
        "increment_epoch": {
          "start_line": 72,
          "end_line": 76,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [],
          "docstring": "Increment the epoch counter",
          "code_snippet": "        pass\n    \n    def increment_epoch(self):\n        \"\"\"Increment the epoch counter\"\"\"\n        self.epoch += 1"
        }
      },
      "class_variables": [],
      "bases": [
        "ABC"
      ],
      "docstring": "\n    Abstract base class for all batch size selection strategies.\n    \n    All batch size selector implementations must implement this interface.\n    "
    }
  },
  "functions": {},
  "constants": {}
}