{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\benchmarks.old\\examples\\run_streamlined_responsive.py",
  "imports": [
    {
      "name": "os",
      "line": 22
    },
    {
      "name": "sys",
      "line": 23
    },
    {
      "name": "time",
      "line": 24
    },
    {
      "name": "logging",
      "line": 25
    },
    {
      "name": "argparse",
      "line": 26
    },
    {
      "name": "json",
      "line": 27
    },
    {
      "name": "torch",
      "line": 28
    },
    {
      "name": "torch.nn",
      "line": 29
    },
    {
      "name": "torch.optim",
      "line": 30
    },
    {
      "name": "torchvision",
      "line": 31
    },
    {
      "name": "torchvision.transforms",
      "line": 32
    },
    {
      "name": "torchvision.models",
      "line": 33
    },
    {
      "name": "matplotlib.pyplot",
      "line": 34
    },
    {
      "name": "numpy",
      "line": 35
    },
    {
      "name": "datetime.datetime",
      "line": 36
    },
    {
      "name": "shutil",
      "line": 37
    },
    {
      "name": "isekaizen.trainer.adaptive_trainer.AdaptiveTrainer",
      "line": 43
    },
    {
      "name": "isekaizen.pattern.data_loading.load_latest_pattern_map",
      "line": 44
    },
    {
      "name": "isekaizen.core.optimizer.enhanced_pattern_responsive.EnhancedPatternResponsiveOptimizer",
      "line": 45
    },
    {
      "name": "isekaizen.optimizers.eve.EVENaturalWeights",
      "line": 46
    },
    {
      "name": "optimizer_utils.configure_optimizer",
      "line": 49
    },
    {
      "name": "optimizer_utils.print_available_optimizers",
      "line": 49
    },
    {
      "name": "optimizer_utils.add_optimizer_arguments",
      "line": 49
    },
    {
      "name": "isekaizen.core.optimizer.pattern_risk_accuracy_tracker.PatternRiskAccuracyTracker",
      "line": 831
    },
    {
      "name": "json",
      "line": 995
    },
    {
      "name": "datetime.datetime",
      "line": 996
    },
    {
      "name": "isekaizen.pattern.detection.PatternRecognitionService",
      "line": 97
    },
    {
      "name": "isekaizen.utils.pattern_map_utils.translate_pattern_map_to_standard_format",
      "line": 1189
    },
    {
      "name": "traceback",
      "line": 1500
    }
  ],
  "classes": {
    "StreamlinedPatternTrainer": {
      "start_line": 55,
      "end_line": 456,
      "methods": {
        "__init__": {
          "start_line": 61,
          "end_line": 104,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "model"
            },
            {
              "name": "criterion"
            },
            {
              "name": "optimizer_class"
            },
            {
              "name": "optimizer"
            },
            {
              "name": "optimizer_kwargs"
            },
            {
              "name": "scheduler_class"
            },
            {
              "name": "scheduler_kwargs"
            },
            {
              "name": "device"
            },
            {
              "name": "pattern_map"
            },
            {
              "name": "batch_optimizer_class"
            },
            {
              "name": "batch_optimizer_kwargs"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "....__init__",
              "line": 80
            },
            {
              "name": "PatternRecognitionService",
              "line": 98
            },
            {
              "name": "logger.info",
              "line": 99
            },
            {
              "name": "super",
              "line": 80
            }
          ],
          "docstring": "\n        Initialize the trainer with early pattern service setup.\n        ",
          "code_snippet": "    \"\"\"\n    \n    def __init__(\n        self,\n        model,\n        criterion,\n        optimizer_class,\n        optimizer=None,\n        optimizer_kwargs=None,\n        scheduler_class=None,\n        scheduler_kwargs=None,\n        device=None,\n        pattern_map=None,\n        batch_optimizer_class=None,\n        batch_optimizer_kwargs=None,\n        **kwargs\n    ):\n        \"\"\"\n        Initialize the trainer with early pattern service setup.\n        \"\"\"\n        # Initialize base trainer\n        super().__init__(\n            model=model,\n            criterion=criterion,\n            optimizer_class=optimizer_class,\n            optimizer=optimizer,\n            optimizer_kwargs=optimizer_kwargs, \n            scheduler_class=scheduler_class,\n            scheduler_kwargs=scheduler_kwargs,\n            device=device,\n            pattern_map=pattern_map,\n            batch_optimizer_class=batch_optimizer_class,\n            batch_optimizer_kwargs=batch_optimizer_kwargs,\n            **kwargs\n        )\n        \n        # Initialize pattern service as soon as we have the pattern map\n        # This prevents on-the-fly creation during training\n        from isekaizen.pattern.detection import PatternRecognitionService\n        self.pattern_service = PatternRecognitionService(pattern_map)\n        logger.info(\"PatternRecognitionService initialized early with pattern map\")\n        \n        # Initialize dataset adaptations tracking\n        self.dataset_adaptations = []\n    \n    def train(\n        self,\n        train_dataset,"
        },
        "train": {
          "start_line": 104,
          "end_line": 335,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "train_dataset"
            },
            {
              "name": "val_dataset"
            },
            {
              "name": "epochs"
            },
            {
              "name": "early_stopping"
            },
            {
              "name": "patience"
            },
            {
              "name": "test_interval"
            },
            {
              "name": "checkpoint_interval"
            },
            {
              "name": "checkpoint_path"
            },
            {
              "name": "callbacks"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "float",
              "line": 143
            },
            {
              "name": "....append",
              "line": 147
            },
            {
              "name": "range",
              "line": 149
            },
            {
              "name": "logger.info",
              "line": 332
            },
            {
              "name": "len",
              "line": 147
            },
            {
              "name": "logger.info",
              "line": 157
            },
            {
              "name": "self.batch_optimizer.get_optimal_batch_size",
              "line": 187
            },
            {
              "name": "....append",
              "line": 188
            },
            {
              "name": "....append",
              "line": 191
            },
            {
              "name": "logger.info",
              "line": 194
            },
            {
              "name": "logger.info",
              "line": 197
            },
            {
              "name": "time.time",
              "line": 200
            },
            {
              "name": "self._train_epoch",
              "line": 201
            },
            {
              "name": "torch.cuda.is_available",
              "line": 205
            },
            {
              "name": "hasattr",
              "line": 220
            },
            {
              "name": "logger.info",
              "line": 224
            },
            {
              "name": "logger.info",
              "line": 225
            },
            {
              "name": "logger.info",
              "line": 226
            },
            {
              "name": "hasattr",
              "line": 230
            },
            {
              "name": "....append",
              "line": 250
            },
            {
              "name": "....append",
              "line": 251
            },
            {
              "name": "....append",
              "line": 252
            },
            {
              "name": "....append",
              "line": 253
            },
            {
              "name": "hasattr",
              "line": 314
            },
            {
              "name": "globals",
              "line": 151
            },
            {
              "name": "print_progress_bar",
              "line": 153
            },
            {
              "name": "hasattr",
              "line": 160
            },
            {
              "name": "self.batch_optimizer.should_adapt_patterns",
              "line": 160
            },
            {
              "name": "logger.info",
              "line": 161
            },
            {
              "name": "self.batch_optimizer.adapt_dataset",
              "line": 162
            },
            {
              "name": "adaptation_metrics.get",
              "line": 164
            },
            {
              "name": "len",
              "line": 191
            },
            {
              "name": "hasattr",
              "line": 195
            },
            {
              "name": "hasattr",
              "line": 195
            },
            {
              "name": "logger.info",
              "line": 196
            },
            {
              "name": "time.time",
              "line": 202
            },
            {
              "name": "self.batch_optimizer.update_with_epoch_metrics",
              "line": 221
            },
            {
              "name": "self.batch_optimizer.pattern_tracker.get_pattern_risks",
              "line": 232
            },
            {
              "name": "hasattr",
              "line": 241
            },
            {
              "name": "hasattr",
              "line": 258
            },
            {
              "name": "self._validate",
              "line": 264
            },
            {
              "name": "....append",
              "line": 265
            },
            {
              "name": "....append",
              "line": 266
            },
            {
              "name": "hasattr",
              "line": 273
            },
            {
              "name": "isinstance",
              "line": 277
            },
            {
              "name": "logger.info",
              "line": 280
            },
            {
              "name": "self.save_model",
              "line": 307
            },
            {
              "name": "self.scheduler.step",
              "line": 311
            },
            {
              "name": "self.batch_optimizer.hardware_analyzer.cleanup_memory",
              "line": 315
            },
            {
              "name": "torch.cuda.is_available",
              "line": 316
            },
            {
              "name": "history.get",
              "line": 152
            },
            {
              "name": "self.dataset_adaptations.append",
              "line": 169
            },
            {
              "name": "logger.info",
              "line": 174
            },
            {
              "name": "logger.info",
              "line": 175
            },
            {
              "name": "logger.info",
              "line": 179
            },
            {
              "name": "logger.info",
              "line": 182
            },
            {
              "name": "torch.cuda.max_memory_allocated",
              "line": 206
            },
            {
              "name": "logger.info",
              "line": 234
            },
            {
              "name": "pattern_risks.items",
              "line": 235
            },
            {
              "name": "....append",
              "line": 238
            },
            {
              "name": "self.batch_optimizer.pattern_tracker.get_pattern_accuracies",
              "line": 242
            },
            {
              "name": "self.batch_optimizer.hardware_analyzer.cleanup_memory",
              "line": 259
            },
            {
              "name": "torch.cuda.is_available",
              "line": 260
            },
            {
              "name": "self.batch_optimizer.update_with_epoch_metrics",
              "line": 274
            },
            {
              "name": "self.optimizer.update_accuracy_metrics",
              "line": 278
            },
            {
              "name": "len",
              "line": 283
            },
            {
              "name": "torch.cuda.empty_cache",
              "line": 317
            },
            {
              "name": "torch.cuda.reset_peak_memory_stats",
              "line": 318
            },
            {
              "name": "callback",
              "line": 324
            },
            {
              "name": "logger.info",
              "line": 329
            },
            {
              "name": "logger.info",
              "line": 178
            },
            {
              "name": "len",
              "line": 197
            },
            {
              "name": "logger.info",
              "line": 236
            },
            {
              "name": "logger.info",
              "line": 244
            },
            {
              "name": "pattern_accuracies.items",
              "line": 245
            },
            {
              "name": "....append",
              "line": 248
            },
            {
              "name": "torch.cuda.empty_cache",
              "line": 261
            },
            {
              "name": "torch.cuda.reset_peak_memory_stats",
              "line": 262
            },
            {
              "name": "logger.info",
              "line": 288
            },
            {
              "name": "logger.info",
              "line": 293
            },
            {
              "name": "self.save_model",
              "line": 302
            },
            {
              "name": "logger.info",
              "line": 303
            },
            {
              "name": "history.get",
              "line": 154
            },
            {
              "name": "logger.info",
              "line": 246
            },
            {
              "name": "logger.info",
              "line": 290
            },
            {
              "name": "logger.info",
              "line": 295
            },
            {
              "name": "adaptation_metrics.get",
              "line": 182
            },
            {
              "name": "....join",
              "line": 178
            },
            {
              "name": "abs",
              "line": 290
            },
            {
              "name": "abs",
              "line": 295
            },
            {
              "name": "pattern_risks.items",
              "line": 178
            }
          ],
          "docstring": "\n        Train the model with dynamic dataset adaptation based on pattern risk.\n        \n        Args:\n            train_dataset: Training dataset\n            val_dataset: Validation dataset\n            epochs: Number of epochs\n            early_stopping: Whether to use early stopping\n            patience: Early stopping patience\n            test_interval: Interval for validation\n            checkpoint_interval: Interval for saving checkpoints\n            checkpoint_path: Path to save checkpoints\n            callbacks: List of callback functions\n            \n        Returns:\n            Training history\n        ",
          "code_snippet": "        self.dataset_adaptations = []\n    \n    def train(\n        self,\n        train_dataset,\n        val_dataset=None,\n        epochs=10,\n        early_stopping=None,  # Parameter kept for compatibility but not used\n        patience=None,        # Parameter kept for compatibility but not used\n        test_interval=1,\n        checkpoint_interval=None,\n        checkpoint_path=None,\n        callbacks=None\n    ):\n        \"\"\"\n        Train the model with dynamic dataset adaptation based on pattern risk.\n        \n        Args:\n            train_dataset: Training dataset\n            val_dataset: Validation dataset\n            epochs: Number of epochs\n            early_stopping: Whether to use early stopping\n            patience: Early stopping patience\n            test_interval: Interval for validation\n            checkpoint_interval: Interval for saving checkpoints\n            checkpoint_path: Path to save checkpoints\n            callbacks: List of callback functions\n            \n        Returns:\n            Training history\n        \"\"\"\n        # Initialize variables\n        history = {\n            'train_loss': [], 'train_acc': [],\n            'val_loss': [], 'val_acc': [],\n            'batch_sizes': [], 'epoch_times': [],\n            'dataset_sizes': [], 'memory_usage': [],\n            'pattern_recognition_rates': [],\n            'pattern_risks': []\n        }\n        \n        best_val_loss = float('inf')\n        no_improve_count = 0\n        \n        # Initial dataset size\n        history['dataset_sizes'].append(len(train_dataset))\n        \n        for epoch in range(epochs):\n            # Update progress bar first\n            if 'print_progress_bar' in globals():\n                test_acc = history['val_acc'][-1] if history.get('val_acc') else None\n                print_progress_bar(epoch+1, epochs, \n                                 train_acc=history['train_acc'][-1] if history.get('train_acc') else None,\n                                 test_acc=test_acc)\n                \n            logger.info(f\"Epoch {epoch+1}/{epochs}\")\n            \n            # Check if we should adapt the dataset\n            if hasattr(self.batch_optimizer, 'should_adapt_patterns') and self.batch_optimizer.should_adapt_patterns():\n                logger.info(\"Adapting dataset based on pattern risk assessment...\")\n                adapted_dataset, adaptation_metrics = self.batch_optimizer.adapt_dataset(train_dataset)\n                \n                if adaptation_metrics.get('adapted', False):\n                    # Use the adapted dataset for this epoch\n                    current_dataset = adapted_dataset\n                    \n                    # Record adaptation\n                    self.dataset_adaptations.append({\n                        'epoch': epoch + 1,\n                        **adaptation_metrics\n                    })\n                    \n                    logger.info(f\"Dataset adapted successfully:\")\n                    logger.info(f\"  Added {adaptation_metrics['examples_added']} new examples\")\n                    if 'pattern_risks' in adaptation_metrics:\n                        pattern_risks = adaptation_metrics['pattern_risks']\n                        logger.info(f\"  Based on pattern risks: {', '.join([f'{k}:{v:.2f}' for k, v in pattern_risks.items()])}\")\n                    logger.info(f\"  New dataset size: {adaptation_metrics['total_size']} examples\")\n                else:\n                    current_dataset = train_dataset\n                    logger.info(f\"Dataset adaptation skipped: {adaptation_metrics.get('reason', 'unknown')}\")\n            else:\n                current_dataset = train_dataset\n            \n            # Get optimal batch size\n            batch_size = self.batch_optimizer.get_optimal_batch_size()\n            history['batch_sizes'].append(batch_size)\n            \n            # Track dataset size\n            history['dataset_sizes'].append(len(current_dataset))\n            \n            # Print more detailed epoch information\n            logger.info(f\"Starting epoch {epoch+1}/{epochs} with batch size {batch_size}\")\n            if hasattr(self.batch_optimizer, 'min_batch') and hasattr(self.batch_optimizer, 'max_batch'):\n                logger.info(f\"  Batch size range: [{self.batch_optimizer.min_batch}, {self.batch_optimizer.max_batch}]\")\n            logger.info(f\"  Dataset size: {len(current_dataset)} examples\")\n            \n            # Train for one epoch\n            start_time = time.time()\n            train_metrics = self._train_epoch(current_dataset, batch_size)\n            epoch_time = time.time() - start_time\n            \n            # Get memory usage\n            if torch.cuda.is_available():\n                memory_usage = torch.cuda.max_memory_allocated(self.device) / (1024 ** 3)  # GB\n            else:\n                memory_usage = 0.0\n                \n            # Add memory usage to metrics\n            train_metrics['memory'] = memory_usage\n            \n            # Add epoch time to metrics\n            train_metrics['time'] = epoch_time\n            \n            # Add batch size to metrics\n            train_metrics['batch_size'] = batch_size\n            \n            # Update optimizer with epoch metrics\n            if hasattr(self.batch_optimizer, 'update_with_epoch_metrics'):\n                self.batch_optimizer.update_with_epoch_metrics(train_metrics)\n            \n            # Print detailed completion information\n            logger.info(f\"Epoch {epoch+1}/{epochs} completed in {epoch_time:.2f} seconds\")\n            logger.info(f\"  Training - Loss: {train_metrics['loss']:.4f}, Accuracy: {train_metrics['accuracy']:.2f}%\")\n            logger.info(f\"  Memory usage: {memory_usage:.2f} GB\")\n            \n            # Pattern statistics if available\n            pattern_risks = {}\n            if hasattr(self.batch_optimizer, 'pattern_tracker'):\n                # Get pattern risks\n                pattern_risks = self.batch_optimizer.pattern_tracker.get_pattern_risks()\n                if pattern_risks:\n                    logger.info(\"  Pattern risks:\")\n                    for pattern_type, risk in pattern_risks.items():\n                        logger.info(f\"    {pattern_type}: {risk:.2f}\")\n                    # Save for history\n                    history['pattern_risks'].append(pattern_risks)\n                    \n                # Get pattern recognition rates\n                if hasattr(self.batch_optimizer.pattern_tracker, 'get_pattern_accuracies'):\n                    pattern_accuracies = self.batch_optimizer.pattern_tracker.get_pattern_accuracies()\n                    if pattern_accuracies:\n                        logger.info(\"  Pattern recognition rates:\")\n                        for pattern_type, rate in pattern_accuracies.items():\n                            logger.info(f\"    {pattern_type}: {rate:.2f}\")\n                        # Save for history\n                        history['pattern_recognition_rates'].append(pattern_accuracies)\n            \n            history['train_loss'].append(train_metrics['loss'])\n            history['train_acc'].append(train_metrics['accuracy'])\n            history['epoch_times'].append(epoch_time)\n            history['memory_usage'].append(memory_usage)\n            \n            # Validate if a validation set is provided\n            if val_dataset is not None and (epoch + 1) % test_interval == 0:\n                # Clean up memory before validation\n                if hasattr(self.batch_optimizer, 'hardware_analyzer'):\n                    self.batch_optimizer.hardware_analyzer.cleanup_memory()\n                elif torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n                    torch.cuda.reset_peak_memory_stats()\n                    \n                val_metrics = self._validate(val_dataset)\n                history['val_loss'].append(val_metrics['loss'])\n                history['val_acc'].append(val_metrics['accuracy'])\n                \n                # Add validation metrics to train_metrics for batch optimizer\n                train_metrics['val_accuracy'] = val_metrics['accuracy']\n                train_metrics['val_loss'] = val_metrics['loss']\n                \n                # Update batch optimizer with complete metrics including validation\n                if hasattr(self.batch_optimizer, 'update_with_epoch_metrics'):\n                    self.batch_optimizer.update_with_epoch_metrics(train_metrics)\n                \n                # Update EVENaturalWeights optimizer with validation accuracy\n                if isinstance(self.optimizer, EVENaturalWeights):\n                    self.optimizer.update_accuracy_metrics(train_metrics['accuracy'], val_metrics['accuracy'])\n                \n                logger.info(f\"  Validation - Loss: {val_metrics['loss']:.4f}, Accuracy: {val_metrics['accuracy']:.2f}%\")\n                \n                # Display improvement/decline compared to previous validation\n                if len(history['val_acc']) > 1:\n                    acc_diff = val_metrics['accuracy'] - history['val_acc'][-2]\n                    loss_diff = history['val_loss'][-2] - val_metrics['loss']  # Loss should decrease\n                    \n                    if acc_diff > 0:\n                        logger.info(f\"  Validation accuracy improved by {acc_diff:.2f}%\")\n                    elif acc_diff < 0:\n                        logger.info(f\"  Validation accuracy decreased by {abs(acc_diff):.2f}%\")\n                    \n                    if loss_diff > 0:\n                        logger.info(f\"  Validation loss improved by {loss_diff:.4f}\")\n                    elif loss_diff < 0:\n                        logger.info(f\"  Validation loss worsened by {abs(loss_diff):.4f}\")\n                \n                # Save best model when validation improves\n                if val_metrics['loss'] < best_val_loss:\n                    best_val_loss = val_metrics['loss']\n                    # Save best model\n                    if checkpoint_path:\n                        self.save_model(f\"{checkpoint_path}_best.pth\")\n                        logger.info(f\"New best model saved at epoch {epoch+1}\")\n            \n            # Save checkpoint if interval is specified\n            if checkpoint_interval and (epoch + 1) % checkpoint_interval == 0 and checkpoint_path:\n                self.save_model(f\"{checkpoint_path}_epoch{epoch+1}.pth\")\n            \n            # Step the scheduler if it exists\n            if self.scheduler:\n                self.scheduler.step()\n            \n            # Clean up memory after epoch\n            if hasattr(self.batch_optimizer, 'hardware_analyzer'):\n                self.batch_optimizer.hardware_analyzer.cleanup_memory()\n            elif torch.cuda.is_available():\n                torch.cuda.empty_cache()\n                torch.cuda.reset_peak_memory_stats()\n            \n            # Execute callbacks if provided\n            if callbacks:\n                stop_training = False\n                for callback in callbacks:\n                    result = callback(epoch, history, self.model, self.optimizer)\n                    if result:  # If callback returns True, stop training\n                        stop_training = True\n                        \n                if stop_training:\n                    logger.info(f\"Training stopped by callback after epoch {epoch+1}\")\n                    break\n        \n        logger.info(\"Training complete\")\n        return history\n        \n    def _train_epoch(self, dataset, batch_size):\n        \"\"\"\n        Train for one epoch."
        },
        "_train_epoch": {
          "start_line": 335,
          "end_line": 415,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "dataset"
            },
            {
              "name": "batch_size"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.model.train",
              "line": 346
            },
            {
              "name": "torch.utils.data.DataLoader",
              "line": 352
            },
            {
              "name": "enumerate",
              "line": 358
            },
            {
              "name": "list",
              "line": 363
            },
            {
              "name": "all_batch_indices.extend",
              "line": 364
            },
            {
              "name": "self.optimizer.zero_grad",
              "line": 367
            },
            {
              "name": "self.model",
              "line": 370
            },
            {
              "name": "self.criterion",
              "line": 371
            },
            {
              "name": "loss.item",
              "line": 375
            },
            {
              "name": "outputs.max",
              "line": 379
            },
            {
              "name": "predicted.eq",
              "line": 380
            },
            {
              "name": "hasattr",
              "line": 383
            },
            {
              "name": "hasattr",
              "line": 387
            },
            {
              "name": "loss.item",
              "line": 391
            },
            {
              "name": "....item",
              "line": 392
            },
            {
              "name": "targets.size",
              "line": 394
            },
            {
              "name": "loss.backward",
              "line": 397
            },
            {
              "name": "isinstance",
              "line": 400
            },
            {
              "name": "len",
              "line": 410
            },
            {
              "name": "inputs.to",
              "line": 359
            },
            {
              "name": "targets.to",
              "line": 359
            },
            {
              "name": "range",
              "line": 363
            },
            {
              "name": "max",
              "line": 374
            },
            {
              "name": "self.batch_optimizer.update_with_pattern_recognition",
              "line": 384
            },
            {
              "name": "self.batch_optimizer.update_with_batch_results",
              "line": 388
            },
            {
              "name": "self.pattern_service.get_batch_pattern_states",
              "line": 403
            },
            {
              "name": "self.optimizer.step",
              "line": 404
            },
            {
              "name": "self.optimizer.step",
              "line": 407
            },
            {
              "name": "min",
              "line": 363
            },
            {
              "name": "....sum",
              "line": 392
            },
            {
              "name": "len",
              "line": 363
            },
            {
              "name": "predicted.eq",
              "line": 392
            }
          ],
          "docstring": "\n        Train for one epoch.\n        \n        Args:\n            dataset: Dataset to train on\n            batch_size: Batch size\n            \n        Returns:\n            Dictionary with training metrics\n        ",
          "code_snippet": "        return history\n        \n    def _train_epoch(self, dataset, batch_size):\n        \"\"\"\n        Train for one epoch.\n        \n        Args:\n            dataset: Dataset to train on\n            batch_size: Batch size\n            \n        Returns:\n            Dictionary with training metrics\n        \"\"\"\n        self.model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        # Create data loader\n        dataloader = torch.utils.data.DataLoader(\n            dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        \n        # For batch-level risk assessment\n        all_batch_indices = []\n        \n        for i, (inputs, targets) in enumerate(dataloader):\n            inputs, targets = inputs.to(self.device), targets.to(self.device)\n            \n            # Get batch indices for pattern tracking\n            batch_start = i * batch_size\n            batch_indices = list(range(batch_start, min(batch_start + batch_size, len(dataset))))\n            all_batch_indices.extend(batch_indices)\n            \n            # Zero the parameter gradients\n            self.optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = self.model(inputs)\n            loss = self.criterion(outputs, targets)\n            \n            # Check loss change for risk assessment\n            prev_loss = running_loss / max(1, i)\n            current_loss = loss.item()\n            loss_decreased = i == 0 or current_loss <= prev_loss\n            \n            # Calculate per-example correctness for pattern recognition\n            _, predicted = outputs.max(1)\n            correct_mask = predicted.eq(targets)\n            \n            # Update pattern recognition tracking\n            if hasattr(self.batch_optimizer, 'update_with_pattern_recognition'):\n                self.batch_optimizer.update_with_pattern_recognition(batch_indices, correct_mask)\n            \n            # Update risk assessment\n            if hasattr(self.batch_optimizer, 'update_with_batch_results'):\n                self.batch_optimizer.update_with_batch_results(batch_indices, batch_size, loss_decreased)\n            \n            # Update metrics\n            running_loss += loss.item()\n            batch_correct = predicted.eq(targets).sum().item()\n            correct += batch_correct\n            total += targets.size(0)\n            \n            # Backward pass and optimize\n            loss.backward()\n            \n            # Special handling for EVE optimizers - pass pattern states\n            if isinstance(self.optimizer, EVENaturalWeights):\n                # Use the pre-initialized pattern service\n                # Get pattern states for this batch\n                pattern_states = self.pattern_service.get_batch_pattern_states(batch_indices)\n                self.optimizer.step(pattern_states=pattern_states)\n            else:\n                # Normal optimization step\n                self.optimizer.step()\n        \n        # Calculate epoch metrics\n        epoch_loss = running_loss / len(dataloader)\n        epoch_acc = 100. * correct / total\n        \n        return {'loss': epoch_loss, 'accuracy': epoch_acc}\n    \n    def _validate(self, dataset, batch_size=128):\n        \"\"\"\n        Validate the model on a dataset."
        },
        "_validate": {
          "start_line": 415,
          "end_line": 456,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "dataset"
            },
            {
              "name": "batch_size"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.model.eval",
              "line": 426
            },
            {
              "name": "torch.utils.data.DataLoader",
              "line": 432
            },
            {
              "name": "torch.no_grad",
              "line": 436
            },
            {
              "name": "len",
              "line": 451
            },
            {
              "name": "self.model",
              "line": 441
            },
            {
              "name": "self.criterion",
              "line": 442
            },
            {
              "name": "loss.item",
              "line": 445
            },
            {
              "name": "outputs.max",
              "line": 446
            },
            {
              "name": "....item",
              "line": 447
            },
            {
              "name": "targets.size",
              "line": 448
            },
            {
              "name": "inputs.to",
              "line": 438
            },
            {
              "name": "targets.to",
              "line": 438
            },
            {
              "name": "....sum",
              "line": 447
            },
            {
              "name": "predicted.eq",
              "line": 447
            }
          ],
          "docstring": "\n        Validate the model on a dataset.\n        \n        Args:\n            dataset: Validation dataset\n            batch_size: Batch size\n            \n        Returns:\n            Dictionary with validation metrics\n        ",
          "code_snippet": "        return {'loss': epoch_loss, 'accuracy': epoch_acc}\n    \n    def _validate(self, dataset, batch_size=128):\n        \"\"\"\n        Validate the model on a dataset.\n        \n        Args:\n            dataset: Validation dataset\n            batch_size: Batch size\n            \n        Returns:\n            Dictionary with validation metrics\n        \"\"\"\n        self.model.eval()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        # Create data loader\n        dataloader = torch.utils.data.DataLoader(\n            dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n        \n        # Disable gradient calculation\n        with torch.no_grad():\n            for inputs, targets in dataloader:\n                inputs, targets = inputs.to(self.device), targets.to(self.device)\n                \n                # Forward pass\n                outputs = self.model(inputs)\n                loss = self.criterion(outputs, targets)\n                \n                # Update metrics\n                running_loss += loss.item()\n                _, predicted = outputs.max(1)\n                correct += predicted.eq(targets).sum().item()\n                total += targets.size(0)\n        \n        # Calculate metrics\n        val_loss = running_loss / len(dataloader)\n        val_acc = 100. * correct / total\n        \n        return {'loss': val_loss, 'accuracy': val_acc}\n\ndef print_progress_bar(epoch, total_epochs, length=50, train_acc=None, test_acc=None):\n    \"\"\"\n    Print a fancy progress bar showing current training progress."
        }
      },
      "class_variables": [],
      "bases": [
        "AdaptiveTrainer"
      ],
      "docstring": "\n    Enhanced adaptive trainer optimized for streamlined pattern maps that uses\n    the risk-accuracy relationship approach for training.\n    "
    }
  },
  "functions": {
    "print_progress_bar": {
      "start_line": 456,
      "end_line": 497,
      "parameters": [
        {
          "name": "epoch"
        },
        {
          "name": "total_epochs"
        },
        {
          "name": "length"
        },
        {
          "name": "train_acc"
        },
        {
          "name": "test_acc"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "int",
          "line": 469
        },
        {
          "name": "print",
          "line": 492
        },
        {
          "name": "print",
          "line": 493
        },
        {
          "name": "print",
          "line": 494
        },
        {
          "name": "print",
          "line": 495
        },
        {
          "name": "round",
          "line": 469
        },
        {
          "name": "min",
          "line": 491
        },
        {
          "name": "shutil.get_terminal_size",
          "line": 473
        }
      ],
      "docstring": "\n    Print a fancy progress bar showing current training progress.\n    \n    Args:\n        epoch: Current epoch (1-based)\n        total_epochs: Total number of epochs\n        length: Length of the progress bar\n        train_acc: Optional training accuracy to display\n        test_acc: Optional test accuracy to display\n    ",
      "code_snippet": "        return {'loss': val_loss, 'accuracy': val_acc}\n\ndef print_progress_bar(epoch, total_epochs, length=50, train_acc=None, test_acc=None):\n    \"\"\"\n    Print a fancy progress bar showing current training progress.\n    \n    Args:\n        epoch: Current epoch (1-based)\n        total_epochs: Total number of epochs\n        length: Length of the progress bar\n        train_acc: Optional training accuracy to display\n        test_acc: Optional test accuracy to display\n    \"\"\"\n    # Calculate progress\n    progress = epoch / total_epochs\n    block = int(round(length * progress))\n    \n    # Try to detect terminal width for better formatting\n    try:\n        term_width = shutil.get_terminal_size().columns\n    except:\n        term_width = 80\n    \n    # Build the progress bar - \u2593 for completed, \u2591 for remaining\n    bar = \"\u2593\" * block + \"\u2591\" * (length - block)\n    \n    # Format the accuracy strings\n    acc_str = \"\"\n    if train_acc is not None:\n        acc_str += f\"Train: {train_acc:.2f}% \"\n    if test_acc is not None:\n        acc_str += f\"Test: {test_acc:.2f}% \"\n    \n    # Print progress text\n    progress_text = f\"Progress |{bar}| {progress:.1%} {acc_str}\"\n    \n    # Print separators and progress\n    divider = \"=\" * min(term_width, 80)\n    print(f\"\\n{divider}\")\n    print(f\"Epoch {epoch}/{total_epochs}\")\n    print(progress_text)\n    print(f\"{divider}\\n\")\n\ndef save_batch_diagnostics(model, device, trainer):\n    \"\"\"\n    Save detailed batch calculation diagnostics to a JSON file."
    },
    "save_batch_diagnostics": {
      "start_line": 497,
      "end_line": 578,
      "parameters": [
        {
          "name": "model"
        },
        {
          "name": "device"
        },
        {
          "name": "trainer"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "sum",
          "line": 511
        },
        {
          "name": "hasattr",
          "line": 537
        },
        {
          "name": "os.path.join",
          "line": 565
        },
        {
          "name": "os.makedirs",
          "line": 566
        },
        {
          "name": "os.path.join",
          "line": 567
        },
        {
          "name": "logger.info",
          "line": 572
        },
        {
          "name": "sum",
          "line": 512
        },
        {
          "name": "hasattr",
          "line": 515
        },
        {
          "name": "hasattr",
          "line": 516
        },
        {
          "name": "hasattr",
          "line": 520
        },
        {
          "name": "optimizer.pattern_map.get",
          "line": 521
        },
        {
          "name": "....strftime",
          "line": 546
        },
        {
          "name": "open",
          "line": 569
        },
        {
          "name": "json.dump",
          "line": 570
        },
        {
          "name": "logger.error",
          "line": 575
        },
        {
          "name": "p.numel",
          "line": 511
        },
        {
          "name": "optimizer.pattern_tracker.get_pattern_risks",
          "line": 539
        },
        {
          "name": "optimizer.pattern_tracker.get_overall_risk",
          "line": 540
        },
        {
          "name": "int",
          "line": 548
        },
        {
          "name": "float",
          "line": 549
        },
        {
          "name": "int",
          "line": 557
        },
        {
          "name": "int",
          "line": 558
        },
        {
          "name": "model.parameters",
          "line": 511
        },
        {
          "name": "sum",
          "line": 525
        },
        {
          "name": "....items",
          "line": 531
        },
        {
          "name": "datetime.now",
          "line": 546
        },
        {
          "name": "torch.cuda.get_device_name",
          "line": 553
        },
        {
          "name": "float",
          "line": 554
        },
        {
          "name": "....strftime",
          "line": 567
        },
        {
          "name": "p.numel",
          "line": 512
        },
        {
          "name": "p.element_size",
          "line": 512
        },
        {
          "name": "model.parameters",
          "line": 512
        },
        {
          "name": "pattern_distribution.values",
          "line": 525
        },
        {
          "name": "data.get",
          "line": 532
        },
        {
          "name": "str",
          "line": 575
        },
        {
          "name": "datetime.now",
          "line": 567
        },
        {
          "name": "torch.cuda.get_device_properties",
          "line": 554
        }
      ],
      "docstring": "\n    Save detailed batch calculation diagnostics to a JSON file.\n    \n    Args:\n        model: The model being optimized\n        device: The compute device being used\n        trainer: The trainer instance with batch optimizer\n    ",
      "code_snippet": "    print(f\"{divider}\\n\")\n\ndef save_batch_diagnostics(model, device, trainer):\n    \"\"\"\n    Save detailed batch calculation diagnostics to a JSON file.\n    \n    Args:\n        model: The model being optimized\n        device: The compute device being used\n        trainer: The trainer instance with batch optimizer\n    \"\"\"\n    try:\n        # Get batch optimizer\n        optimizer = trainer.batch_optimizer\n        \n        # Get basic model info\n        model_params = sum(p.numel() for p in model.parameters())\n        model_size_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024**2)\n        \n        # Get batch range\n        min_batch = optimizer.min_batch if hasattr(optimizer, 'min_batch') else 0\n        max_batch = optimizer.max_batch if hasattr(optimizer, 'max_batch') else 0\n        \n        # Get pattern map info if available\n        pattern_map_info = {}\n        if hasattr(optimizer, 'pattern_map') and optimizer.pattern_map:\n            pattern_distribution = optimizer.pattern_map.get('pattern_distribution', {})\n            if pattern_distribution:\n                pattern_map_info = {\n                    'pattern_distribution': pattern_distribution,\n                    'total_patterns': sum(pattern_distribution.values())\n                }\n                \n                # Include complexity information for streamlined maps\n                if 'pattern_complexities' in optimizer.pattern_map:\n                    complexities = {}\n                    for pattern_type, data in optimizer.pattern_map['pattern_complexities'].items():\n                        complexities[pattern_type] = data.get('avg_complexity', 2.5)\n                    pattern_map_info['pattern_complexities'] = complexities\n        \n        # Get risk assessment info if available\n        risk_assessment_info = {}\n        if hasattr(optimizer, 'pattern_tracker'):\n            risk_info = {\n                'pattern_risks': optimizer.pattern_tracker.get_pattern_risks(),\n                'overall_risk': optimizer.pattern_tracker.get_overall_risk()\n            }\n            risk_assessment_info = risk_info\n        \n        # Create diagnostics data\n        diagnostics = {\n            'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n            'model_info': {\n                'parameters': int(model_params),\n                'size_mb': float(model_size_mb),\n            },\n            'device_info': {\n                'type': device.type,\n                'name': torch.cuda.get_device_name(device) if device.type == 'cuda' else 'CPU',\n                'memory_gb': float(torch.cuda.get_device_properties(device).total_memory / (1024**3)) if device.type == 'cuda' else 0\n            },\n            'batch_info': {\n                'min_batch': int(min_batch),\n                'max_batch': int(max_batch)\n            },\n            'pattern_map_info': pattern_map_info,\n            'risk_assessment_info': risk_assessment_info\n        }\n        \n        # Save to file\n        output_dir = os.path.join(\"examples\", \"output\")\n        os.makedirs(output_dir, exist_ok=True)\n        filepath = os.path.join(output_dir, f\"streamlined_responsive_diagnostics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n        \n        with open(filepath, 'w') as f:\n            json.dump(diagnostics, f, indent=2)\n            \n        logger.info(f\"Batch diagnostics saved to: {filepath}\")\n        return filepath\n    except Exception as e:\n        logger.error(f\"Error saving diagnostics: {str(e)}\")\n        return None\n\ndef load_cifar10_data():\n    \"\"\"\n    Load and prepare CIFAR-10 dataset."
    },
    "load_cifar10_data": {
      "start_line": 578,
      "end_line": 607,
      "parameters": [],
      "return_type": null,
      "calls": [
        {
          "name": "transforms.Compose",
          "line": 586
        },
        {
          "name": "transforms.Compose",
          "line": 593
        },
        {
          "name": "torchvision.datasets.CIFAR10",
          "line": 599
        },
        {
          "name": "torchvision.datasets.CIFAR10",
          "line": 602
        },
        {
          "name": "transforms.RandomCrop",
          "line": 587
        },
        {
          "name": "transforms.RandomHorizontalFlip",
          "line": 588
        },
        {
          "name": "transforms.ToTensor",
          "line": 589
        },
        {
          "name": "transforms.Normalize",
          "line": 590
        },
        {
          "name": "transforms.ToTensor",
          "line": 594
        },
        {
          "name": "transforms.Normalize",
          "line": 595
        }
      ],
      "docstring": "\n    Load and prepare CIFAR-10 dataset.\n    \n    Returns:\n        Train dataset, test dataset\n    ",
      "code_snippet": "        return None\n\ndef load_cifar10_data():\n    \"\"\"\n    Load and prepare CIFAR-10 dataset.\n    \n    Returns:\n        Train dataset, test dataset\n    \"\"\"\n    # Data transforms\n    transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n\n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n\n    # Load datasets\n    trainset = torchvision.datasets.CIFAR10(\n        root='./data', train=True, download=True, transform=transform_train)\n\n    testset = torchvision.datasets.CIFAR10(\n        root='./data', train=False, download=True, transform=transform_test)\n\n    return trainset, testset\n\ndef create_model():\n    \"\"\"\n    Create a model for CIFAR-10."
    },
    "create_model": {
      "start_line": 607,
      "end_line": 626,
      "parameters": [],
      "return_type": null,
      "calls": [
        {
          "name": "models.resnet18",
          "line": 615
        },
        {
          "name": "nn.Conv2d",
          "line": 618
        },
        {
          "name": "nn.Identity",
          "line": 619
        },
        {
          "name": "nn.Linear",
          "line": 622
        }
      ],
      "docstring": "\n    Create a model for CIFAR-10.\n    \n    Returns:\n        Model instance\n    ",
      "code_snippet": "    return trainset, testset\n\ndef create_model():\n    \"\"\"\n    Create a model for CIFAR-10.\n    \n    Returns:\n        Model instance\n    \"\"\"\n    # Use ResNet-18 with appropriate output layer for CIFAR-10\n    model = models.resnet18(pretrained=False)\n\n    # Modify first conv layer to handle CIFAR-10's 32x32 images\n    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n    model.maxpool = nn.Identity()  # Remove maxpool as it's too aggressive for 32x32\n\n    # Modify the final fully connected layer for 10 classes\n    model.fc = nn.Linear(model.fc.in_features, 10)\n\n    return model\n\ndef visualize_training_results(history, output_path=None):\n    \"\"\"\n    Visualize training results including pattern risks and recognition rates."
    },
    "visualize_training_results": {
      "start_line": 626,
      "end_line": 825,
      "parameters": [
        {
          "name": "history"
        },
        {
          "name": "output_path"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "plt.figure",
          "line": 635
        },
        {
          "name": "range",
          "line": 638
        },
        {
          "name": "plt.subplot",
          "line": 646
        },
        {
          "name": "plt.plot",
          "line": 647
        },
        {
          "name": "plt.title",
          "line": 650
        },
        {
          "name": "plt.xlabel",
          "line": 651
        },
        {
          "name": "plt.ylabel",
          "line": 652
        },
        {
          "name": "plt.legend",
          "line": 653
        },
        {
          "name": "plt.grid",
          "line": 654
        },
        {
          "name": "plt.subplot",
          "line": 657
        },
        {
          "name": "plt.plot",
          "line": 658
        },
        {
          "name": "plt.title",
          "line": 661
        },
        {
          "name": "plt.xlabel",
          "line": 662
        },
        {
          "name": "plt.ylabel",
          "line": 663
        },
        {
          "name": "plt.legend",
          "line": 664
        },
        {
          "name": "plt.grid",
          "line": 665
        },
        {
          "name": "plt.subplot",
          "line": 668
        },
        {
          "name": "plt.plot",
          "line": 669
        },
        {
          "name": "plt.title",
          "line": 670
        },
        {
          "name": "plt.xlabel",
          "line": 671
        },
        {
          "name": "plt.ylabel",
          "line": 672
        },
        {
          "name": "plt.grid",
          "line": 673
        },
        {
          "name": "plt.subplot",
          "line": 676
        },
        {
          "name": "plt.plot",
          "line": 677
        },
        {
          "name": "plt.title",
          "line": 678
        },
        {
          "name": "plt.xlabel",
          "line": 679
        },
        {
          "name": "plt.ylabel",
          "line": 680
        },
        {
          "name": "plt.grid",
          "line": 681
        },
        {
          "name": "plt.subplot",
          "line": 684
        },
        {
          "name": "plt.plot",
          "line": 685
        },
        {
          "name": "plt.title",
          "line": 686
        },
        {
          "name": "plt.xlabel",
          "line": 687
        },
        {
          "name": "plt.ylabel",
          "line": 688
        },
        {
          "name": "plt.grid",
          "line": 689
        },
        {
          "name": "plt.subplot",
          "line": 692
        },
        {
          "name": "plt.grid",
          "line": 719
        },
        {
          "name": "plt.subplot",
          "line": 722
        },
        {
          "name": "plt.grid",
          "line": 749
        },
        {
          "name": "plt.subplot",
          "line": 752
        },
        {
          "name": "plt.tight_layout",
          "line": 816
        },
        {
          "name": "plt.close",
          "line": 823
        },
        {
          "name": "plt.plot",
          "line": 649
        },
        {
          "name": "plt.plot",
          "line": 660
        },
        {
          "name": "range",
          "line": 685
        },
        {
          "name": "set",
          "line": 695
        },
        {
          "name": "plt.title",
          "line": 709
        },
        {
          "name": "plt.xlabel",
          "line": 710
        },
        {
          "name": "plt.ylabel",
          "line": 711
        },
        {
          "name": "plt.legend",
          "line": 712
        },
        {
          "name": "plt.text",
          "line": 715
        },
        {
          "name": "plt.title",
          "line": 717
        },
        {
          "name": "set",
          "line": 725
        },
        {
          "name": "plt.title",
          "line": 739
        },
        {
          "name": "plt.xlabel",
          "line": 740
        },
        {
          "name": "plt.ylabel",
          "line": 741
        },
        {
          "name": "plt.legend",
          "line": 742
        },
        {
          "name": "plt.text",
          "line": 745
        },
        {
          "name": "plt.title",
          "line": 747
        },
        {
          "name": "set",
          "line": 756
        },
        {
          "name": "plt.title",
          "line": 776
        },
        {
          "name": "plt.xlabel",
          "line": 777
        },
        {
          "name": "plt.ylabel",
          "line": 778
        },
        {
          "name": "plt.grid",
          "line": 779
        },
        {
          "name": "plt.legend",
          "line": 780
        },
        {
          "name": "plt.text",
          "line": 783
        },
        {
          "name": "plt.title",
          "line": 785
        },
        {
          "name": "plt.subplot",
          "line": 789
        },
        {
          "name": "set",
          "line": 792
        },
        {
          "name": "plt.title",
          "line": 810
        },
        {
          "name": "plt.xlabel",
          "line": 811
        },
        {
          "name": "plt.ylabel",
          "line": 812
        },
        {
          "name": "plt.grid",
          "line": 813
        },
        {
          "name": "plt.legend",
          "line": 814
        },
        {
          "name": "plt.savefig",
          "line": 820
        },
        {
          "name": "logger.info",
          "line": 821
        },
        {
          "name": "len",
          "line": 638
        },
        {
          "name": "len",
          "line": 685
        },
        {
          "name": "pattern_types.update",
          "line": 697
        },
        {
          "name": "pattern_types.update",
          "line": 727
        },
        {
          "name": "pattern_types.update",
          "line": 758
        },
        {
          "name": "min",
          "line": 766
        },
        {
          "name": "range",
          "line": 768
        },
        {
          "name": "pattern_types.update",
          "line": 794
        },
        {
          "name": "enumerate",
          "line": 802
        },
        {
          "name": "rates.keys",
          "line": 697
        },
        {
          "name": "epoch_rates.get",
          "line": 700
        },
        {
          "name": "len",
          "line": 702
        },
        {
          "name": "len",
          "line": 702
        },
        {
          "name": "plt.plot",
          "line": 703
        },
        {
          "name": "plt.plot",
          "line": 707
        },
        {
          "name": "risks.keys",
          "line": 727
        },
        {
          "name": "epoch_risks.get",
          "line": 730
        },
        {
          "name": "len",
          "line": 732
        },
        {
          "name": "len",
          "line": 732
        },
        {
          "name": "plt.plot",
          "line": 733
        },
        {
          "name": "plt.plot",
          "line": 737
        },
        {
          "name": "risks.keys",
          "line": 758
        },
        {
          "name": "len",
          "line": 766
        },
        {
          "name": "len",
          "line": 766
        },
        {
          "name": "plt.scatter",
          "line": 774
        },
        {
          "name": "decay_data.keys",
          "line": 794
        },
        {
          "name": "plt.plot",
          "line": 808
        },
        {
          "name": "risks.append",
          "line": 770
        },
        {
          "name": "accuracies.append",
          "line": 771
        },
        {
          "name": "decay_values.append",
          "line": 804
        },
        {
          "name": "decay_epochs.append",
          "line": 805
        },
        {
          "name": "len",
          "line": 703
        },
        {
          "name": "len",
          "line": 706
        },
        {
          "name": "len",
          "line": 706
        },
        {
          "name": "len",
          "line": 733
        },
        {
          "name": "len",
          "line": 736
        },
        {
          "name": "len",
          "line": 736
        }
      ],
      "docstring": "\n    Visualize training results including pattern risks and recognition rates.\n    \n    Args:\n        history: Training history dictionary\n        output_path: Optional output file path\n    ",
      "code_snippet": "    return model\n\ndef visualize_training_results(history, output_path=None):\n    \"\"\"\n    Visualize training results including pattern risks and recognition rates.\n    \n    Args:\n        history: Training history dictionary\n        output_path: Optional output file path\n    \"\"\"\n    # Create figure\n    plt.figure(figsize=(15, 26))  # Further increased height for additional plots\n    \n    # Create epochs range\n    epochs = range(1, len(history['train_loss']) + 1)\n    \n    # Determine the number of subplots based on available data\n    num_plots = 8  # Default number of plots\n    if 'dynamic_weight_decays' in history and history['dynamic_weight_decays']:\n        num_plots = 9  # Add one more plot for weight decays\n    \n    # 1. Training metrics: Loss\n    plt.subplot(4, 2, 1)\n    plt.plot(epochs, history['train_loss'], 'b-', label='Training Loss')\n    if 'val_loss' in history and history['val_loss']:\n        plt.plot(epochs, history['val_loss'], 'r-', label='Validation Loss')\n    plt.title('Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n    \n    # 2. Training metrics: Accuracy\n    plt.subplot(4, 2, 2)\n    plt.plot(epochs, history['train_acc'], 'b-', label='Training Accuracy')\n    if 'val_acc' in history and history['val_acc']:\n        plt.plot(epochs, history['val_acc'], 'r-', label='Validation Accuracy')\n    plt.title('Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy (%)')\n    plt.legend()\n    plt.grid(True)\n    \n    # 3. Batch sizes\n    plt.subplot(4, 2, 3)\n    plt.plot(epochs, history['batch_sizes'], 'g-o', label='Batch Size')\n    plt.title('Batch Size')\n    plt.xlabel('Epoch')\n    plt.ylabel('Batch Size')\n    plt.grid(True)\n    \n    # 4. Epoch times\n    plt.subplot(4, 2, 4)\n    plt.plot(epochs, history['epoch_times'], 'm-o', label='Epoch Time')\n    plt.title('Training Time')\n    plt.xlabel('Epoch')\n    plt.ylabel('Time (s)')\n    plt.grid(True)\n    \n    # 5. Dataset growth\n    plt.subplot(4, 2, 5)\n    plt.plot(range(len(history['dataset_sizes'])), history['dataset_sizes'], 'c-o', label='Dataset Size')\n    plt.title('Dataset Growth')\n    plt.xlabel('Checkpoint')\n    plt.ylabel('Number of Examples')\n    plt.grid(True)\n    \n    # 6. Pattern recognition rates\n    plt.subplot(4, 2, 6)\n    if 'pattern_recognition_rates' in history and history['pattern_recognition_rates']:\n        # Extract pattern types and create separate lines for each\n        pattern_types = set()\n        for rates in history['pattern_recognition_rates']:\n            pattern_types.update(rates.keys())\n        \n        for pattern_type in pattern_types:\n            rates = [epoch_rates.get(pattern_type, 0) for epoch_rates in history['pattern_recognition_rates']]\n            # Only plot if we have enough data points\n            if len(rates) >= len(epochs):\n                plt.plot(epochs[:len(rates)], rates, marker='o', alpha=0.7, label=pattern_type)\n            else:\n                # Pad with zeros if needed\n                padded_rates = [0] * (len(epochs) - len(rates)) + rates\n                plt.plot(epochs, padded_rates, marker='o', alpha=0.7, label=pattern_type)\n        \n        plt.title('Pattern Recognition Rates')\n        plt.xlabel('Epoch')\n        plt.ylabel('Recognition Rate')\n        plt.legend(loc='lower right')\n    else:\n        # Fallback\n        plt.text(0.5, 0.5, 'No pattern recognition data available', \n                horizontalalignment='center', verticalalignment='center')\n        plt.title('Pattern Recognition Rates')\n    \n    plt.grid(True)\n    \n    # 7. Pattern risks\n    plt.subplot(4, 2, 7)\n    if 'pattern_risks' in history and history['pattern_risks']:\n        # Extract pattern types and create separate lines for each\n        pattern_types = set()\n        for risks in history['pattern_risks']:\n            pattern_types.update(risks.keys())\n        \n        for pattern_type in pattern_types:\n            risks = [epoch_risks.get(pattern_type, 0.5) for epoch_risks in history['pattern_risks']]\n            # Only plot if we have enough data points\n            if len(risks) >= len(epochs):\n                plt.plot(epochs[:len(risks)], risks, marker='o', alpha=0.7, label=pattern_type)\n            else:\n                # Pad with default value (0.5) if needed\n                padded_risks = [0.5] * (len(epochs) - len(risks)) + risks\n                plt.plot(epochs, padded_risks, marker='o', alpha=0.7, label=pattern_type)\n        \n        plt.title('Pattern Risk Levels')\n        plt.xlabel('Epoch')\n        plt.ylabel('Risk Level')\n        plt.legend(loc='upper right')\n    else:\n        # Fallback\n        plt.text(0.5, 0.5, 'No pattern risk data available', \n                horizontalalignment='center', verticalalignment='center')\n        plt.title('Pattern Risk Levels')\n    \n    plt.grid(True)\n    \n    # 8. Risk vs. Accuracy relationship\n    plt.subplot(num_plots//2, 2, 8)\n    if 'pattern_risks' in history and history['pattern_risks'] and 'pattern_recognition_rates' in history and history['pattern_recognition_rates']:\n        # We want to visualize the inverse relationship between risk and accuracy\n        # Get all pattern types\n        pattern_types = set()\n        for risks in history['pattern_risks']:\n            pattern_types.update(risks.keys())\n        \n        for pattern_type in pattern_types:\n            # Extract data for this pattern\n            risks = []\n            accuracies = []\n            \n            # Only use epochs where we have both risk and accuracy data\n            min_data_points = min(len(history['pattern_risks']), len(history['pattern_recognition_rates']))\n            \n            for i in range(min_data_points):\n                if pattern_type in history['pattern_risks'][i] and pattern_type in history['pattern_recognition_rates'][i]:\n                    risks.append(history['pattern_risks'][i][pattern_type])\n                    accuracies.append(history['pattern_recognition_rates'][i][pattern_type])\n            \n            if risks and accuracies:\n                plt.scatter(risks, accuracies, alpha=0.7, label=pattern_type)\n        \n        plt.title('Risk-Accuracy Relationship')\n        plt.xlabel('Risk Level')\n        plt.ylabel('Accuracy')\n        plt.grid(True)\n        plt.legend(loc='upper right')\n    else:\n        # Fallback\n        plt.text(0.5, 0.5, 'No risk-accuracy relationship data available', \n                horizontalalignment='center', verticalalignment='center')\n        plt.title('Risk-Accuracy Relationship')\n    \n    # 9. Dynamic Weight Decay (if available)\n    if 'dynamic_weight_decays' in history and history['dynamic_weight_decays']:\n        plt.subplot(5, 2, 9)\n        \n        # Get all pattern types with weight decay data\n        pattern_types = set()\n        for decay_data in history['dynamic_weight_decays']:\n            pattern_types.update(decay_data.keys())\n        \n        # Plot weight decay for each pattern type\n        for pattern_type in pattern_types:\n            decay_values = []\n            decay_epochs = []\n            \n            # Extract data points for this pattern\n            for i, decay_data in enumerate(history['dynamic_weight_decays']):\n                if pattern_type in decay_data:\n                    decay_values.append(decay_data[pattern_type])\n                    decay_epochs.append(i + 1)  # 1-indexed epochs\n            \n            if decay_values:\n                plt.plot(decay_epochs, decay_values, marker='o', alpha=0.7, label=f\"{pattern_type}\")\n        \n        plt.title('Dynamic Weight Decay by Pattern')\n        plt.xlabel('Epoch')\n        plt.ylabel('Weight Decay')\n        plt.grid(True)\n        plt.legend(loc='upper left')\n    \n    plt.tight_layout()\n    \n    # Save figure if output path provided\n    if output_path:\n        plt.savefig(output_path)\n        logger.info(f\"Training visualization saved to: {output_path}\")\n    \n    plt.close()\n\ndef modify_pattern_risk_accuracy_tracker():\n    \"\"\"\n    Monkey patch the PatternRiskAccuracyTracker to initialize risk from complexity."
    },
    "modify_pattern_risk_accuracy_tracker": {
      "start_line": 825,
      "end_line": 882,
      "parameters": [],
      "return_type": null,
      "calls": [
        {
          "name": "logging.info",
          "line": 880
        },
        {
          "name": "original_init",
          "line": 839
        },
        {
          "name": "logging.info",
          "line": 852
        },
        {
          "name": "complexities.items",
          "line": 855
        },
        {
          "name": "self._initialize_risks_from_complexities",
          "line": 843
        },
        {
          "name": "logging.warning",
          "line": 849
        },
        {
          "name": "min",
          "line": 870
        },
        {
          "name": "logging.info",
          "line": 872
        },
        {
          "name": "isinstance",
          "line": 858
        },
        {
          "name": "max",
          "line": 870
        },
        {
          "name": "isinstance",
          "line": 861
        }
      ],
      "docstring": "\n    Monkey patch the PatternRiskAccuracyTracker to initialize risk from complexity.\n    \n    This ensures compatibility with the streamlined pattern map format.\n    ",
      "code_snippet": "    plt.close()\n\ndef modify_pattern_risk_accuracy_tracker():\n    \"\"\"\n    Monkey patch the PatternRiskAccuracyTracker to initialize risk from complexity.\n    \n    This ensures compatibility with the streamlined pattern map format.\n    \"\"\"\n    from isekaizen.core.optimizer.pattern_risk_accuracy_tracker import PatternRiskAccuracyTracker\n    \n    # Store the original __init__ method\n    original_init = PatternRiskAccuracyTracker.__init__\n    \n    # Define a new initialization method that handles complexity information\n    def new_init(self, pattern_map=None):\n        # Call the original initialization\n        original_init(self, pattern_map)\n        \n        # Initialize risks from complexity if available\n        if pattern_map and 'pattern_complexities' in pattern_map:\n            self._initialize_risks_from_complexities()\n    \n    # Define the new method to initialize risks from complexities\n    def initialize_risks_from_complexities(self):\n        \"\"\"Initialize risks from complexity scores if available.\"\"\"\n        if not self.pattern_map or 'pattern_complexities' not in self.pattern_map:\n            logging.warning(\"No pattern complexities found for risk initialization\")\n            return\n            \n        logging.info(\"Initializing pattern risks from complexity scores\")\n        complexities = self.pattern_map['pattern_complexities']\n        \n        for pattern_type, complexity_info in complexities.items():\n            if pattern_type in self.pattern_stats:\n                # Use complexity score to initialize risk\n                if isinstance(complexity_info, dict) and 'avg_complexity' in complexity_info:\n                    avg_complexity = complexity_info['avg_complexity']\n                else:\n                    avg_complexity = complexity_info if isinstance(complexity_info, (int, float)) else 2.5\n                    \n                # Since the complexity in the pattern map is very low (~0.1), scale it up\n                if avg_complexity < 0.2:\n                    # Scale up low complexities to get meaningful risk values\n                    scaled_complexity = avg_complexity * 2.5\n                else:\n                    scaled_complexity = avg_complexity / 5.0  # Keep original scale for higher values\n                    \n                initial_risk = min(1.0, max(0.1, scaled_complexity))  # Minimum risk of 0.1\n                self.pattern_stats[pattern_type]['risk'] = initial_risk\n                logging.info(f\"Initialized {pattern_type} risk to {initial_risk:.2f} based on complexity {avg_complexity}\")\n    \n    # Add the new method to the class\n    PatternRiskAccuracyTracker._initialize_risks_from_complexities = initialize_risks_from_complexities\n    \n    # Replace the __init__ method\n    PatternRiskAccuracyTracker.__init__ = new_init\n    \n    logging.info(\"Enhanced PatternRiskAccuracyTracker to initialize risk from complexity scores\")\n\ndef track_dynamic_weight_decays(trainer, history):\n    \"\"\"\n    Track dynamic weight decays from EVENaturalWeights optimizer and add to history."
    },
    "track_dynamic_weight_decays": {
      "start_line": 882,
      "end_line": 905,
      "parameters": [
        {
          "name": "trainer"
        },
        {
          "name": "history"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "trainer.optimizer.get_pattern_weight_decays",
          "line": 898
        },
        {
          "name": "isinstance",
          "line": 890
        },
        {
          "name": "....append",
          "line": 902
        },
        {
          "name": "logger.info",
          "line": 903
        },
        {
          "name": "....join",
          "line": 903
        },
        {
          "name": "weight_decays.items",
          "line": 903
        }
      ],
      "docstring": "\n    Track dynamic weight decays from EVENaturalWeights optimizer and add to history.\n    \n    Args:\n        trainer: The trainer instance\n        history: Training history dictionary to update\n    ",
      "code_snippet": "    logging.info(\"Enhanced PatternRiskAccuracyTracker to initialize risk from complexity scores\")\n\ndef track_dynamic_weight_decays(trainer, history):\n    \"\"\"\n    Track dynamic weight decays from EVENaturalWeights optimizer and add to history.\n    \n    Args:\n        trainer: The trainer instance\n        history: Training history dictionary to update\n    \"\"\"\n    if not isinstance(trainer.optimizer, EVENaturalWeights):\n        return\n    \n    # Initialize dynamic weight decays tracking if not already present\n    if 'dynamic_weight_decays' not in history:\n        history['dynamic_weight_decays'] = []\n    \n    # Get current dynamic weight decays\n    weight_decays = trainer.optimizer.get_pattern_weight_decays()\n    \n    # Add to history\n    if weight_decays:\n        history['dynamic_weight_decays'].append(weight_decays)\n        logger.info(f\"Dynamic weight decays: {', '.join([f'{k}:{v:.5f}' for k, v in weight_decays.items()])}\")\n\ndef evaluate_model_detailed(model, device):\n    \"\"\"\n    Evaluate model with per-class accuracy breakdown."
    },
    "evaluate_model_detailed": {
      "start_line": 905,
      "end_line": 984,
      "parameters": [
        {
          "name": "model"
        },
        {
          "name": "device"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "model.eval",
          "line": 916
        },
        {
          "name": "torch.utils.data.DataLoader",
          "line": 920
        },
        {
          "name": "logger.info",
          "line": 923
        },
        {
          "name": "range",
          "line": 955
        },
        {
          "name": "logger.info",
          "line": 968
        },
        {
          "name": "logger.info",
          "line": 969
        },
        {
          "name": "logger.info",
          "line": 970
        },
        {
          "name": "logger.info",
          "line": 971
        },
        {
          "name": "logger.info",
          "line": 972
        },
        {
          "name": "logger.info",
          "line": 973
        },
        {
          "name": "logger.info",
          "line": 974
        },
        {
          "name": "logger.info",
          "line": 975
        },
        {
          "name": "per_class_accuracy.items",
          "line": 977
        },
        {
          "name": "logger.info",
          "line": 980
        },
        {
          "name": "load_cifar10_data",
          "line": 919
        },
        {
          "name": "torch.no_grad",
          "line": 932
        },
        {
          "name": "logger.info",
          "line": 978
        },
        {
          "name": "model",
          "line": 935
        },
        {
          "name": "outputs.max",
          "line": 936
        },
        {
          "name": "targets.size",
          "line": 939
        },
        {
          "name": "....item",
          "line": 940
        },
        {
          "name": "predicted.eq",
          "line": 943
        },
        {
          "name": "range",
          "line": 944
        },
        {
          "name": "max",
          "line": 956
        },
        {
          "name": "len",
          "line": 923
        },
        {
          "name": "inputs.to",
          "line": 934
        },
        {
          "name": "targets.to",
          "line": 934
        },
        {
          "name": "targets.size",
          "line": 944
        },
        {
          "name": "....item",
          "line": 945
        },
        {
          "name": "....item",
          "line": 946
        },
        {
          "name": "....sum",
          "line": 940
        },
        {
          "name": "predicted.eq",
          "line": 940
        }
      ],
      "docstring": "\n    Evaluate model with per-class accuracy breakdown.\n    \n    Args:\n        model: PyTorch model to evaluate\n        device: Compute device\n        \n    Returns:\n        Dictionary with evaluation results\n    ",
      "code_snippet": "        logger.info(f\"Dynamic weight decays: {', '.join([f'{k}:{v:.5f}' for k, v in weight_decays.items()])}\")\n\ndef evaluate_model_detailed(model, device):\n    \"\"\"\n    Evaluate model with per-class accuracy breakdown.\n    \n    Args:\n        model: PyTorch model to evaluate\n        device: Compute device\n        \n    Returns:\n        Dictionary with evaluation results\n    \"\"\"\n    model.eval()\n    \n    # Load test data\n    testset = load_cifar10_data()[1]  # Just get the test set\n    testloader = torch.utils.data.DataLoader(\n        testset, batch_size=100, shuffle=False, num_workers=2)\n    \n    logger.info(f\"Performing detailed evaluation on {len(testset)} test samples...\")\n    \n    # Initialize counters\n    correct = 0\n    total = 0\n    class_correct = [0] * 10\n    class_total = [0] * 10\n    \n    # Evaluate\n    with torch.no_grad():\n        for inputs, targets in testloader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            _, predicted = outputs.max(1)\n            \n            # Overall accuracy\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n            \n            # Per-class accuracy\n            c = predicted.eq(targets)\n            for i in range(targets.size(0)):\n                label = targets[i].item()\n                class_correct[label] += c[i].item()\n                class_total[label] += 1\n    \n    # Calculate overall accuracy\n    test_accuracy = 100. * correct / total\n    \n    # Calculate per-class accuracy\n    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n    per_class_accuracy = {}\n    for i in range(10):\n        class_acc = 100. * class_correct[i] / max(1, class_total[i])\n        per_class_accuracy[classes[i]] = class_acc\n    \n    # Prepare results\n    results = {\n        'accuracy': test_accuracy,\n        'error_rate': 100.0 - test_accuracy,\n        'per_class_accuracy': per_class_accuracy,\n        'samples_evaluated': total\n    }\n    \n    # Print results\n    logger.info(\"\\n\" + \"=\" * 50)\n    logger.info(\"DETAILED EVALUATION RESULTS\")\n    logger.info(\"=\" * 50)\n    logger.info(f\"Model:                     ResNet-18\")\n    logger.info(f\"Test Set:                  CIFAR-10 ({total} samples)\")\n    logger.info(f\"Overall Test Accuracy:     {test_accuracy:.2f}%\")\n    logger.info(f\"Overall Test Error Rate:   {(100.0 - test_accuracy):.2f}%\")\n    logger.info(\"\\nPer-Class Accuracy:\")\n    \n    for cls, acc in per_class_accuracy.items():\n        logger.info(f\"  {cls:8s}: {acc:.2f}%\")\n    \n    logger.info(\"=\" * 50)\n    \n    return results\n\ndef save_training_results_to_json(history, evaluation_results, optimizer_info, pattern_map_info, output_path):\n    \"\"\"\n    Save training results and model evaluation data to a JSON file for later analysis."
    },
    "save_training_results_to_json": {
      "start_line": 984,
      "end_line": 1092,
      "parameters": [
        {
          "name": "history"
        },
        {
          "name": "evaluation_results"
        },
        {
          "name": "optimizer_info"
        },
        {
          "name": "pattern_map_info"
        },
        {
          "name": "output_path"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "max",
          "line": 1012
        },
        {
          "name": "os.makedirs",
          "line": 1083
        },
        {
          "name": "logger.info",
          "line": 1089
        },
        {
          "name": "len",
          "line": 1004
        },
        {
          "name": "list",
          "line": 1015
        },
        {
          "name": "set",
          "line": 1027
        },
        {
          "name": "set",
          "line": 1041
        },
        {
          "name": "set",
          "line": 1055
        },
        {
          "name": "....strftime",
          "line": 1069
        },
        {
          "name": "os.path.dirname",
          "line": 1083
        },
        {
          "name": "open",
          "line": 1086
        },
        {
          "name": "json.dump",
          "line": 1087
        },
        {
          "name": "len",
          "line": 1012
        },
        {
          "name": "range",
          "line": 1015
        },
        {
          "name": "pattern_types.update",
          "line": 1029
        },
        {
          "name": "pattern_types.update",
          "line": 1043
        },
        {
          "name": "pattern_types.update",
          "line": 1057
        },
        {
          "name": "sum",
          "line": 1005
        },
        {
          "name": "len",
          "line": 1005
        },
        {
          "name": "len",
          "line": 1007
        },
        {
          "name": "range",
          "line": 1016
        },
        {
          "name": "range",
          "line": 1018
        },
        {
          "name": "rates.keys",
          "line": 1029
        },
        {
          "name": "....append",
          "line": 1035
        },
        {
          "name": "risks.keys",
          "line": 1043
        },
        {
          "name": "....append",
          "line": 1049
        },
        {
          "name": "decays.keys",
          "line": 1057
        },
        {
          "name": "....append",
          "line": 1063
        },
        {
          "name": "datetime.now",
          "line": 1069
        },
        {
          "name": "len",
          "line": 1015
        },
        {
          "name": "len",
          "line": 1016
        },
        {
          "name": "range",
          "line": 1017
        },
        {
          "name": "len",
          "line": 1018
        },
        {
          "name": "range",
          "line": 1019
        },
        {
          "name": "rates.get",
          "line": 1035
        },
        {
          "name": "risks.get",
          "line": 1049
        },
        {
          "name": "decays.get",
          "line": 1063
        },
        {
          "name": "len",
          "line": 1017
        },
        {
          "name": "len",
          "line": 1019
        },
        {
          "name": "isinstance",
          "line": 1087
        },
        {
          "name": "float",
          "line": 1087
        },
        {
          "name": "str",
          "line": 1087
        }
      ],
      "docstring": "\n    Save training results and model evaluation data to a JSON file for later analysis.\n    \n    Args:\n        history: Training history dictionary\n        evaluation_results: Evaluation results dictionary\n        optimizer_info: Information about the optimizer configuration\n        pattern_map_info: Information about the pattern map used\n        output_path: Path to save the JSON file\n    ",
      "code_snippet": "    return results\n\ndef save_training_results_to_json(history, evaluation_results, optimizer_info, pattern_map_info, output_path):\n    \"\"\"\n    Save training results and model evaluation data to a JSON file for later analysis.\n    \n    Args:\n        history: Training history dictionary\n        evaluation_results: Evaluation results dictionary\n        optimizer_info: Information about the optimizer configuration\n        pattern_map_info: Information about the pattern map used\n        output_path: Path to save the JSON file\n    \"\"\"\n    import json\n    from datetime import datetime\n    \n    # Prepare training metrics summary\n    training_summary = {\n        'train_acc_final': history['train_acc'][-1] if history['train_acc'] else None,\n        'val_acc_final': history['val_acc'][-1] if history['val_acc'] else None,\n        'train_loss_final': history['train_loss'][-1] if history['train_loss'] else None,\n        'val_loss_final': history['val_loss'][-1] if history['val_loss'] else None,\n        'epochs_completed': len(history['train_acc']),\n        'avg_epoch_time': sum(history['epoch_times']) / len(history['epoch_times']) if history['epoch_times'] else 0,\n        'dataset_final_size': history['dataset_sizes'][-1] if history['dataset_sizes'] else 0,\n        'dataset_growth': history['dataset_sizes'][-1] - history['dataset_sizes'][0] if len(history['dataset_sizes']) > 1 else 0,\n    }\n    \n    # Extract training curves (with reasonable sampling for large runs)\n    max_points = 50  # Max points to include for large training runs\n    sampling_rate = max(1, len(history['train_acc']) // max_points)\n    \n    training_curves = {\n        'epochs': list(range(1, len(history['train_acc']) + 1, sampling_rate)),\n        'train_acc': [history['train_acc'][i] for i in range(0, len(history['train_acc']), sampling_rate)],\n        'val_acc': [history['val_acc'][i] for i in range(0, len(history['val_acc']), sampling_rate)] if history['val_acc'] else [],\n        'train_loss': [history['train_loss'][i] for i in range(0, len(history['train_loss']), sampling_rate)],\n        'val_loss': [history['val_loss'][i] for i in range(0, len(history['val_loss']), sampling_rate)] if history['val_loss'] else [],\n    }\n    \n    # Include pattern-specific information\n    pattern_data = {}\n    \n    # Pattern recognition rates\n    if 'pattern_recognition_rates' in history and history['pattern_recognition_rates']:\n        pattern_types = set()\n        for rates in history['pattern_recognition_rates']:\n            pattern_types.update(rates.keys())\n            \n        recognition_rates = {pattern_type: [] for pattern_type in pattern_types}\n        \n        for rates in history['pattern_recognition_rates'][::sampling_rate]:\n            for pattern_type in pattern_types:\n                recognition_rates[pattern_type].append(rates.get(pattern_type, None))\n                \n        pattern_data['recognition_rates'] = recognition_rates\n    \n    # Pattern risks\n    if 'pattern_risks' in history and history['pattern_risks']:\n        pattern_types = set()\n        for risks in history['pattern_risks']:\n            pattern_types.update(risks.keys())\n            \n        risk_values = {pattern_type: [] for pattern_type in pattern_types}\n        \n        for risks in history['pattern_risks'][::sampling_rate]:\n            for pattern_type in pattern_types:\n                risk_values[pattern_type].append(risks.get(pattern_type, None))\n                \n        pattern_data['risks'] = risk_values\n    \n    # Dynamic weight decays\n    if 'dynamic_weight_decays' in history and history['dynamic_weight_decays']:\n        pattern_types = set()\n        for decays in history['dynamic_weight_decays']:\n            pattern_types.update(decays.keys())\n            \n        decay_values = {pattern_type: [] for pattern_type in pattern_types}\n        \n        for decays in history['dynamic_weight_decays']:\n            for pattern_type in pattern_types:\n                decay_values[pattern_type].append(decays.get(pattern_type, None))\n                \n        pattern_data['weight_decays'] = decay_values\n    \n    # Create complete results dictionary\n    results = {\n        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n        'model_info': {\n            'architecture': 'ResNet-18',\n            'dataset': 'CIFAR-10',\n        },\n        'optimizer_info': optimizer_info,\n        'pattern_map_info': pattern_map_info,\n        'training_summary': training_summary,\n        'training_curves': training_curves,\n        'pattern_data': pattern_data,\n        'evaluation_results': evaluation_results\n    }\n    \n    # Ensure directory exists\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    \n    # Save to JSON file\n    with open(output_path, 'w') as f:\n        json.dump(results, f, indent=2, default=lambda x: float(x) if isinstance(x, (torch.Tensor, np.ndarray, np.float32, np.float64)) else str(x))\n    \n    logger.info(f\"Training and evaluation results saved to: {output_path}\")\n    return output_path\n\ndef main():\n    \"\"\"\n    Run streamlined pattern-responsive training with risk-accuracy relationship."
    },
    "main": {
      "start_line": 1092,
      "end_line": 1493,
      "parameters": [],
      "return_type": null,
      "calls": [
        {
          "name": "modify_pattern_risk_accuracy_tracker",
          "line": 1097
        },
        {
          "name": "argparse.ArgumentParser",
          "line": 1100
        },
        {
          "name": "parser.add_argument",
          "line": 1101
        },
        {
          "name": "parser.add_argument",
          "line": 1102
        },
        {
          "name": "parser.add_argument",
          "line": 1104
        },
        {
          "name": "parser.add_argument",
          "line": 1106
        },
        {
          "name": "parser.add_argument",
          "line": 1108
        },
        {
          "name": "add_optimizer_arguments",
          "line": 1112
        },
        {
          "name": "parser.parse_args",
          "line": 1114
        },
        {
          "name": "print_available_optimizers",
          "line": 1117
        },
        {
          "name": "logger.info",
          "line": 1120
        },
        {
          "name": "logger.info",
          "line": 1121
        },
        {
          "name": "logger.info",
          "line": 1122
        },
        {
          "name": "logger.info",
          "line": 1123
        },
        {
          "name": "torch.device",
          "line": 1138
        },
        {
          "name": "logger.info",
          "line": 1139
        },
        {
          "name": "logger.info",
          "line": 1142
        },
        {
          "name": "logger.info",
          "line": 1211
        },
        {
          "name": "load_cifar10_data",
          "line": 1212
        },
        {
          "name": "logger.info",
          "line": 1213
        },
        {
          "name": "logger.info",
          "line": 1216
        },
        {
          "name": "create_model",
          "line": 1217
        },
        {
          "name": "model.to",
          "line": 1218
        },
        {
          "name": "logger.info",
          "line": 1221
        },
        {
          "name": "configure_optimizer",
          "line": 1224
        },
        {
          "name": "isinstance",
          "line": 1232
        },
        {
          "name": "StreamlinedPatternTrainer",
          "line": 1240
        },
        {
          "name": "logger.info",
          "line": 1263
        },
        {
          "name": "save_batch_diagnostics",
          "line": 1264
        },
        {
          "name": "logger.info",
          "line": 1265
        },
        {
          "name": "logger.info",
          "line": 1268
        },
        {
          "name": "logger.info",
          "line": 1269
        },
        {
          "name": "logger.info",
          "line": 1270
        },
        {
          "name": "logger.info",
          "line": 1271
        },
        {
          "name": "logger.info",
          "line": 1272
        },
        {
          "name": "logger.info",
          "line": 1273
        },
        {
          "name": "logger.info",
          "line": 1274
        },
        {
          "name": "logger.info",
          "line": 1275
        },
        {
          "name": "logger.info",
          "line": 1276
        },
        {
          "name": "hasattr",
          "line": 1279
        },
        {
          "name": "logger.info",
          "line": 1286
        },
        {
          "name": "callbacks.append",
          "line": 1343
        },
        {
          "name": "isinstance",
          "line": 1346
        },
        {
          "name": "trainer.train",
          "line": 1366
        },
        {
          "name": "logger.info",
          "line": 1374
        },
        {
          "name": "logger.info",
          "line": 1375
        },
        {
          "name": "logger.info",
          "line": 1376
        },
        {
          "name": "os.path.join",
          "line": 1379
        },
        {
          "name": "os.makedirs",
          "line": 1380
        },
        {
          "name": "os.path.join",
          "line": 1381
        },
        {
          "name": "visualize_training_results",
          "line": 1382
        },
        {
          "name": "logger.info",
          "line": 1385
        },
        {
          "name": "evaluate_model_detailed",
          "line": 1386
        },
        {
          "name": "logger.info",
          "line": 1392
        },
        {
          "name": "os.path.join",
          "line": 1395
        },
        {
          "name": "trainer.save_model",
          "line": 1396
        },
        {
          "name": "logger.info",
          "line": 1397
        },
        {
          "name": "logger.info",
          "line": 1400
        },
        {
          "name": "hasattr",
          "line": 1412
        },
        {
          "name": "logger.info",
          "line": 1424
        },
        {
          "name": "logger.info",
          "line": 1425
        },
        {
          "name": "logger.info",
          "line": 1426
        },
        {
          "name": "logger.info",
          "line": 1427
        },
        {
          "name": "logger.info",
          "line": 1428
        },
        {
          "name": "logger.info",
          "line": 1429
        },
        {
          "name": "logger.info",
          "line": 1430
        },
        {
          "name": "logger.info",
          "line": 1431
        },
        {
          "name": "logger.info",
          "line": 1445
        },
        {
          "name": "logger.info",
          "line": 1453
        },
        {
          "name": "....strftime",
          "line": 1483
        },
        {
          "name": "os.path.join",
          "line": 1484
        },
        {
          "name": "save_training_results_to_json",
          "line": 1485
        },
        {
          "name": "logger.info",
          "line": 1487
        },
        {
          "name": "logger.info",
          "line": 1488
        },
        {
          "name": "logger.info",
          "line": 1489
        },
        {
          "name": "logger.info",
          "line": 1490
        },
        {
          "name": "logger.info",
          "line": 1491
        },
        {
          "name": "logger.info",
          "line": 1125
        },
        {
          "name": "logger.info",
          "line": 1135
        },
        {
          "name": "logger.info",
          "line": 1144
        },
        {
          "name": "logger.info",
          "line": 1154
        },
        {
          "name": "load_latest_pattern_map",
          "line": 1155
        },
        {
          "name": "logger.info",
          "line": 1186
        },
        {
          "name": "logger.warning",
          "line": 1207
        },
        {
          "name": "logger.info",
          "line": 1233
        },
        {
          "name": "optimizer.initialize_from_pattern_map",
          "line": 1235
        },
        {
          "name": "logger.info",
          "line": 1236
        },
        {
          "name": "trainer.batch_optimizer.hardware_analyzer.cleanup_memory",
          "line": 1280
        },
        {
          "name": "torch.cuda.is_available",
          "line": 1281
        },
        {
          "name": "callbacks.append",
          "line": 1363
        },
        {
          "name": "hasattr",
          "line": 1401
        },
        {
          "name": "logger.info",
          "line": 1409
        },
        {
          "name": "os.path.join",
          "line": 1418
        },
        {
          "name": "logger.info",
          "line": 1421
        },
        {
          "name": "logger.info",
          "line": 1435
        },
        {
          "name": "last_decays.items",
          "line": 1437
        },
        {
          "name": "logger.info",
          "line": 1443
        },
        {
          "name": "hasattr",
          "line": 1448
        },
        {
          "name": "sum",
          "line": 1449
        },
        {
          "name": "logger.info",
          "line": 1450
        },
        {
          "name": "logger.info",
          "line": 1451
        },
        {
          "name": "logger.info",
          "line": 1127
        },
        {
          "name": "logger.info",
          "line": 1129
        },
        {
          "name": "logger.info",
          "line": 1131
        },
        {
          "name": "logger.info",
          "line": 1133
        },
        {
          "name": "torch.cuda.is_available",
          "line": 1138
        },
        {
          "name": "logger.info",
          "line": 1148
        },
        {
          "name": "logger.info",
          "line": 1191
        },
        {
          "name": "translate_pattern_map_to_standard_format",
          "line": 1192
        },
        {
          "name": "logger.info",
          "line": 1193
        },
        {
          "name": "nn.CrossEntropyLoss",
          "line": 1242
        },
        {
          "name": "torch.cuda.empty_cache",
          "line": 1283
        },
        {
          "name": "torch.cuda.reset_peak_memory_stats",
          "line": 1284
        },
        {
          "name": "isinstance",
          "line": 1349
        },
        {
          "name": "logger.info",
          "line": 1403
        },
        {
          "name": "logger.info",
          "line": 1407
        },
        {
          "name": "trainer.batch_optimizer.pattern_tracker.get_pattern_risks",
          "line": 1414
        },
        {
          "name": "trainer.batch_optimizer.pattern_tracker.get_pattern_accuracies",
          "line": 1415
        },
        {
          "name": "trainer.batch_optimizer.pattern_tracker.get_overall_risk",
          "line": 1416
        },
        {
          "name": "open",
          "line": 1419
        },
        {
          "name": "json.dump",
          "line": 1420
        },
        {
          "name": "logger.info",
          "line": 1438
        },
        {
          "name": "sum",
          "line": 1442
        },
        {
          "name": "len",
          "line": 1442
        },
        {
          "name": "....items",
          "line": 1475
        },
        {
          "name": "datetime.now",
          "line": 1483
        },
        {
          "name": "open",
          "line": 1146
        },
        {
          "name": "json.load",
          "line": 1147
        },
        {
          "name": "logger.error",
          "line": 1150
        },
        {
          "name": "logger.info",
          "line": 1151
        },
        {
          "name": "load_latest_pattern_map",
          "line": 1152
        },
        {
          "name": "os.path.join",
          "line": 1161
        },
        {
          "name": "os.path.exists",
          "line": 1162
        },
        {
          "name": "logger.info",
          "line": 1197
        },
        {
          "name": "logger.warning",
          "line": 1202
        },
        {
          "name": "logger.error",
          "line": 1204
        },
        {
          "name": "logger.warning",
          "line": 1205
        },
        {
          "name": "len",
          "line": 1213
        },
        {
          "name": "len",
          "line": 1213
        },
        {
          "name": "len",
          "line": 1272
        },
        {
          "name": "len",
          "line": 1294
        },
        {
          "name": "len",
          "line": 1294
        },
        {
          "name": "logger.info",
          "line": 1301
        },
        {
          "name": "hasattr",
          "line": 1304
        },
        {
          "name": "optimizer.get_pattern_weight_decays",
          "line": 1355
        },
        {
          "name": "....strftime",
          "line": 1381
        },
        {
          "name": "logger.info",
          "line": 1406
        },
        {
          "name": "len",
          "line": 1428
        },
        {
          "name": "len",
          "line": 1445
        },
        {
          "name": "logger.info",
          "line": 1173
        },
        {
          "name": "logger.warning",
          "line": 1181
        },
        {
          "name": "logger.error",
          "line": 1183
        },
        {
          "name": "logger.info",
          "line": 1199
        },
        {
          "name": "logger.info",
          "line": 1305
        },
        {
          "name": "hasattr",
          "line": 1308
        },
        {
          "name": "trainer.batch_optimizer.should_adapt_patterns",
          "line": 1312
        },
        {
          "name": "hasattr",
          "line": 1337
        },
        {
          "name": "....append",
          "line": 1359
        },
        {
          "name": "logger.info",
          "line": 1360
        },
        {
          "name": "....strftime",
          "line": 1418
        },
        {
          "name": "isinstance",
          "line": 1476
        },
        {
          "name": "open",
          "line": 1163
        },
        {
          "name": "....strip",
          "line": 1164
        },
        {
          "name": "os.path.isabs",
          "line": 1167
        },
        {
          "name": "os.path.join",
          "line": 1168
        },
        {
          "name": "logger.info",
          "line": 1177
        },
        {
          "name": "logger.info",
          "line": 1315
        },
        {
          "name": "trainer.batch_optimizer.adapt_dataset",
          "line": 1318
        },
        {
          "name": "adaptation_metrics.get",
          "line": 1320
        },
        {
          "name": "logger.info",
          "line": 1334
        },
        {
          "name": "datetime.now",
          "line": 1381
        },
        {
          "name": "str",
          "line": 1150
        },
        {
          "name": "os.getcwd",
          "line": 1168
        },
        {
          "name": "open",
          "line": 1175
        },
        {
          "name": "json.load",
          "line": 1176
        },
        {
          "name": "logger.error",
          "line": 1179
        },
        {
          "name": "len",
          "line": 1197
        },
        {
          "name": "str",
          "line": 1204
        },
        {
          "name": "logger.info",
          "line": 1321
        },
        {
          "name": "....join",
          "line": 1406
        },
        {
          "name": "datetime.now",
          "line": 1418
        },
        {
          "name": "f.read",
          "line": 1164
        },
        {
          "name": "str",
          "line": 1183
        },
        {
          "name": "len",
          "line": 1199
        },
        {
          "name": "hasattr",
          "line": 1324
        },
        {
          "name": "trainer.dataset_adaptations.append",
          "line": 1328
        },
        {
          "name": "....join",
          "line": 1360
        },
        {
          "name": "str",
          "line": 1179
        },
        {
          "name": "pattern_risks.items",
          "line": 1406
        },
        {
          "name": "weight_decays.items",
          "line": 1360
        }
      ],
      "docstring": "\n    Run streamlined pattern-responsive training with risk-accuracy relationship.\n    ",
      "code_snippet": "    return output_path\n\ndef main():\n    \"\"\"\n    Run streamlined pattern-responsive training with risk-accuracy relationship.\n    \"\"\"\n    # Modify the PatternRiskAccuracyTracker to handle complexity information\n    modify_pattern_risk_accuracy_tracker()\n    \n    # Parse command line arguments\n    parser = argparse.ArgumentParser(description=\"Run streamlined pattern-responsive training\")\n    parser.add_argument(\"--epochs\", type=int, default=15, help=\"Number of epochs for training (default: 15)\")\n    parser.add_argument(\"--target-accuracy\", type=float, default=None, help=\"Target accuracy to dynamically adjust training behavior\")\n    # When target accuracy is set, allow configuration of stagnation parameters\n    parser.add_argument(\"--stagnation-window\", type=int, default=None, \n                      help=\"Number of epochs to consider for stagnation detection (only used when target-accuracy is set)\")\n    parser.add_argument(\"--stagnation-threshold\", type=float, default=None, \n                      help=\"Minimum accuracy improvement required over the window (only used when target-accuracy is set)\")\n    parser.add_argument(\"--pattern-map-path\", type=str, default=None, \n                      help=\"Path to a specific pattern map file (default: use latest)\")\n    \n    # Add optimizer options\n    add_optimizer_arguments(parser)\n    \n    args = parser.parse_args()\n    \n    # Show available optimizers if requested\n    print_available_optimizers()\n    \n    # Show configuration\n    logger.info(\"=== Streamlined Pattern-Responsive Training with Risk-Accuracy Relationship ===\")\n    logger.info(f\"Epochs: {args.epochs}\")\n    logger.info(f\"Optimizer: {args.optimizer} ({args.optimizer_variant})\")\n    logger.info(\"Pattern adaptation is self-regulated based on risk-accuracy relationship\")\n    if args.target_accuracy:\n        logger.info(f\"Target accuracy: {args.target_accuracy}\")\n        if args.stagnation_window:\n            logger.info(f\"Stagnation window: {args.stagnation_window}\")\n        else:\n            logger.info(\"Stagnation window: auto-determined based on target accuracy\")\n        if args.stagnation_threshold:\n            logger.info(f\"Stagnation threshold: {args.stagnation_threshold}\")\n        else:\n            logger.info(\"Stagnation threshold: auto-determined based on target accuracy\")\n    else:\n        logger.info(\"Target accuracy: none (using fully automated stagnation detection)\")\n    \n    # Set device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    logger.info(f\"Using device: {device}\")\n    \n    # Load pattern map\n    logger.info(\"Loading pattern map...\")\n    if args.pattern_map_path:\n        logger.info(f\"Loading pattern map from specified path: {args.pattern_map_path}\")\n        try:\n            with open(args.pattern_map_path, 'r') as f:\n                pattern_map = json.load(f)\n            logger.info(\"Pattern map loaded successfully from specified path\")\n        except Exception as e:\n            logger.error(f\"Error loading pattern map from {args.pattern_map_path}: {str(e)}\")\n            logger.info(\"Falling back to latest pattern map\")\n            pattern_map = load_latest_pattern_map()\n    else:\n        logger.info(\"Loading latest pattern map from default location\")\n        pattern_map = load_latest_pattern_map()\n        \n        # If still no pattern map found, try to find it directly\n        if not pattern_map:\n            try:\n                # Check for specific path from latest_pattern_map_path.txt\n                map_path_file = os.path.join(\"benchmarks\", \"semantic_maps\", \"latest_pattern_map_path.txt\")\n                if os.path.exists(map_path_file):\n                    with open(map_path_file, 'r') as f:\n                        relative_path = f.read().strip()\n                    \n                    # Convert to absolute path if needed\n                    if not os.path.isabs(relative_path):\n                        map_path = os.path.join(os.getcwd(), relative_path)\n                    else:\n                        map_path = relative_path\n                        \n                    # Load from direct path\n                    logger.info(f\"Trying to load pattern map directly from: {map_path}\")\n                    try:\n                        with open(map_path, 'r') as f:\n                            pattern_map = json.load(f)\n                        logger.info(\"Pattern map loaded successfully from direct path\")\n                    except Exception as e:\n                        logger.error(f\"Error loading pattern map from direct path: {str(e)}\")\n                else:\n                    logger.warning(\"No latest_pattern_map_path.txt file found\")\n            except Exception as e:\n                logger.error(f\"Error finding pattern map: {str(e)}\")\n    \n    if pattern_map:\n        logger.info(\"Pattern map loaded successfully\")\n        \n        # Convert to standardized format\n        from isekaizen.utils.pattern_map_utils import translate_pattern_map_to_standard_format\n        try:\n            logger.info(\"Converting pattern map to standardized format...\")\n            standardized_pattern_map = translate_pattern_map_to_standard_format(pattern_map)\n            logger.info(\"Pattern map successfully converted to standardized format\")\n            \n            # Check if standardization worked correctly\n            if 'format_version' in standardized_pattern_map and 'pattern_distribution' in standardized_pattern_map:\n                logger.info(f\"Standardized map contains {len(standardized_pattern_map['pattern_distribution'])} pattern types\")\n                if 'pattern_risks' in standardized_pattern_map:\n                    logger.info(f\"Standardized map includes {len(standardized_pattern_map['pattern_risks'])} pattern risks\")\n                pattern_map = standardized_pattern_map\n            else:\n                logger.warning(\"Standardization may not have worked correctly, proceeding with original pattern map\")\n        except Exception as e:\n            logger.error(f\"Error converting pattern map to standardized format: {str(e)}\")\n            logger.warning(\"Proceeding with original pattern map\")\n    else:\n        logger.warning(\"No pattern map found. Creating a new map will be required.\")\n        pattern_map = None\n    \n    # Load dataset\n    logger.info(\"Loading CIFAR-10 dataset...\")\n    trainset, testset = load_cifar10_data()\n    logger.info(f\"Dataset loaded: {len(trainset)} training samples, {len(testset)} test samples\")\n    \n    # Create model\n    logger.info(\"Creating model...\")\n    model = create_model()\n    model = model.to(device)\n    \n    # Configure optimizer based on args\n    logger.info(f\"Configuring {args.optimizer} optimizer with {args.optimizer_variant} variant...\")\n    \n    # Create optimizer and scheduler - no manual parameter overrides to respect dynamic optimization\n    optimizer, scheduler = configure_optimizer(\n        model, \n        optimizer_type=args.optimizer,\n        optimizer_variant=args.optimizer_variant,\n        custom_params={}\n    )\n    \n    # Initialize optimizer with pattern data for EVE optimizers\n    if isinstance(optimizer, EVENaturalWeights):\n        logger.info(\"Initializing EVE optimizer with standardized pattern map...\")\n        # Initialize pattern tracker with standardized pattern map\n        optimizer.initialize_from_pattern_map(pattern_map)\n        logger.info(\"EVE optimizer initialized with standardized pattern map data\")\n    \n    # Create enhanced adaptive trainer\n    # Make sure we pass the standardized pattern map directly to the trainer\n    trainer = StreamlinedPatternTrainer(\n        model=model,\n        criterion=nn.CrossEntropyLoss(),\n        optimizer_class=optimizer.__class__,  # Add optimizer class\n        optimizer=optimizer,\n        scheduler=scheduler,\n        device=device,\n        pattern_map=pattern_map,  # This ensures PatternRecognitionService gets initialized properly\n        batch_optimizer_class=EnhancedPatternResponsiveOptimizer,\n        batch_optimizer_kwargs={\n            \"pattern_map\": pattern_map,  # Pass standardized pattern map\n            \"target_accuracy\": args.target_accuracy,\n            \"stagnation_window\": args.stagnation_window if args.target_accuracy and args.stagnation_window else None,\n            \"stagnation_threshold\": args.stagnation_threshold if args.target_accuracy and args.stagnation_threshold else None,\n            \"run_diagnostics\": True,  # Always run diagnostics\n            \"total_epochs\": args.epochs\n        }\n    )\n    \n    # Initialize dataset adaptations tracking\n    trainer.dataset_adaptations = []\n    \n    # Always run diagnostics (mandatory)\n    logger.info(\"Running batch diagnostics (mandatory)...\")\n    diagnostics_path = save_batch_diagnostics(model, device, trainer)\n    logger.info(f\"Diagnostics saved to: {diagnostics_path}\")\n    \n    # Train the model\n    logger.info(\"===================================================\")\n    logger.info(f\"STARTING STREAMLINED PATTERN-RESPONSIVE TRAINING FOR {args.epochs} EPOCHS\")\n    logger.info(\"===================================================\")\n    logger.info(f\"Model: {model.__class__.__name__}\")\n    logger.info(f\"Dataset: CIFAR-10 with {len(trainset)} training samples\")\n    logger.info(f\"Using device: {device}\")\n    logger.info(f\"Optimizer: {args.optimizer} ({args.optimizer_variant})\")\n    logger.info(f\"Batch optimizer: {trainer.batch_optimizer.__class__.__name__}\")\n    logger.info(\"===================================================\")\n    \n    # Clean up any residual memory before starting\n    if hasattr(trainer.batch_optimizer, 'hardware_analyzer'):\n        trainer.batch_optimizer.hardware_analyzer.cleanup_memory()\n    elif torch.cuda.is_available():\n        # Direct cleanup if no analyzer\n        torch.cuda.empty_cache()\n        torch.cuda.reset_peak_memory_stats()\n        \n    logger.info(\"Starting training...\")\n    \n    # Create callbacks for tracking weight decays and train-test gap\n    callbacks = []\n    \n    # Define train-test gap monitoring callback\n    def check_train_test_gap_callback(epoch, history, model, optimizer):\n        # Only check if we have validation data\n        if 'val_acc' in history and len(history['val_acc']) >= 2 and 'train_acc' in history and len(history['train_acc']) >= 2:\n            # Calculate current and previous gap\n            current_gap = history['train_acc'][-1] - history['val_acc'][-1]\n            prev_gap = history['train_acc'][-2] - history['val_acc'][-2]\n            \n            # If gap is growing\n            if current_gap > prev_gap and current_gap > 5.0:  # Gap > 5% and growing\n                logger.info(f\"Growing train-test gap detected: {prev_gap:.2f}% \u2192 {current_gap:.2f}%\")\n                \n                # Force dataset adaptation\n                if hasattr(trainer.batch_optimizer, 'adapt_dataset'):\n                    logger.info(\"Forcing dataset adaptation due to growing train-test gap\")\n                    \n                    # Set force_high_risk in epoch_metrics to trigger adaptation\n                    if hasattr(trainer.batch_optimizer, 'epoch_metrics'):\n                        trainer.batch_optimizer.epoch_metrics['force_high_risk'] = True\n                    \n                    # Call should_adapt_patterns again\n                    should_adapt = trainer.batch_optimizer.should_adapt_patterns()\n                    \n                    if should_adapt:\n                        logger.info(\"Gap-based adaptation triggered successfully\")\n                        \n                        # Actually perform the adaptation\n                        adapted_dataset, adaptation_metrics = trainer.batch_optimizer.adapt_dataset(trainer.train_dataset)\n                        \n                        if adaptation_metrics.get('adapted', False):\n                            logger.info(f\"Gap-based adaptation successful! Added {adaptation_metrics['examples_added']} examples\")\n                            \n                            # Update trainer's dataset\n                            if not hasattr(trainer, 'train_dataset'):\n                                trainer.train_dataset = adapted_dataset\n                            else:\n                                # Record adaptation in trainer's history\n                                trainer.dataset_adaptations.append({\n                                    'epoch': epoch + 1,\n                                    **adaptation_metrics,\n                                    'trigger': 'train_test_gap'\n                                })\n                    else:\n                        logger.info(\"Gap-based adaptation attempt unsuccessful\")\n                    \n                    # Reset force flag\n                    if hasattr(trainer.batch_optimizer, 'epoch_metrics'):\n                        trainer.batch_optimizer.epoch_metrics['force_high_risk'] = False\n        \n        return False  # Continue training\n    \n    # Add gap checking callback first\n    callbacks.append(check_train_test_gap_callback)\n    \n    # Add weight decay tracking if using EVENaturalWeights\n    if isinstance(trainer.optimizer, EVENaturalWeights):\n        # Define weight decay tracking callback\n        def track_weight_decays_callback(epoch, history, model, optimizer):\n            if isinstance(optimizer, EVENaturalWeights):\n                # Initialize dynamic weight decays tracking if not already present\n                if 'dynamic_weight_decays' not in history:\n                    history['dynamic_weight_decays'] = []\n                \n                # Get current dynamic weight decays\n                weight_decays = optimizer.get_pattern_weight_decays()\n                \n                # Add to history\n                if weight_decays:\n                    history['dynamic_weight_decays'].append(weight_decays)\n                    logger.info(f\"Dynamic weight decays: {', '.join([f'{k}:{v:.5f}' for k, v in weight_decays.items()])}\")\n            return False  # Continue training\n        \n        callbacks.append(track_weight_decays_callback)\n    \n    # Start training without early stopping (removed as per requirements)\n    history = trainer.train(\n        train_dataset=trainset,\n        val_dataset=testset,\n        epochs=args.epochs,\n        early_stopping=False,  # Early stopping has been removed from the interface\n        patience=0,  # Not used since early stopping is disabled\n        callbacks=callbacks\n    )\n    logger.info(\"===================================================\")\n    logger.info(\"TRAINING COMPLETED\")\n    logger.info(\"===================================================\")\n    \n    # Visualize results\n    output_dir = os.path.join(\"examples\", \"output\")\n    os.makedirs(output_dir, exist_ok=True)\n    output_path = os.path.join(output_dir, f\"streamlined_responsive_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\")\n    visualize_training_results(history, output_path)\n    \n    # Detailed final evaluation\n    logger.info(\"Performing detailed model evaluation...\")\n    evaluation_results = evaluate_model_detailed(model, device)\n    \n    # Get accuracy string for display\n    final_accuracy = evaluation_results['accuracy']\n    accuracy_str = f\"{final_accuracy:.2f}%\"\n    \n    logger.info(f\"Final test accuracy: {accuracy_str}\")\n    \n    # Save model\n    model_path = os.path.join(output_dir, \"streamlined_responsive_model.pth\")\n    trainer.save_model(model_path)\n    logger.info(f\"Model saved to {model_path}\")\n    \n    # Print adaptation stats\n    logger.info(\"Dataset adaptation statistics:\")\n    if hasattr(trainer, 'dataset_adaptations') and trainer.dataset_adaptations:\n        for adaptation in trainer.dataset_adaptations:\n            logger.info(f\"Epoch {adaptation['epoch']}: Added {adaptation['examples_added']} examples\")\n            if 'pattern_risks' in adaptation:\n                pattern_risks = adaptation['pattern_risks']\n                logger.info(f\"  Pattern risks: {', '.join([f'{k}:{v:.2f}' for k, v in pattern_risks.items()])}\")\n            logger.info(f\"  Total dataset size: {adaptation['total_size']}\")\n    else:\n        logger.info(\"No dataset adaptations performed\")\n    \n    # Save final risk assessment metrics if available\n    if hasattr(trainer.batch_optimizer, 'pattern_tracker'):\n        risk_metrics = {\n            'pattern_risks': trainer.batch_optimizer.pattern_tracker.get_pattern_risks(),\n            'pattern_accuracies': trainer.batch_optimizer.pattern_tracker.get_pattern_accuracies(),\n            'overall_risk': trainer.batch_optimizer.pattern_tracker.get_overall_risk()\n        }\n        metrics_path = os.path.join(output_dir, f\"risk_metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n        with open(metrics_path, 'w') as f:\n            json.dump(risk_metrics, f, indent=2)\n        logger.info(f\"Risk assessment metrics saved to: {metrics_path}\")\n    \n    # Print comprehensive training summary\n    logger.info(\"\\n\" + \"=\" * 50)\n    logger.info(\"TRAINING SUMMARY\")\n    logger.info(\"=\" * 50)\n    logger.info(f\"Model:                     ResNet-18\")\n    logger.info(f\"Dataset:                   CIFAR-10 ({len(trainset)} samples)\")\n    logger.info(f\"Optimizer:                 {args.optimizer} ({args.optimizer_variant})\")\n    logger.info(f\"Final Training Accuracy:   {history['train_acc'][-1]:.2f}%\")\n    logger.info(f\"Final Test Accuracy:       {accuracy_str}\")\n    \n    # Add dynamic weight decay info if available\n    if 'dynamic_weight_decays' in history and history['dynamic_weight_decays']:\n        logger.info(\"\\nDynamic Weight Decay Summary:\")\n        last_decays = history['dynamic_weight_decays'][-1]\n        for pattern_type, decay in last_decays.items():\n            logger.info(f\"  {pattern_type}: {decay:.6f}\")\n    \n    # Calculate average epoch time\n    if history['epoch_times']:\n        avg_epoch_time = sum(history['epoch_times']) / len(history['epoch_times'])\n        logger.info(f\"Average Epoch Time:        {avg_epoch_time:.2f} seconds\")\n    \n    logger.info(f\"Total Epochs Completed:    {len(history['train_acc'])}\")\n    \n    # Show dataset adaptation info\n    if hasattr(trainer, 'dataset_adaptations') and trainer.dataset_adaptations:\n        total_added = sum(adaptation['examples_added'] for adaptation in trainer.dataset_adaptations)\n        logger.info(f\"Total Examples Added:       {total_added}\")\n        logger.info(f\"Final Dataset Size:        {history['dataset_sizes'][-1]}\")\n    \n    logger.info(\"=\" * 50)\n    \n    # Save full results to JSON file for later analysis\n    optimizer_info = {\n        'type': args.optimizer,\n        'variant': args.optimizer_variant,\n        'class_name': optimizer.__class__.__name__\n    }\n    \n    # If using EVE, add note about dynamic optimization\n    if args.optimizer == 'eve':\n        optimizer_info['dynamic_optimization'] = True\n        optimizer_info['note'] = 'Parameters are dynamically adjusted based on pattern performance and risk assessment'\n    \n    # Extract pattern map info\n    pattern_map_info = {}\n    if pattern_map:\n        if 'pattern_distribution' in pattern_map:\n            pattern_map_info['pattern_distribution'] = pattern_map['pattern_distribution']\n            \n        if 'pattern_complexities' in pattern_map:\n            complexities = {}\n            for pattern_type, data in pattern_map['pattern_complexities'].items():\n                if isinstance(data, dict) and 'avg_complexity' in data:\n                    complexities[pattern_type] = data['avg_complexity']\n                else:\n                    complexities[pattern_type] = data\n            pattern_map_info['pattern_complexities'] = complexities\n    \n    # Save to JSON file\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    json_output_path = os.path.join(output_dir, f\"streamlined_training_results_{timestamp}.json\")\n    save_training_results_to_json(history, evaluation_results, optimizer_info, pattern_map_info, json_output_path)\n    \n    logger.info(\"Streamlined pattern-responsive training completed successfully\")\n    logger.info(f\"Results saved to JSON file: {json_output_path}\")\n    logger.info(f\"Visualization saved to: {output_path}\")\n    logger.info(f\"Model saved to: {model_path}\")\n    logger.info(\"=\" * 50)\n\nif __name__ == \"__main__\":\n    try:\n        main()"
    }
  },
  "constants": {}
}