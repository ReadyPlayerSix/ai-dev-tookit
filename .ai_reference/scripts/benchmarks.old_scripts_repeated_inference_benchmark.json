{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\benchmarks.old\\scripts\\repeated_inference_benchmark.py",
  "imports": [
    {
      "name": "torch",
      "line": 2
    },
    {
      "name": "torch.nn",
      "line": 3
    },
    {
      "name": "time",
      "line": 4
    },
    {
      "name": "numpy",
      "line": 5
    },
    {
      "name": "pandas",
      "line": 6
    },
    {
      "name": "datetime.datetime",
      "line": 7
    },
    {
      "name": "argparse",
      "line": 251
    }
  ],
  "classes": {
    "SimpleModel": {
      "start_line": 10,
      "end_line": 22,
      "methods": {
        "__init__": {
          "start_line": 12,
          "end_line": 19,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "....__init__",
              "line": 13
            },
            {
              "name": "nn.Sequential",
              "line": 14
            },
            {
              "name": "nn.Linear",
              "line": 14
            },
            {
              "name": "nn.ReLU",
              "line": 14
            },
            {
              "name": "nn.Linear",
              "line": 15
            },
            {
              "name": "nn.ReLU",
              "line": 15
            },
            {
              "name": "nn.Linear",
              "line": 16
            },
            {
              "name": "nn.ReLU",
              "line": 16
            },
            {
              "name": "nn.Linear",
              "line": 17
            },
            {
              "name": "super",
              "line": 13
            }
          ],
          "code_snippet": "class SimpleModel(nn.Module):\n\n    def __init__(self):\n        super(SimpleModel, self).__init__()\n        self.layers = nn.Sequential(nn.Linear(784, 512), nn.ReLU(),\n                                    nn.Linear(512, 256), nn.ReLU(),\n                                    nn.Linear(256, 128), nn.ReLU(),\n                                    nn.Linear(128, 10))\n\n    def forward(self, x):\n        return self.layers(x)\n"
        },
        "forward": {
          "start_line": 19,
          "end_line": 22,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "x"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.layers",
              "line": 20
            }
          ],
          "code_snippet": "                                    nn.Linear(128, 10))\n\n    def forward(self, x):\n        return self.layers(x)\n\n\ndef run_single_benchmark(model, device, batch_sizes, input_dim, run_index):\n    \"\"\"Run a single full benchmark across all batch sizes\"\"\""
        }
      },
      "class_variables": [],
      "bases": [
        "..."
      ]
    }
  },
  "functions": {
    "run_single_benchmark": {
      "start_line": 23,
      "end_line": 79,
      "parameters": [
        {
          "name": "model"
        },
        {
          "name": "device"
        },
        {
          "name": "batch_sizes"
        },
        {
          "name": "input_dim"
        },
        {
          "name": "run_index"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "torch.randn",
          "line": 30
        },
        {
          "name": "range",
          "line": 33
        },
        {
          "name": "range",
          "line": 44
        },
        {
          "name": "np.mean",
          "line": 57
        },
        {
          "name": "np.percentile",
          "line": 58
        },
        {
          "name": "np.percentile",
          "line": 59
        },
        {
          "name": "torch.cuda.synchronize",
          "line": 38
        },
        {
          "name": "time.time",
          "line": 45
        },
        {
          "name": "time.time",
          "line": 53
        },
        {
          "name": "latencies.append",
          "line": 54
        },
        {
          "name": "torch.no_grad",
          "line": 34
        },
        {
          "name": "model",
          "line": 35
        },
        {
          "name": "torch.no_grad",
          "line": 47
        },
        {
          "name": "model",
          "line": 48
        },
        {
          "name": "torch.cuda.synchronize",
          "line": 51
        },
        {
          "name": "sum",
          "line": 61
        },
        {
          "name": "torch.cuda.memory_allocated",
          "line": 64
        }
      ],
      "docstring": "Run a single full benchmark across all batch sizes",
      "code_snippet": "\n\ndef run_single_benchmark(model, device, batch_sizes, input_dim, run_index):\n    \"\"\"Run a single full benchmark across all batch sizes\"\"\"\n    run_results = {}\n\n    # Test with different batch sizes\n    for batch_size in batch_sizes:\n        # Create input data\n        input_data = torch.randn(batch_size, input_dim, device=device)\n\n        # Warmup runs\n        for _ in range(10):\n            with torch.no_grad():\n                _ = model(input_data)\n\n        if device == \"cuda\":\n            torch.cuda.synchronize()\n\n        # Measure inference time\n        iterations = 100\n        latencies = []\n\n        for i in range(iterations):\n            start_time = time.time()\n\n            with torch.no_grad():\n                _ = model(input_data)\n\n            if device == \"cuda\":\n                torch.cuda.synchronize()\n\n            end_time = time.time()\n            latencies.append((end_time - start_time) * 1000)  # Convert to ms\n\n        # Calculate statistics\n        avg_latency = np.mean(latencies)\n        p95_latency = np.percentile(latencies, 95)\n        p99_latency = np.percentile(latencies, 99)\n        throughput = (batch_size *\n                      iterations) / sum(latencies) * 1000  # samples/second\n\n        if device == \"cuda\":\n            memory_allocated = torch.cuda.memory_allocated() / 1e6  # MB\n        else:\n            memory_allocated = 0\n\n        run_results[batch_size] = {\n            'avg_latency': avg_latency,\n            'p95_latency': p95_latency,\n            'p99_latency': p99_latency,\n            'throughput': throughput,\n            'memory_allocated': memory_allocated,\n            'run_index': run_index\n        }\n\n    return run_results\n\n\ndef run_inference_benchmark(num_runs=5):\n    \"\"\"Run the full benchmark multiple times and average the results\"\"\""
    },
    "run_inference_benchmark": {
      "start_line": 80,
      "end_line": 248,
      "parameters": [
        {
          "name": "num_runs"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "print",
          "line": 83
        },
        {
          "name": "print",
          "line": 84
        },
        {
          "name": "....to",
          "line": 89
        },
        {
          "name": "model.eval",
          "line": 92
        },
        {
          "name": "range",
          "line": 102
        },
        {
          "name": "pd.DataFrame",
          "line": 120
        },
        {
          "name": "....agg",
          "line": 123
        },
        {
          "name": "print",
          "line": 133
        },
        {
          "name": "print",
          "line": 136
        },
        {
          "name": "print",
          "line": 138
        },
        {
          "name": "print",
          "line": 139
        },
        {
          "name": "print",
          "line": 140
        },
        {
          "name": "print",
          "line": 147
        },
        {
          "name": "print",
          "line": 148
        },
        {
          "name": "print",
          "line": 149
        },
        {
          "name": "print",
          "line": 157
        },
        {
          "name": "range",
          "line": 161
        },
        {
          "name": "pd.DataFrame",
          "line": 212
        },
        {
          "name": "rt_df.mean",
          "line": 213
        },
        {
          "name": "rt_df.std",
          "line": 214
        },
        {
          "name": "print",
          "line": 216
        },
        {
          "name": "print",
          "line": 217
        },
        {
          "name": "print",
          "line": 220
        },
        {
          "name": "print",
          "line": 221
        },
        {
          "name": "print",
          "line": 222
        },
        {
          "name": "print",
          "line": 223
        },
        {
          "name": "print",
          "line": 224
        },
        {
          "name": "print",
          "line": 225
        },
        {
          "name": "print",
          "line": 226
        },
        {
          "name": "....strftime",
          "line": 231
        },
        {
          "name": "df.to_csv",
          "line": 235
        },
        {
          "name": "print",
          "line": 236
        },
        {
          "name": "torch.cuda.is_available",
          "line": 82
        },
        {
          "name": "print",
          "line": 103
        },
        {
          "name": "run_single_benchmark",
          "line": 110
        },
        {
          "name": "run_results.items",
          "line": 114
        },
        {
          "name": "print",
          "line": 117
        },
        {
          "name": "print",
          "line": 143
        },
        {
          "name": "print",
          "line": 152
        },
        {
          "name": "print",
          "line": 162
        },
        {
          "name": "torch.randn",
          "line": 163
        },
        {
          "name": "range",
          "line": 166
        },
        {
          "name": "range",
          "line": 177
        },
        {
          "name": "np.mean",
          "line": 190
        },
        {
          "name": "np.std",
          "line": 191
        },
        {
          "name": "np.min",
          "line": 192
        },
        {
          "name": "np.max",
          "line": 193
        },
        {
          "name": "np.percentile",
          "line": 194
        },
        {
          "name": "np.percentile",
          "line": 195
        },
        {
          "name": "np.percentile",
          "line": 196
        },
        {
          "name": "real_time_results.append",
          "line": 199
        },
        {
          "name": "print",
          "line": 239
        },
        {
          "name": "SimpleModel",
          "line": 89
        },
        {
          "name": "torch.cuda.empty_cache",
          "line": 107
        },
        {
          "name": "all_results.append",
          "line": 115
        },
        {
          "name": "df.groupby",
          "line": 123
        },
        {
          "name": "torch.cuda.synchronize",
          "line": 171
        },
        {
          "name": "time.time",
          "line": 178
        },
        {
          "name": "time.time",
          "line": 186
        },
        {
          "name": "latencies.append",
          "line": 187
        },
        {
          "name": "datetime.now",
          "line": 231
        },
        {
          "name": "print",
          "line": 241
        },
        {
          "name": "print",
          "line": 245
        },
        {
          "name": "torch.no_grad",
          "line": 167
        },
        {
          "name": "model",
          "line": 168
        },
        {
          "name": "torch.no_grad",
          "line": 180
        },
        {
          "name": "model",
          "line": 181
        },
        {
          "name": "torch.cuda.synchronize",
          "line": 184
        }
      ],
      "docstring": "Run the full benchmark multiple times and average the results",
      "code_snippet": "\n\ndef run_inference_benchmark(num_runs=5):\n    \"\"\"Run the full benchmark multiple times and average the results\"\"\"\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Running inference benchmark on: {device}\")\n    print(\n        f\"Will run the complete benchmark {num_runs} times for more reliable results\"\n    )\n\n    # Create and \"train\" the model (we'll just use random weights for the benchmark)\n    model = SimpleModel().to(device)\n\n    # Put the model in evaluation mode (important for inference)\n    model.eval()\n\n    # Use different batch sizes to test scaling characteristics\n    batch_sizes = [1, 16, 32, 64, 128, 256]\n    input_dim = 784  # Same as our training example\n\n    # Store all results across runs\n    all_results = []\n\n    # Run the complete benchmark multiple times\n    for run in range(num_runs):\n        print(f\"\\n--- Starting benchmark run {run+1}/{num_runs} ---\")\n\n        # Clear GPU memory between runs\n        if device == \"cuda\":\n            torch.cuda.empty_cache()\n\n        # Run a complete benchmark\n        run_results = run_single_benchmark(model, device, batch_sizes,\n                                           input_dim, run)\n\n        # Add to our collection\n        for batch_size, metrics in run_results.items():\n            all_results.append({'batch_size': batch_size, **metrics})\n\n        print(f\"--- Completed benchmark run {run+1}/{num_runs} ---\")\n\n    # Convert to DataFrame for easier analysis\n    df = pd.DataFrame(all_results)\n\n    # Calculate aggregate statistics across runs\n    aggregated = df.groupby('batch_size').agg({\n        'avg_latency': ['mean', 'std', 'min', 'max'],\n        'p95_latency': ['mean', 'std'],\n        'p99_latency': ['mean', 'std'],\n        'throughput': ['mean', 'std', 'min', 'max'],\n        'memory_allocated':\n        'mean'\n    })\n\n    # Print a detailed summary\n    print(\n        \"\\n===== INFERENCE PERFORMANCE SUMMARY (AVERAGED OVER MULTIPLE RUNS) =====\"\n    )\n    print(f\"Number of complete benchmark runs: {num_runs}\")\n\n    print(\"\\n--- Average Latency (ms) ---\")\n    print(\"Batch Size | Mean  | Std Dev | Min   | Max \")\n    print(\"-\" * 50)\n    for batch_size in batch_sizes:\n        stats = aggregated.loc[batch_size]['avg_latency']\n        print(\n            f\"{batch_size:^10} | {stats['mean']:5.2f} | {stats['std']:7.2f} | {stats['min']:5.2f} | {stats['max']:5.2f}\"\n        )\n\n    print(\"\\n--- Throughput (samples/second) ---\")\n    print(\"Batch Size | Mean    | Std Dev  | Min     | Max\")\n    print(\"-\" * 50)\n    for batch_size in batch_sizes:\n        stats = aggregated.loc[batch_size]['throughput']\n        print(\n            f\"{batch_size:^10} | {stats['mean']:7.1f} | {stats['std']:8.1f} | {stats['min']:7.1f} | {stats['max']:7.1f}\"\n        )\n\n    # Test real-time inference capability with more runs (batch size of 1)\n    print(\"\\n===== REAL-TIME INFERENCE TEST (MULTIPLE RUNS) =====\")\n\n    real_time_results = []\n\n    for run in range(num_runs):\n        print(f\"Real-time test run {run+1}/{num_runs}...\")\n        input_data = torch.randn(1, input_dim, device=device)\n\n        # Warm up\n        for _ in range(10):\n            with torch.no_grad():\n                _ = model(input_data)\n\n        if device == \"cuda\":\n            torch.cuda.synchronize()\n\n        # Measure inference time with specific focus on consistency\n        iterations = 1000\n        latencies = []\n\n        for i in range(iterations):\n            start_time = time.time()\n\n            with torch.no_grad():\n                _ = model(input_data)\n\n            if device == \"cuda\":\n                torch.cuda.synchronize()\n\n            end_time = time.time()\n            latencies.append((end_time - start_time) * 1000)  # Convert to ms\n\n        # Calculate statistics for this run\n        avg_latency = np.mean(latencies)\n        std_latency = np.std(latencies)\n        min_latency = np.min(latencies)\n        max_latency = np.max(latencies)\n        p50_latency = np.percentile(latencies, 50)  # median\n        p95_latency = np.percentile(latencies, 95)\n        p99_latency = np.percentile(latencies, 99)\n        fps = 1000 / avg_latency\n\n        real_time_results.append({\n            'run': run,\n            'avg_latency': avg_latency,\n            'std_latency': std_latency,\n            'min_latency': min_latency,\n            'max_latency': max_latency,\n            'p50_latency': p50_latency,\n            'p95_latency': p95_latency,\n            'p99_latency': p99_latency,\n            'fps': fps\n        })\n\n    # Average the real-time test results\n    rt_df = pd.DataFrame(real_time_results)\n    avg_rt = rt_df.mean()\n    std_rt = rt_df.std()\n\n    print(\"\\n--- Real-Time Inference Performance (averaged over runs) ---\")\n    print(\n        f\"Average latency: {avg_rt['avg_latency']:.3f} ms (\u00b1{std_rt['avg_latency']:.3f})\"\n    )\n    print(f\"Standard deviation within runs: {avg_rt['std_latency']:.3f} ms\")\n    print(f\"Min latency: {avg_rt['min_latency']:.3f} ms\")\n    print(f\"Max latency: {avg_rt['max_latency']:.3f} ms\")\n    print(f\"Median (P50) latency: {avg_rt['p50_latency']:.3f} ms\")\n    print(f\"P95 latency: {avg_rt['p95_latency']:.3f} ms\")\n    print(f\"P99 latency: {avg_rt['p99_latency']:.3f} ms\")\n    print(\n        f\"Effective FPS (frames per second): {avg_rt['fps']:.1f} (\u00b1{std_rt['fps']:.1f})\"\n    )\n\n    # Generate a timestamp for the results file\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n    # Save the detailed results to CSV for further analysis if needed\n    csv_filename = f\"inference_results_{timestamp}.csv\"\n    df.to_csv(csv_filename, index=False)\n    print(f\"\\nDetailed results saved to {csv_filename}\")\n\n    if avg_rt['fps'] >= 30:\n        print(\"\\n\u2713 Model meets real-time requirements (30+ FPS)\")\n    elif avg_rt['fps'] >= 15:\n        print(\n            \"\\n\u26a0 Model may be suitable for near-real-time applications (15-30 FPS)\"\n        )\n    else:\n        print(\n            \"\\n\u2717 Model is likely too slow for real-time applications (<15 FPS)\"\n        )\n\n\nif __name__ == \"__main__\":"
    }
  },
  "constants": {}
}