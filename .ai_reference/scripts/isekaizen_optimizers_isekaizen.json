{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\isekaizen\\optimizers\\isekaizen.py",
  "imports": [
    {
      "name": "torch",
      "line": 8
    },
    {
      "name": "torch.nn",
      "line": 9
    },
    {
      "name": "logging",
      "line": 10
    },
    {
      "name": "time",
      "line": 11
    },
    {
      "name": "math",
      "line": 12
    },
    {
      "name": "random",
      "line": 13
    },
    {
      "name": "typing.Union",
      "line": 14
    },
    {
      "name": "typing.Optional",
      "line": 14
    },
    {
      "name": "typing.Dict",
      "line": 14
    },
    {
      "name": "typing.List",
      "line": 14
    },
    {
      "name": "typing.Tuple",
      "line": 14
    },
    {
      "name": "typing.Any",
      "line": 14
    },
    {
      "name": "core.optimizers.BatchSizeSelector",
      "line": 15
    }
  ],
  "classes": {
    "IsekaiZenOptimizer": {
      "start_line": 19,
      "end_line": 2162,
      "methods": {
        "__init__": {
          "start_line": 28,
          "end_line": 164,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "model"
            },
            {
              "name": "device"
            },
            {
              "name": "run_diagnostics",
              "type": "bool"
            },
            {
              "name": "min_batch"
            },
            {
              "name": "max_batch"
            },
            {
              "name": "total_epochs",
              "type": "int"
            },
            {
              "name": "accuracy_window",
              "type": "int"
            },
            {
              "name": "accuracy_threshold",
              "type": "float"
            },
            {
              "name": "use_lanes",
              "type": "bool"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "....__init__",
              "line": 64
            },
            {
              "name": "logger.info",
              "line": 98
            },
            {
              "name": "sum",
              "line": 101
            },
            {
              "name": "logger.info",
              "line": 102
            },
            {
              "name": "torch.cuda.get_device_capability",
              "line": 106
            },
            {
              "name": "logger.info",
              "line": 108
            },
            {
              "name": "logger.info",
              "line": 109
            },
            {
              "name": "logger.info",
              "line": 110
            },
            {
              "name": "max",
              "line": 115
            },
            {
              "name": "logger.info",
              "line": 116
            },
            {
              "name": "logger.info",
              "line": 161
            },
            {
              "name": "self._run_diagnostics",
              "line": 162
            },
            {
              "name": "super",
              "line": 64
            },
            {
              "name": "p.numel",
              "line": 101
            },
            {
              "name": "torch.cuda.get_device_properties",
              "line": 111
            },
            {
              "name": "float",
              "line": 140
            },
            {
              "name": "model.parameters",
              "line": 101
            },
            {
              "name": "torch.cuda.get_device_properties",
              "line": 107
            },
            {
              "name": "hasattr",
              "line": 115
            },
            {
              "name": "torch.get_num_threads",
              "line": 115
            },
            {
              "name": "torch.cuda.get_device_name",
              "line": 108
            }
          ],
          "docstring": "\n        Initialize the framework optimizer with epoch-aware approach.\n        \n        Args:\n            model: PyTorch model to optimize\n            device: Target device for optimization (default: auto-detect)\n            run_diagnostics: Whether to run initial diagnostics\n            min_batch: Optional minimum batch size to try (if None, will be determined automatically)\n            max_batch: Optional maximum batch size to try (if None, will be determined automatically)\n            total_epochs: The total expected number of training epochs\n            accuracy_window: Number of epochs to consider for accuracy trend analysis\n            accuracy_threshold: Threshold for accuracy decrease to trigger batch size adjustment\n            use_lanes: Whether to use frequency lanes for optimization\n        ",
          "code_snippet": "    \"\"\"\n    \n    def __init__(\n        self, \n        model: torch.nn.Module, \n        device: Optional[torch.device] = None,\n        run_diagnostics: bool = True,\n        min_batch: Optional[int] = None,\n        max_batch: Optional[int] = None,\n        total_epochs: int = 10,\n        accuracy_window: int = 3,\n        accuracy_threshold: float = 0.01,\n        use_lanes: bool = True\n    ):\n        \"\"\"\n        Initialize the framework optimizer with epoch-aware approach.\n        \n        Args:\n            model: PyTorch model to optimize\n            device: Target device for optimization (default: auto-detect)\n            run_diagnostics: Whether to run initial diagnostics\n            min_batch: Optional minimum batch size to try (if None, will be determined automatically)\n            max_batch: Optional maximum batch size to try (if None, will be determined automatically)\n            total_epochs: The total expected number of training epochs\n            accuracy_window: Number of epochs to consider for accuracy trend analysis\n            accuracy_threshold: Threshold for accuracy decrease to trigger batch size adjustment\n            use_lanes: Whether to use frequency lanes for optimization\n        \"\"\"\n        # Set initial placeholder values - these will be dynamically calculated \n        # during diagnostics based on model characteristics and hardware capabilities\n        self.min_batch = min_batch  # Will be determined automatically if None\n        self.max_batch = max_batch  # Will be determined automatically if None\n        \n        # Initialize with temporary values just for parent class constructor\n        temp_min = min_batch if min_batch is not None else 1\n        temp_max = max_batch if max_batch is not None else 1024\n        \n        # Call parent constructor with temporary values\n        super().__init__(model, device, temp_min, temp_max)\n        \n        # Store intent to run diagnostics - the actual min/max batch will be set there\n        self.run_diagnostics = run_diagnostics\n        \n        # Store epoch-aware optimization parameters\n        self.total_epochs = total_epochs\n        self.accuracy_window = accuracy_window\n        self.accuracy_threshold = accuracy_threshold\n        self.use_lanes = use_lanes\n        \n        # Track accuracy history for trend analysis\n        self.accuracy_history = []\n        \n        # Track best batch size found so far\n        self.best_batch_size = None\n        self.best_accuracy = 0.0\n        \n        # Initially selected batch size (will be set to max viable during diagnostics)\n        self.selected_batch_size = None\n        self.initial_batch_size_set = False\n        \n        # Initialize selected lane\n        self.selected_lane = \"medium\"  # Default lane\n        \n        # Batch size adjustment multiplier (used when accuracy plateaus/decreases)\n        self.batch_adjustment_factor = 0.75  # Reduce by 25% when needed\n        \n        # Resonance tracking - to identify optimal zones where batch sizes\n        # and frequency bands align\n        self.resonance_map = {}  # Maps frequency bands to their optimal batch size ranges\n        self.resonance_zone = (0, 0)  # The overlapping sweet spot (min, max) batch size\n        \n        # Log initial values\n        logger.info(f\"IsekaiZen optimizer initialized with batch size range: {self.min_batch}-{self.max_batch} (will be refined during diagnostics)\")\n        \n        # Calculate parameter count to use in formulas\n        self.param_count = sum(p.numel() for p in model.parameters())\n        logger.info(f\"Model has {self.param_count:,} parameters\")\n        \n        # Determine available compute resources\n        if self.device.type == \"cuda\":\n            self.gpu_compute_capability = torch.cuda.get_device_capability(self.device)\n            self.gpu_memory = torch.cuda.get_device_properties(self.device).total_memory / (1024**3)  # GB\n            logger.info(f\"GPU: {torch.cuda.get_device_name(self.device)}\")\n            logger.info(f\"Compute capability: {self.gpu_compute_capability}\")\n            logger.info(f\"Available memory: {self.gpu_memory:.2f} GB\")\n            self.parallel_processors = torch.cuda.get_device_properties(self.device).multi_processor_count\n        else:\n            self.gpu_compute_capability = (0, 0)\n            self.gpu_memory = 0\n            self.parallel_processors = max(1, (torch.get_num_threads() if hasattr(torch, 'get_num_threads') else 1))\n            logger.info(f\"CPU with {self.parallel_processors} logical processors\")\n            \n        # Training state tracking\n        self.last_gradient_norm = 0.0\n        self.gradient_history = []\n        self.batch_history = []\n        self.efficiency_history = {}  # Track (batch_size, efficiency) pairs\n        self.loss_history = []\n        self.epoch = 0\n        self.iteration = 0\n        \n        # Framework parameters from documentation\n        self.L_c = 8.0\n        self.alpha_w = 0.15\n        self.alpha_a = 0.18\n        self.alpha_kv = 0.25\n        self.alpha_par = 0.22\n        self.gamma_w = 2.6745\n        self.gamma_a = 2.2102\n        self.gamma_kv = 0.9578\n        self.sigma = 0.8\n        \n        # Multi-lane approach: Define frequency bands and lanes\n        self.frequency_bands = {\n            \"ultra\": (100, float('inf')),   # Ultra-high frequency (>100Hz)\n            \"high\": (30, 100),              # High frequency (30-100Hz)\n            \"medium\": (5, 30),              # Medium frequency (5-30Hz)\n            \"low\": (0.1, 5),                # Low frequency (0.1-5Hz)\n            \"background\": (0, 0.1),         # Background frequency (<0.1Hz)\n        }\n        \n        # Mapping of model layers to frequency bands (will be populated during diagnostics)\n        self.layer_frequency_map = {}\n        \n        # Initialize band layer counts (used for calculations before diagnostics run)\n        self.band_layer_counts = {band: 1 for band in self.frequency_bands}  # Default to 1 per band\n        \n        # Track lane-specific metrics\n        self.lane_metrics = {band: {\"updates\": 0, \"efficiency\": 0.0} for band in self.frequency_bands}\n        \n        # Observer effect compensation\n        self.observer_overhead = {band: 0.0 for band in self.frequency_bands}\n        \n        # Run initial diagnostics if requested\n        if self.run_diagnostics:\n            logger.info(\"Running initial framework diagnostics...\")\n            self._run_diagnostics()\n    \n    def _run_diagnostics(self):\n        \"\"\"\n        Run initial diagnostics to calibrate the framework."
        },
        "_run_diagnostics": {
          "start_line": 164,
          "end_line": 205,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logger.info",
              "line": 174
            },
            {
              "name": "self._analyze_layer_frequencies",
              "line": 175
            },
            {
              "name": "logger.info",
              "line": 178
            },
            {
              "name": "self._calculate_observer_overhead",
              "line": 179
            },
            {
              "name": "logger.info",
              "line": 183
            },
            {
              "name": "self._initial_hardware_mapping",
              "line": 184
            },
            {
              "name": "logger.info",
              "line": 187
            },
            {
              "name": "self._find_resonance_zone",
              "line": 188
            },
            {
              "name": "logger.info",
              "line": 191
            },
            {
              "name": "self._refine_batch_size_range",
              "line": 192
            },
            {
              "name": "self._report_diagnostics",
              "line": 195
            },
            {
              "name": "logger.info",
              "line": 198
            },
            {
              "name": "self._report_diagnostics",
              "line": 201
            },
            {
              "name": "logger.warning",
              "line": 203
            }
          ],
          "docstring": "\n        Run initial diagnostics to calibrate the framework.\n        \n        This performs hardware capability mapping, layer frequency analysis,\n        and establishes baseline metrics for optimization.\n        ",
          "code_snippet": "            self._run_diagnostics()\n    \n    def _run_diagnostics(self):\n        \"\"\"\n        Run initial diagnostics to calibrate the framework.\n        \n        This performs hardware capability mapping, layer frequency analysis,\n        and establishes baseline metrics for optimization.\n        \"\"\"\n        # Check for keyboard interrupts during lengthy operations\n        try:\n            # 1. Analyze model layer frequencies first (needed for resonance calculations)\n            logger.info(\"Analyzing model layer natural frequencies...\")\n            self._analyze_layer_frequencies()\n            \n            # 2. Calculate observer effect overhead\n            logger.info(\"Calculating measurement overhead...\")\n            self._calculate_observer_overhead()\n            \n            # 3. Map hardware capability curve with basic settings\n            # This just collects raw data, we'll refine settings after resonance analysis\n            logger.info(\"Mapping initial hardware capabilities...\")\n            self._initial_hardware_mapping()\n            \n            # 4. Identify the resonance zone where batch sizes and frequencies align\n            logger.info(\"Finding resonance zone between batch sizes and frequencies...\")\n            self._find_resonance_zone()\n            \n            # 5. Now refine batch size range based on resonance zones and efficiency\n            logger.info(\"Refining batch size range based on resonance analysis...\")\n            self._refine_batch_size_range()\n            \n            # 6. Report diagnostic results\n            self._report_diagnostics()\n            \n        except KeyboardInterrupt:\n            logger.info(\"Diagnostics interrupted by user. Using partial results.\")\n            # Still try to report whatever diagnostics we have\n            try:\n                self._report_diagnostics()\n            except:\n                logger.warning(\"Could not generate diagnostic report after interruption\")\n    \n    def _initial_hardware_mapping(self):\n        \"\"\"\n        Map hardware capability curve and determine optimal batch size ranges."
        },
        "_initial_hardware_mapping": {
          "start_line": 205,
          "end_line": 455,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logger.info",
              "line": 233
            },
            {
              "name": "logger.info",
              "line": 245
            },
            {
              "name": "sorted",
              "line": 246
            },
            {
              "name": "self.model.parameters",
              "line": 451
            },
            {
              "name": "logger.info",
              "line": 300
            },
            {
              "name": "sorted",
              "line": 301
            },
            {
              "name": "sorted",
              "line": 346
            },
            {
              "name": "logger.info",
              "line": 427
            },
            {
              "name": "logger.warning",
              "line": 434
            },
            {
              "name": "torch.cuda.empty_cache",
              "line": 249
            },
            {
              "name": "torch.randn",
              "line": 253
            },
            {
              "name": "time.time",
              "line": 260
            },
            {
              "name": "range",
              "line": 261
            },
            {
              "name": "time.time",
              "line": 267
            },
            {
              "name": "logger.info",
              "line": 284
            },
            {
              "name": "self.hw_time_curve.keys",
              "line": 301
            },
            {
              "name": "sum",
              "line": 308
            },
            {
              "name": "self.estimate_available_energy",
              "line": 312
            },
            {
              "name": "self.hw_efficiency_curve.keys",
              "line": 346
            },
            {
              "name": "sorted",
              "line": 364
            },
            {
              "name": "logger.warning",
              "line": 386
            },
            {
              "name": "int",
              "line": 392
            },
            {
              "name": "logger.info",
              "line": 393
            },
            {
              "name": "logger.info",
              "line": 422
            },
            {
              "name": "param.grad.zero_",
              "line": 453
            },
            {
              "name": "torch.cuda.reset_peak_memory_stats",
              "line": 257
            },
            {
              "name": "band_weights.values",
              "line": 308
            },
            {
              "name": "logger.info",
              "line": 330
            },
            {
              "name": "self.hw_efficiency_curve.items",
              "line": 364
            },
            {
              "name": "len",
              "line": 367
            },
            {
              "name": "max",
              "line": 397
            },
            {
              "name": "sorted",
              "line": 409
            },
            {
              "name": "max",
              "line": 420
            },
            {
              "name": "torch.enable_grad",
              "line": 262
            },
            {
              "name": "self.model",
              "line": 263
            },
            {
              "name": "output.mean",
              "line": 265
            },
            {
              "name": "loss.backward",
              "line": 266
            },
            {
              "name": "torch.cuda.max_memory_allocated",
              "line": 278
            },
            {
              "name": "logger.warning",
              "line": 290
            },
            {
              "name": "logger.warning",
              "line": 294
            },
            {
              "name": "self.estimate_cognitive_load",
              "line": 319
            },
            {
              "name": "self.cognitive_efficiency",
              "line": 322
            },
            {
              "name": "logger.error",
              "line": 333
            },
            {
              "name": "logger.warning",
              "line": 336
            },
            {
              "name": "self.hw_efficiency_curve.items",
              "line": 397
            },
            {
              "name": "max",
              "line": 416
            },
            {
              "name": "int",
              "line": 420
            },
            {
              "name": "str",
              "line": 289
            },
            {
              "name": "str",
              "line": 289
            },
            {
              "name": "viable_items.append",
              "line": 375
            },
            {
              "name": "max",
              "line": 379
            },
            {
              "name": "len",
              "line": 386
            },
            {
              "name": "int",
              "line": 416
            },
            {
              "name": "str",
              "line": 294
            },
            {
              "name": "str",
              "line": 333
            }
          ],
          "docstring": "\n        Map hardware capability curve and determine optimal batch size ranges.\n        \n        This method follows the IsekaiZen framework's mathematical principles \n        to automatically determine the optimal batch size ranges based on the\n        model architecture and available hardware resources.\n        ",
          "code_snippet": "                logger.warning(\"Could not generate diagnostic report after interruption\")\n    \n    def _initial_hardware_mapping(self):\n        \"\"\"\n        Map hardware capability curve and determine optimal batch size ranges.\n        \n        This method follows the IsekaiZen framework's mathematical principles \n        to automatically determine the optimal batch size ranges based on the\n        model architecture and available hardware resources.\n        \"\"\"\n        # Dynamically determine batch sizes to test based on model characteristics\n        # Start with a small value and increase exponentially until we hit hardware limits\n        \n        # Calculate an appropriate range based on model complexity and hardware\n        model_params = self.param_count\n        is_cuda = self.device.type == \"cuda\"\n        \n        # Create a dynamically sized test range with MUCH more diversity\n        # Include smaller batches and use a wider range with more testing points\n        if model_params > 100_000_000:  # Very large models (100M+ params)\n            max_test = 1024 if is_cuda else 256\n            # Create a more diverse range with smaller batch sizes too\n            test_batch_sizes = [1, 2, 4, 8, 12, 16, 24, 32, 48, 64, 96, 128, 192, 256, 384, 512, max_test]\n        elif model_params > 10_000_000:  # Large models (10M-100M params)\n            max_test = 2048 if is_cuda else 512 \n            test_batch_sizes = [1, 2, 4, 8, 12, 16, 24, 32, 48, 64, 96, 128, 192, 256, 384, 512, 768, 1024, max_test]\n        else:  # Small to medium models\n            max_test = 4096 if is_cuda else 1024\n            test_batch_sizes = [1, 2, 4, 8, 12, 16, 24, 32, 48, 64, 96, 128, 192, 256, 384, 512, 768, 1024, 1536, 2048, max_test]\n            \n        logger.info(f\"Testing batch sizes: {test_batch_sizes}\")\n            \n        self.hw_memory_curve = {}\n        self.hw_time_curve = {}\n        self.hw_throughput_curve = {}\n        self.hw_efficiency_curve = {}\n        \n        # Determine maximum viable batch size\n        max_viable_batch = None\n        memory_limited = False\n        \n        # First pass: collect raw hardware performance data\n        logger.info(\"Testing batch sizes to find hardware limits and throughput curve\")\n        for batch_size in sorted(test_batch_sizes):\n            # Clear memory before test\n            if self.device.type == \"cuda\":\n                torch.cuda.empty_cache()\n                \n            try:\n                # Create random input of appropriate size\n                sample_input = torch.randn(batch_size, 3, 32, 32, device=self.device)\n                \n                # Measure memory usage\n                if self.device.type == \"cuda\":\n                    torch.cuda.reset_peak_memory_stats(self.device)\n                    \n                # Time a few forward/backward passes\n                start_time = time.time()\n                for _ in range(3):\n                    with torch.enable_grad():\n                        output = self.model(sample_input)\n                        # Use mean as a simple loss\n                        loss = output.mean()\n                        loss.backward()\n                end_time = time.time()\n                \n                # Record metrics\n                time_per_sample = (end_time - start_time) / (3 * batch_size)\n                self.hw_time_curve[batch_size] = time_per_sample\n                \n                # Calculate throughput (samples per second)\n                throughput = 1.0 / time_per_sample\n                self.hw_throughput_curve[batch_size] = throughput\n                \n                if self.device.type == \"cuda\":\n                    peak_mem = torch.cuda.max_memory_allocated(self.device) / (1024**3)  # GB\n                    self.hw_memory_curve[batch_size] = peak_mem\n                \n                # Update max viable batch size\n                max_viable_batch = batch_size\n                \n                logger.info(f\"Batch size {batch_size}: {throughput:.1f} samples/s, \"\n                           f\"{peak_mem:.2f} GB memory\" if self.device.type == \"cuda\" else \"\")\n                \n            except RuntimeError as e:\n                # OOM or other runtime error\n                if \"CUDA out of memory\" in str(e) or \"not enough memory\" in str(e):\n                    logger.warning(f\"Batch size {batch_size} exceeds available memory\")\n                    memory_limited = True\n                    break\n                else:\n                    logger.warning(f\"Error testing batch size {batch_size}: {str(e)}\")\n                    break\n        \n        # Second pass: calculate cognitive efficiency for each batch size\n        if max_viable_batch:\n            # Calculate efficiency curve using our framework formulas\n            logger.info(\"Calculating cognitive efficiency curve for tested batch sizes\")\n            for batch_size in sorted(self.hw_time_curve.keys()):\n                # Instead of just medium frequency, test across multiple bands for a more balanced assessment\n                frequency_bands_to_test = [\"low\", \"medium\", \"high\"]\n                band_weights = {\"low\": 0.3, \"medium\": 0.5, \"high\": 0.2}  # Weight medium frequency more\n                \n                # Track weighted efficiencies\n                weighted_efficiency = 0.0\n                total_weight = sum(band_weights.values())\n                \n                # Calculate efficiency for each frequency band and average\n                p = self.parallel_processors\n                E_available = self.estimate_available_energy()\n                \n                # Calculate cognitive efficiency with error handling\n                try:\n                    # Test each frequency band and get weighted average\n                    for band in frequency_bands_to_test:\n                        # Get load for this band\n                        L = self.estimate_cognitive_load(batch_size, band)\n                        \n                        # Calculate efficiency for this band\n                        band_efficiency = self.cognitive_efficiency(E_available, batch_size, p, L, band)\n                        \n                        # Add weighted contribution\n                        band_weight = band_weights[band] / total_weight\n                        weighted_efficiency += band_efficiency * band_weight\n                    \n                    # Store the weighted result\n                    self.hw_efficiency_curve[batch_size] = weighted_efficiency\n                    logger.info(f\"Batch size {batch_size}: efficiency = {weighted_efficiency:.4f} (weighted across multiple frequency bands)\")\n                    \n                except Exception as e:\n                    logger.error(f\"Error calculating efficiency for batch size {batch_size}: {str(e)}\")\n                    # Use a fallback efficiency calculation\n                    self.hw_efficiency_curve[batch_size] = 0.5  # Default moderate efficiency\n                    logger.warning(f\"Using fallback efficiency for batch size {batch_size}\")\n                \n            # Third pass: use the efficiency/throughput data to determine optimal batch size range\n            # per framework mathematical principles\n            \n            # Collect raw data first, the actual thresholds will be applied in _refine_batch_size_range\n            # This is just initial mapping to gather data points\n            min_efficiency_threshold = 0.01  # Even more permissive for data collection\n            \n            # Sort by batch size\n            sorted_batch_sizes = sorted(self.hw_efficiency_curve.keys())\n            \n            # Find the smallest batch size that meets our efficiency threshold\n            # DO NOT force a specific minimum batch size - let the algorithm decide\n            min_viable_batch = sorted_batch_sizes[0]  # Start with the smallest tested\n            efficiency_found = False\n            \n            # Check if ANY batch sizes meet the efficiency threshold\n            for batch_size in sorted_batch_sizes:\n                if self.hw_efficiency_curve[batch_size] >= min_efficiency_threshold:\n                    min_viable_batch = batch_size\n                    efficiency_found = True\n                    break\n                    \n            # If no batch size meets the threshold, don't be picky - try a range of batch sizes\n            if not efficiency_found and sorted_batch_sizes:\n                # Rather than just picking the most efficient, get a good mix of batch sizes\n                # Use the most efficient as a starting point, but consider larger batch sizes too\n                best_efficiency_items = sorted(self.hw_efficiency_curve.items(), key=lambda x: x[1], reverse=True)\n                \n                # Consider the top 3 most efficient batch sizes and prefer larger ones if efficiency is close\n                if len(best_efficiency_items) >= 3:\n                    top_items = best_efficiency_items[:3]\n                    # If the top items are close in efficiency (within 10%), prefer larger batch\n                    top_efficiency = top_items[0][1]\n                    viable_items = []\n                    \n                    for batch, eff in top_items:\n                        if eff >= 0.9 * top_efficiency:\n                            viable_items.append((batch, eff))\n                    \n                    if viable_items:\n                        # Prefer larger batch sizes when efficiency is similar\n                        most_efficient = max(viable_items, key=lambda x: x[0])[0]\n                    else:\n                        most_efficient = top_items[0][0]\n                else:\n                    most_efficient = best_efficiency_items[0][0]\n                \n                min_viable_batch = most_efficient\n                logger.warning(f\"No batch size met original efficiency threshold of {min_efficiency_threshold:.2f}, using batch size: {most_efficient} (best efficiency: {best_efficiency_items[0][1]:.4f}, considered {len(best_efficiency_items)} alternatives)\")\n            \n            # Find maximum batch size based on memory limits or efficiency curve\n            # Per the framework, we should use the point where marginal efficiency gain becomes minimal\n            if memory_limited and max_viable_batch:\n                # We hit memory limits, so use the largest successful batch size with a safety margin\n                safe_max = int(max_viable_batch * 0.9)\n                logger.info(f\"Setting memory-limited max batch size: {safe_max}\")\n                self.max_batch = safe_max\n            else:\n                # Find the batch size with highest efficiency\n                best_batch = max(self.hw_efficiency_curve.items(), key=lambda x: x[1])[0]\n                \n                # Find where efficiency starts to significantly drop after the peak\n                # Use a very permissive threshold to allow much wider batch size range\n                efficiency_threshold = 0.5 * self.hw_efficiency_curve[best_batch]  # Lowered from 0.75\n                larger_batches = [b for b in sorted_batch_sizes if b > best_batch]\n                \n                # Set a minimum range between min and max - at least 4x difference if possible\n                desired_range_multiple = 4.0\n                \n                if larger_batches:\n                    # Find largest batch size that still maintains efficiency above the threshold\n                    for batch_size in sorted(larger_batches, reverse=True):\n                        if self.hw_efficiency_curve[batch_size] >= efficiency_threshold:\n                            self.max_batch = batch_size\n                            break\n                    else:\n                        # If all larger batches are below threshold, use the best efficiency point\n                        # but ensure max_batch is at least 4x min_batch if possible\n                        self.max_batch = max(best_batch, int(min_viable_batch * desired_range_multiple))\n                else:\n                    # No larger batches were tested, use the best efficiency point\n                    # but ensure max_batch is at least 4x min_batch if possible\n                    self.max_batch = max(best_batch, int(min_viable_batch * desired_range_multiple))\n                \n                logger.info(f\"Setting efficiency-based max batch size: {self.max_batch}\")\n            \n            # Set the min batch size\n            self.min_batch = min_viable_batch\n            \n            logger.info(f\"Initially determined batch size range: {self.min_batch} - {self.max_batch}\")\n            \n            # Store initial values to be refined after resonance analysis\n            self.initial_min_batch = self.min_batch\n            self.initial_max_batch = self.max_batch\n        else:\n            # If we couldn't test any batch size successfully, set reasonable defaults\n            logger.warning(\"Could not determine optimal batch sizes, using framework defaults\")\n            # Apply Cognitive Efficiency model defaults\n            model_size = self.param_count\n            if model_size > 100_000_000:  # Very large model\n                self.min_batch = 8    # Reduced from 16 to allow smaller batch sizes\n                self.max_batch = 256  # Increased to allow much larger batch sizes\n            elif model_size > 10_000_000:  # Large model\n                self.min_batch = 8   # Reduced from 16\n                self.max_batch = 256 # Increased from 128\n            elif model_size > 1_000_000:  # Medium model\n                self.min_batch = 16  # Reduced from 32\n                self.max_batch = 384 # Increased from 256\n            else:  # Small model\n                self.min_batch = 32  # Reduced from 64\n                self.max_batch = 512 # Kept the same\n        \n        # Reset model gradients after diagnostics\n        for param in self.model.parameters():\n            if param.grad is not None:\n                param.grad.zero_()\n    \n    def _refine_batch_size_range(self):\n        \"\"\"\n        Refine batch size range based on resonance zones and natural frequencies."
        },
        "_refine_batch_size_range": {
          "start_line": 455,
          "end_line": 657,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logger.info",
              "line": 464
            },
            {
              "name": "hasattr",
              "line": 468
            },
            {
              "name": "hasattr",
              "line": 469
            },
            {
              "name": "logger.info",
              "line": 524
            },
            {
              "name": "hasattr",
              "line": 527
            },
            {
              "name": "sorted",
              "line": 528
            },
            {
              "name": "logger.info",
              "line": 652
            },
            {
              "name": "logger.warning",
              "line": 655
            },
            {
              "name": "hasattr",
              "line": 481
            },
            {
              "name": "list",
              "line": 482
            },
            {
              "name": "min",
              "line": 510
            },
            {
              "name": "logger.info",
              "line": 512
            },
            {
              "name": "logger.info",
              "line": 513
            },
            {
              "name": "logger.info",
              "line": 514
            },
            {
              "name": "logger.info",
              "line": 515
            },
            {
              "name": "logger.info",
              "line": 520
            },
            {
              "name": "self.hw_efficiency_curve.keys",
              "line": 528
            },
            {
              "name": "max",
              "line": 536
            },
            {
              "name": "min",
              "line": 537
            },
            {
              "name": "logger.info",
              "line": 538
            },
            {
              "name": "logger.info",
              "line": 539
            },
            {
              "name": "sorted",
              "line": 569
            },
            {
              "name": "min",
              "line": 572
            },
            {
              "name": "logger.info",
              "line": 573
            },
            {
              "name": "range",
              "line": 574
            },
            {
              "name": "logger.info",
              "line": 621
            },
            {
              "name": "max",
              "line": 645
            },
            {
              "name": "self.hw_efficiency_curve.values",
              "line": 482
            },
            {
              "name": "max",
              "line": 483
            },
            {
              "name": "min",
              "line": 484
            },
            {
              "name": "max",
              "line": 510
            },
            {
              "name": "self.hw_efficiency_curve.values",
              "line": 536
            },
            {
              "name": "self.hw_efficiency_curve.values",
              "line": 537
            },
            {
              "name": "logger.info",
              "line": 548
            },
            {
              "name": "logger.info",
              "line": 549
            },
            {
              "name": "logger.info",
              "line": 558
            },
            {
              "name": "self.hw_efficiency_curve.items",
              "line": 569
            },
            {
              "name": "len",
              "line": 572
            },
            {
              "name": "logger.info",
              "line": 576
            },
            {
              "name": "len",
              "line": 579
            },
            {
              "name": "logger.info",
              "line": 618
            },
            {
              "name": "logger.info",
              "line": 631
            },
            {
              "name": "max",
              "line": 649
            },
            {
              "name": "logger.info",
              "line": 650
            },
            {
              "name": "sum",
              "line": 485
            },
            {
              "name": "len",
              "line": 485
            },
            {
              "name": "math.exp",
              "line": 502
            },
            {
              "name": "efficiencies.append",
              "line": 556
            },
            {
              "name": "len",
              "line": 594
            },
            {
              "name": "logger.info",
              "line": 613
            },
            {
              "name": "logger.info",
              "line": 635
            },
            {
              "name": "math.sin",
              "line": 506
            },
            {
              "name": "viable_items.append",
              "line": 588
            },
            {
              "name": "max",
              "line": 601
            },
            {
              "name": "logger.info",
              "line": 602
            },
            {
              "name": "next",
              "line": 606
            },
            {
              "name": "logger.info",
              "line": 608
            },
            {
              "name": "....join",
              "line": 558
            },
            {
              "name": "min",
              "line": 605
            },
            {
              "name": "abs",
              "line": 605
            }
          ],
          "docstring": "\n        Refine batch size range based on resonance zones and natural frequencies.\n        \n        This method uses the resonance information to dynamically adapt the efficiency\n        threshold instead of using a fixed value, creating a more principled approach.\n        ",
          "code_snippet": "                param.grad.zero_()\n    \n    def _refine_batch_size_range(self):\n        \"\"\"\n        Refine batch size range based on resonance zones and natural frequencies.\n        \n        This method uses the resonance information to dynamically adapt the efficiency\n        threshold instead of using a fixed value, creating a more principled approach.\n        \"\"\"\n        # Only proceed if we have valid resonance zones\n        if self.resonance_zone[0] <= 0 or self.resonance_zone[1] <= 0:\n            logger.info(\"No valid resonance zone found, keeping initial batch size range\")\n            return\n            \n        # Get the initial batch range for reference\n        initial_min = self.initial_min_batch if hasattr(self, 'initial_min_batch') else self.min_batch\n        initial_max = self.initial_max_batch if hasattr(self, 'initial_max_batch') else self.max_batch\n        \n        # 1. Calculate a mathematically sound efficiency threshold based on resonance properties\n        # Start with natural resonance width as a proportion of overall range\n        resonance_width = self.resonance_zone[1] - self.resonance_zone[0]\n        batch_range_width = initial_max - initial_min\n        \n        if batch_range_width > 0 and resonance_width > 0:\n            # First derive relative resonance - how much of the range is in the \"sweet spot\"\n            relative_resonance = resonance_width / batch_range_width\n            \n            # Get the actual efficiency values for reference\n            if hasattr(self, 'hw_efficiency_curve') and self.hw_efficiency_curve:\n                eff_values = list(self.hw_efficiency_curve.values())\n                max_efficiency = max(eff_values) if eff_values else 0.03\n                min_efficiency = min(eff_values) if eff_values else 0.01\n                mean_efficiency = sum(eff_values) / len(eff_values) if eff_values else 0.02\n                \n                # Calculate harmonic parameters from resonance zone\n                # Framework principle: threshold is proportional to resonance characteristics\n                # The mathematical basis uses a harmonic function:\n                # threshold = base_eff * (1 - A * sin(B * relative_resonance))\n                # Where:\n                # - base_eff is the reference efficiency (max observed in good systems)\n                # - A controls amplitude of variation (how much resonance affects threshold)\n                # - B controls the shape of the response curve\n                \n                # Calculate base efficiency term - use max observed with a safety margin\n                base_eff = max_efficiency * 0.9  # 90% of observed maximum\n                \n                # Calculate shape parameters based on relative resonance\n                # Wider resonance zones need lower thresholds (more permissive)\n                # Narrower resonance zones need higher thresholds (more selective)\n                amplitude = 0.4 + 0.2 * math.exp(-relative_resonance)  # Range ~0.4-0.6\n                shape = math.pi * (1.0 + 0.5 * relative_resonance)      # Range ~\u03c0-1.5\u03c0\n                \n                # Calculate the actual threshold\n                adaptive_threshold = base_eff * (1.0 - amplitude * math.sin(shape * relative_resonance))\n                \n                # Ensure threshold is within reasonable bounds\n                # Must be less than max observed but high enough to differentiate\n                adaptive_threshold = min(max_efficiency * 0.95, max(min_efficiency * 1.2, adaptive_threshold))\n                \n                logger.info(f\"Framework-derived adaptive threshold: {adaptive_threshold:.5f}\")\n                logger.info(f\"  \u2022 Based on resonance ratio: {relative_resonance:.3f}\")\n                logger.info(f\"  \u2022 Efficiency range: {min_efficiency:.5f} to {max_efficiency:.5f} (mean: {mean_efficiency:.5f})\")\n                logger.info(f\"  \u2022 Harmonic parameters: base={base_eff:.5f}, amplitude={amplitude:.3f}, shape={shape:.3f}\")\n            else:\n                # If no efficiency data, use a resonance-based heuristic\n                # For wider resonance zones, be more permissive (lower threshold)\n                adaptive_threshold = 0.02 * (1.0 - 0.5 * relative_resonance)\n                logger.info(f\"Using resonance-based heuristic threshold: {adaptive_threshold:.5f} (no efficiency data available)\")\n        else:\n            # Fallback to moderate threshold\n            adaptive_threshold = 0.02  # Adjusted to realistic values\n            logger.info(f\"Using default efficiency threshold: {adaptive_threshold:.3f} (couldn't calculate resonance ratio)\")\n            \n        # 2. Refine min and max batch sizes using this adaptive threshold\n        if hasattr(self, 'hw_efficiency_curve') and self.hw_efficiency_curve:\n            sorted_batch_sizes = sorted(self.hw_efficiency_curve.keys())\n            \n            # Check if any batch sizes meet our adaptive threshold\n            threshold_met = False\n            min_viable_batch = 0\n            \n            # Log the actual efficiency values for analysis\n            if sorted_batch_sizes:\n                best_eff = max(self.hw_efficiency_curve.values())\n                worst_eff = min(self.hw_efficiency_curve.values())\n                logger.info(f\"Evaluating batch sizes against threshold {adaptive_threshold:.5f}\")\n                logger.info(f\"Batch size efficiencies: [{worst_eff:.5f} to {best_eff:.5f}]\")\n                \n                # Smart adaptation: If no batch sizes would meet the threshold,\n                # adjust to ensure at least one viable option\n                if best_eff < adaptive_threshold:\n                    # Choose a threshold that selects the best batch size plus any that \n                    # are at least 90% as efficient\n                    old_threshold = adaptive_threshold\n                    adaptive_threshold = best_eff * 0.9  # 90% of best efficiency\n                    logger.info(f\"No batch sizes meet the calculated threshold, adjusting from {old_threshold:.5f} to {adaptive_threshold:.5f}\")\n                    logger.info(f\"This will select batch sizes with efficiency \u2265 {adaptive_threshold:.5f} (best is {best_eff:.5f})\")\n                \n                # For very close thresholds, display what batch sizes will be selected\n                efficiencies = []\n                for batch_size in sorted_batch_sizes:\n                    eff = self.hw_efficiency_curve[batch_size]\n                    if eff >= adaptive_threshold:\n                        efficiencies.append(f\"{batch_size}:{eff:.5f}\")\n                if efficiencies:\n                    logger.info(f\"Batch sizes meeting threshold: {', '.join(efficiencies)}\")\n            \n            for batch_size in sorted_batch_sizes:\n                if self.hw_efficiency_curve[batch_size] >= adaptive_threshold:\n                    min_viable_batch = batch_size\n                    threshold_met = True\n                    break\n                    \n            # Handle case where no batch size meets the threshold\n            if not threshold_met and sorted_batch_sizes:\n                # Find the batch size with highest efficiency and report more details\n                best_efficiency_items = sorted(self.hw_efficiency_curve.items(), key=lambda x: x[1], reverse=True)\n                \n                # Report top batch sizes for transparency\n                top_count = min(5, len(best_efficiency_items))\n                logger.info(f\"Top {top_count} batch sizes by efficiency:\")\n                for i in range(top_count):\n                    batch, eff = best_efficiency_items[i]\n                    logger.info(f\"  {i+1}. Batch {batch}: {eff:.5f}\")\n                \n                # Consider the top batch sizes and prefer those within or closer to resonance zone\n                if len(best_efficiency_items) >= 3:\n                    # Get top performers (at least top 3)\n                    top_items = best_efficiency_items[:3]\n                    top_efficiency = top_items[0][1]\n                    \n                    # Include any batch sizes with efficiency at least 85% of the best\n                    viable_items = []\n                    for batch, eff in best_efficiency_items:\n                        if eff >= 0.85 * top_efficiency:  # Within 15% of best\n                            viable_items.append((batch, eff))\n                    \n                    # Calculate resonance center for reference\n                    resonance_center = (self.resonance_zone[0] + self.resonance_zone[1]) / 2\n                    \n                    # If we have multiple viable batch sizes, prefer those closest to resonance zone\n                    if len(viable_items) > 1:\n                        # Try to find batch sizes within resonance zone first\n                        in_zone_items = [(b, e) for b, e in viable_items \n                                        if self.resonance_zone[0] <= b <= self.resonance_zone[1]]\n                        \n                        if in_zone_items:\n                            # If we have batch sizes in the zone, prefer the most efficient\n                            best_batch, best_eff = max(in_zone_items, key=lambda x: x[1])\n                            logger.info(f\"Selected batch size {best_batch} (efficiency: {best_eff:.5f}) from resonance zone\")\n                        else:\n                            # Otherwise, prefer the one closest to resonance center\n                            closest_batch = min(viable_items, key=lambda x: abs(x[0] - resonance_center))[0]\n                            closest_eff = next(e for b, e in viable_items if b == closest_batch)\n                            best_batch = closest_batch\n                            logger.info(f\"Selected batch size {best_batch} (efficiency: {closest_eff:.5f}) - closest to resonance center {resonance_center}\")\n                    else:\n                        # With just one viable batch size, use it\n                        best_batch = viable_items[0][0]\n                        best_eff = viable_items[0][1]\n                        logger.info(f\"Selected batch size {best_batch} (efficiency: {best_eff:.5f}) - only viable option\")\n                else:\n                    # With very few options, just pick the most efficient\n                    best_batch = best_efficiency_items[0][0]\n                    best_eff = best_efficiency_items[0][1]\n                    logger.info(f\"Selected batch size {best_batch} (efficiency: {best_eff:.5f}) - most efficient of limited options\")\n                    \n                min_viable_batch = best_batch\n                logger.info(f\"Final selection: batch size {min_viable_batch} (efficiency: {self.hw_efficiency_curve[min_viable_batch]:.5f})\")\n            \n            # Incorporate resonance zones into final batch size range\n            # If min_viable_batch is outside the resonance zone, adjust toward the zone\n            if self.resonance_zone[0] > 0 and self.resonance_zone[1] > 0:\n                resonance_min, resonance_max = self.resonance_zone\n                \n                # Refine the minimum batch size - prefer values within resonance zone\n                if min_viable_batch < resonance_min:\n                    # Below resonance zone - use lower end of resonance\n                    logger.info(f\"Efficiency-based min batch ({min_viable_batch}) below resonance zone, using resonance min: {resonance_min}\")\n                    min_viable_batch = resonance_min\n                elif min_viable_batch > resonance_max:\n                    # Above resonance zone - use resonance max instead\n                    logger.info(f\"Efficiency-based min batch ({min_viable_batch}) above resonance zone, using resonance max: {resonance_max}\")\n                    min_viable_batch = resonance_max\n            \n            # Update minimum batch size\n            self.min_batch = min_viable_batch\n            \n            # Determine maximum batch size\n            # Start with resonance zone maximum if available\n            if self.resonance_zone[1] > 0:\n                # Use resonance zone max as a starting point\n                self.max_batch = max(self.max_batch, self.resonance_zone[1])\n                \n                # Make sure max is at least 3x min to give adequate range\n                if self.max_batch < 3 * self.min_batch:\n                    self.max_batch = max(self.max_batch, 3 * self.min_batch)\n                    logger.info(f\"Increased max batch to {self.max_batch} (3x min_batch)\")\n                    \n            logger.info(f\"Refined batch size range: {self.min_batch} - {self.max_batch} (incorporating resonance and efficiency)\")\n            \n        else:\n            logger.warning(\"No efficiency data available, keeping initial batch size range\")\n    \n    def _analyze_layer_frequencies(self):\n        \"\"\"Analyze natural update frequencies for different layer types.\"\"\"\n        # This is a simplified implementation for the demo"
        },
        "_analyze_layer_frequencies": {
          "start_line": 657,
          "end_line": 692,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.model.named_modules",
              "line": 663
            },
            {
              "name": "self.layer_frequency_map.values",
              "line": 689
            },
            {
              "name": "len",
              "line": 664
            },
            {
              "name": "....append",
              "line": 668
            },
            {
              "name": "isinstance",
              "line": 671
            },
            {
              "name": "self.band_layer_counts.get",
              "line": 690
            },
            {
              "name": "list",
              "line": 664
            },
            {
              "name": "isinstance",
              "line": 676
            },
            {
              "name": "module.children",
              "line": 664
            },
            {
              "name": "isinstance",
              "line": 678
            },
            {
              "name": "name.endswith",
              "line": 679
            }
          ],
          "docstring": "Analyze natural update frequencies for different layer types.",
          "code_snippet": "            logger.warning(\"No efficiency data available, keeping initial batch size range\")\n    \n    def _analyze_layer_frequencies(self):\n        \"\"\"Analyze natural update frequencies for different layer types.\"\"\"\n        # This is a simplified implementation for the demo\n        # A full implementation would use layer-wise gradient analysis\n        \n        layer_types = {}\n        for name, module in self.model.named_modules():\n            if len(list(module.children())) == 0:  # Leaf module\n                layer_type = module.__class__.__name__\n                if layer_type not in layer_types:\n                    layer_types[layer_type] = []\n                layer_types[layer_type].append(name)\n                \n                # Map layers to frequency bands based on type\n                if isinstance(module, nn.Conv2d):\n                    if \"conv1\" in name or \".0.\" in name:  # Early layers\n                        self.layer_frequency_map[name] = \"high\"\n                    else:\n                        self.layer_frequency_map[name] = \"medium\"\n                elif isinstance(module, nn.BatchNorm2d):\n                    self.layer_frequency_map[name] = \"ultra\"  # BN needs frequent updates\n                elif isinstance(module, nn.Linear):\n                    if \"fc\" in name or name.endswith(\"fc\"):  # Final classification layer\n                        self.layer_frequency_map[name] = \"low\"\n                    else:\n                        self.layer_frequency_map[name] = \"medium\"\n                else:\n                    # Default to medium frequency for other layers\n                    self.layer_frequency_map[name] = \"medium\"\n        \n        # Count layers in each frequency band\n        self.band_layer_counts = {band: 0 for band in self.frequency_bands}\n        for band in self.layer_frequency_map.values():\n            self.band_layer_counts[band] = self.band_layer_counts.get(band, 0) + 1\n    \n    def _calculate_observer_overhead(self):\n        \"\"\"Calculate the overhead introduced by measurement itself.\"\"\"\n        # This is approximate - a real implementation would measure more precisely"
        },
        "_calculate_observer_overhead": {
          "start_line": 692,
          "end_line": 706,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [],
          "docstring": "Calculate the overhead introduced by measurement itself.",
          "code_snippet": "            self.band_layer_counts[band] = self.band_layer_counts.get(band, 0) + 1\n    \n    def _calculate_observer_overhead(self):\n        \"\"\"Calculate the overhead introduced by measurement itself.\"\"\"\n        # This is approximate - a real implementation would measure more precisely\n        for band in self.frequency_bands:\n            # Higher frequency bands have more observer overhead\n            if band == \"ultra\":\n                self.observer_overhead[band] = 0.15  # 15% overhead\n            elif band == \"high\":\n                self.observer_overhead[band] = 0.10  # 10% overhead\n            elif band == \"medium\":\n                self.observer_overhead[band] = 0.05  # 5% overhead\n            else:\n                self.observer_overhead[band] = 0.02  # 2% overhead\n    \n    def _find_resonance_zone(self):\n        \"\"\"\n        Identify the resonance zone where batch sizes and frequency bands optimally align."
        },
        "_find_resonance_zone": {
          "start_line": 706,
          "end_line": 983,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logger.info",
              "line": 719
            },
            {
              "name": "sorted",
              "line": 722
            },
            {
              "name": "self.frequency_bands.keys",
              "line": 735
            },
            {
              "name": "self.estimate_available_energy",
              "line": 739
            },
            {
              "name": "self.frequency_bands.keys",
              "line": 763
            },
            {
              "name": "self.frequency_bands.keys",
              "line": 771
            },
            {
              "name": "float",
              "line": 843
            },
            {
              "name": "sum",
              "line": 848
            },
            {
              "name": "sum",
              "line": 889
            },
            {
              "name": "logger.info",
              "line": 895
            },
            {
              "name": "sorted",
              "line": 896
            },
            {
              "name": "self.resonance_map.items",
              "line": 900
            },
            {
              "name": "self.resonance_map.items",
              "line": 914
            },
            {
              "name": "logger.info",
              "line": 978
            },
            {
              "name": "sorted",
              "line": 979
            },
            {
              "name": "logger.warning",
              "line": 715
            },
            {
              "name": "self.hw_time_curve.keys",
              "line": 722
            },
            {
              "name": "len",
              "line": 723
            },
            {
              "name": "logger.warning",
              "line": 724
            },
            {
              "name": "self.frequency_bands.keys",
              "line": 750
            },
            {
              "name": "self.estimate_model_complexity",
              "line": 798
            },
            {
              "name": "max",
              "line": 799
            },
            {
              "name": "max",
              "line": 803
            },
            {
              "name": "logger.debug",
              "line": 805
            },
            {
              "name": "self.band_layer_counts.values",
              "line": 848
            },
            {
              "name": "self.band_layer_counts.get",
              "line": 852
            },
            {
              "name": "band_weights.values",
              "line": 889
            },
            {
              "name": "band_weights.items",
              "line": 896
            },
            {
              "name": "logger.info",
              "line": 897
            },
            {
              "name": "max",
              "line": 906
            },
            {
              "name": "min",
              "line": 907
            },
            {
              "name": "band_weights.get",
              "line": 920
            },
            {
              "name": "int",
              "line": 929
            },
            {
              "name": "int",
              "line": 930
            },
            {
              "name": "logger.warning",
              "line": 935
            },
            {
              "name": "logger.info",
              "line": 975
            },
            {
              "name": "self.resonance_map.items",
              "line": 979
            },
            {
              "name": "max",
              "line": 728
            },
            {
              "name": "list",
              "line": 729
            },
            {
              "name": "logger.info",
              "line": 732
            },
            {
              "name": "self.estimate_cognitive_load",
              "line": 752
            },
            {
              "name": "self.cognitive_efficiency",
              "line": 755
            },
            {
              "name": "min",
              "line": 799
            },
            {
              "name": "min",
              "line": 803
            },
            {
              "name": "max",
              "line": 855
            },
            {
              "name": "float",
              "line": 933
            },
            {
              "name": "logger.info",
              "line": 940
            },
            {
              "name": "logger.info",
              "line": 981
            },
            {
              "name": "range",
              "line": 729
            },
            {
              "name": "batch_sizes.append",
              "line": 731
            },
            {
              "name": "max",
              "line": 767
            },
            {
              "name": "max",
              "line": 832
            },
            {
              "name": "logger.info",
              "line": 839
            },
            {
              "name": "logger.info",
              "line": 944
            },
            {
              "name": "logger.warning",
              "line": 947
            },
            {
              "name": "self.estimate_model_complexity",
              "line": 950
            },
            {
              "name": "len",
              "line": 724
            },
            {
              "name": "band.upper",
              "line": 805
            },
            {
              "name": "band.upper",
              "line": 897
            },
            {
              "name": "int",
              "line": 966
            },
            {
              "name": "max",
              "line": 969
            },
            {
              "name": "int",
              "line": 969
            },
            {
              "name": "band.upper",
              "line": 981
            },
            {
              "name": "band.upper",
              "line": 839
            }
          ],
          "docstring": "\n        Identify the resonance zone where batch sizes and frequency bands optimally align.\n        \n        This creates a map of optimal batch size ranges for each frequency band,\n        then finds the overlapping \"sweet spot\" where all frequency bands can operate effectively.\n        ",
          "code_snippet": "                self.observer_overhead[band] = 0.02  # 2% overhead\n    \n    def _find_resonance_zone(self):\n        \"\"\"\n        Identify the resonance zone where batch sizes and frequency bands optimally align.\n        \n        This creates a map of optimal batch size ranges for each frequency band,\n        then finds the overlapping \"sweet spot\" where all frequency bands can operate effectively.\n        \"\"\"\n        # Ensure we have some test batch sizes\n        if not self.hw_time_curve:\n            logger.warning(\"No batch size tests available, skipping resonance zone calculation\")\n            return\n            \n        # Step 1: Calculate optimal batch size ranges for each frequency band\n        logger.info(\"Finding optimal batch size ranges for each frequency band...\")\n        \n        # Get test batch sizes with at least 6 points for reliable analysis\n        batch_sizes = sorted(self.hw_time_curve.keys())\n        if len(batch_sizes) < 6:\n            logger.warning(f\"Too few batch sizes ({len(batch_sizes)}) for reliable resonance zone calculation\")\n            # Create a reasonable test range if we don't have enough points\n            if self.min_batch is not None and self.max_batch is not None:\n                # Create a range based on min and max batch\n                step = max(1, (self.max_batch - self.min_batch) // 5)\n                batch_sizes = list(range(self.min_batch, self.max_batch + 1, step))\n                if self.max_batch not in batch_sizes:\n                    batch_sizes.append(self.max_batch)\n                logger.info(f\"Created synthetic batch range for resonance analysis: {batch_sizes}\")\n        \n        # Initialize resonance map with empty ranges for all frequency bands\n        for band in self.frequency_bands.keys():\n            self.resonance_map[band] = (0, 0)  # (min, max) \n        \n        # Get total energy and processors for calculations\n        total_energy = self.estimate_available_energy()\n        p = self.parallel_processors\n        \n        # Store all band efficiencies for each batch size for a more stable analysis\n        batch_efficiencies = {}\n        \n        # For each batch size, evaluate each frequency band\n        for batch_size in batch_sizes:\n            band_efficiencies = {}\n            \n            # Test this batch size with each frequency band\n            for band in self.frequency_bands.keys():\n                # Calculate cognitive load\n                L = self.estimate_cognitive_load(batch_size, band)\n                \n                # Calculate efficiency for this batch+band combination\n                band_efficiency = self.cognitive_efficiency(total_energy, batch_size, p, L, band)\n                band_efficiencies[band] = band_efficiency\n            \n            # Store all band efficiencies for this batch size\n            batch_efficiencies[batch_size] = band_efficiencies\n            \n        # Find the maximum efficiency achieved for each band across all batch sizes\n        max_band_efficiencies = {}\n        for band in self.frequency_bands.keys():\n            max_eff = 0.0\n            for batch_size in batch_sizes:\n                if batch_size in batch_efficiencies and band in batch_efficiencies[batch_size]:\n                    max_eff = max(max_eff, batch_efficiencies[batch_size][band])\n            max_band_efficiencies[band] = max_eff\n            \n        # Now determine the viable range for each frequency band\n        for band in self.frequency_bands.keys():\n            # Calculate an adaptive threshold based on the band's characteristics\n            # Use the principles of cognitive resonance from the framework's mathematical foundation\n            \n            # Start with a base threshold that adapts to the band frequency\n            if band == \"ultra\":\n                # Ultra high frequency bands need higher precision (more selective)\n                base_threshold = 0.75\n            elif band == \"high\":\n                base_threshold = 0.7\n            elif band == \"medium\":\n                base_threshold = 0.65\n            elif band == \"low\":\n                base_threshold = 0.6\n            else:  # background\n                # Background frequencies can operate with more flexibility\n                base_threshold = 0.55\n                \n            # Adjust for hardware type (CPU vs GPU resonance differences)\n            if self.device.type == \"cuda\":\n                # GPUs have sharper resonance peaks (more selective)\n                hw_factor = 1.0\n            else:\n                # CPUs have broader resonance bands (less selective)\n                hw_factor = 0.9\n                \n            # Adjust for model complexity - more complex models have broader resonance zones\n            complexity = self.estimate_model_complexity()\n            complexity_factor = max(0.8, min(1.1, 1.0 - (complexity - 1.0) * 0.1))\n            \n            # Calculate final adaptive threshold with natural boundaries\n            # Upper bound lowered from 0.85 to 0.7 to encourage model growth and wider resonance zone\n            threshold = max(0.45, min(0.7, base_threshold * hw_factor * complexity_factor))\n            \n            logger.debug(f\"{band.upper()} band threshold: {threshold:.2f} (from {base_threshold:.2f} \u00d7 {hw_factor:.2f} \u00d7 {complexity_factor:.2f})\")\n            \n            min_viable = 0\n            max_viable = 0\n            \n            # Skip bands with no meaningful efficiency\n            if max_band_efficiencies[band] <= 0.01:\n                continue\n                \n            # Find the range where efficiency is above threshold\n            for batch_size in batch_sizes:\n                if batch_size in batch_efficiencies and band in batch_efficiencies[batch_size]:\n                    efficiency = batch_efficiencies[batch_size][band]\n                    \n                    # Check if this batch size is viable for this band\n                    if efficiency >= threshold * max_band_efficiencies[band]:\n                        # Update the range\n                        if min_viable == 0 or batch_size < min_viable:\n                            min_viable = batch_size\n                        if max_viable == 0 or batch_size > max_viable:\n                            max_viable = batch_size\n            \n            # Store the viable range for this band\n            if min_viable > 0 and max_viable > 0:\n                self.resonance_map[band] = (min_viable, max_viable)\n                \n                # Log the best batch size for this band\n                best_batch = max(\n                    [(batch, batch_efficiencies[batch][band]) for batch in batch_sizes if batch in batch_efficiencies and band in batch_efficiencies[batch]], \n                    key=lambda x: x[1], \n                    default=(0, 0)\n                )[0]\n                \n                if best_batch > 0:\n                    logger.info(f\"{band.upper()} band optimal: {best_batch} (range: {min_viable}-{max_viable})\")\n        \n        # Step 2: Find the overlapping range across all frequency bands\n        min_viable = 0\n        max_viable = float('inf')\n        \n        # Calculate importance weights for each frequency band \n        # based on how many layers are in each band\n        band_weights = {}\n        total_layers = sum(self.band_layer_counts.values())\n        \n        for band in self.frequency_bands:\n            # Use layer counts to determine importance\n            layer_count = self.band_layer_counts.get(band, 0)\n            if total_layers > 0:\n                # Weight by number of layers, but ensure at least some weight\n                raw_weight = max(0.1, layer_count / total_layers)\n            else:\n                # Default weight if no layer counts\n                raw_weight = 0.2\n                \n            # Adjust weight based on hardware and band preference\n            if self.device.type == \"cuda\":\n                # GPUs do well with higher frequency bands\n                if band == \"ultra\":\n                    raw_weight *= 1.3\n                elif band == \"high\":\n                    raw_weight *= 1.2\n                elif band == \"medium\":\n                    raw_weight *= 1.0\n                elif band == \"low\":\n                    raw_weight *= 0.9\n                else:  # background\n                    raw_weight *= 0.8\n            else:\n                # CPUs do better with lower frequency bands\n                if band == \"ultra\":\n                    raw_weight *= 0.8\n                elif band == \"high\":\n                    raw_weight *= 0.9\n                elif band == \"medium\":\n                    raw_weight *= 1.0\n                elif band == \"low\":\n                    raw_weight *= 1.2\n                else:  # background\n                    raw_weight *= 1.3\n                    \n            band_weights[band] = raw_weight\n            \n        # Normalize weights to sum to 1.0\n        weight_sum = sum(band_weights.values())\n        if weight_sum > 0:\n            for band in band_weights:\n                band_weights[band] /= weight_sum\n                \n        # Log the band weights\n        logger.info(\"Frequency band weights:\")\n        for band, weight in sorted(band_weights.items(), key=lambda x: x[1], reverse=True):\n            logger.info(f\"  {band.upper()}: {weight:.2f}\")\n            \n        # Method 1: Strict Intersection (traditional approach)\n        for band, (band_min, band_max) in self.resonance_map.items():\n            # Ignore bands with no viable range\n            if band_min == 0 and band_max == 0:\n                continue\n                \n            # Update the global resonance zone\n            min_viable = max(min_viable, band_min)\n            max_viable = min(max_viable, band_max)\n            \n        # Method 2: Weighted Midpoint (alternative if strict intersection fails)\n        weighted_min = 0\n        weighted_max = 0\n        weight_total = 0\n        \n        for band, (band_min, band_max) in self.resonance_map.items():\n            # Skip bands with no viable range\n            if band_min == 0 and band_max == 0:\n                continue\n                \n            # Get the weight for this band\n            weight = band_weights.get(band, 0.2)  # Default to 0.2 if no weight\n            \n            # Add weighted contribution to min and max\n            weighted_min += band_min * weight\n            weighted_max += band_max * weight\n            weight_total += weight\n            \n        # Calculate final weighted values if we have weights\n        if weight_total > 0:\n            weighted_min = int(weighted_min / weight_total)\n            weighted_max = int(weighted_max / weight_total)\n        \n        # Ensure we have a valid resonance zone\n        if min_viable == 0 or max_viable == float('inf') or min_viable > max_viable:\n            # No strict overlapping zone found - use weighted approach instead\n            logger.warning(f\"No strictly overlapping resonance zone - using weighted approach\")\n            \n            if weighted_min > 0 and weighted_max > 0 and weighted_min <= weighted_max:\n                # Use the weighted values\n                self.resonance_zone = (weighted_min, weighted_max)\n                logger.info(f\"Using weighted resonance zone: {weighted_min}-{weighted_max}\")\n            elif 'medium' in self.resonance_map and self.resonance_map['medium'] != (0, 0):\n                # Fallback to medium frequency band's range\n                self.resonance_zone = self.resonance_map['medium']\n                logger.info(f\"Using medium frequency band range: {self.resonance_zone[0]}-{self.resonance_zone[1]}\")\n            else:\n                # Final fallback to adaptive defaults based on model characteristics\n                logger.warning(\"No viable ranges found - using adaptive defaults\")\n                \n                # Calculate adaptive resonance zone based on model complexity\n                complexity = self.estimate_model_complexity()\n                \n                # For more complex models, use wider range with larger values\n                if complexity > 2.0:  # More complex model\n                    min_batch = 16\n                    max_batch = 96\n                elif complexity > 1.5:  # Moderately complex\n                    min_batch = 12 \n                    max_batch = 64\n                else:  # Simpler model\n                    min_batch = 8\n                    max_batch = 48\n                    \n                # Further adjust based on device type\n                if self.device.type == \"cuda\":\n                    # GPUs can handle larger batch sizes\n                    max_batch = int(max_batch * 1.5)\n                else:\n                    # CPUs benefit from smaller batches\n                    min_batch = max(1, int(min_batch * 0.75))\n                    \n                self.resonance_zone = (min_batch, max_batch)\n        else:\n            # We found a valid strict overlapping zone\n            self.resonance_zone = (min_viable, max_viable)\n            logger.info(f\"Found strict overlapping resonance zone: {min_viable}-{max_viable}\")\n        \n        # Log all the band ranges for reference\n        logger.info(\"Frequency band optimal ranges:\")\n        for band, (band_min, band_max) in sorted(self.resonance_map.items()):\n            if band_min > 0 and band_max > 0:\n                logger.info(f\"  {band.upper()}: {band_min}-{band_max}\")\n    \n    def _report_diagnostics(self):\n        \"\"\"Report diagnostic results and set initial batch size.\"\"\"\n        logger.info(\"\\n--- IsekaiZen Framework Diagnostics ---\")"
        },
        "_report_diagnostics": {
          "start_line": 983,
          "end_line": 1053,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logger.info",
              "line": 985
            },
            {
              "name": "logger.info",
              "line": 986
            },
            {
              "name": "logger.info",
              "line": 987
            },
            {
              "name": "min",
              "line": 1040
            },
            {
              "name": "logger.info",
              "line": 1045
            },
            {
              "name": "logger.info",
              "line": 1046
            },
            {
              "name": "logger.info",
              "line": 1051
            },
            {
              "name": "self.hw_memory_curve.keys",
              "line": 992
            },
            {
              "name": "logger.info",
              "line": 998
            },
            {
              "name": "sorted",
              "line": 999
            },
            {
              "name": "logger.info",
              "line": 1007
            },
            {
              "name": "max",
              "line": 1040
            },
            {
              "name": "mem_info.append",
              "line": 996
            },
            {
              "name": "logger.info",
              "line": 1000
            },
            {
              "name": "max",
              "line": 1013
            },
            {
              "name": "logger.info",
              "line": 1037
            },
            {
              "name": "self.hw_memory_curve.keys",
              "line": 1013
            },
            {
              "name": "sorted",
              "line": 1021
            },
            {
              "name": "self.hw_memory_curve.items",
              "line": 1021
            },
            {
              "name": "max",
              "line": 1030
            },
            {
              "name": "viable_sizes.append",
              "line": 1023
            },
            {
              "name": "max",
              "line": 1034
            },
            {
              "name": "self.hw_efficiency_curve.items",
              "line": 1034
            }
          ],
          "docstring": "Report diagnostic results and set initial batch size.",
          "code_snippet": "                logger.info(f\"  {band.upper()}: {band_min}-{band_max}\")\n    \n    def _report_diagnostics(self):\n        \"\"\"Report diagnostic results and set initial batch size.\"\"\"\n        logger.info(\"\\n--- IsekaiZen Framework Diagnostics ---\")\n        logger.info(f\"Hardware: {'GPU' if self.device.type == 'cuda' else 'CPU'} with {self.parallel_processors} processors\")\n        logger.info(f\"Practical batch size range: {self.min_batch} - {self.max_batch}\")\n        \n        # Calculate memory-based maximum batch size\n        if self.device.type == \"cuda\":\n            mem_info = []\n            for batch_size in self.hw_memory_curve.keys():\n                memory_used = self.hw_memory_curve[batch_size]\n                available_memory = self.gpu_memory\n                mem_usage_percent = (memory_used / available_memory) * 100\n                mem_info.append((batch_size, memory_used, mem_usage_percent))\n                \n            logger.info(\"\\nMemory usage by batch size:\")\n            for batch, mem, percent in sorted(mem_info):\n                logger.info(f\"  Batch {batch}: {mem:.2f} GB ({percent:.1f}% of available)\")\n        \n        # Use the resonance zone to guide batch size selection\n        if self.resonance_zone[0] > 0 and self.resonance_zone[1] > 0:\n            # Use the middle of the resonance zone as our selected batch size\n            midpoint = (self.resonance_zone[0] + self.resonance_zone[1]) // 2\n            self.selected_batch_size = midpoint\n            logger.info(f\"\\nSelected initial batch size: {self.selected_batch_size} (from resonance zone)\")\n        else:\n            # Fall back to the original method if resonance zone isn't available\n            # Find the maximum viable batch size based on hardware constraints\n            if self.hw_memory_curve:\n                # Default to the maximum batch size we could test\n                self.selected_batch_size = max(self.hw_memory_curve.keys())\n                \n                # Find the sweet spot based on memory usage or efficiency\n                if self.device.type == \"cuda\":\n                    # For GPU, try to find a batch size that uses around 80% of memory\n                    target_usage = 0.8 * self.gpu_memory\n                    viable_sizes = []\n                    \n                    for batch, memory in sorted(self.hw_memory_curve.items()):\n                        if memory <= target_usage:\n                            viable_sizes.append(batch)\n                        else:\n                            # We've found the first batch size that exceeds our target\n                            # Use the previous one (which is within our target)\n                            break\n                    \n                    if viable_sizes:\n                        self.selected_batch_size = max(viable_sizes)\n                else:\n                    # For CPU, use the one with best efficiency if available\n                    if self.hw_efficiency_curve:\n                        best_batch = max(self.hw_efficiency_curve.items(), key=lambda x: x[1])[0]\n                        self.selected_batch_size = best_batch\n                \n                logger.info(f\"\\nSelected initial batch size: {self.selected_batch_size} (from hardware constraints)\")\n        \n        # Ensure the selected batch is within our diagnostic range\n        self.selected_batch_size = min(max(self.min_batch, self.selected_batch_size), self.max_batch)\n        \n        # Consider the current epoch relative to total training length\n        phase = \"early\"  # Default to early phase\n        \n        logger.info(f\"Training plan: {self.total_epochs} total epochs\")\n        logger.info(f\"Accuracy monitoring: {self.accuracy_window} epoch window, {self.accuracy_threshold} threshold\")\n        \n        # Start with the selected batch size as our best seen\n        self.best_batch_size = self.selected_batch_size\n            \n        logger.info(\"\\nDiagnostics complete. Framework initialized.\\n\")\n    \n    def base_cognitive_efficiency(self, L: float) -> float:\n        \"\"\"\n        Calculate base cognitive efficiency using sigmoidal transition."
        },
        "base_cognitive_efficiency": {
          "start_line": 1053,
          "end_line": 1067,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "L",
              "type": "float"
            }
          ],
          "return_type": "float",
          "calls": [
            {
              "name": "math.exp",
              "line": 1065
            }
          ],
          "docstring": "\n        Calculate base cognitive efficiency using sigmoidal transition.\n        \n        Implements \u03c6(L) = 1/(1 + e^((L-L_c/2)/\u03c3))\n        \n        Args:\n            L: Cognitive load\n            \n        Returns:\n            Base cognitive efficiency\n        ",
          "code_snippet": "        logger.info(\"\\nDiagnostics complete. Framework initialized.\\n\")\n    \n    def base_cognitive_efficiency(self, L: float) -> float:\n        \"\"\"\n        Calculate base cognitive efficiency using sigmoidal transition.\n        \n        Implements \u03c6(L) = 1/(1 + e^((L-L_c/2)/\u03c3))\n        \n        Args:\n            L: Cognitive load\n            \n        Returns:\n            Base cognitive efficiency\n        \"\"\"\n        return 1 / (1 + math.exp((L - self.L_c/2) / self.sigma))\n    \n    def parallel_processing_penalty(self, p: int, L: float) -> float:\n        \"\"\"\n        Calculate parallel processing penalty."
        },
        "parallel_processing_penalty": {
          "start_line": 1067,
          "end_line": 1085,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "p",
              "type": "int"
            },
            {
              "name": "L",
              "type": "float"
            }
          ],
          "return_type": "float",
          "calls": [
            {
              "name": "math.log",
              "line": 1081
            },
            {
              "name": "max",
              "line": 1081
            },
            {
              "name": "math.sqrt",
              "line": 1082
            },
            {
              "name": "math.exp",
              "line": 1082
            }
          ],
          "docstring": "\n        Calculate parallel processing penalty.\n        \n        Implements \u03c8(p, L) = (1/p)(1 + \u03b1_w(p - 1)log(L))(1 - \u03b1_par\u221ap(1 - e^(-L/L_c)))\n        \n        Args:\n            p: Number of parallel processors\n            L: Cognitive load\n            \n        Returns:\n            Parallel processing penalty factor\n        ",
          "code_snippet": "        return 1 / (1 + math.exp((L - self.L_c/2) / self.sigma))\n    \n    def parallel_processing_penalty(self, p: int, L: float) -> float:\n        \"\"\"\n        Calculate parallel processing penalty.\n        \n        Implements \u03c8(p, L) = (1/p)(1 + \u03b1_w(p - 1)log(L))(1 - \u03b1_par\u221ap(1 - e^(-L/L_c)))\n        \n        Args:\n            p: Number of parallel processors\n            L: Cognitive load\n            \n        Returns:\n            Parallel processing penalty factor\n        \"\"\"\n        term1 = 1/p\n        term2 = 1 + self.alpha_w * (p - 1) * math.log(max(1.0, L))\n        term3 = 1 - self.alpha_par * math.sqrt(p) * (1 - math.exp(-L / self.L_c))\n        return term1 * term2 * term3\n    \n    def energy_requirement(self, batch_size: int, p: int, L: float, frequency_band: str = \"medium\") -> float:\n        \"\"\"\n        Calculate energy requirement for given batch size, adjusted for frequency band."
        },
        "energy_requirement": {
          "start_line": 1085,
          "end_line": 1121,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "batch_size",
              "type": "int"
            },
            {
              "name": "p",
              "type": "int"
            },
            {
              "name": "L",
              "type": "float"
            },
            {
              "name": "frequency_band",
              "type": "str"
            }
          ],
          "return_type": "float",
          "calls": [
            {
              "name": "math.sqrt",
              "line": 1105
            },
            {
              "name": "math.sqrt",
              "line": 1107
            },
            {
              "name": "math.log",
              "line": 1107
            },
            {
              "name": "max",
              "line": 1107
            }
          ],
          "docstring": "\n        Calculate energy requirement for given batch size, adjusted for frequency band.\n        \n        Based on E_total(n, p) = kT\u00b7ln(2) \u00b7 n \u00b7 [1 + \u03b1(p)log(n) + \u03b2(p)\u221aL]\n        \n        Args:\n            batch_size: Batch size (n)\n            p: Number of parallel processors\n            L: Cognitive load\n            frequency_band: Frequency band to calculate energy for\n            \n        Returns:\n            Energy requirement\n        ",
          "code_snippet": "        return term1 * term2 * term3\n    \n    def energy_requirement(self, batch_size: int, p: int, L: float, frequency_band: str = \"medium\") -> float:\n        \"\"\"\n        Calculate energy requirement for given batch size, adjusted for frequency band.\n        \n        Based on E_total(n, p) = kT\u00b7ln(2) \u00b7 n \u00b7 [1 + \u03b1(p)log(n) + \u03b2(p)\u221aL]\n        \n        Args:\n            batch_size: Batch size (n)\n            p: Number of parallel processors\n            L: Cognitive load\n            frequency_band: Frequency band to calculate energy for\n            \n        Returns:\n            Energy requirement\n        \"\"\"\n        # Constants (normalized for implementation)\n        kT_ln2 = 1.0\n        \n        # Base calculation\n        alpha_p = self.alpha_w * (p - 1)\n        beta_p = self.alpha_par * math.sqrt(p)\n        \n        base_energy = kT_ln2 * batch_size * (1 + alpha_p * math.log(max(1.0, batch_size)) + beta_p * math.sqrt(L))\n        \n        # Adjust for frequency band\n        if frequency_band == \"ultra\":\n            return base_energy * 1.3  # Ultra-high frequency needs more energy\n        elif frequency_band == \"high\":\n            return base_energy * 1.1  # High frequency needs slightly more energy\n        elif frequency_band == \"low\":\n            return base_energy * 0.9  # Low frequency needs less energy\n        elif frequency_band == \"background\":\n            return base_energy * 0.7  # Background needs even less energy\n        else:\n            return base_energy  # Medium is baseline\n    \n    def _get_lane_energy_allocation(self, band: str, total_energy: float) -> float:\n        \"\"\"\n        Calculate energy allocation for a specific frequency lane."
        },
        "_get_lane_energy_allocation": {
          "start_line": 1121,
          "end_line": 1156,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "band",
              "type": "str"
            },
            {
              "name": "total_energy",
              "type": "float"
            }
          ],
          "return_type": "float",
          "calls": [
            {
              "name": "sum",
              "line": 1133
            },
            {
              "name": "self.band_layer_counts.values",
              "line": 1133
            },
            {
              "name": "self.band_layer_counts.get",
              "line": 1138
            },
            {
              "name": "self.observer_overhead.get",
              "line": 1154
            }
          ],
          "docstring": "\n        Calculate energy allocation for a specific frequency lane.\n        \n        Args:\n            band: Frequency band\n            total_energy: Total available energy\n            \n        Returns:\n            Energy allocation for the band\n        ",
          "code_snippet": "            return base_energy  # Medium is baseline\n    \n    def _get_lane_energy_allocation(self, band: str, total_energy: float) -> float:\n        \"\"\"\n        Calculate energy allocation for a specific frequency lane.\n        \n        Args:\n            band: Frequency band\n            total_energy: Total available energy\n            \n        Returns:\n            Energy allocation for the band\n        \"\"\"\n        # Count total layers to establish baseline\n        total_layers = sum(self.band_layer_counts.values())\n        if total_layers == 0:\n            return 0\n            \n        # Start with layer-count proportional allocation\n        layer_proportion = self.band_layer_counts.get(band, 0) / total_layers\n        \n        # Adjust based on frequency - higher frequencies need more energy\n        if band == \"ultra\":\n            freq_factor = 1.5\n        elif band == \"high\":\n            freq_factor = 1.2\n        elif band == \"medium\":\n            freq_factor = 1.0\n        elif band == \"low\":\n            freq_factor = 0.8\n        else:  # background\n            freq_factor = 0.5\n            \n        # Calculate adjusted allocation and compensate for observer overhead\n        raw_allocation = total_energy * layer_proportion * freq_factor\n        return raw_allocation / (1 - self.observer_overhead.get(band, 0))\n    \n    def cognitive_efficiency(self, E_available: float, batch_size: int, p: int, L: float, \n                           frequency_band: str = \"medium\") -> float:\n        \"\"\""
        },
        "cognitive_efficiency": {
          "start_line": 1156,
          "end_line": 1244,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "E_available",
              "type": "float"
            },
            {
              "name": "batch_size",
              "type": "int"
            },
            {
              "name": "p",
              "type": "int"
            },
            {
              "name": "L",
              "type": "float"
            },
            {
              "name": "frequency_band",
              "type": "str"
            }
          ],
          "return_type": "float",
          "calls": [
            {
              "name": "self._get_lane_energy_allocation",
              "line": 1174
            },
            {
              "name": "min",
              "line": 1187
            },
            {
              "name": "self.base_cognitive_efficiency",
              "line": 1194
            },
            {
              "name": "self.parallel_processing_penalty",
              "line": 1197
            },
            {
              "name": "self.energy_requirement",
              "line": 1178
            },
            {
              "name": "max",
              "line": 1187
            },
            {
              "name": "max",
              "line": 1209
            },
            {
              "name": "max",
              "line": 1210
            },
            {
              "name": "float",
              "line": 1238
            },
            {
              "name": "math.isnan",
              "line": 1179
            },
            {
              "name": "math.isinf",
              "line": 1179
            },
            {
              "name": "logger.warning",
              "line": 1180
            },
            {
              "name": "logger.error",
              "line": 1183
            },
            {
              "name": "self.observer_overhead.get",
              "line": 1187
            },
            {
              "name": "math.exp",
              "line": 1191
            },
            {
              "name": "math.isnan",
              "line": 1204
            },
            {
              "name": "math.isinf",
              "line": 1204
            },
            {
              "name": "logger.warning",
              "line": 1205
            },
            {
              "name": "math.exp",
              "line": 1216
            },
            {
              "name": "math.isnan",
              "line": 1234
            },
            {
              "name": "math.isinf",
              "line": 1234
            },
            {
              "name": "logger.warning",
              "line": 1235
            },
            {
              "name": "logger.error",
              "line": 1241
            },
            {
              "name": "math.exp",
              "line": 1219
            },
            {
              "name": "max",
              "line": 1191
            },
            {
              "name": "math.exp",
              "line": 1222
            },
            {
              "name": "str",
              "line": 1183
            },
            {
              "name": "math.exp",
              "line": 1225
            },
            {
              "name": "math.exp",
              "line": 1228
            },
            {
              "name": "str",
              "line": 1241
            },
            {
              "name": "abs",
              "line": 1219
            },
            {
              "name": "abs",
              "line": 1222
            },
            {
              "name": "abs",
              "line": 1225
            },
            {
              "name": "abs",
              "line": 1228
            }
          ],
          "docstring": "\n        Calculate cognitive efficiency with lane-specific adjustments.\n        \n        Implements \u03b7_cognitive(E_available, L) = 0.95 \u00b7 (1 - e^(-E_available/E_total(n,p))) \u00b7 \u03c6(L) \u00b7 \u03c8(p, L)\n        \n        Args:\n            E_available: Available energy\n            batch_size: Batch size (n)\n            p: Number of parallel processors\n            L: Cognitive load\n            frequency_band: Frequency band to calculate efficiency for\n            \n        Returns:\n            Cognitive efficiency\n        ",
          "code_snippet": "        return raw_allocation / (1 - self.observer_overhead.get(band, 0))\n    \n    def cognitive_efficiency(self, E_available: float, batch_size: int, p: int, L: float, \n                           frequency_band: str = \"medium\") -> float:\n        \"\"\"\n        Calculate cognitive efficiency with lane-specific adjustments.\n        \n        Implements \u03b7_cognitive(E_available, L) = 0.95 \u00b7 (1 - e^(-E_available/E_total(n,p))) \u00b7 \u03c6(L) \u00b7 \u03c8(p, L)\n        \n        Args:\n            E_available: Available energy\n            batch_size: Batch size (n)\n            p: Number of parallel processors\n            L: Cognitive load\n            frequency_band: Frequency band to calculate efficiency for\n            \n        Returns:\n            Cognitive efficiency\n        \"\"\"\n        # Adjust available energy for this frequency band\n        lane_energy = self._get_lane_energy_allocation(frequency_band, E_available)\n        \n        # Calculate energy requirement for this batch size (with safety check)\n        try:\n            E_total = self.energy_requirement(batch_size, p, L, frequency_band)\n            if math.isnan(E_total) or math.isinf(E_total) or E_total <= 0:\n                logger.warning(f\"Invalid energy requirement: {E_total}, using default value\")\n                E_total = 1.0  # Use safe default\n        except Exception as e:\n            logger.error(f\"Error calculating energy requirement: {str(e)}\")\n            E_total = 1.0  # Use safe default\n        \n        # Compensate for observer overhead (with safety)\n        overhead = min(0.95, max(0, self.observer_overhead.get(frequency_band, 0)))  # Keep between 0-0.95\n        adjusted_energy = lane_energy * (1 - overhead)\n        \n        # Energy utilization term (with safety for division)\n        energy_term = 0.95 * (1 - math.exp(-adjusted_energy / max(1e-6, E_total)))\n        \n        # Cognitive load efficiency\n        phi_term = self.base_cognitive_efficiency(L)\n        \n        # Parallel processing penalty\n        psi_term = self.parallel_processing_penalty(p, L)\n        \n        # Calculate base efficiency with safety checks\n        try:\n            efficiency = energy_term * phi_term * psi_term\n            \n            # Check for NaN or Inf\n            if math.isnan(efficiency) or math.isinf(efficiency):\n                logger.warning(f\"Invalid efficiency value detected: {efficiency}\")\n                return 0.5  # Return a moderate default value\n                \n            # Ensure batch_size is safe for calculations\n            safe_batch_size = max(1, batch_size)\n            safe_max_batch = max(1, self.max_batch)\n            \n            # Adjust efficiency based on natural frequency alignment\n            # Batch sizes have different efficiency impacts in different frequency bands\n            if frequency_band == \"ultra\":\n                # Ultra-high frequency prefers smaller batches\n                alignment_factor = math.exp(-safe_batch_size / (safe_max_batch * 0.1))\n            elif frequency_band == \"high\":\n                # High frequency prefers moderate-small batches\n                alignment_factor = math.exp(-abs(safe_batch_size - safe_max_batch * 0.2) / (safe_max_batch * 0.2))\n            elif frequency_band == \"medium\":\n                # Medium frequency prefers moderate batches\n                alignment_factor = math.exp(-abs(safe_batch_size - safe_max_batch * 0.4) / (safe_max_batch * 0.3))\n            elif frequency_band == \"low\":\n                # Low frequency prefers larger batches\n                alignment_factor = math.exp(-abs(safe_batch_size - safe_max_batch * 0.7) / (safe_max_batch * 0.3))\n            else:  # background\n                # Background frequency prefers maximum batches\n                alignment_factor = math.exp(-abs(safe_batch_size - safe_max_batch) / (safe_max_batch * 0.3))\n            \n            # Apply natural frequency alignment adjustment\n            final_efficiency = efficiency * alignment_factor**0.5\n            \n            # Check for NaN or Inf in final result\n            if math.isnan(final_efficiency) or math.isinf(final_efficiency):\n                logger.warning(f\"Invalid final efficiency value: {final_efficiency}\")\n                return 0.5  # Return a moderate default value\n                \n            return float(final_efficiency)\n            \n        except Exception as e:\n            logger.error(f\"Error in cognitive_efficiency calculation: {str(e)}\")\n            return 0.5  # Return a moderate default value\n    \n    def estimate_model_complexity(self) -> float:\n        \"\"\"\n        Estimate model complexity based on parameters and structure."
        },
        "estimate_model_complexity": {
          "start_line": 1244,
          "end_line": 1278,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": "float",
          "calls": [
            {
              "name": "self.model.modules",
              "line": 1257
            },
            {
              "name": "isinstance",
              "line": 1258
            },
            {
              "name": "math.log",
              "line": 1274
            },
            {
              "name": "isinstance",
              "line": 1260
            },
            {
              "name": "max",
              "line": 1274
            },
            {
              "name": "isinstance",
              "line": 1262
            },
            {
              "name": "....lower",
              "line": 1264
            },
            {
              "name": "str",
              "line": 1264
            }
          ],
          "docstring": "\n        Estimate model complexity based on parameters and structure.\n        \n        Returns:\n            Complexity factor (higher means more complex)\n        ",
          "code_snippet": "            return 0.5  # Return a moderate default value\n    \n    def estimate_model_complexity(self) -> float:\n        \"\"\"\n        Estimate model complexity based on parameters and structure.\n        \n        Returns:\n            Complexity factor (higher means more complex)\n        \"\"\"\n        # Count different layer types to better estimate complexity\n        conv_layers = 0\n        linear_layers = 0\n        norm_layers = 0\n        attention_layers = 0\n        \n        for m in self.model.modules():\n            if isinstance(m, nn.Conv2d):\n                conv_layers += 1\n            elif isinstance(m, nn.Linear):\n                linear_layers += 1\n            elif isinstance(m, (nn.BatchNorm2d, nn.LayerNorm)):\n                norm_layers += 1\n            elif \"attention\" in str(m.__class__).lower():\n                attention_layers += 1\n        \n        # Calculate architectural complexity\n        arch_complexity = (conv_layers * 1.5 + \n                          linear_layers * 1.0 + \n                          norm_layers * 0.5 + \n                          attention_layers * 3.0)\n        \n        # Parameter complexity (log scale to handle wide range of model sizes)\n        param_complexity = math.log(max(1, self.param_count)) / 16\n        \n        return param_complexity * (1 + arch_complexity / 50)\n    \n    def estimate_available_energy(self) -> float:\n        \"\"\"\n        Estimate available energy based on hardware."
        },
        "estimate_available_energy": {
          "start_line": 1278,
          "end_line": 1302,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": "float",
          "calls": [
            {
              "name": "math.sqrt",
              "line": 1291
            },
            {
              "name": "math.sqrt",
              "line": 1297
            }
          ],
          "docstring": "\n        Estimate available energy based on hardware.\n        \n        Returns:\n            Available energy (normalized)\n        ",
          "code_snippet": "        return param_complexity * (1 + arch_complexity / 50)\n    \n    def estimate_available_energy(self) -> float:\n        \"\"\"\n        Estimate available energy based on hardware.\n        \n        Returns:\n            Available energy (normalized)\n        \"\"\"\n        # Base energy estimation on hardware capabilities\n        if self.device.type == \"cuda\":\n            total_memory_gb = self.gpu_memory\n            \n            # Consider compute capability, memory, and multiprocessors\n            cc_factor = self.gpu_compute_capability[0] + self.gpu_compute_capability[1] / 10\n            mp_factor = math.sqrt(self.parallel_processors)\n            \n            # Adjusted formula based on diagnostic results\n            energy = total_memory_gb * cc_factor * mp_factor * 50\n        else:\n            # CPU energy estimation\n            proc_factor = math.sqrt(self.parallel_processors)\n            energy = 32 * proc_factor\n            \n        return energy  # No energy regime restriction\n    \n    def estimate_gradient_impact(self, frequency_band: str = \"medium\") -> float:\n        \"\"\"\n        Estimate the impact of gradient behavior on cognitive load."
        },
        "estimate_gradient_impact": {
          "start_line": 1302,
          "end_line": 1378,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "frequency_band",
              "type": "str"
            }
          ],
          "return_type": "float",
          "calls": [
            {
              "name": "len",
              "line": 1312
            },
            {
              "name": "len",
              "line": 1317
            },
            {
              "name": "math.sqrt",
              "line": 1321
            },
            {
              "name": "len",
              "line": 1327
            },
            {
              "name": "len",
              "line": 1329
            },
            {
              "name": "list",
              "line": 1330
            },
            {
              "name": "sum",
              "line": 1331
            },
            {
              "name": "sum",
              "line": 1332
            },
            {
              "name": "sum",
              "line": 1333
            },
            {
              "name": "sum",
              "line": 1334
            },
            {
              "name": "min",
              "line": 1368
            },
            {
              "name": "sum",
              "line": 1319
            },
            {
              "name": "len",
              "line": 1319
            },
            {
              "name": "sum",
              "line": 1320
            },
            {
              "name": "len",
              "line": 1320
            },
            {
              "name": "range",
              "line": 1330
            },
            {
              "name": "max",
              "line": 1368
            },
            {
              "name": "min",
              "line": 1370
            },
            {
              "name": "math.exp",
              "line": 1342
            },
            {
              "name": "math.exp",
              "line": 1342
            },
            {
              "name": "max",
              "line": 1370
            },
            {
              "name": "min",
              "line": 1372
            },
            {
              "name": "range",
              "line": 1333
            },
            {
              "name": "range",
              "line": 1334
            },
            {
              "name": "math.exp",
              "line": 1345
            },
            {
              "name": "math.exp",
              "line": 1345
            },
            {
              "name": "math.exp",
              "line": 1348
            },
            {
              "name": "math.exp",
              "line": 1348
            },
            {
              "name": "max",
              "line": 1372
            },
            {
              "name": "min",
              "line": 1374
            },
            {
              "name": "min",
              "line": 1376
            },
            {
              "name": "max",
              "line": 1374
            },
            {
              "name": "max",
              "line": 1376
            }
          ],
          "docstring": "\n        Estimate the impact of gradient behavior on cognitive load.\n        \n        Args:\n            frequency_band: Frequency band to calculate impact for\n            \n        Returns:\n            Gradient impact factor\n        ",
          "code_snippet": "        return energy  # No energy regime restriction\n    \n    def estimate_gradient_impact(self, frequency_band: str = \"medium\") -> float:\n        \"\"\"\n        Estimate the impact of gradient behavior on cognitive load.\n        \n        Args:\n            frequency_band: Frequency band to calculate impact for\n            \n        Returns:\n            Gradient impact factor\n        \"\"\"\n        if len(self.gradient_history) < 3:\n            return 1.0\n        \n        # Calculate gradient volatility (using last few iterations)\n        recent_gradients = self.gradient_history[-5:]\n        if len(recent_gradients) > 1:\n            # Calculate standard deviation manually to avoid numpy dependency\n            mean_gradient = sum(recent_gradients) / len(recent_gradients)\n            variance = sum((g - mean_gradient) ** 2 for g in recent_gradients) / len(recent_gradients)\n            std_dev = math.sqrt(variance)\n            volatility = std_dev / (mean_gradient + 1e-8)\n        else:\n            volatility = 0\n            \n        # Calculate gradient trend (increasing or decreasing)\n        if len(recent_gradients) >= 3:\n            # Simple linear regression slope calculated manually\n            n = len(recent_gradients)\n            x = list(range(n))\n            sum_x = sum(x)\n            sum_y = sum(recent_gradients)\n            sum_xy = sum(x[i] * recent_gradients[i] for i in range(n))\n            sum_x2 = sum(x[i] ** 2 for i in range(n))\n            \n            # Calculate slope: (n*sum_xy - sum_x*sum_y) / (n*sum_x2 - sum_x*sum_x)\n            slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x * sum_x)\n            \n            # Different frequency bands respond differently to gradient trends\n            if frequency_band == \"ultra\" or frequency_band == \"high\":\n                # Higher frequencies are more sensitive to gradient increases\n                trend_factor = math.exp(slope * 3) if slope < 0 else math.exp(slope * 1.5)\n            elif frequency_band == \"low\" or frequency_band == \"background\":\n                # Lower frequencies are less sensitive to gradient changes\n                trend_factor = math.exp(slope) if slope < 0 else math.exp(slope * 0.5)\n            else:\n                # Medium frequency - balanced response\n                trend_factor = math.exp(slope * 2) if slope < 0 else math.exp(slope)\n        else:\n            trend_factor = 1.0\n            \n        # Combine into impact factor - adjust volatility weight by frequency\n        if frequency_band == \"ultra\":\n            vol_weight = 0.3  # Most sensitive to volatility\n        elif frequency_band == \"high\":\n            vol_weight = 0.25\n        elif frequency_band == \"low\":\n            vol_weight = 0.15\n        elif frequency_band == \"background\":\n            vol_weight = 0.1  # Least sensitive to volatility\n        else:\n            vol_weight = 0.2  # Medium\n            \n        impact = (1.0 + vol_weight * volatility) * trend_factor\n        \n        # Clamp range - different ranges for different bands\n        if frequency_band == \"ultra\":\n            return min(4.0, max(0.3, impact))\n        elif frequency_band == \"high\":\n            return min(3.5, max(0.4, impact))\n        elif frequency_band == \"low\":\n            return min(2.5, max(0.6, impact))\n        elif frequency_band == \"background\":\n            return min(2.0, max(0.7, impact))\n        else:\n            return min(3.0, max(0.5, impact))\n    \n    def estimate_cognitive_load(self, batch_size: int, frequency_band: str = \"medium\") -> float:\n        \"\"\"\n        Estimate cognitive load based on batch size, model complexity,"
        },
        "estimate_cognitive_load": {
          "start_line": 1378,
          "end_line": 1450,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "batch_size",
              "type": "int"
            },
            {
              "name": "frequency_band",
              "type": "str"
            }
          ],
          "return_type": "float",
          "calls": [
            {
              "name": "self.estimate_model_complexity",
              "line": 1391
            },
            {
              "name": "max",
              "line": 1394
            },
            {
              "name": "math.log",
              "line": 1422
            },
            {
              "name": "self.estimate_gradient_impact",
              "line": 1433
            },
            {
              "name": "self.precision_impact",
              "line": 1428
            },
            {
              "name": "max",
              "line": 1438
            },
            {
              "name": "math.log",
              "line": 1404
            },
            {
              "name": "max",
              "line": 1441
            },
            {
              "name": "max",
              "line": 1444
            },
            {
              "name": "math.log",
              "line": 1408
            },
            {
              "name": "math.log",
              "line": 1412
            },
            {
              "name": "math.log",
              "line": 1419
            },
            {
              "name": "math.sqrt",
              "line": 1438
            },
            {
              "name": "math.log",
              "line": 1416
            },
            {
              "name": "math.sqrt",
              "line": 1441
            },
            {
              "name": "math.sqrt",
              "line": 1444
            },
            {
              "name": "max",
              "line": 1438
            },
            {
              "name": "max",
              "line": 1441
            },
            {
              "name": "max",
              "line": 1444
            }
          ],
          "docstring": "\n        Estimate cognitive load based on batch size, model complexity,\n        and current training state following framework principles.\n        \n        Args:\n            batch_size: Batch size\n            frequency_band: Frequency band to calculate load for\n            \n        Returns:\n            Estimated cognitive load\n        ",
          "code_snippet": "            return min(3.0, max(0.5, impact))\n    \n    def estimate_cognitive_load(self, batch_size: int, frequency_band: str = \"medium\") -> float:\n        \"\"\"\n        Estimate cognitive load based on batch size, model complexity,\n        and current training state following framework principles.\n        \n        Args:\n            batch_size: Batch size\n            frequency_band: Frequency band to calculate load for\n            \n        Returns:\n            Estimated cognitive load\n        \"\"\"\n        # Get base model complexity\n        complexity = self.estimate_model_complexity()\n        \n        # Ensure batch_size is safe for logarithm\n        safe_batch_size = max(1, batch_size)\n        \n        # Calculate the median batch size from min/max for reference\n        median_batch = (self.min_batch + self.max_batch) / 2 if self.min_batch and self.max_batch else 64\n        \n        # Adjust batch factor based on frequency band - now accounts for batch size range\n        if frequency_band == \"ultra\":\n            # Ultra-high frequency is more sensitive to batch size\n            # Prefer smaller batches - increase penalty for larger ones\n            normalized_size = safe_batch_size / median_batch\n            batch_factor = math.log(safe_batch_size) / 1.5 * normalized_size**0.3\n        elif frequency_band == \"high\":\n            # High frequency prefers moderate-small batches\n            normalized_size = safe_batch_size / median_batch \n            batch_factor = math.log(safe_batch_size) / 1.8 * normalized_size**0.2\n        elif frequency_band == \"low\":\n            # Low frequency prefers larger batches\n            normalized_size = median_batch / safe_batch_size if safe_batch_size > 0 else 1.0\n            batch_factor = math.log(safe_batch_size) / 2.2 * normalized_size**0.2\n        elif frequency_band == \"background\":\n            # Background frequency strongly prefers large batches\n            normalized_size = median_batch / safe_batch_size if safe_batch_size > 0 else 1.0\n            batch_factor = math.log(safe_batch_size) / 2.5 * normalized_size**0.3\n        else:\n            # Medium frequency - balanced response\n            batch_factor = math.log(safe_batch_size) / 2\n        \n        # Parameter count contribution\n        param_factor = math.log(self.param_count) / 20\n        \n        # Calculate base cognitive load\n        base_load = param_factor * batch_factor * complexity\n        \n        # Adjust for numerical precision impact\n        precision_factor = 1.0 + 0.5 * self.precision_impact(batch_size, frequency_band)\n        \n        # Calculate final cognitive load\n        if self.iteration > 2:\n            # Include gradient behavior in cognitive load\n            gradient_factor = self.estimate_gradient_impact(frequency_band)\n            \n            # Training progress impact varies by frequency band\n            if frequency_band == \"ultra\" or frequency_band == \"high\":\n                # Higher frequencies benefit more from training progress\n                progress_factor = max(0.6, 1.0 - 0.2 * math.sqrt(max(0.1, self.epoch) / 10))\n            elif frequency_band == \"low\" or frequency_band == \"background\":\n                # Lower frequencies benefit less from training progress\n                progress_factor = max(0.8, 1.0 - 0.1 * math.sqrt(max(0.1, self.epoch) / 10))\n            else:\n                # Medium frequency - balanced benefit\n                progress_factor = max(0.7, 1.0 - 0.15 * math.sqrt(max(0.1, self.epoch) / 10))\n            \n            return base_load * precision_factor * gradient_factor * progress_factor\n        else:\n            return base_load * precision_factor\n    \n    def precision_impact(self, batch_size: int, frequency_band: str = \"medium\") -> float:\n        \"\"\"\n        Calculate the impact of batch size on numerical precision."
        },
        "precision_impact": {
          "start_line": 1450,
          "end_line": 1494,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "batch_size",
              "type": "int"
            },
            {
              "name": "frequency_band",
              "type": "str"
            }
          ],
          "return_type": "float",
          "calls": [
            {
              "name": "max",
              "line": 1485
            },
            {
              "name": "max",
              "line": 1464
            },
            {
              "name": "math.exp",
              "line": 1486
            },
            {
              "name": "math.exp",
              "line": 1489
            }
          ],
          "docstring": "\n        Calculate the impact of batch size on numerical precision.\n        \n        Based on N_eff(P) \u2248 N(1 - e^(-P/\u03b3))\u00b3\n        \n        Args:\n            batch_size: Batch size\n            frequency_band: Frequency band to calculate impact for\n            \n        Returns:\n            Effective precision factor\n        ",
          "code_snippet": "            return base_load * precision_factor\n    \n    def precision_impact(self, batch_size: int, frequency_band: str = \"medium\") -> float:\n        \"\"\"\n        Calculate the impact of batch size on numerical precision.\n        \n        Based on N_eff(P) \u2248 N(1 - e^(-P/\u03b3))\u00b3\n        \n        Args:\n            batch_size: Batch size\n            frequency_band: Frequency band to calculate impact for\n            \n        Returns:\n            Effective precision factor\n        \"\"\"\n        # Normalize batch size relative to max\n        norm_batch = batch_size / max(1, self.max_batch)\n        \n        # Adjust sensitivity parameters based on frequency band\n        if frequency_band == \"ultra\":\n            # Ultra-high frequency is more sensitive to precision\n            gamma_w_adj = self.gamma_w * 0.8\n            gamma_a_adj = self.gamma_a * 0.8\n        elif frequency_band == \"high\":\n            gamma_w_adj = self.gamma_w * 0.9\n            gamma_a_adj = self.gamma_a * 0.9\n        elif frequency_band == \"low\":\n            gamma_w_adj = self.gamma_w * 1.1\n            gamma_a_adj = self.gamma_a * 1.1\n        elif frequency_band == \"background\":\n            gamma_w_adj = self.gamma_w * 1.2\n            gamma_a_adj = self.gamma_a * 1.2\n        else:  # medium\n            gamma_w_adj = self.gamma_w\n            gamma_a_adj = self.gamma_a\n        \n        # Calculate weight precision impact (with safety checks)\n        safe_norm_batch = max(0.001, norm_batch)  # Prevent division by zero\n        weight_precision = (1 - math.exp(-1/safe_norm_batch/gamma_w_adj)) ** 3\n        \n        # Calculate activation precision impact\n        activation_precision = (1 - math.exp(-1/safe_norm_batch/gamma_a_adj)) ** 3\n        \n        # Combined precision impact\n        return self.alpha_w * weight_precision + self.alpha_a * activation_precision\n    \n    def update_training_state(self, loss: float, gradient_norm: float, accuracy: Optional[float] = None):\n        \"\"\"\n        Update internal state based on current training metrics."
        },
        "update_training_state": {
          "start_line": 1494,
          "end_line": 1541,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "loss",
              "type": "float"
            },
            {
              "name": "gradient_norm",
              "type": "float"
            },
            {
              "name": "accuracy"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.loss_history.append",
              "line": 1504
            },
            {
              "name": "self.gradient_history.append",
              "line": 1507
            },
            {
              "name": "len",
              "line": 1508
            },
            {
              "name": "self.gradient_history.pop",
              "line": 1509
            },
            {
              "name": "self.accuracy_history.append",
              "line": 1516
            },
            {
              "name": "len",
              "line": 1528
            },
            {
              "name": "....append",
              "line": 1536
            },
            {
              "name": "logger.debug",
              "line": 1539
            },
            {
              "name": "len",
              "line": 1524
            }
          ],
          "docstring": "\n        Update internal state based on current training metrics.\n        \n        Args:\n            loss: Current loss value\n            gradient_norm: Norm of current gradients\n            accuracy: Current accuracy (optional, but needed for batch adjustment)\n        ",
          "code_snippet": "        return self.alpha_w * weight_precision + self.alpha_a * activation_precision\n    \n    def update_training_state(self, loss: float, gradient_norm: float, accuracy: Optional[float] = None):\n        \"\"\"\n        Update internal state based on current training metrics.\n        \n        Args:\n            loss: Current loss value\n            gradient_norm: Norm of current gradients\n            accuracy: Current accuracy (optional, but needed for batch adjustment)\n        \"\"\"\n        # Track loss history\n        self.loss_history.append(loss)\n        \n        # Track gradient history\n        self.gradient_history.append(gradient_norm)\n        if len(self.gradient_history) > 20:\n            self.gradient_history.pop(0)\n        \n        self.last_gradient_norm = gradient_norm\n        self.iteration += 1\n        \n        # Track accuracy if provided\n        if accuracy is not None:\n            self.accuracy_history.append(accuracy)\n            \n            # Update best accuracy seen\n            if accuracy > self.best_accuracy:\n                self.best_accuracy = accuracy\n                self.best_batch_size = self.selected_batch_size\n            \n            # Keep only the window of accuracy values we need for trend analysis\n            if len(self.accuracy_history) > self.accuracy_window * 2:  # Keep twice the window for context\n                self.accuracy_history = self.accuracy_history[-self.accuracy_window * 2:]\n        \n        # Update batch size / efficiency history\n        if len(self.batch_history) > 0:\n            last_batch = self.batch_history[-1]\n            if last_batch not in self.efficiency_history:\n                self.efficiency_history[last_batch] = []\n            \n            # Use normalized loss as a proxy for efficiency (scale from 0-1 rather than unbounded)\n            # A high inverse_loss means better efficiency\n            normalized_efficiency = 1.0 / (10.0 * loss + 1.0)  # Scales to 0-1 range better than raw inverse\n            self.efficiency_history[last_batch].append(normalized_efficiency)\n            \n            # Log this so we can see actual efficiency values\n            logger.debug(f\"Batch size {last_batch} iteration efficiency: {normalized_efficiency:.4f} (loss: {loss:.4f})\")\n    \n    def get_sweet_spot_batch(self) -> int:\n        \"\"\"\n        Find the batch size with the best historical efficiency."
        },
        "get_sweet_spot_batch": {
          "start_line": 1541,
          "end_line": 1568,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": "int",
          "calls": [
            {
              "name": "self.efficiency_history.items",
              "line": 1553
            },
            {
              "name": "max",
              "line": 1549
            },
            {
              "name": "max",
              "line": 1558
            },
            {
              "name": "max",
              "line": 1563
            },
            {
              "name": "len",
              "line": 1554
            },
            {
              "name": "max",
              "line": 1562
            },
            {
              "name": "int",
              "line": 1563
            },
            {
              "name": "logger.warning",
              "line": 1565
            },
            {
              "name": "max",
              "line": 1566
            },
            {
              "name": "sum",
              "line": 1555
            },
            {
              "name": "len",
              "line": 1555
            },
            {
              "name": "avg_efficiency.items",
              "line": 1562
            },
            {
              "name": "str",
              "line": 1565
            }
          ],
          "docstring": "\n        Find the batch size with the best historical efficiency.\n        \n        Returns:\n            Best historical batch size (or default minimum if no history)\n        ",
          "code_snippet": "            logger.debug(f\"Batch size {last_batch} iteration efficiency: {normalized_efficiency:.4f} (loss: {loss:.4f})\")\n    \n    def get_sweet_spot_batch(self) -> int:\n        \"\"\"\n        Find the batch size with the best historical efficiency.\n        \n        Returns:\n            Best historical batch size (or default minimum if no history)\n        \"\"\"\n        if not self.efficiency_history:\n            return max(16, self.min_batch)  # Return a safe default instead of 0\n            \n        # Calculate average efficiency for each batch size\n        avg_efficiency = {}\n        for batch, efficiencies in self.efficiency_history.items():\n            if len(efficiencies) >= 2:  # Need multiple points for reliability\n                avg_efficiency[batch] = sum(efficiencies) / len(efficiencies)\n        \n        if not avg_efficiency:\n            return max(16, self.min_batch)  # Return a safe default instead of 0\n            \n        # Return batch size with highest average efficiency\n        try:\n            best_batch = max(avg_efficiency.items(), key=lambda x: x[1])[0]\n            return max(1, int(best_batch))  # Ensure it's a positive integer\n        except Exception as e:\n            logger.warning(f\"Error finding sweet spot batch: {str(e)}\")\n            return max(16, self.min_batch)  # Return a safe default\n    \n    def _select_lane_for_batch(self, batch_size: int) -> None:\n        \"\"\"\n        Select the appropriate frequency lane for a given batch size."
        },
        "_select_lane_for_batch": {
          "start_line": 1568,
          "end_line": 1610,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "batch_size",
              "type": "int"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "max",
              "line": 1584
            },
            {
              "name": "max",
              "line": 1585
            },
            {
              "name": "logger.info",
              "line": 1607
            },
            {
              "name": "new_lane.upper",
              "line": 1607
            }
          ],
          "docstring": "\n        Select the appropriate frequency lane for a given batch size.\n        \n        Args:\n            batch_size: Current batch size\n        ",
          "code_snippet": "            return max(16, self.min_batch)  # Return a safe default\n    \n    def _select_lane_for_batch(self, batch_size: int) -> None:\n        \"\"\"\n        Select the appropriate frequency lane for a given batch size.\n        \n        Args:\n            batch_size: Current batch size\n        \"\"\"\n        # Determine which lane is most appropriate for this batch size\n        # Different lanes prefer different batch sizes\n        # - Ultra frequency prefers very small batch sizes\n        # - High frequency prefers small-to-medium batch sizes\n        # - Medium frequency prefers medium batch sizes\n        # - Low frequency prefers medium-to-large batch sizes\n        # - Background frequency prefers large batch sizes\n        \n        # Calculate relative position in batch size range\n        min_batch = max(1, self.min_batch if self.min_batch is not None else 1)\n        max_batch = max(min_batch + 1, self.max_batch if self.max_batch is not None else 512)\n        \n        # Calculate where in the range this batch size falls (0.0 to 1.0)\n        if max_batch == min_batch:  # Avoid division by zero\n            position = 0.5\n        else:\n            position = (batch_size - min_batch) / (max_batch - min_batch)\n        \n        # Select lane based on position in range\n        if position < 0.15:\n            new_lane = \"ultra\"  # Very small batch sizes\n        elif position < 0.35:\n            new_lane = \"high\"   # Small-to-medium batch sizes\n        elif position < 0.65:\n            new_lane = \"medium\" # Medium batch sizes \n        elif position < 0.85:\n            new_lane = \"low\"    # Medium-to-large batch sizes\n        else:\n            new_lane = \"background\"  # Large batch sizes\n            \n        # Log change if we're changing lanes\n        if new_lane != self.selected_lane:\n            logger.info(f\"Switching to {new_lane.upper()} frequency lane for batch size {batch_size}\")\n            self.selected_lane = new_lane\n            \n    def _select_optimal_lane(self, band_energies: Dict[str, float]) -> None:\n        \"\"\"\n        Select the optimal frequency lane based on model and hardware characteristics."
        },
        "_select_optimal_lane": {
          "start_line": 1610,
          "end_line": 1704,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "band_energies"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logger.info",
              "line": 1698
            },
            {
              "name": "logger.warning",
              "line": 1702
            },
            {
              "name": "self.band_layer_counts.get",
              "line": 1627
            },
            {
              "name": "self.estimate_model_complexity",
              "line": 1660
            },
            {
              "name": "logger.debug",
              "line": 1692
            },
            {
              "name": "max",
              "line": 1697
            },
            {
              "name": "max",
              "line": 1629
            },
            {
              "name": "band_energies.get",
              "line": 1630
            },
            {
              "name": "max",
              "line": 1630
            },
            {
              "name": "lane_scores.items",
              "line": 1697
            },
            {
              "name": "sum",
              "line": 1629
            },
            {
              "name": "sum",
              "line": 1630
            },
            {
              "name": "self.band_layer_counts.values",
              "line": 1629
            },
            {
              "name": "band_energies.values",
              "line": 1630
            }
          ],
          "docstring": "\n        Select the optimal frequency lane based on model and hardware characteristics.\n        \n        Args:\n            band_energies: Dictionary of energy allocations for each frequency band\n        ",
          "code_snippet": "            self.selected_lane = new_lane\n            \n    def _select_optimal_lane(self, band_energies: Dict[str, float]) -> None:\n        \"\"\"\n        Select the optimal frequency lane based on model and hardware characteristics.\n        \n        Args:\n            band_energies: Dictionary of energy allocations for each frequency band\n        \"\"\"\n        # Start with a scoring system for each lane\n        lane_scores = {}\n        \n        # Consider several factors:\n        # 1. Layer count in each band (more layers = higher preference)\n        # 2. Energy allocation (more energy = higher preference)\n        # 3. Hardware characteristics (GPU favors higher frequencies, CPU lower)\n        # 4. Model complexity (complex models favor lower frequencies)\n        \n        for band in self.frequency_bands:\n            if self.band_layer_counts.get(band, 0) > 0:\n                # Calculate a score\n                layer_factor = self.band_layer_counts[band] / max(1, sum(self.band_layer_counts.values()))\n                energy_factor = band_energies.get(band, 0) / max(1, sum(band_energies.values()))\n                \n                # Hardware factor - GPUs handle high frequencies better\n                hw_factor = 1.0\n                if self.device.type == \"cuda\":\n                    # GPU: favor higher frequencies\n                    if band == \"ultra\":\n                        hw_factor = 1.4\n                    elif band == \"high\":\n                        hw_factor = 1.3\n                    elif band == \"medium\":\n                        hw_factor = 1.1\n                    elif band == \"low\":\n                        hw_factor = 0.8\n                    else:  # background\n                        hw_factor = 0.6\n                else:\n                    # CPU: favor lower frequencies\n                    if band == \"ultra\":\n                        hw_factor = 0.6\n                    elif band == \"high\":\n                        hw_factor = 0.8\n                    elif band == \"medium\":\n                        hw_factor = 1.1\n                    elif band == \"low\":\n                        hw_factor = 1.3\n                    else:  # background\n                        hw_factor = 1.4\n                \n                # Model complexity factor\n                complexity = self.estimate_model_complexity()\n                complexity_factor = 1.0\n                if complexity > 2.0:  # More complex model\n                    # Complex models do better with lower frequencies\n                    if band == \"ultra\":\n                        complexity_factor = 0.7\n                    elif band == \"high\":\n                        complexity_factor = 0.8\n                    elif band == \"medium\":\n                        complexity_factor = 1.0\n                    elif band == \"low\":\n                        complexity_factor = 1.2\n                    else:  # background\n                        complexity_factor = 1.3\n                else:  # Simpler model\n                    # Simpler models do better with higher frequencies\n                    if band == \"ultra\":\n                        complexity_factor = 1.2\n                    elif band == \"high\":\n                        complexity_factor = 1.1\n                    elif band == \"medium\":\n                        complexity_factor = 1.0\n                    elif band == \"low\":\n                        complexity_factor = 0.8\n                    else:  # background\n                        complexity_factor = 0.7\n                \n                # Calculate final score\n                score = layer_factor * energy_factor * hw_factor * complexity_factor\n                lane_scores[band] = score\n                \n                # Log the score components for debugging\n                logger.debug(f\"Lane {band} score: {score:.4f} = {layer_factor:.2f} (layers) * {energy_factor:.2f} (energy) * {hw_factor:.2f} (hw) * {complexity_factor:.2f} (complexity)\")\n        \n        # Select the lane with the highest score\n        if lane_scores:\n            # Get the highest scoring lane\n            self.selected_lane = max(lane_scores.items(), key=lambda x: x[1])[0]\n            logger.info(f\"Selected lane {self.selected_lane} with score {lane_scores[self.selected_lane]:.4f}\")\n        else:\n            # Fallback to medium if no scores were calculated\n            self.selected_lane = \"medium\"\n            logger.warning(\"No lane scores calculated, defaulting to medium frequency lane\")\n    \n    def get_optimal_batch_size(self) -> int:\n        \"\"\"\n        Calculate the optimal batch size using the resonance zone approach."
        },
        "get_optimal_batch_size": {
          "start_line": 1704,
          "end_line": 2158,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": "int",
          "calls": [
            {
              "name": "max",
              "line": 1766
            },
            {
              "name": "self.estimate_available_energy",
              "line": 1925
            },
            {
              "name": "self.estimate_available_energy",
              "line": 1993
            },
            {
              "name": "self.band_layer_counts.items",
              "line": 2026
            },
            {
              "name": "logger.debug",
              "line": 2033
            },
            {
              "name": "self.batch_history.append",
              "line": 2127
            },
            {
              "name": "hasattr",
              "line": 1712
            },
            {
              "name": "hasattr",
              "line": 1712
            },
            {
              "name": "len",
              "line": 1777
            },
            {
              "name": "min",
              "line": 1856
            },
            {
              "name": "logger.info",
              "line": 1857
            },
            {
              "name": "logger.info",
              "line": 1895
            },
            {
              "name": "self.get_sweet_spot_batch",
              "line": 1932
            },
            {
              "name": "min",
              "line": 1952
            },
            {
              "name": "sorted",
              "line": 1972
            },
            {
              "name": "logger.info",
              "line": 1986
            },
            {
              "name": "list",
              "line": 2068
            },
            {
              "name": "min",
              "line": 2069
            },
            {
              "name": "max",
              "line": 2070
            },
            {
              "name": "self.estimate_model_complexity",
              "line": 2088
            },
            {
              "name": "min",
              "line": 2095
            },
            {
              "name": "logger.debug",
              "line": 2100
            },
            {
              "name": "max",
              "line": 2105
            },
            {
              "name": "max",
              "line": 2106
            },
            {
              "name": "logger.info",
              "line": 2108
            },
            {
              "name": "logger.warning",
              "line": 2113
            },
            {
              "name": "logger.info",
              "line": 2123
            },
            {
              "name": "logger.warning",
              "line": 2131
            },
            {
              "name": "logger.info",
              "line": 2146
            },
            {
              "name": "logger.info",
              "line": 1720
            },
            {
              "name": "len",
              "line": 1783
            },
            {
              "name": "list",
              "line": 1785
            },
            {
              "name": "len",
              "line": 1788
            },
            {
              "name": "sum",
              "line": 1789
            },
            {
              "name": "sum",
              "line": 1790
            },
            {
              "name": "sum",
              "line": 1791
            },
            {
              "name": "sum",
              "line": 1792
            },
            {
              "name": "abs",
              "line": 1802
            },
            {
              "name": "max",
              "line": 1807
            },
            {
              "name": "max",
              "line": 1856
            },
            {
              "name": "self._select_lane_for_batch",
              "line": 1860
            },
            {
              "name": "max",
              "line": 1863
            },
            {
              "name": "logger.warning",
              "line": 1873
            },
            {
              "name": "logger.info",
              "line": 1902
            },
            {
              "name": "sorted",
              "line": 1917
            },
            {
              "name": "logger.info",
              "line": 1921
            },
            {
              "name": "max",
              "line": 1947
            },
            {
              "name": "max",
              "line": 1948
            },
            {
              "name": "max",
              "line": 1952
            },
            {
              "name": "list",
              "line": 1956
            },
            {
              "name": "range",
              "line": 1960
            },
            {
              "name": "list",
              "line": 1972
            },
            {
              "name": "logger.warning",
              "line": 1975
            },
            {
              "name": "hasattr",
              "line": 1984
            },
            {
              "name": "logger.info",
              "line": 2014
            },
            {
              "name": "active_bands.append",
              "line": 2028
            },
            {
              "name": "self.estimate_cognitive_load",
              "line": 2043
            },
            {
              "name": "self.cognitive_efficiency",
              "line": 2047
            },
            {
              "name": "logger.debug",
              "line": 2050
            },
            {
              "name": "....values",
              "line": 2068
            },
            {
              "name": "sum",
              "line": 2071
            },
            {
              "name": "len",
              "line": 2071
            },
            {
              "name": "max",
              "line": 2095
            },
            {
              "name": "resonance_values.items",
              "line": 2105
            },
            {
              "name": "....values",
              "line": 2106
            },
            {
              "name": "logger.warning",
              "line": 2118
            },
            {
              "name": "max",
              "line": 2119
            },
            {
              "name": "isinstance",
              "line": 2130
            },
            {
              "name": "logger.info",
              "line": 2136
            },
            {
              "name": "logger.info",
              "line": 2140
            },
            {
              "name": "hasattr",
              "line": 2145
            },
            {
              "name": "logger.info",
              "line": 1724
            },
            {
              "name": "logger.info",
              "line": 1728
            },
            {
              "name": "logger.info",
              "line": 1751
            },
            {
              "name": "max",
              "line": 1754
            },
            {
              "name": "logger.info",
              "line": 1755
            },
            {
              "name": "self._select_lane_for_batch",
              "line": 1761
            },
            {
              "name": "range",
              "line": 1785
            },
            {
              "name": "int",
              "line": 1807
            },
            {
              "name": "logger.info",
              "line": 1821
            },
            {
              "name": "int",
              "line": 1863
            },
            {
              "name": "logger.info",
              "line": 1865
            },
            {
              "name": "max",
              "line": 1913
            },
            {
              "name": "list",
              "line": 1917
            },
            {
              "name": "self.batch_history.append",
              "line": 1939
            },
            {
              "name": "logger.info",
              "line": 1940
            },
            {
              "name": "range",
              "line": 1956
            },
            {
              "name": "batch_sizes.append",
              "line": 1967
            },
            {
              "name": "batch_sizes.append",
              "line": 1969
            },
            {
              "name": "set",
              "line": 1972
            },
            {
              "name": "filtered_batch_sizes.append",
              "line": 2007
            },
            {
              "name": "filtered_batch_sizes.append",
              "line": 2010
            },
            {
              "name": "len",
              "line": 2033
            },
            {
              "name": "max",
              "line": 2084
            },
            {
              "name": "min",
              "line": 2119
            },
            {
              "name": "len",
              "line": 1785
            },
            {
              "name": "max",
              "line": 1813
            },
            {
              "name": "logger.info",
              "line": 1814
            },
            {
              "name": "self._select_lane_for_batch",
              "line": 1825
            },
            {
              "name": "max",
              "line": 1830
            },
            {
              "name": "self._select_lane_for_batch",
              "line": 1869
            },
            {
              "name": "set",
              "line": 1917
            },
            {
              "name": "len",
              "line": 1920
            },
            {
              "name": "int",
              "line": 1961
            },
            {
              "name": "batch_sizes.append",
              "line": 1963
            },
            {
              "name": "self.selected_lane.upper",
              "line": 1985
            },
            {
              "name": "len",
              "line": 1986
            },
            {
              "name": "efficiency_matrix.get",
              "line": 2106
            },
            {
              "name": "len",
              "line": 2111
            },
            {
              "name": "int",
              "line": 1746
            },
            {
              "name": "zip",
              "line": 1791
            },
            {
              "name": "min",
              "line": 1817
            },
            {
              "name": "logger.info",
              "line": 1818
            },
            {
              "name": "min",
              "line": 1830
            },
            {
              "name": "max",
              "line": 1835
            },
            {
              "name": "logger.info",
              "line": 1836
            },
            {
              "name": "logger.info",
              "line": 1839
            },
            {
              "name": "logger.info",
              "line": 1847
            },
            {
              "name": "len",
              "line": 1921
            },
            {
              "name": "abs",
              "line": 1936
            },
            {
              "name": "str",
              "line": 1975
            },
            {
              "name": "band.upper",
              "line": 2050
            },
            {
              "name": "random.random",
              "line": 1829
            },
            {
              "name": "int",
              "line": 1830
            },
            {
              "name": "min",
              "line": 1835
            },
            {
              "name": "self._select_lane_for_batch",
              "line": 1843
            },
            {
              "name": "self._select_lane_for_batch",
              "line": 1851
            },
            {
              "name": "hash",
              "line": 1936
            },
            {
              "name": "len",
              "line": 1921
            },
            {
              "name": "str",
              "line": 1936
            },
            {
              "name": "max",
              "line": 2146
            }
          ],
          "docstring": "\n        Calculate the optimal batch size using the resonance zone approach.\n        \n        Returns:\n            Current optimal batch size\n        ",
          "code_snippet": "            logger.warning(\"No lane scores calculated, defaulting to medium frequency lane\")\n    \n    def get_optimal_batch_size(self) -> int:\n        \"\"\"\n        Calculate the optimal batch size using the resonance zone approach.\n        \n        Returns:\n            Current optimal batch size\n        \"\"\"\n        # Use cached batch size if available for this epoch\n        if hasattr(self, '_cached_batch_size') and hasattr(self, '_last_batch_calc_epoch') and self._last_batch_calc_epoch == self.epoch:\n            return self._cached_batch_size\n        \n        # If no batch size selected yet, start with the middle of the resonance zone if available\n        if self.selected_batch_size is None:\n            if self.resonance_zone[0] > 0 and self.resonance_zone[1] > 0:\n                # Use the middle of the resonance zone\n                self.selected_batch_size = (self.resonance_zone[0] + self.resonance_zone[1]) // 2\n                logger.info(f\"Starting with batch size {self.selected_batch_size} (from resonance zone)\")\n            elif self.max_batch is not None:\n                # Fallback to the max batch if resonance zone isn't available\n                self.selected_batch_size = self.max_batch\n                logger.info(f\"Starting with batch size {self.selected_batch_size} (maximum available)\")\n            else:\n                # Final fallback to a decent default\n                self.selected_batch_size = 64\n                logger.info(f\"Starting with batch size {self.selected_batch_size} (default)\")\n            \n            # Mark that we haven't set the initial halved batch size yet\n            self.initial_batch_size_set = False\n            \n        # Special case for the first epoch - use half the maximum batch size if we haven't set it yet\n        if self.epoch == 0:\n            if not self.initial_batch_size_set:\n                original_batch = self.selected_batch_size\n                \n                # Make sure we stay within the resonance zone if available\n                if self.resonance_zone[0] > 0 and self.resonance_zone[1] > 0:\n                    half_size = original_batch // 2\n                    # Calculate a balanced first epoch batch size within the resonance zone\n                    # Use a weighted approach that favors the middle of the zone\n                    zone_width = self.resonance_zone[1] - self.resonance_zone[0]\n                    if zone_width > 0:\n                        # Use ~40% up from min for typical zones (favors midpoint, not min)\n                        target_point = self.resonance_zone[0] + int(zone_width * 0.4)\n                        self.selected_batch_size = target_point\n                    else:\n                        # For single-point zones, use that point\n                        self.selected_batch_size = self.resonance_zone[0]\n                    logger.info(f\"First epoch: using resonance-aligned batch size {self.selected_batch_size}\")\n                else:\n                    # Without resonance zone, just use half the original\n                    self.selected_batch_size = max(self.min_batch, original_batch // 2)\n                    logger.info(f\"First epoch: using half of maximum batch size ({original_batch} \u2192 {self.selected_batch_size})\")\n                \n                self.initial_batch_size_set = True\n                \n                # Select appropriate lane for this batch size\n                if self.use_lanes:\n                    self._select_lane_for_batch(self.selected_batch_size)\n            \n            return self.selected_batch_size\n            \n        # Determine training phase based on current epoch / total epochs\n        total = max(self.total_epochs, 1)  # Prevent division by zero\n        progress = self.epoch / total\n        \n        if progress < 0.2:\n            phase = \"early\"\n        elif progress < 0.8:\n            phase = \"middle\"\n        else:\n            phase = \"final\"\n            \n        # Check if we need to adjust batch size based on accuracy trends\n        if len(self.accuracy_history) >= self.accuracy_window:\n            # Get the window of recent accuracies\n            recent_accuracies = self.accuracy_history[-self.accuracy_window:]\n            \n            # Check for accuracy plateau or decrease\n            accuracy_trend = 0.0\n            if len(recent_accuracies) >= 3:\n                # Simple linear regression to detect trend\n                x = list(range(len(recent_accuracies)))\n                y = recent_accuracies\n                \n                n = len(x)\n                sum_x = sum(x)\n                sum_y = sum(y)\n                sum_xy = sum(xi * yi for xi, yi in zip(x, y))\n                sum_x2 = sum(xi * xi for xi in x)\n                \n                # Calculate slope for trend\n                try:\n                    slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x * sum_x)\n                    accuracy_trend = slope * 100  # Convert to percentage points per epoch\n                except ZeroDivisionError:\n                    accuracy_trend = 0.0\n            \n            # Detect if we're plateauing or decreasing\n            is_plateauing = abs(accuracy_trend) < self.accuracy_threshold\n            is_decreasing = accuracy_trend < -self.accuracy_threshold\n            \n            if is_decreasing:\n                # Significant accuracy decrease - adjust batch size down\n                new_batch = max(self.min_batch, int(self.selected_batch_size * self.batch_adjustment_factor))\n                \n                # Make sure we stay within the resonance zone if available\n                if self.resonance_zone[0] > 0 and self.resonance_zone[1] > 0:\n                    if new_batch < self.resonance_zone[0]:\n                        # If we would go below the resonance zone, stay at the minimum of the zone\n                        new_batch = max(new_batch, self.resonance_zone[0])\n                        logger.info(f\"Adjustment constrained by resonance zone minimum: {self.resonance_zone[0]}\")\n                    elif new_batch > self.resonance_zone[1]:\n                        # If we would go above the resonance zone, stay at the maximum of the zone\n                        new_batch = min(new_batch, self.resonance_zone[1])\n                        logger.info(f\"Adjustment constrained by resonance zone maximum: {self.resonance_zone[1]}\")\n                \n                if new_batch != self.selected_batch_size:\n                    logger.info(f\"Accuracy decreasing ({accuracy_trend:.4f}% per epoch) - reducing batch size from {self.selected_batch_size} to {new_batch}\")\n                    self.selected_batch_size = new_batch\n                    # Update lane for new batch size if using lanes\n                    if self.use_lanes:\n                        self._select_lane_for_batch(self.selected_batch_size)\n            elif is_plateauing and phase == \"middle\":\n                # Stagnant progress in middle phase - try slight batch adjustment\n                if self.epoch % 3 == 0:  # Don't adjust too frequently\n                    adjustment = self.batch_adjustment_factor if random.random() < 0.7 else 1/self.batch_adjustment_factor\n                    new_batch = max(self.min_batch, min(self.max_batch, int(self.selected_batch_size * adjustment)))\n                    \n                    # Make sure we stay within the resonance zone if available\n                    if self.resonance_zone[0] > 0 and self.resonance_zone[1] > 0:\n                        # Constrain to resonance zone\n                        new_batch = max(self.resonance_zone[0], min(self.resonance_zone[1], new_batch))\n                        logger.info(f\"Adjustment constrained by resonance zone: {self.resonance_zone[0]}-{self.resonance_zone[1]}\")\n                    \n                    if new_batch != self.selected_batch_size:\n                        logger.info(f\"Accuracy plateau ({accuracy_trend:.4f}% per epoch) - adjusting batch size from {self.selected_batch_size} to {new_batch}\")\n                        self.selected_batch_size = new_batch\n                        # Update lane for new batch size if using lanes\n                        if self.use_lanes:\n                            self._select_lane_for_batch(self.selected_batch_size)\n            elif phase == \"final\" and not is_decreasing:\n                # Final phase, not decreasing - consider using best batch size seen\n                if self.best_batch_size is not None and self.best_batch_size != self.selected_batch_size:\n                    logger.info(f\"Final training phase - switching to best performing batch size {self.best_batch_size}\")\n                    self.selected_batch_size = self.best_batch_size\n                    # Update lane for new batch size if using lanes\n                    if self.use_lanes:\n                        self._select_lane_for_batch(self.selected_batch_size)\n        \n        # Phase-specific adjustments\n        if phase == \"early\" and self.epoch <= 1:\n            # For very early training, use max batch for speed\n            self.selected_batch_size = min(self.max_batch, max(self.min_batch, self.selected_batch_size))\n            logger.info(f\"Early phase: using large batch size {self.selected_batch_size} for speed\")\n            # Update lane for new batch size if using lanes\n            if self.use_lanes:\n                self._select_lane_for_batch(self.selected_batch_size)\n        elif phase == \"final\" and progress > 0.9 and self.selected_batch_size > self.min_batch * 2:\n            # For very final tuning, consider a smaller batch size\n            new_batch = max(self.min_batch, int(self.selected_batch_size * 0.75))\n            if new_batch != self.selected_batch_size:\n                logger.info(f\"Final tuning phase: reducing batch size from {self.selected_batch_size} to {new_batch}\")\n                self.selected_batch_size = new_batch\n                # Update lane for new batch size if using lanes\n                if self.use_lanes:\n                    self._select_lane_for_batch(self.selected_batch_size)\n        # Make sure min_batch and max_batch are set - use reasonable defaults if not\n        if self.min_batch is None or self.max_batch is None:\n            if not self.run_diagnostics:\n                logger.warning(\"Batch size range not determined - using defaults\")\n                \n            # Set reasonable defaults based on model size if min/max not determined yet\n            # Use a broad range by default to ensure we don't get stuck on extreme values\n            model_size = self.param_count\n            if self.min_batch is None:\n                if model_size > 100_000_000:  # Very large model\n                    self.min_batch = 8\n                elif model_size > 10_000_000:  # Large model\n                    self.min_batch = 4\n                else:  # Small/medium model\n                    self.min_batch = 1\n            \n            if self.max_batch is None:\n                if model_size > 100_000_000:  # Very large model\n                    self.max_batch = 128\n                elif model_size > 10_000_000:  # Large model\n                    self.max_batch = 256\n                else:  # Small/medium model\n                    self.max_batch = 512\n                \n            # Log the default range\n            logger.info(f\"Using batch size range: {self.min_batch}-{self.max_batch}\")\n                \n        # During diagnostic phase, return a moderate batch size\n        if self.iteration < 3 and self.epoch == 0 and not self.run_diagnostics:\n            # Start with a batch size slightly above the middle for balanced performance\n            middle_batch = (self.min_batch + self.max_batch) // 2\n            if self.iteration == 0:\n                logger.info(f\"Initial batch size: {middle_batch} (middle of range)\")\n                return middle_batch\n            else:\n                # Try different batch sizes based on iteration to gather diverse data points\n                # Cover a wide range of values to better understand the performance characteristics\n                batch_options = [\n                    self.min_batch,                                # Minimum\n                    self.min_batch * 2,                           # Low end\n                    (self.min_batch + middle_batch) // 2,         # Below middle\n                    middle_batch,                                 # Middle\n                    (middle_batch + self.max_batch) // 2,         # Above middle\n                    max(self.min_batch * 4, self.max_batch // 2), # Higher\n                    self.max_batch                                # Maximum\n                ]\n                # Remove any duplicates\n                batch_options = sorted(list(set(batch_options)))\n                \n                # Ensure we don't go out of bounds\n                batch_choice = batch_options[self.iteration % len(batch_options)]\n                logger.info(f\"Exploration batch size: {batch_choice} (option {self.iteration % len(batch_options) + 1} of {len(batch_options)})\")\n                return batch_choice\n        \n        # Get total available energy - no artificial energy regime restriction\n        total_energy = self.estimate_available_energy()\n        p = self.parallel_processors\n        \n        # Skip using historical sweet spot if we have a resonance zone \n        # This avoids the conflict between resonance zone and historical data\n        if self.resonance_zone[0] <= 0 or self.resonance_zone[1] <= 0:\n            # Only use sweet spot from history if we don't have a resonance zone\n            sweet_spot = self.get_sweet_spot_batch()\n            if sweet_spot > 0 and self.iteration > 10:\n                # Use it 70% of the time for exploitation, explore 30% of the time\n                # Fixed random value generation to be more reliable\n                random_value = 0.3 + (abs(hash(str(self.iteration))) % 100) / 100.0\n                if random_value < 0.7:\n                    # Add to history for tracking\n                    self.batch_history.append(sweet_spot)\n                    logger.info(f\"Using sweet spot batch size from history: {sweet_spot}\")\n                    return sweet_spot\n        \n        # Generate batch sizes using a simpler, more robust approach\n        batch_sizes = []\n        try:\n            # Ensure min and max batch are valid\n            min_batch = max(1, self.min_batch) if self.min_batch is not None else 1\n            max_batch = max(min_batch + 1, self.max_batch) if self.max_batch is not None else 512\n            \n            # Use a simple linear range instead of logarithmic to avoid precision issues\n            # This ensures we get predictable values and don't overflow\n            num_points = min(12, max(6, max_batch - min_batch + 1))  # Reduced from 30 to 12 points for efficiency\n            \n            if num_points >= (max_batch - min_batch + 1):\n                # If range is small enough, use all integer values\n                batch_sizes = list(range(min_batch, max_batch + 1))\n            else:\n                # Otherwise, space them evenly\n                step = (max_batch - min_batch) / (num_points - 1)\n                for i in range(num_points):\n                    batch = min_batch + int(i * step)\n                    if batch not in batch_sizes:\n                        batch_sizes.append(batch)\n                \n                # Always include min and max\n                if min_batch not in batch_sizes:\n                    batch_sizes.append(min_batch)\n                if max_batch not in batch_sizes:\n                    batch_sizes.append(max_batch)\n                    \n            # Sort and ensure unique values\n            batch_sizes = sorted(list(set(batch_sizes)))\n            \n        except Exception as e:\n            logger.warning(f\"Error generating batch sizes: {str(e)}, using basic range\")\n            # Simple emergency fallback with just a few safe values\n            default_min = 16\n            default_max = 64\n            batch_sizes = [default_min, \n                          (default_min + default_max) // 2, \n                          default_max]\n        \n        # Log what we're testing (only at epoch boundaries or first call)\n        if self.iteration == 0 or (hasattr(self, '_last_batch_calc_epoch') and self._last_batch_calc_epoch != self.epoch):\n            lane_info = f\" for lane {self.selected_lane.upper()}\" if self.use_lanes else \"\"\n            logger.info(f\"Testing batch sizes in range [{batch_sizes[0]}-{batch_sizes[-1]}]{lane_info}: {len(batch_sizes)} values\")\n        \n        # Initialize best metrics\n        best_batch = self.min_batch\n        best_efficiency = 0.0\n        \n        # Get available energy\n        total_energy = self.estimate_available_energy()\n        p = self.parallel_processors\n        \n        # If we have a resonance zone, filter the batch sizes to stay within it\n        if self.resonance_zone[0] > 0 and self.resonance_zone[1] > 0:\n            # Filter batch sizes to only include those in the resonance zone\n            filtered_batch_sizes = [b for b in batch_sizes if self.resonance_zone[0] <= b <= self.resonance_zone[1]]\n            \n            # If we filtered out all batch sizes, add the resonance zone bounds\n            if not filtered_batch_sizes and self.resonance_zone[0] <= self.resonance_zone[1]:\n                filtered_batch_sizes = [self.resonance_zone[0]]\n                # Add the midpoint if the zone is wide enough\n                if self.resonance_zone[1] > self.resonance_zone[0] + 1:\n                    mid = (self.resonance_zone[0] + self.resonance_zone[1]) // 2\n                    filtered_batch_sizes.append(mid)\n                # Add the upper bound if different from lower\n                if self.resonance_zone[1] > self.resonance_zone[0]:\n                    filtered_batch_sizes.append(self.resonance_zone[1])\n                \n            # Use the filtered batch sizes if we have any\n            if filtered_batch_sizes:\n                logger.info(f\"Constraining batch size search to resonance zone: {self.resonance_zone[0]}-{self.resonance_zone[1]}\")\n                batch_sizes = filtered_batch_sizes\n        \n        # Apply the true IsekaiZen framework by calculating across all frequency bands simultaneously\n        # This is how the framework operates in direct component applications\n        \n        # Create calculation matrices - efficiency values for each (batch size, frequency band) combo\n        efficiency_matrix = {}\n        cognitive_load_matrix = {}\n        \n        # Gather all frequency bands with meaningful layer counts\n        active_bands = []\n        for band, count in self.band_layer_counts.items():\n            if count > 0:\n                active_bands.append(band)\n        \n        if not active_bands:\n            active_bands = [\"medium\"]  # Fallback if no bands have layers assigned\n        \n        logger.debug(f\"Evaluating batches across {len(active_bands)} frequency bands: {active_bands}\")\n        \n        # First pass: Gather raw efficiencies across all bands\n        for batch_size in batch_sizes:\n            batch_eff = {}\n            batch_load = {}\n            \n            # Calculate efficiency across all active frequency bands\n            for band in active_bands:\n                # Calculate cognitive load for this batch+band combination\n                L = self.estimate_cognitive_load(batch_size, band)\n                batch_load[band] = L\n                \n                # Calculate efficiency for this batch+band combination\n                eff = self.cognitive_efficiency(total_energy, batch_size, p, L, band)\n                batch_eff[band] = eff\n                \n                logger.debug(f\"Batch {batch_size}, {band.upper()} band: load={L:.2f}, efficiency={eff:.4f}\")\n            \n            # Store the efficiency and load data for this batch size\n            efficiency_matrix[batch_size] = batch_eff\n            cognitive_load_matrix[batch_size] = batch_load\n        \n        # Second pass: Apply framework's resonance calculation\n        resonance_values = {}\n        \n        for batch_size in batch_sizes:\n            # Calculate the resonance value across all frequency bands\n            # Resonance occurs when efficiency is balanced across frequency bands\n            # while maintaining high total efficiency\n            \n            if not efficiency_matrix[batch_size]:\n                continue\n                \n            # Get min and max efficiency across bands\n            efficiencies = list(efficiency_matrix[batch_size].values())\n            min_eff = min(efficiencies)\n            max_eff = max(efficiencies)\n            avg_eff = sum(efficiencies) / len(efficiencies)\n            \n            # Calculate uniformity (how consistent efficiency is across bands)\n            # When all bands have similar efficiency, we have good resonance\n            if max_eff > 0:\n                uniformity = min_eff / max_eff  # Ratio between 0-1\n            else:\n                uniformity = 0\n                \n            # Calculate resonance value using the mathematical foundation:\n            # 1. Reward high average efficiency\n            # 2. Reward uniformity across frequency bands\n            # 3. Add small preference for larger batch sizes (throughput term)\n            throughput_term = 0.05 * (batch_size / max(batch_sizes))\n            \n            # The fundamental resonance equation from the framework's mathematical foundation\n            # Calculate an adaptive threshold based on the model complexity and hardware\n            complexity = self.estimate_model_complexity()\n            \n            # Hardware-specific adjustment \n            hw_factor = 0.8 if self.device.type == \"cuda\" else 0.65\n            \n            # Calculate final adaptive threshold (range adjusted to 0.55-0.7)\n            # Upper bound lowered from 0.8 to 0.7 for consistency with earlier threshold change\n            adaptive_threshold = min(0.7, max(0.55, hw_factor + (complexity - 1.0) * 0.05))\n            \n            resonance = avg_eff * (adaptive_threshold + (1.0 - adaptive_threshold) * uniformity) + throughput_term\n            \n            resonance_values[batch_size] = resonance\n            logger.debug(f\"Batch {batch_size}: resonance={resonance:.4f} (avg_eff={avg_eff:.4f}, uniformity={uniformity:.2f})\")\n        \n        # Find the batch size with maximum resonance\n        if resonance_values:\n            # Get the batch size with highest resonance\n            best_batch, best_resonance = max(resonance_values.items(), key=lambda x: x[1])\n            best_efficiency = max(efficiency_matrix.get(best_batch, {}).values(), default=0.0)\n            \n            logger.info(f\"Selected batch size {best_batch} with resonance {best_resonance:.4f}, efficiency {best_efficiency:.4f}\")\n        else:\n            # Fallback to middle of range if no resonance values calculated\n            best_batch = batch_sizes[len(batch_sizes) // 2] if batch_sizes else self.min_batch\n            best_efficiency = 0.0\n            logger.warning(f\"No valid resonance values found, using fallback batch size {best_batch}\")\n        \n        # Check if the best batch is within the resonance zone\n        if self.resonance_zone[0] > 0 and self.resonance_zone[1] > 0:\n            if best_batch < self.resonance_zone[0] or best_batch > self.resonance_zone[1]:\n                logger.warning(f\"Best batch {best_batch} is outside resonance zone - adjusting to nearest bound\")\n                best_batch = max(self.resonance_zone[0], min(self.resonance_zone[1], best_batch))\n        \n        # Use the best batch size we found\n        if best_batch != self.selected_batch_size:\n            logger.info(f\"Selected optimal batch size: {best_batch} (efficiency: {best_efficiency:.4f})\")\n            self.selected_batch_size = best_batch\n        \n        # Store the result in our batch history for tracking\n        self.batch_history.append(self.selected_batch_size)\n        \n        # Safety check for validity\n        if not isinstance(self.selected_batch_size, int) or self.selected_batch_size <= 0:\n            logger.warning(f\"Invalid batch size: {self.selected_batch_size}, using default value\")\n            \n            # Use the middle of the resonance zone if available\n            if self.resonance_zone[0] > 0 and self.resonance_zone[1] > 0:\n                middle_batch = (self.resonance_zone[0] + self.resonance_zone[1]) // 2\n                logger.info(f\"Falling back to middle batch size from resonance zone: {middle_batch}\")\n            else:\n                # Otherwise use the middle of the allowed range\n                middle_batch = (self.min_batch + self.max_batch) // 2\n                logger.info(f\"Falling back to middle batch size: {middle_batch}\")\n                \n            self.selected_batch_size = middle_batch\n        \n        # Log the selected batch size\n        if self.iteration == 0 or (hasattr(self, '_last_batch_calc_epoch') and self._last_batch_calc_epoch != self.epoch):\n            logger.info(f\"Epoch {self.epoch}/{self.total_epochs} ({(self.epoch/max(1, self.total_epochs))*100:.1f}%) \" +\n                       f\"- Using batch size: {self.selected_batch_size}\")\n        \n        # Track when we last calculated batch size\n        self._last_batch_calc_epoch = self.epoch\n        \n        # Cache the result\n        self._cached_batch_size = self.selected_batch_size\n        \n        # Return the selected batch size\n        return self.selected_batch_size\n        \n    def increment_epoch(self):\n        \"\"\"Increment the epoch counter\"\"\"\n        super().increment_epoch()"
        },
        "increment_epoch": {
          "start_line": 2158,
          "end_line": 2162,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "....increment_epoch",
              "line": 2160
            },
            {
              "name": "super",
              "line": 2160
            }
          ],
          "docstring": "Increment the epoch counter",
          "code_snippet": "        return self.selected_batch_size\n        \n    def increment_epoch(self):\n        \"\"\"Increment the epoch counter\"\"\"\n        super().increment_epoch()\n        # Don't reset iteration counter to maintain gradient history across epochs"
        }
      },
      "class_variables": [],
      "bases": [
        "BatchSizeSelector"
      ],
      "docstring": "\n    Dynamic batch size optimizer using the IsekaiZen framework with multi-lane approach.\n    \n    This implementation applies the IsekaiZen optimization framework's advanced\n    cognitive efficiency principles and frequency-based techniques to determine\n    the optimal batch size in real-time during training.\n    "
    }
  },
  "functions": {},
  "constants": {}
}