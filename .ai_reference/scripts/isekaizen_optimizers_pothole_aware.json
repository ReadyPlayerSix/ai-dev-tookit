{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\isekaizen\\optimizers\\pothole_aware.py",
  "imports": [
    {
      "name": "torch",
      "line": 9
    },
    {
      "name": "logging",
      "line": 10
    },
    {
      "name": "math",
      "line": 11
    },
    {
      "name": "random",
      "line": 12
    },
    {
      "name": "typing.Dict",
      "line": 13
    },
    {
      "name": "typing.List",
      "line": 13
    },
    {
      "name": "typing.Tuple",
      "line": 13
    },
    {
      "name": "typing.Optional",
      "line": 13
    },
    {
      "name": "typing.Set",
      "line": 13
    },
    {
      "name": "collections.defaultdict",
      "line": 14
    },
    {
      "name": "isekaizen.IsekaiZenOptimizer",
      "line": 15
    }
  ],
  "classes": {
    "PotholeAwareIsekaiZen": {
      "start_line": 19,
      "end_line": 420,
      "methods": {
        "__init__": {
          "start_line": 31,
          "end_line": 100,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "model"
            },
            {
              "name": "device"
            },
            {
              "name": "run_diagnostics",
              "type": "bool"
            },
            {
              "name": "min_batch"
            },
            {
              "name": "max_batch"
            },
            {
              "name": "total_epochs",
              "type": "int"
            },
            {
              "name": "accuracy_window",
              "type": "int"
            },
            {
              "name": "accuracy_threshold",
              "type": "float"
            },
            {
              "name": "use_lanes",
              "type": "bool"
            },
            {
              "name": "pothole_memory_factor",
              "type": "float"
            },
            {
              "name": "pothole_severity_threshold",
              "type": "float"
            },
            {
              "name": "exploration_rate",
              "type": "float"
            },
            {
              "name": "pothole_avoidance_weight",
              "type": "float"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "....__init__",
              "line": 66
            },
            {
              "name": "defaultdict",
              "line": 85
            },
            {
              "name": "set",
              "line": 92
            },
            {
              "name": "logger.info",
              "line": 96
            },
            {
              "name": "super",
              "line": 66
            },
            {
              "name": "defaultdict",
              "line": 85
            }
          ],
          "docstring": "\n        Initialize the PotholeAwareIsekaiZen optimizer.\n        \n        Args:\n            model: PyTorch model to optimize\n            device: Target device for optimization (default: auto-detect)\n            run_diagnostics: Whether to run initial diagnostics\n            min_batch: Optional minimum batch size to try\n            max_batch: Optional maximum batch size to try\n            total_epochs: The total expected number of training epochs\n            accuracy_window: Number of epochs to consider for accuracy trend analysis\n            accuracy_threshold: Threshold for accuracy decrease to trigger batch size adjustment\n            use_lanes: Whether to use frequency lanes for optimization\n            pothole_memory_factor: Controls how quickly pothole memory fades (0-1)\n            pothole_severity_threshold: Multiplier of average loss to identify potholes\n            exploration_rate: How often to randomly explore batch sizes\n            pothole_avoidance_weight: Weight given to pothole avoidance (0-1)\n        ",
          "code_snippet": "    \"\"\"\n    \n    def __init__(\n        self,\n        model: torch.nn.Module,\n        device: Optional[torch.device] = None,\n        run_diagnostics: bool = True,\n        min_batch: Optional[int] = None,\n        max_batch: Optional[int] = None,\n        total_epochs: int = 10,\n        accuracy_window: int = 3,\n        accuracy_threshold: float = 0.01,\n        use_lanes: bool = True,\n        pothole_memory_factor: float = 0.8,  # Controls how quickly pothole memory fades\n        pothole_severity_threshold: float = 0.75,  # Threshold for identifying potholes\n        exploration_rate: float = 0.2,  # How often to randomly explore batch sizes\n        pothole_avoidance_weight: float = 0.6  # Weight given to pothole avoidance (0-1)\n    ):\n        \"\"\"\n        Initialize the PotholeAwareIsekaiZen optimizer.\n        \n        Args:\n            model: PyTorch model to optimize\n            device: Target device for optimization (default: auto-detect)\n            run_diagnostics: Whether to run initial diagnostics\n            min_batch: Optional minimum batch size to try\n            max_batch: Optional maximum batch size to try\n            total_epochs: The total expected number of training epochs\n            accuracy_window: Number of epochs to consider for accuracy trend analysis\n            accuracy_threshold: Threshold for accuracy decrease to trigger batch size adjustment\n            use_lanes: Whether to use frequency lanes for optimization\n            pothole_memory_factor: Controls how quickly pothole memory fades (0-1)\n            pothole_severity_threshold: Multiplier of average loss to identify potholes\n            exploration_rate: How often to randomly explore batch sizes\n            pothole_avoidance_weight: Weight given to pothole avoidance (0-1)\n        \"\"\"\n        # Initialize parent class first\n        super().__init__(\n            model=model,\n            device=device,\n            run_diagnostics=run_diagnostics,\n            min_batch=min_batch,\n            max_batch=max_batch,\n            total_epochs=total_epochs,\n            accuracy_window=accuracy_window,\n            accuracy_threshold=accuracy_threshold,\n            use_lanes=use_lanes\n        )\n        \n        # Pothole tracking parameters\n        self.pothole_memory_factor = pothole_memory_factor\n        self.pothole_severity_threshold = pothole_severity_threshold\n        self.exploration_rate = exploration_rate\n        self.pothole_avoidance_weight = pothole_avoidance_weight\n        \n        # Pothole map: {batch_size: {data_region: severity}}\n        self.pothole_map = defaultdict(lambda: defaultdict(float))\n        \n        # Loss tracking\n        self.last_batch_losses = []  # Per-example losses from the last batch\n        self.last_data_indices = []  # Indices of data points from the last batch\n        \n        # Data region tracking - identifies which regions of data we're currently training on\n        self.current_data_regions = set()\n        self.num_data_regions = 10  # Divide dataset into 10 regions by default\n        \n        # Log initialization\n        logger.info(f\"PotholeAwareIsekaiZen initialized with pothole_memory_factor={pothole_memory_factor}, \" +\n                   f\"pothole_severity_threshold={pothole_severity_threshold}, \" +\n                   f\"exploration_rate={exploration_rate}\")\n    \n    def update_with_loss_info(self, per_example_losses: List[float], data_indices: List[int]) -> None:\n        \"\"\"\n        Update the pothole map with information from the latest batch."
        },
        "update_with_loss_info": {
          "start_line": 100,
          "end_line": 144,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "per_example_losses"
            },
            {
              "name": "data_indices"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "len",
              "line": 116
            },
            {
              "name": "self._identify_data_regions",
              "line": 119
            },
            {
              "name": "enumerate",
              "line": 125
            },
            {
              "name": "....update_training_state",
              "line": 142
            },
            {
              "name": "sum",
              "line": 122
            },
            {
              "name": "len",
              "line": 122
            },
            {
              "name": "self._get_region_for_index",
              "line": 128
            },
            {
              "name": "logger.debug",
              "line": 138
            },
            {
              "name": "super",
              "line": 142
            }
          ],
          "docstring": "\n        Update the pothole map with information from the latest batch.\n        \n        This method should be called after each forward/backward pass with the\n        per-example losses and corresponding data indices.\n        \n        Args:\n            per_example_losses: List of loss values for each example in the batch\n            data_indices: Indices or identifiers for the data points in this batch\n        ",
          "code_snippet": "                   f\"exploration_rate={exploration_rate}\")\n    \n    def update_with_loss_info(self, per_example_losses: List[float], data_indices: List[int]) -> None:\n        \"\"\"\n        Update the pothole map with information from the latest batch.\n        \n        This method should be called after each forward/backward pass with the\n        per-example losses and corresponding data indices.\n        \n        Args:\n            per_example_losses: List of loss values for each example in the batch\n            data_indices: Indices or identifiers for the data points in this batch\n        \"\"\"\n        # Store the loss information for later use\n        self.last_batch_losses = per_example_losses\n        self.last_data_indices = data_indices\n        \n        # Store the current batch size\n        current_batch_size = len(per_example_losses)\n        \n        # Identify current data regions (could be based on data indices or other heuristics)\n        self.current_data_regions = self._identify_data_regions(data_indices)\n        \n        # Calculate average loss for this batch\n        avg_loss = sum(per_example_losses) / len(per_example_losses)\n        \n        # Identify potholes - examples with loss significantly higher than average\n        for i, loss in enumerate(per_example_losses):\n            if loss > avg_loss * self.pothole_severity_threshold:\n                # This is a pothole - update the severity in all the regions this data point belongs to\n                data_region = self._get_region_for_index(data_indices[i])\n                \n                # Increase severity based on how bad the loss is compared to average\n                severity_increase = (loss / avg_loss) - 1\n                \n                # Update pothole map with decay for existing values\n                current_severity = self.pothole_map[current_batch_size][data_region]\n                new_severity = (current_severity * self.pothole_memory_factor) + severity_increase\n                self.pothole_map[current_batch_size][data_region] = new_severity\n                \n                logger.debug(f\"Pothole detected in region {data_region} with batch size {current_batch_size}, \" +\n                            f\"severity: {new_severity:.2f} (loss: {loss:.4f}, avg: {avg_loss:.4f})\")\n        \n        # Update the base optimizer with the average loss\n        super().update_training_state(avg_loss, 0.0, None)\n    \n    def _identify_data_regions(self, data_indices: List[int]) -> Set[int]:\n        \"\"\"\n        Identify the regions of the dataset that the current batch falls into."
        },
        "_identify_data_regions": {
          "start_line": 144,
          "end_line": 167,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "data_indices"
            }
          ],
          "return_type": "complex_type",
          "calls": [
            {
              "name": "set",
              "line": 157
            },
            {
              "name": "max",
              "line": 159
            },
            {
              "name": "max",
              "line": 158
            },
            {
              "name": "regions.add",
              "line": 163
            }
          ],
          "docstring": "\n        Identify the regions of the dataset that the current batch falls into.\n        This is a simplified implementation - in practice, this could use\n        clustering, dataset metadata, or other approaches.\n        \n        Args:\n            data_indices: Indices of data points in the current batch\n            \n        Returns:\n            Set of region identifiers\n        ",
          "code_snippet": "        super().update_training_state(avg_loss, 0.0, None)\n    \n    def _identify_data_regions(self, data_indices: List[int]) -> Set[int]:\n        \"\"\"\n        Identify the regions of the dataset that the current batch falls into.\n        This is a simplified implementation - in practice, this could use\n        clustering, dataset metadata, or other approaches.\n        \n        Args:\n            data_indices: Indices of data points in the current batch\n            \n        Returns:\n            Set of region identifiers\n        \"\"\"\n        # Simple implementation: divide the dataset into regions\n        regions = set()\n        max_index = max(data_indices) if data_indices else 1000\n        region_size = max(1, max_index // self.num_data_regions)\n        \n        for idx in data_indices:\n            region = idx // region_size\n            regions.add(region)\n        \n        return regions\n    \n    def _get_region_for_index(self, idx: int) -> int:\n        \"\"\"\n        Convert a data index to a region identifier."
        },
        "_get_region_for_index": {
          "start_line": 167,
          "end_line": 183,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "idx",
              "type": "int"
            }
          ],
          "return_type": "int",
          "calls": [
            {
              "name": "max",
              "line": 180
            },
            {
              "name": "max",
              "line": 179
            }
          ],
          "docstring": "\n        Convert a data index to a region identifier.\n        \n        Args:\n            idx: Data index\n            \n        Returns:\n            Region identifier\n        ",
          "code_snippet": "        return regions\n    \n    def _get_region_for_index(self, idx: int) -> int:\n        \"\"\"\n        Convert a data index to a region identifier.\n        \n        Args:\n            idx: Data index\n            \n        Returns:\n            Region identifier\n        \"\"\"\n        # Simple region identification based on index ranges\n        # This should match the logic in _identify_data_regions\n        max_index = max(self.last_data_indices) if self.last_data_indices else 1000\n        region_size = max(1, max_index // self.num_data_regions)\n        return idx // region_size\n    \n    def _random_batch_size_in_resonance(self) -> int:\n        \"\"\"\n        Select a random batch size from one of the resonance zones."
        },
        "_random_batch_size_in_resonance": {
          "start_line": 183,
          "end_line": 200,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": "int",
          "calls": [
            {
              "name": "hasattr",
              "line": 190
            },
            {
              "name": "random.choice",
              "line": 191
            },
            {
              "name": "random.randint",
              "line": 192
            },
            {
              "name": "random.randint",
              "line": 195
            },
            {
              "name": "random.randint",
              "line": 198
            }
          ],
          "docstring": "\n        Select a random batch size from one of the resonance zones.\n        \n        Returns:\n            Random batch size within a resonance zone\n        ",
          "code_snippet": "        return idx // region_size\n    \n    def _random_batch_size_in_resonance(self) -> int:\n        \"\"\"\n        Select a random batch size from one of the resonance zones.\n        \n        Returns:\n            Random batch size within a resonance zone\n        \"\"\"\n        if hasattr(self, 'resonance_zones') and self.resonance_zones:\n            zone = random.choice(self.resonance_zones)\n            return random.randint(zone[0], zone[1])\n        elif self.resonance_zone[0] > 0 and self.resonance_zone[1] > 0:\n            # Use the main resonance zone if resonance_zones isn't available\n            return random.randint(self.resonance_zone[0], self.resonance_zone[1])\n        else:\n            # Fallback to a random batch size in the allowed range\n            return random.randint(self.min_batch, self.max_batch)\n    \n    def get_pothole_severity_for_region(self, batch_size: int, data_region: int) -> float:\n        \"\"\"\n        Get the current severity of potholes for a specific batch size and data region."
        },
        "get_pothole_severity_for_region": {
          "start_line": 200,
          "end_line": 213,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "batch_size",
              "type": "int"
            },
            {
              "name": "data_region",
              "type": "int"
            }
          ],
          "return_type": "float",
          "calls": [],
          "docstring": "\n        Get the current severity of potholes for a specific batch size and data region.\n        \n        Args:\n            batch_size: Batch size to check\n            data_region: Data region to check\n            \n        Returns:\n            Pothole severity value (0 if no pothole)\n        ",
          "code_snippet": "            return random.randint(self.min_batch, self.max_batch)\n    \n    def get_pothole_severity_for_region(self, batch_size: int, data_region: int) -> float:\n        \"\"\"\n        Get the current severity of potholes for a specific batch size and data region.\n        \n        Args:\n            batch_size: Batch size to check\n            data_region: Data region to check\n            \n        Returns:\n            Pothole severity value (0 if no pothole)\n        \"\"\"\n        return self.pothole_map[batch_size][data_region]\n    \n    def get_overall_pothole_map(self) -> Dict[int, Dict[int, float]]:\n        \"\"\"\n        Return the entire pothole map for analysis."
        },
        "get_overall_pothole_map": {
          "start_line": 213,
          "end_line": 222,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": "complex_type",
          "calls": [
            {
              "name": "dict",
              "line": 220
            }
          ],
          "docstring": "\n        Return the entire pothole map for analysis.\n        \n        Returns:\n            Dictionary mapping batch sizes to region severity maps\n        ",
          "code_snippet": "        return self.pothole_map[batch_size][data_region]\n    \n    def get_overall_pothole_map(self) -> Dict[int, Dict[int, float]]:\n        \"\"\"\n        Return the entire pothole map for analysis.\n        \n        Returns:\n            Dictionary mapping batch sizes to region severity maps\n        \"\"\"\n        return dict(self.pothole_map)\n    \n    def visualize_pothole_map(self) -> str:\n        \"\"\"\n        Create a simple text visualization of the pothole map."
        },
        "visualize_pothole_map": {
          "start_line": 222,
          "end_line": 268,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": "str",
          "calls": [
            {
              "name": "sorted",
              "line": 233
            },
            {
              "name": "set",
              "line": 234
            },
            {
              "name": "self.pothole_map.values",
              "line": 235
            },
            {
              "name": "sorted",
              "line": 237
            },
            {
              "name": "set",
              "line": 233
            },
            {
              "name": "all_regions.update",
              "line": 236
            },
            {
              "name": "self.pothole_map.keys",
              "line": 233
            },
            {
              "name": "batch_dict.keys",
              "line": 236
            },
            {
              "name": "len",
              "line": 247
            }
          ],
          "docstring": "\n        Create a simple text visualization of the pothole map.\n        \n        Returns:\n            A string representation of the pothole map\n        ",
          "code_snippet": "        return dict(self.pothole_map)\n    \n    def visualize_pothole_map(self) -> str:\n        \"\"\"\n        Create a simple text visualization of the pothole map.\n        \n        Returns:\n            A string representation of the pothole map\n        \"\"\"\n        result = \"Pothole Map Visualization:\\n\"\n        result += \"=\" * 50 + \"\\n\"\n        \n        # Find all regions and batch sizes with data\n        all_batch_sizes = sorted(set(self.pothole_map.keys()))\n        all_regions = set()\n        for batch_dict in self.pothole_map.values():\n            all_regions.update(batch_dict.keys())\n        all_regions = sorted(all_regions)\n        \n        if not all_batch_sizes or not all_regions:\n            return result + \"No potholes detected yet.\"\n        \n        # Create a header\n        result += \"Region |\"\n        for bs in all_batch_sizes:\n            result += f\" BS:{bs} |\"\n        result += \"\\n\"\n        result += \"-\" * (8 + 9 * len(all_batch_sizes)) + \"\\n\"\n        \n        # Add data rows\n        for region in all_regions:\n            result += f\"{region:6d} |\"\n            for bs in all_batch_sizes:\n                severity = self.pothole_map[bs][region]\n                # Convert severity to a symbol for readability\n                if severity == 0:\n                    symbol = \"   .   \"\n                elif severity < 0.5:\n                    symbol = f\" {severity:.2f} \"\n                elif severity < 1.0:\n                    symbol = f\"*{severity:.2f}*\"\n                else:\n                    symbol = f\"!{severity:.2f}!\"\n                result += f\" {symbol} |\"\n            result += \"\\n\"\n        \n        return result\n    \n    def get_optimal_batch_size(self) -> int:\n        \"\"\"\n        Calculate the optimal batch size using both resonance zone and pothole avoidance."
        },
        "get_optimal_batch_size": {
          "start_line": 268,
          "end_line": 378,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": "int",
          "calls": [
            {
              "name": "....get_optimal_batch_size",
              "line": 321
            },
            {
              "name": "self._get_candidate_batch_sizes",
              "line": 328
            },
            {
              "name": "hasattr",
              "line": 280
            },
            {
              "name": "hasattr",
              "line": 286
            },
            {
              "name": "logger.debug",
              "line": 332
            },
            {
              "name": "abs",
              "line": 349
            },
            {
              "name": "max",
              "line": 350
            },
            {
              "name": "logger.debug",
              "line": 361
            },
            {
              "name": "len",
              "line": 286
            },
            {
              "name": "abs",
              "line": 290
            },
            {
              "name": "super",
              "line": 321
            },
            {
              "name": "abs",
              "line": 350
            },
            {
              "name": "abs",
              "line": 350
            },
            {
              "name": "max",
              "line": 366
            },
            {
              "name": "logger.info",
              "line": 370
            },
            {
              "name": "min",
              "line": 303
            },
            {
              "name": "random.random",
              "line": 345
            },
            {
              "name": "max",
              "line": 351
            },
            {
              "name": "batch_scores.items",
              "line": 366
            },
            {
              "name": "max",
              "line": 304
            },
            {
              "name": "logger.info",
              "line": 316
            },
            {
              "name": "int",
              "line": 305
            },
            {
              "name": "logger.info",
              "line": 311
            }
          ],
          "docstring": "\n        Calculate the optimal batch size using both resonance zone and pothole avoidance.\n        \n        This extends the base IsekaiZen approach by considering the pothole map\n        when selecting batch sizes. It also includes adaptive exploration to overcome\n        periods of slow progress.\n        \n        Returns:\n            Current optimal batch size\n        ",
          "code_snippet": "        return result\n    \n    def get_optimal_batch_size(self) -> int:\n        \"\"\"\n        Calculate the optimal batch size using both resonance zone and pothole avoidance.\n        \n        This extends the base IsekaiZen approach by considering the pothole map\n        when selecting batch sizes. It also includes adaptive exploration to overcome\n        periods of slow progress.\n        \n        Returns:\n            Current optimal batch size\n        \"\"\"\n        # Initialize progress tracking variables if they don't exist\n        if not hasattr(self, 'slow_progress_counter'):\n            self.slow_progress_counter = 0\n            self.last_accuracy = 0.0\n            self.last_significant_change_epoch = 0\n        \n        # Check for slow progress conditions - using the same threshold as early stopping\n        if hasattr(self, 'accuracy_history') and len(self.accuracy_history) > 0:\n            current_accuracy = self.accuracy_history[-1] if self.accuracy_history else 0\n            \n            # Detect if we have minimal improvement (using the same threshold as early stopping)\n            if abs(current_accuracy - self.last_accuracy) < self.accuracy_threshold:\n                self.slow_progress_counter += 1\n            else:\n                self.slow_progress_counter = 0\n                self.last_significant_change_epoch = self.epoch\n            \n            self.last_accuracy = current_accuracy\n            \n            # If we have exploration enabled (rate > 0) and 8 epochs of slow progress, try a larger batch\n            if self.slow_progress_counter >= 8 and self.exploration_rate > 0:\n                # Only do this if we're not at max batch already\n                if self.selected_batch_size < self.max_batch:\n                    # Try a larger batch size - 25% larger or at least +8\n                    larger_batch = min(self.max_batch, \n                                        max(self.selected_batch_size + 8, \n                                            int(self.selected_batch_size * 1.25)))\n                    \n                    # Check if this batch size is within resonance zone upper limit\n                    if self.resonance_zone[1] > 0 and larger_batch > self.resonance_zone[1]:\n                        # If beyond resonance zone, but still have slow progress, try it anyway after more epochs\n                        if self.slow_progress_counter >= 12:  # Extra patience for outside resonance zone\n                            logger.info(f\"Trying larger batch size {larger_batch} to overcome slow progress after {self.slow_progress_counter} epochs\")\n                            self.slow_progress_counter = 0  # Reset counter\n                            return larger_batch\n                    else:\n                        # Within resonance zone, proceed with the larger batch size\n                        logger.info(f\"Trying larger batch size {larger_batch} to overcome slow progress after {self.slow_progress_counter} epochs\")\n                        self.slow_progress_counter = 0  # Reset counter\n                        return larger_batch\n        \n        # Get the base optimal batch size from parent class\n        base_optimal = super().get_optimal_batch_size()\n        \n        # If we don't have enough loss data yet, just use the base optimal\n        if not self.current_data_regions or not self.pothole_map:\n            return base_optimal\n        \n        # Get potential batch sizes from resonance zones\n        candidate_batch_sizes = self._get_candidate_batch_sizes()\n        \n        if not candidate_batch_sizes:\n            # If no candidates, use base optimal\n            logger.debug(\"No candidate batch sizes, using base optimal\")\n            return base_optimal\n        \n        # Score each candidate based on pothole severity\n        batch_scores = {}\n        for batch_size in candidate_batch_sizes:\n            # Score inversely proportional to pothole severity\n            total_severity = 0\n            for region in self.current_data_regions:\n                total_severity += self.pothole_map[batch_size][region]\n            \n            # Higher score is better (less severe potholes)\n            # Add small random factor to break ties\n            pothole_score = 1.0 / (1.0 + total_severity) + random.random() * 0.01\n            \n            # Get base optimizer score (normalized to 0-1 range)\n            # We'll use distance from base optimal as a proxy for base score\n            distance = abs(batch_size - base_optimal)\n            max_distance = max(abs(self.max_batch - base_optimal), abs(self.min_batch - base_optimal))\n            base_score = 1.0 - (distance / max(1, max_distance))\n            \n            # Combine scores based on pothole avoidance weight\n            combined_score = (\n                (self.pothole_avoidance_weight * pothole_score) + \n                ((1 - self.pothole_avoidance_weight) * base_score)\n            )\n            \n            batch_scores[batch_size] = combined_score\n            \n            logger.debug(f\"Batch {batch_size}: pothole_score={pothole_score:.4f}, \" +\n                        f\"base_score={base_score:.4f}, combined={combined_score:.4f}\")\n        \n        # Choose batch size with highest score (best compromise between base optimal and pothole avoidance)\n        if batch_scores:\n            best_batch_size = max(batch_scores.items(), key=lambda x: x[1])[0]\n            \n            # Only log changes\n            if best_batch_size != base_optimal:\n                logger.info(f\"PotholeAware adjustment: {base_optimal} \u2192 {best_batch_size} \" +\n                           f\"(avoiding potholes in regions {self.current_data_regions})\")\n            \n            return best_batch_size\n        else:\n            # Fallback to base optimal\n            return base_optimal\n    \n    def _get_candidate_batch_sizes(self) -> List[int]:\n        \"\"\"\n        Get candidate batch sizes based on resonance zones."
        },
        "_get_candidate_batch_sizes": {
          "start_line": 378,
          "end_line": 420,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": "complex_type",
          "calls": [
            {
              "name": "list",
              "line": 415
            },
            {
              "name": "max",
              "line": 392
            },
            {
              "name": "range",
              "line": 393
            },
            {
              "name": "candidates.append",
              "line": 404
            },
            {
              "name": "candidates.append",
              "line": 405
            },
            {
              "name": "candidates.append",
              "line": 406
            },
            {
              "name": "max",
              "line": 410
            },
            {
              "name": "range",
              "line": 411
            },
            {
              "name": "set",
              "line": 415
            },
            {
              "name": "candidates.append",
              "line": 394
            },
            {
              "name": "candidates.append",
              "line": 398
            },
            {
              "name": "candidates.append",
              "line": 400
            },
            {
              "name": "max",
              "line": 405
            },
            {
              "name": "min",
              "line": 406
            },
            {
              "name": "candidates.append",
              "line": 412
            }
          ],
          "docstring": "\n        Get candidate batch sizes based on resonance zones.\n        \n        Returns:\n            List of candidate batch sizes to consider\n        ",
          "code_snippet": "            return base_optimal\n    \n    def _get_candidate_batch_sizes(self) -> List[int]:\n        \"\"\"\n        Get candidate batch sizes based on resonance zones.\n        \n        Returns:\n            List of candidate batch sizes to consider\n        \"\"\"\n        candidates = []\n        \n        # Get resonance zone\n        if self.resonance_zone[0] > 0 and self.resonance_zone[1] > 0:\n            lower, upper = self.resonance_zone\n            \n            # Sample values from the resonance zone\n            step = max(1, (upper - lower) // 5)\n            for batch in range(lower, upper + 1, step):\n                candidates.append(batch)\n            \n            # Always include the lower and upper bounds\n            if lower not in candidates:\n                candidates.append(lower)\n            if upper not in candidates:\n                candidates.append(upper)\n        \n        # Add current batch size and neighbors\n        if self.selected_batch_size:\n            candidates.append(self.selected_batch_size)\n            candidates.append(max(self.min_batch, self.selected_batch_size - 8))\n            candidates.append(min(self.max_batch, self.selected_batch_size + 8))\n        \n        # If still empty, use range between min and max\n        if not candidates and self.min_batch is not None and self.max_batch is not None:\n            step = max(1, (self.max_batch - self.min_batch) // 10)\n            for batch in range(self.min_batch, self.max_batch + 1, step):\n                candidates.append(batch)\n        \n        # Remove duplicates and ensure they're within bounds\n        candidates = list(set(candidates))\n        candidates = [bs for bs in candidates if self.min_batch <= bs <= self.max_batch]\n        \n        return candidates"
        }
      },
      "class_variables": [],
      "bases": [
        "IsekaiZenOptimizer"
      ],
      "docstring": "\n    Enhanced IsekaiZen optimizer that tracks training 'potholes' (areas of high loss)\n    and navigates the training landscape by finding the path of least resistance.\n    \n    This class extends the standard IsekaiZenOptimizer with the ability to:\n    1. Track which data regions have unusually high losses (\"potholes\")\n    2. Remember the severity of potholes for different batch sizes\n    3. Adjust batch size selection to avoid or minimize the impact of potholes\n    4. Gradually forget about potholes as training progresses\n    "
    }
  },
  "functions": {},
  "constants": {}
}