{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\benchmarks.old\\scripts\\inference_benchmark.py",
  "imports": [
    {
      "name": "torch",
      "line": 2
    },
    {
      "name": "torch.nn",
      "line": 3
    },
    {
      "name": "time",
      "line": 4
    },
    {
      "name": "numpy",
      "line": 5
    }
  ],
  "classes": {
    "SimpleModel": {
      "start_line": 8,
      "end_line": 20,
      "methods": {
        "__init__": {
          "start_line": 10,
          "end_line": 17,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "....__init__",
              "line": 11
            },
            {
              "name": "nn.Sequential",
              "line": 12
            },
            {
              "name": "nn.Linear",
              "line": 12
            },
            {
              "name": "nn.ReLU",
              "line": 12
            },
            {
              "name": "nn.Linear",
              "line": 13
            },
            {
              "name": "nn.ReLU",
              "line": 13
            },
            {
              "name": "nn.Linear",
              "line": 14
            },
            {
              "name": "nn.ReLU",
              "line": 14
            },
            {
              "name": "nn.Linear",
              "line": 15
            },
            {
              "name": "super",
              "line": 11
            }
          ],
          "code_snippet": "class SimpleModel(nn.Module):\n\n    def __init__(self):\n        super(SimpleModel, self).__init__()\n        self.layers = nn.Sequential(nn.Linear(784, 512), nn.ReLU(),\n                                    nn.Linear(512, 256), nn.ReLU(),\n                                    nn.Linear(256, 128), nn.ReLU(),\n                                    nn.Linear(128, 10))\n\n    def forward(self, x):\n        return self.layers(x)\n"
        },
        "forward": {
          "start_line": 17,
          "end_line": 20,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "x"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.layers",
              "line": 18
            }
          ],
          "code_snippet": "                                    nn.Linear(128, 10))\n\n    def forward(self, x):\n        return self.layers(x)\n\n\ndef run_inference_benchmark():\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
        }
      },
      "class_variables": [],
      "bases": [
        "..."
      ]
    }
  },
  "functions": {
    "run_inference_benchmark": {
      "start_line": 21,
      "end_line": 168,
      "parameters": [],
      "return_type": null,
      "calls": [
        {
          "name": "print",
          "line": 23
        },
        {
          "name": "....to",
          "line": 26
        },
        {
          "name": "model.eval",
          "line": 29
        },
        {
          "name": "print",
          "line": 94
        },
        {
          "name": "print",
          "line": 95
        },
        {
          "name": "print",
          "line": 98
        },
        {
          "name": "print",
          "line": 106
        },
        {
          "name": "print",
          "line": 107
        },
        {
          "name": "torch.randn",
          "line": 111
        },
        {
          "name": "range",
          "line": 114
        },
        {
          "name": "range",
          "line": 125
        },
        {
          "name": "np.mean",
          "line": 138
        },
        {
          "name": "np.std",
          "line": 139
        },
        {
          "name": "np.min",
          "line": 140
        },
        {
          "name": "np.max",
          "line": 141
        },
        {
          "name": "np.percentile",
          "line": 142
        },
        {
          "name": "np.percentile",
          "line": 143
        },
        {
          "name": "np.percentile",
          "line": 144
        },
        {
          "name": "print",
          "line": 146
        },
        {
          "name": "print",
          "line": 147
        },
        {
          "name": "print",
          "line": 148
        },
        {
          "name": "print",
          "line": 149
        },
        {
          "name": "print",
          "line": 150
        },
        {
          "name": "print",
          "line": 151
        },
        {
          "name": "print",
          "line": 152
        },
        {
          "name": "print",
          "line": 156
        },
        {
          "name": "torch.cuda.is_available",
          "line": 22
        },
        {
          "name": "print",
          "line": 39
        },
        {
          "name": "torch.randn",
          "line": 42
        },
        {
          "name": "print",
          "line": 45
        },
        {
          "name": "range",
          "line": 46
        },
        {
          "name": "print",
          "line": 54
        },
        {
          "name": "range",
          "line": 58
        },
        {
          "name": "np.mean",
          "line": 71
        },
        {
          "name": "np.percentile",
          "line": 72
        },
        {
          "name": "np.percentile",
          "line": 73
        },
        {
          "name": "print",
          "line": 77
        },
        {
          "name": "print",
          "line": 78
        },
        {
          "name": "print",
          "line": 79
        },
        {
          "name": "print",
          "line": 80
        },
        {
          "name": "print",
          "line": 101
        },
        {
          "name": "torch.cuda.synchronize",
          "line": 119
        },
        {
          "name": "time.time",
          "line": 126
        },
        {
          "name": "time.time",
          "line": 134
        },
        {
          "name": "latencies.append",
          "line": 135
        },
        {
          "name": "print",
          "line": 159
        },
        {
          "name": "SimpleModel",
          "line": 26
        },
        {
          "name": "torch.cuda.synchronize",
          "line": 51
        },
        {
          "name": "time.time",
          "line": 59
        },
        {
          "name": "time.time",
          "line": 67
        },
        {
          "name": "latencies.append",
          "line": 68
        },
        {
          "name": "print",
          "line": 84
        },
        {
          "name": "torch.no_grad",
          "line": 115
        },
        {
          "name": "model",
          "line": 116
        },
        {
          "name": "torch.no_grad",
          "line": 128
        },
        {
          "name": "model",
          "line": 129
        },
        {
          "name": "torch.cuda.synchronize",
          "line": 132
        },
        {
          "name": "print",
          "line": 161
        },
        {
          "name": "print",
          "line": 165
        },
        {
          "name": "torch.no_grad",
          "line": 47
        },
        {
          "name": "model",
          "line": 48
        },
        {
          "name": "torch.no_grad",
          "line": 61
        },
        {
          "name": "model",
          "line": 62
        },
        {
          "name": "torch.cuda.synchronize",
          "line": 65
        },
        {
          "name": "sum",
          "line": 75
        },
        {
          "name": "torch.cuda.memory_allocated",
          "line": 83
        }
      ],
      "code_snippet": "\n\ndef run_inference_benchmark():\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Running inference benchmark on: {device}\")\n\n    # Create and \"train\" the model (we'll just use random weights for the benchmark)\n    model = SimpleModel().to(device)\n\n    # Put the model in evaluation mode (important for inference)\n    model.eval()\n\n    # Use different batch sizes to test scaling characteristics\n    batch_sizes = [1, 16, 32, 64, 128, 256]\n    input_dim = 784  # Same as our training example\n\n    results = {}\n\n    # Test with different batch sizes\n    for batch_size in batch_sizes:\n        print(f\"\\nTesting inference with batch size: {batch_size}\")\n\n        # Create input data\n        input_data = torch.randn(batch_size, input_dim, device=device)\n\n        # Warmup runs\n        print(\"Warming up...\")\n        for _ in range(10):\n            with torch.no_grad():  # This is critical for inference\n                _ = model(input_data)\n\n        if device == \"cuda\":\n            torch.cuda.synchronize()\n\n        # Measure inference time\n        print(\"Running timed inference...\")\n        iterations = 100\n        latencies = []\n\n        for i in range(iterations):\n            start_time = time.time()\n\n            with torch.no_grad():\n                _ = model(input_data)\n\n            if device == \"cuda\":\n                torch.cuda.synchronize()\n\n            end_time = time.time()\n            latencies.append((end_time - start_time) * 1000)  # Convert to ms\n\n        # Calculate statistics\n        avg_latency = np.mean(latencies)\n        p95_latency = np.percentile(latencies, 95)\n        p99_latency = np.percentile(latencies, 99)\n        throughput = (batch_size *\n                      iterations) / sum(latencies) * 1000  # samples/second\n\n        print(f\"Average latency: {avg_latency:.2f} ms\")\n        print(f\"95th percentile latency: {p95_latency:.2f} ms\")\n        print(f\"99th percentile latency: {p99_latency:.2f} ms\")\n        print(f\"Throughput: {throughput:.2f} samples/second\")\n\n        if device == \"cuda\":\n            memory_allocated = torch.cuda.memory_allocated() / 1e6  # MB\n            print(f\"GPU memory used: {memory_allocated:.2f} MB\")\n\n        results[batch_size] = {\n            'avg_latency': avg_latency,\n            'p95_latency': p95_latency,\n            'p99_latency': p99_latency,\n            'throughput': throughput\n        }\n\n    # Print summary\n    print(\"\\n===== INFERENCE PERFORMANCE SUMMARY =====\")\n    print(\n        \"Batch Size | Avg Latency (ms) | P95 Latency (ms) | P99 Latency (ms) | Throughput (samples/s)\"\n    )\n    print(\"-\" * 90)\n    for batch_size in batch_sizes:\n        r = results[batch_size]\n        print(\n            f\"{batch_size:^10} | {r['avg_latency']:^15.2f} | {r['p95_latency']:^15.2f} | {r['p99_latency']:^15.2f} | {r['throughput']:^20.2f}\"\n        )\n\n    # Test real-time inference capability (batch size of 1)\n    print(\"\\n===== REAL-TIME INFERENCE TEST =====\")\n    print(\n        \"This simulates real-time inference needs (like for an interactive application)\"\n    )\n\n    input_data = torch.randn(1, input_dim, device=device)\n\n    # Warm up\n    for _ in range(10):\n        with torch.no_grad():\n            _ = model(input_data)\n\n    if device == \"cuda\":\n        torch.cuda.synchronize()\n\n    # Measure inference time with specific focus on consistency\n    iterations = 1000\n    latencies = []\n\n    for i in range(iterations):\n        start_time = time.time()\n\n        with torch.no_grad():\n            _ = model(input_data)\n\n        if device == \"cuda\":\n            torch.cuda.synchronize()\n\n        end_time = time.time()\n        latencies.append((end_time - start_time) * 1000)  # Convert to ms\n\n    # Calculate more detailed statistics for real-time performance\n    avg_latency = np.mean(latencies)\n    std_latency = np.std(latencies)\n    min_latency = np.min(latencies)\n    max_latency = np.max(latencies)\n    p50_latency = np.percentile(latencies, 50)  # median\n    p95_latency = np.percentile(latencies, 95)\n    p99_latency = np.percentile(latencies, 99)\n\n    print(f\"Average latency: {avg_latency:.3f} ms\")\n    print(f\"Standard deviation: {std_latency:.3f} ms\")\n    print(f\"Min latency: {min_latency:.3f} ms\")\n    print(f\"Max latency: {max_latency:.3f} ms\")\n    print(f\"Median (P50) latency: {p50_latency:.3f} ms\")\n    print(f\"P95 latency: {p95_latency:.3f} ms\")\n    print(f\"P99 latency: {p99_latency:.3f} ms\")\n\n    # Calculate frames per second (FPS) for real-time applications\n    fps = 1000 / avg_latency\n    print(f\"Effective FPS (frames per second): {fps:.1f}\")\n\n    if fps >= 30:\n        print(\"\u2713 Model meets real-time requirements (30+ FPS)\")\n    elif fps >= 15:\n        print(\n            \"\u26a0 Model may be suitable for near-real-time applications (15-30 FPS)\"\n        )\n    else:\n        print(\n            \"\u2717 Model is likely too slow for real-time applications (<15 FPS)\")\n\n\nif __name__ == \"__main__\":\n    run_inference_benchmark()"
    }
  },
  "constants": {}
}