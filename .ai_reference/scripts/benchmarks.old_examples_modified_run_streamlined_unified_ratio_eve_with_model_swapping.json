{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\benchmarks.old\\examples\\modified\\run_streamlined_unified_ratio_eve_with_model_swapping.py",
  "imports": [
    {
      "name": "os",
      "line": 19
    },
    {
      "name": "sys",
      "line": 20
    },
    {
      "name": "time",
      "line": 21
    },
    {
      "name": "logging",
      "line": 22
    },
    {
      "name": "argparse",
      "line": 23
    },
    {
      "name": "json",
      "line": 24
    },
    {
      "name": "torch",
      "line": 25
    },
    {
      "name": "torch.nn",
      "line": 26
    },
    {
      "name": "torch.optim",
      "line": 27
    },
    {
      "name": "torchvision",
      "line": 28
    },
    {
      "name": "torchvision.transforms",
      "line": 29
    },
    {
      "name": "torchvision.models",
      "line": 30
    },
    {
      "name": "matplotlib.pyplot",
      "line": 31
    },
    {
      "name": "numpy",
      "line": 32
    },
    {
      "name": "datetime.datetime",
      "line": 33
    },
    {
      "name": "shutil",
      "line": 34
    },
    {
      "name": "math",
      "line": 35
    },
    {
      "name": "multiprocessing",
      "line": 36
    },
    {
      "name": "psutil",
      "line": 37
    },
    {
      "name": "inspect",
      "line": 38
    },
    {
      "name": "isekaizen.trainer.adaptive_trainer.AdaptiveTrainer",
      "line": 241
    },
    {
      "name": "isekaizen.pattern.data_loading.load_latest_pattern_map",
      "line": 242
    },
    {
      "name": "isekaizen.core.optimizer.enhanced_pattern_responsive.EnhancedPatternResponsiveOptimizer",
      "line": 243
    },
    {
      "name": "isekaizen.optimizers.eve.EVENaturalWeights",
      "line": 244
    },
    {
      "name": "isekaizen.optimizers.eve_unified_ratio.EVEUnifiedRatio",
      "line": 245
    },
    {
      "name": "isekaizen.optimizers.lr_boundary.LRBoundaryCalculator",
      "line": 246
    },
    {
      "name": "isekaizen.mediators.augmentation.AugmentationMediator",
      "line": 247
    },
    {
      "name": "isekaizen.data.augmented_dataset.AugmentedDataset",
      "line": 248
    },
    {
      "name": "isekaizen.mediators.pattern.data_mediator.PatternDataMediator",
      "line": 249
    },
    {
      "name": "optimizer_utils.configure_optimizer",
      "line": 253
    },
    {
      "name": "optimizer_utils.print_available_optimizers",
      "line": 253
    },
    {
      "name": "optimizer_configs.get_optimizer_config",
      "line": 254
    },
    {
      "name": "optimizer_configs.ALL_CONFIGS",
      "line": 254
    },
    {
      "name": "isekaizen.core.optimizer.pattern_risk_accuracy_tracker.PatternRiskAccuracyTracker",
      "line": 1645
    },
    {
      "name": "isekaizen.optimizers.eve_unified_ratio.EVEUnifiedRatio",
      "line": 2234
    },
    {
      "name": "multiprocessing",
      "line": 1325
    },
    {
      "name": "psutil",
      "line": 1326
    },
    {
      "name": "math",
      "line": 1327
    },
    {
      "name": "isekaizen.pattern.detection.PatternRecognitionService",
      "line": 438
    },
    {
      "name": "lazy_augmentation",
      "line": 492
    },
    {
      "name": "isekaizen.utils.pattern_map_utils.translate_pattern_map_to_standard_format",
      "line": 2441
    },
    {
      "name": "sys",
      "line": 2464
    },
    {
      "name": "traceback",
      "line": 2751
    },
    {
      "name": "traceback",
      "line": 2762
    },
    {
      "name": "isekaizen.pattern.detection.PatternRecognitionService",
      "line": 424
    },
    {
      "name": "traceback",
      "line": 604
    },
    {
      "name": "multiprocessing",
      "line": 799
    },
    {
      "name": "psutil",
      "line": 800
    },
    {
      "name": "math",
      "line": 801
    },
    {
      "name": "multiprocessing",
      "line": 1060
    },
    {
      "name": "psutil",
      "line": 1061
    },
    {
      "name": "math",
      "line": 1062
    },
    {
      "name": "types",
      "line": 1462
    },
    {
      "name": "lazy_augmentation",
      "line": 2467
    },
    {
      "name": "traceback",
      "line": 2538
    },
    {
      "name": "lazy_augmentation.create_lazy_augmented_dataset",
      "line": 2473
    },
    {
      "name": "multiprocessing",
      "line": 965
    },
    {
      "name": "psutil",
      "line": 966
    },
    {
      "name": "math",
      "line": 967
    }
  ],
  "classes": {
    "PatternDataMediator": {
      "start_line": 88,
      "end_line": 237,
      "methods": {
        "__init__": {
          "start_line": 94,
          "end_line": 105,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [],
          "code_snippet": "    \"\"\"\n    \n    def __init__(self):\n        # Raw data storage - organized by epoch for better management\n        self.batch_data_by_epoch = {}  # {epoch: [(batch_indices, correct_mask), ...]}\n        \n        # Processed metrics cache - also organized by epoch\n        self.metrics_by_epoch = {}  # {epoch: {'accuracies': {}, 'risks': {}}}\n        \n        # Current state\n        self.current_epoch = 0\n        self.pattern_service = None\n    \n    def set_pattern_service(self, pattern_service):\n        \"\"\"Set the pattern service reference for mapping indices to pattern types.\"\"\"\n        self.pattern_service = pattern_service"
        },
        "set_pattern_service": {
          "start_line": 105,
          "end_line": 109,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "pattern_service"
            }
          ],
          "return_type": null,
          "calls": [],
          "docstring": "Set the pattern service reference for mapping indices to pattern types.",
          "code_snippet": "        self.pattern_service = None\n    \n    def set_pattern_service(self, pattern_service):\n        \"\"\"Set the pattern service reference for mapping indices to pattern types.\"\"\"\n        self.pattern_service = pattern_service\n    \n    def update_with_batch_recognition(self, batch_indices, correct_mask, epoch):\n        \"\"\"Store raw batch recognition data from isekaiZen with epoch tracking.\"\"\"\n        # Update current epoch"
        },
        "update_with_batch_recognition": {
          "start_line": 109,
          "end_line": 126,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "batch_indices"
            },
            {
              "name": "correct_mask"
            },
            {
              "name": "epoch"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "....append",
              "line": 120
            },
            {
              "name": "logger.debug",
              "line": 121
            },
            {
              "name": "self._cleanup_old_epochs",
              "line": 124
            },
            {
              "name": "logger.debug",
              "line": 117
            },
            {
              "name": "....format",
              "line": 121
            },
            {
              "name": "....format",
              "line": 117
            },
            {
              "name": "len",
              "line": 121
            }
          ],
          "docstring": "Store raw batch recognition data from isekaiZen with epoch tracking.",
          "code_snippet": "        self.pattern_service = pattern_service\n    \n    def update_with_batch_recognition(self, batch_indices, correct_mask, epoch):\n        \"\"\"Store raw batch recognition data from isekaiZen with epoch tracking.\"\"\"\n        # Update current epoch\n        self.current_epoch = epoch\n        \n        # Initialize epoch data if needed\n        if epoch not in self.batch_data_by_epoch:\n            self.batch_data_by_epoch[epoch] = []\n            logger.debug(\"PatternDataMediator: Initialized data storage for epoch {}\".format(epoch))\n        \n        # Store the batch data\n        self.batch_data_by_epoch[epoch].append((batch_indices, correct_mask))\n        logger.debug(\"PatternDataMediator: Added batch with {} examples for epoch {}\".format(len(batch_indices), epoch))\n        \n        # Clean up old epochs - keep only current and previous\n        self._cleanup_old_epochs()\n    \n    def _process_epoch_data(self, epoch):\n        \"\"\"Process data for a specific epoch.\"\"\"\n        if not self.pattern_service or epoch not in self.batch_data_by_epoch:"
        },
        "_process_epoch_data": {
          "start_line": 126,
          "end_line": 183,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "epoch"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logger.debug",
              "line": 132
            },
            {
              "name": "pattern_total.items",
              "line": 158
            },
            {
              "name": "logger.debug",
              "line": 172
            },
            {
              "name": "logger.isEnabledFor",
              "line": 173
            },
            {
              "name": "logger.warning",
              "line": 129
            },
            {
              "name": "....format",
              "line": 132
            },
            {
              "name": "enumerate",
              "line": 141
            },
            {
              "name": "....format",
              "line": 172
            },
            {
              "name": "accuracies.items",
              "line": 174
            },
            {
              "name": "....format",
              "line": 129
            },
            {
              "name": "self.pattern_service.get_pattern_type",
              "line": 142
            },
            {
              "name": "max",
              "line": 162
            },
            {
              "name": "logger.debug",
              "line": 175
            },
            {
              "name": "min",
              "line": 162
            },
            {
              "name": "....format",
              "line": 175
            },
            {
              "name": "risks.get",
              "line": 176
            },
            {
              "name": "pattern_total.get",
              "line": 176
            },
            {
              "name": "len",
              "line": 152
            }
          ],
          "docstring": "Process data for a specific epoch.",
          "code_snippet": "        self._cleanup_old_epochs()\n    \n    def _process_epoch_data(self, epoch):\n        \"\"\"Process data for a specific epoch.\"\"\"\n        if not self.pattern_service or epoch not in self.batch_data_by_epoch:\n            logger.warning(\"PatternDataMediator: Cannot process epoch {} - pattern service not available or no data\".format(epoch))\n            return False\n            \n        logger.debug(\"PatternDataMediator: Processing data for epoch {}\".format(epoch))\n        \n        # Initialize counters\n        pattern_correct = {}\n        pattern_total = {}\n        \n        # Process all batch data for this epoch\n        for batch_indices, correct_mask in self.batch_data_by_epoch[epoch]:\n            # Map batch indices to pattern types\n            for i, idx in enumerate(batch_indices):\n                pattern_type = self.pattern_service.get_pattern_type(idx)\n                \n                if pattern_type:\n                    # Initialize counters if needed\n                    if pattern_type not in pattern_total:\n                        pattern_total[pattern_type] = 0\n                        pattern_correct[pattern_type] = 0\n                    \n                    # Update counters\n                    pattern_total[pattern_type] += 1\n                    if i < len(correct_mask) and correct_mask[i]:\n                        pattern_correct[pattern_type] += 1\n        \n        # Calculate accuracies and risks\n        accuracies = {}\n        risks = {}\n        for pattern_type, total in pattern_total.items():\n            if total > 0:\n                accuracy = pattern_correct[pattern_type] / total\n                accuracies[pattern_type] = accuracy\n                risks[pattern_type] = max(0.1, min(0.9, 1.0 - accuracy))\n        \n        # Store the processed metrics\n        self.metrics_by_epoch[epoch] = {\n            'accuracies': accuracies,\n            'risks': risks,\n            'processed': True\n        }\n        \n        # Log the results\n        logger.debug(\"PatternDataMediator: Processed epoch {} data:\".format(epoch))\n        if logger.isEnabledFor(logging.DEBUG):\n            for pattern_type, accuracy in accuracies.items():\n                logger.debug(\"  Pattern '{}': accuracy={:.4f}, risk={:.4f}, count={}\".format(\n                    pattern_type, accuracy, risks.get(pattern_type, 0), pattern_total.get(pattern_type, 0)))\n        \n        # Clear the raw data to save memory\n        self.batch_data_by_epoch[epoch] = [('processed', True)]\n        \n        return True\n    \n    def _cleanup_old_epochs(self):\n        \"\"\"Remove data from epochs except current and previous.\"\"\"\n        # Keep only current and previous epoch"
        },
        "_cleanup_old_epochs": {
          "start_line": 183,
          "end_line": 198,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "list",
              "line": 189
            },
            {
              "name": "list",
              "line": 194
            },
            {
              "name": "sorted",
              "line": 186
            },
            {
              "name": "self.batch_data_by_epoch.keys",
              "line": 189
            },
            {
              "name": "self.metrics_by_epoch.keys",
              "line": 194
            },
            {
              "name": "self.batch_data_by_epoch.keys",
              "line": 186
            }
          ],
          "docstring": "Remove data from epochs except current and previous.",
          "code_snippet": "        return True\n    \n    def _cleanup_old_epochs(self):\n        \"\"\"Remove data from epochs except current and previous.\"\"\"\n        # Keep only current and previous epoch\n        epochs_to_keep = sorted(self.batch_data_by_epoch.keys(), reverse=True)[:2]\n        \n        # Remove older epochs\n        for epoch in list(self.batch_data_by_epoch.keys()):\n            if epoch not in epochs_to_keep:\n                del self.batch_data_by_epoch[epoch]\n        \n        # Also clean up metrics\n        for epoch in list(self.metrics_by_epoch.keys()):\n            if epoch not in epochs_to_keep:\n                del self.metrics_by_epoch[epoch]\n    \n    def get_pattern_accuracies(self, epoch=None, force_recalculate=False):\n        \"\"\"Get pattern accuracies for the specified epoch (defaults to current).\"\"\"\n        epoch = self.current_epoch if epoch is None else epoch"
        },
        "get_pattern_accuracies": {
          "start_line": 198,
          "end_line": 212,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "epoch"
            },
            {
              "name": "force_recalculate"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self._process_epoch_data",
              "line": 205
            }
          ],
          "docstring": "Get pattern accuracies for the specified epoch (defaults to current).",
          "code_snippet": "                del self.metrics_by_epoch[epoch]\n    \n    def get_pattern_accuracies(self, epoch=None, force_recalculate=False):\n        \"\"\"Get pattern accuracies for the specified epoch (defaults to current).\"\"\"\n        epoch = self.current_epoch if epoch is None else epoch\n        \n        # Check if we need to process the data\n        if force_recalculate or epoch not in self.metrics_by_epoch:\n            # We need to process this epoch\n            self._process_epoch_data(epoch)\n        \n        # Return the metrics (empty dict if not available)\n        if epoch in self.metrics_by_epoch:\n            return self.metrics_by_epoch[epoch]['accuracies']\n        return {}\n    \n    def get_pattern_risks(self, epoch=None, force_recalculate=False):\n        \"\"\"Get pattern risks for the specified epoch.\"\"\"\n        epoch = self.current_epoch if epoch is None else epoch"
        },
        "get_pattern_risks": {
          "start_line": 212,
          "end_line": 224,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "epoch"
            },
            {
              "name": "force_recalculate"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.get_pattern_accuracies",
              "line": 217
            }
          ],
          "docstring": "Get pattern risks for the specified epoch.",
          "code_snippet": "        return {}\n    \n    def get_pattern_risks(self, epoch=None, force_recalculate=False):\n        \"\"\"Get pattern risks for the specified epoch.\"\"\"\n        epoch = self.current_epoch if epoch is None else epoch\n        \n        # Ensure metrics are calculated\n        self.get_pattern_accuracies(epoch, force_recalculate)\n        \n        # Return the risks\n        if epoch in self.metrics_by_epoch:\n            return self.metrics_by_epoch[epoch]['risks']\n        return {}\n    \n    def end_epoch(self, epoch):\n        \"\"\"Signal the end of an epoch to ensure all data is processed.\"\"\"\n        logger.debug(\"PatternDataMediator: End of epoch {} signaled, processing remaining data\".format(epoch))"
        },
        "end_epoch": {
          "start_line": 224,
          "end_line": 237,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "epoch"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logger.debug",
              "line": 226
            },
            {
              "name": "self._process_epoch_data",
              "line": 229
            },
            {
              "name": "self._cleanup_old_epochs",
              "line": 235
            },
            {
              "name": "....format",
              "line": 226
            }
          ],
          "docstring": "Signal the end of an epoch to ensure all data is processed.",
          "code_snippet": "        return {}\n    \n    def end_epoch(self, epoch):\n        \"\"\"Signal the end of an epoch to ensure all data is processed.\"\"\"\n        logger.debug(\"PatternDataMediator: End of epoch {} signaled, processing remaining data\".format(epoch))\n        \n        # Process any remaining data for this epoch\n        self._process_epoch_data(epoch)\n        \n        # Move to the next epoch\n        self.current_epoch = epoch + 1\n        \n        # Clean up old epochs\n        self._cleanup_old_epochs()\n\n# Add parent directory to path to import isekaizen package\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))\n"
        }
      },
      "class_variables": [],
      "bases": [],
      "docstring": "\n    Mediator component that handles data transfer between isekaiZen pattern tracking \n    and EVE optimizer, with efficient caching and calculation management.\n    "
    },
    "UnifiedRatioTrainer": {
      "start_line": 359,
      "end_line": 1313,
      "methods": {
        "__init__": {
          "start_line": 365,
          "end_line": 488,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "model"
            },
            {
              "name": "criterion"
            },
            {
              "name": "optimizer_class"
            },
            {
              "name": "optimizer"
            },
            {
              "name": "optimizer_kwargs"
            },
            {
              "name": "scheduler_class"
            },
            {
              "name": "scheduler_kwargs"
            },
            {
              "name": "device"
            },
            {
              "name": "pattern_map"
            },
            {
              "name": "batch_optimizer_class"
            },
            {
              "name": "batch_optimizer_kwargs"
            },
            {
              "name": "val_dataset"
            },
            {
              "name": "pattern_mediator"
            },
            {
              "name": "pattern_service"
            },
            {
              "name": "use_augmentation"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "....__init__",
              "line": 388
            },
            {
              "name": "kwargs.get",
              "line": 407
            },
            {
              "name": "hasattr",
              "line": 418
            },
            {
              "name": "self._initialize_lazy_augmentation",
              "line": 474
            },
            {
              "name": "kwargs.get",
              "line": 411
            },
            {
              "name": "kwargs.get",
              "line": 412
            },
            {
              "name": "kwargs.get",
              "line": 413
            },
            {
              "name": "kwargs.get",
              "line": 414
            },
            {
              "name": "logger.debug",
              "line": 420
            },
            {
              "name": "logger.warning",
              "line": 433
            },
            {
              "name": "PatternRecognitionService",
              "line": 439
            },
            {
              "name": "logger.debug",
              "line": 440
            },
            {
              "name": "hasattr",
              "line": 443
            },
            {
              "name": "hasattr",
              "line": 479
            },
            {
              "name": "logger.info",
              "line": 484
            },
            {
              "name": "super",
              "line": 388
            },
            {
              "name": "hasattr",
              "line": 423
            },
            {
              "name": "PatternRecognitionService",
              "line": 425
            },
            {
              "name": "hasattr",
              "line": 426
            },
            {
              "name": "logger.debug",
              "line": 431
            },
            {
              "name": "logger.debug",
              "line": 445
            },
            {
              "name": "logger.warning",
              "line": 486
            },
            {
              "name": "self.pattern_mediator.set_pattern_service",
              "line": 427
            }
          ],
          "docstring": "\n        Initialize the trainer with unified ratio tracking.\n        ",
          "code_snippet": "    \"\"\"\n    \n    def __init__(\n        self,\n        model,\n        criterion,\n        optimizer_class,\n        optimizer=None,\n        optimizer_kwargs=None,\n        scheduler_class=None,\n        scheduler_kwargs=None,\n        device=None,\n        pattern_map=None,\n        batch_optimizer_class=None,\n        batch_optimizer_kwargs=None,\n        val_dataset=None,\n        pattern_mediator=None,  # Accept pre-initialized mediator\n        pattern_service=None,   # Accept pre-initialized service\n        use_augmentation=True,  # Whether to use augmentation mediator\n        **kwargs\n    ):\n        \"\"\"\n        Initialize the trainer with unified ratio tracking.\n        \"\"\"\n        # Initialize base trainer\n        super().__init__(\n            model=model,\n            criterion=criterion,\n            optimizer_class=optimizer_class,\n            optimizer=optimizer,\n            optimizer_kwargs=optimizer_kwargs, \n            scheduler_class=scheduler_class,\n            scheduler_kwargs=scheduler_kwargs,\n            device=device,\n            pattern_map=pattern_map,\n            batch_optimizer_class=batch_optimizer_class,\n            batch_optimizer_kwargs=batch_optimizer_kwargs,\n            **kwargs\n        )\n        \n        # Store the batch optimizer kwargs explicitly\n        self.batch_optimizer_kwargs = batch_optimizer_kwargs if batch_optimizer_kwargs is not None else {}\n        \n        # Store the model architecture type for swapping\n        self.current_model_type = kwargs.get('model_type', 'resnet18')\n        \n        # Store model configuration parameters\n        self.model_config = {\n            'use_pretrained': kwargs.get('use_pretrained', False),\n            'num_classes': kwargs.get('num_classes', 10),\n            'input_channels': kwargs.get('input_channels', 3),\n            'input_size': kwargs.get('input_size', 32)\n        }\n        \n        # Connect to the optimizer's internal pattern mediator\n        if hasattr(self.optimizer, 'pattern_mediator'):\n            self.pattern_mediator = self.optimizer.pattern_mediator\n            logger.debug(\"Connected to optimizer's internal pattern mediator\")\n            \n            # If we have a pattern map, ensure the mediator has a pattern service\n            if pattern_map and hasattr(self.pattern_mediator, 'pattern_service') and self.pattern_mediator.pattern_service is None:\n                from isekaizen.pattern.detection import PatternRecognitionService\n                pattern_service = PatternRecognitionService(pattern_map)\n                if hasattr(self.pattern_mediator, 'set_pattern_service'):\n                    self.pattern_mediator.set_pattern_service(pattern_service)\n                else:\n                    # Direct assignment if no setter method\n                    self.pattern_mediator.pattern_service = pattern_service\n                logger.debug(\"Set pattern service on optimizer's mediator\")\n        else:\n            logger.warning(\"Optimizer does not have a pattern mediator\")\n            self.pattern_mediator = None\n        \n        # Initialize pattern service if needed\n        if pattern_map:\n            from isekaizen.pattern.detection import PatternRecognitionService\n            self.pattern_service = PatternRecognitionService(pattern_map)\n            logger.debug(\"PatternRecognitionService initialized for trainer\")\n            \n            # Connect pattern service to optimizer's mediator\n            if hasattr(self.optimizer, 'pattern_service'):\n                self.optimizer.pattern_service = self.pattern_service\n                logger.debug(\"Connected pattern service to optimizer\")\n\n        # We don't need to connect mediator to optimizer since it has its own internal mediator\n        \n        # Initialize dataset adaptations tracking\n        self.dataset_adaptations = []\n        \n        # Store validation dataset reference for mini-validation  \n        self.val_dataset = val_dataset\n        \n        # Current epoch tracking\n        self.current_epoch = 0\n        self.last_val_acc = 0  # Initialize last validation accuracy\n        self.last_train_acc = 0  # Initialize last training accuracy\n        \n        # Initialize model swapping metrics\n        self.model_swaps = []\n        self.swap_thresholds = {\n            'accuracy_stagnation': 0.005,  # Less than 0.5% improvement\n            'consecutive_epochs': 3,       # For 3 consecutive epochs\n            'min_epoch': 5                 # Don't swap before epoch 5\n        }\n        \n        # Set use_augmentation flag for lazy augmentation\n        self.use_augmentation = use_augmentation\n        # We don't need the augmentation_mediator, we'll use lazy_augmentation instead\n        self.augmentation_mediator = None\n            \n        # Initialize lazy augmentation support\n        self._initialize_lazy_augmentation()\n        \n        # Flag to track if using lazy augmentation\n        self.using_lazy_augmentation = (\n            use_augmentation and \n            hasattr(self, 'lazy_augmentation_available') and\n            self.lazy_augmentation_available\n        )\n        \n        if self.using_lazy_augmentation:\n            logger.info(\"Lazy pattern augmentation is enabled\")\n        elif use_augmentation:\n            logger.warning(\"Augmentation requested but lazy pattern augmentation not available\")\n    \n    def _initialize_lazy_augmentation(self):\n        \"\"\"Initialize lazy augmentation support.\"\"\"\n        try:"
        },
        "_initialize_lazy_augmentation": {
          "start_line": 488,
          "end_line": 506,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logger.info",
              "line": 501
            },
            {
              "name": "logger.warning",
              "line": 503
            },
            {
              "name": "....format",
              "line": 503
            },
            {
              "name": "str",
              "line": 503
            }
          ],
          "docstring": "Initialize lazy augmentation support.",
          "code_snippet": "            logger.warning(\"Augmentation requested but lazy pattern augmentation not available\")\n    \n    def _initialize_lazy_augmentation(self):\n        \"\"\"Initialize lazy augmentation support.\"\"\"\n        try:\n            # Import the lazy augmentation module\n            import lazy_augmentation\n            \n            # Store references to required classes and functions\n            self.LazyPatternAugmentedDataset = lazy_augmentation.LazyPatternAugmentedDataset\n            self.create_optimized_dataloader = lazy_augmentation.create_optimized_dataloader\n            self.create_lazy_augmented_dataset = lazy_augmentation.create_lazy_augmented_dataset\n            \n            # Flag that lazy augmentation is available\n            self.lazy_augmentation_available = True\n            logger.info(\"Lazy pattern augmentation is available\")\n        except ImportError as e:\n            logger.warning(\"Could not import lazy augmentation module: {}\".format(str(e)))\n            self.lazy_augmentation_available = False\n    \n    def swap_model(self, new_model_type):\n        \"\"\"\n        Swap the current model for a different architecture and transfer weights where possible."
        },
        "swap_model": {
          "start_line": 506,
          "end_line": 617,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "new_model_type"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logger.info",
              "line": 520
            },
            {
              "name": "logger.warning",
              "line": 517
            },
            {
              "name": "....format",
              "line": 520
            },
            {
              "name": "hasattr",
              "line": 525
            },
            {
              "name": "create_model",
              "line": 529
            },
            {
              "name": "new_model.to",
              "line": 538
            },
            {
              "name": "old_model.state_dict",
              "line": 541
            },
            {
              "name": "new_model.state_dict",
              "line": 542
            },
            {
              "name": "old_state.items",
              "line": 546
            },
            {
              "name": "len",
              "line": 551
            },
            {
              "name": "len",
              "line": 552
            },
            {
              "name": "logger.info",
              "line": 555
            },
            {
              "name": "self.model_swaps.append",
              "line": 587
            },
            {
              "name": "....format",
              "line": 517
            },
            {
              "name": "....format",
              "line": 555
            },
            {
              "name": "new_state.update",
              "line": 560
            },
            {
              "name": "new_model.load_state_dict",
              "line": 561
            },
            {
              "name": "logger.info",
              "line": 562
            },
            {
              "name": "logger.warning",
              "line": 564
            },
            {
              "name": "self.optimizer_class",
              "line": 578
            },
            {
              "name": "logger.info",
              "line": 579
            },
            {
              "name": "logger.error",
              "line": 603
            },
            {
              "name": "logger.error",
              "line": 605
            },
            {
              "name": "....format",
              "line": 562
            },
            {
              "name": "hasattr",
              "line": 571
            },
            {
              "name": "self.model.parameters",
              "line": 578
            },
            {
              "name": "hasattr",
              "line": 582
            },
            {
              "name": "hasattr",
              "line": 582
            },
            {
              "name": "self.scheduler_class",
              "line": 583
            },
            {
              "name": "logger.info",
              "line": 584
            },
            {
              "name": "....format",
              "line": 603
            },
            {
              "name": "traceback.format_exc",
              "line": 605
            },
            {
              "name": "hasattr",
              "line": 608
            },
            {
              "name": "logger.warning",
              "line": 609
            },
            {
              "name": "str",
              "line": 603
            }
          ],
          "docstring": "\n        Swap the current model for a different architecture and transfer weights where possible.\n        \n        Args:\n            new_model_type: Type of the new model (e.g., \"resnet34\", \"vgg16\")\n            \n        Returns:\n            True if swapped successfully, False otherwise\n        ",
          "code_snippet": "            self.lazy_augmentation_available = False\n    \n    def swap_model(self, new_model_type):\n        \"\"\"\n        Swap the current model for a different architecture and transfer weights where possible.\n        \n        Args:\n            new_model_type: Type of the new model (e.g., \"resnet34\", \"vgg16\")\n            \n        Returns:\n            True if swapped successfully, False otherwise\n        \"\"\"\n        if new_model_type == self.current_model_type:\n            logger.warning(\"Model is already {}, no need to swap\".format(new_model_type))\n            return False\n        \n        logger.info(\"Swapping model from {} to {}\".format(self.current_model_type, new_model_type))\n        \n        # Save reference to old model in case we need to revert\n        old_model = self.model\n        old_optimizer = self.optimizer\n        old_scheduler = self.scheduler if hasattr(self, 'scheduler') else None\n        \n        try:\n            # Create new model\n            new_model = create_model(\n                model_type=new_model_type,\n                use_pretrained=self.model_config['use_pretrained'],\n                num_classes=self.model_config['num_classes'],\n                input_channels=self.model_config['input_channels'],\n                input_size=self.model_config['input_size']\n            )\n            \n            # Move to the same device\n            new_model = new_model.to(self.device)\n            \n            # Extract state from old model\n            old_state = old_model.state_dict()\n            new_state = new_model.state_dict()\n            \n            # Try to transfer weights for layers with matching names and shapes\n            compatible_layers = {}\n            for name, param in old_state.items():\n                if name in new_state and new_state[name].shape == param.shape:\n                    compatible_layers[name] = param\n            \n            # Log transfer statistics\n            total_params = len(new_state)\n            transferred_params = len(compatible_layers)\n            transfer_percentage = (transferred_params / total_params) * 100\n            \n            logger.info(\"Transferring weights: {}/{} layers ({:.1f}% of parameters)\".format(\n                transferred_params, total_params, transfer_percentage))\n            \n            # Load compatible weights\n            if compatible_layers:\n                new_state.update(compatible_layers)\n                new_model.load_state_dict(new_state)\n                logger.info(\"Successfully transferred weights to {}\".format(new_model_type))\n            else:\n                logger.warning(\"No compatible layers found for weight transfer\")\n            \n            # Replace the model\n            self.model = new_model\n            \n            # Create a new optimizer with the new model parameters\n            if self.optimizer_class:\n                if hasattr(self, 'optimizer_kwargs') and self.optimizer_kwargs:\n                    optimizer_kwargs = self.optimizer_kwargs\n                else:\n                    # Use current optimizer's parameters as a starting point\n                    optimizer_kwargs = {'lr': old_optimizer.param_groups[0]['lr']}\n                    \n                # Create a new optimizer\n                self.optimizer = self.optimizer_class(self.model.parameters(), **optimizer_kwargs)\n                logger.info(\"Created new optimizer for the swapped model\")\n                \n                # Recreate the scheduler if needed\n                if hasattr(self, 'scheduler_class') and self.scheduler_class and hasattr(self, 'scheduler_kwargs'):\n                    self.scheduler = self.scheduler_class(self.optimizer, **self.scheduler_kwargs)\n                    logger.info(\"Created new scheduler for the swapped model\")\n            \n            # Record the swap\n            self.model_swaps.append({\n                'epoch': self.current_epoch,\n                'old_model': self.current_model_type,\n                'new_model': new_model_type,\n                'transferred_params': transferred_params,\n                'total_params': total_params,\n                'transfer_percentage': transfer_percentage\n            })\n            \n            # Update current model type\n            self.current_model_type = new_model_type\n            \n            return True\n            \n        except Exception as e:\n            # Log the error\n            logger.error(\"Error swapping model: {}\".format(str(e)))\n            import traceback\n            logger.error(traceback.format_exc())\n            \n            # Revert to old model/optimizer/scheduler if we got to the replacement stage\n            if hasattr(self, 'model') and self.model is not old_model:\n                logger.warning(\"Error occurred after model replacement, reverting to previous model\")\n                self.model = old_model\n                self.optimizer = old_optimizer\n                if old_scheduler:\n                    self.scheduler = old_scheduler\n            \n            return False\n            \n    def check_and_swap_model(self, history):\n        \"\"\"\n        Check if model swapping is needed based on training history and performance."
        },
        "check_and_swap_model": {
          "start_line": 617,
          "end_line": 691,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "history"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "history.get",
              "line": 632
            },
            {
              "name": "all",
              "line": 645
            },
            {
              "name": "len",
              "line": 636
            },
            {
              "name": "logger.debug",
              "line": 637
            },
            {
              "name": "logger.warning",
              "line": 649
            },
            {
              "name": "logger.info",
              "line": 652
            },
            {
              "name": "logger.info",
              "line": 653
            },
            {
              "name": "self._get_model_size_category",
              "line": 656
            },
            {
              "name": "logger.info",
              "line": 683
            },
            {
              "name": "self.swap_model",
              "line": 687
            },
            {
              "name": "....format",
              "line": 637
            },
            {
              "name": "....format",
              "line": 649
            },
            {
              "name": "....format",
              "line": 651
            },
            {
              "name": "....format",
              "line": 652
            },
            {
              "name": "....format",
              "line": 653
            },
            {
              "name": "self._get_next_model_size",
              "line": 662
            },
            {
              "name": "....format",
              "line": 683
            },
            {
              "name": "len",
              "line": 637
            },
            {
              "name": "....join",
              "line": 652
            }
          ],
          "docstring": "\n        Check if model swapping is needed based on training history and performance.\n        \n        Args:\n            history: Training history dictionary\n            \n        Returns:\n            True if model was swapped, False otherwise\n        ",
          "code_snippet": "            return False\n            \n    def check_and_swap_model(self, history):\n        \"\"\"\n        Check if model swapping is needed based on training history and performance.\n        \n        Args:\n            history: Training history dictionary\n            \n        Returns:\n            True if model was swapped, False otherwise\n        \"\"\"\n        # Don't swap too early in training\n        if self.current_epoch < self.swap_thresholds['min_epoch']:\n            return False\n        \n        # Check for accuracy stagnation over the last few epochs\n        val_acc = history.get('val_acc', [])\n        \n        # Need enough epochs of validation data to make a decision\n        required_epochs = self.swap_thresholds['consecutive_epochs'] + 1\n        if len(val_acc) < required_epochs:\n            logger.debug(\"Not enough validation data for model swap decision: {}/{}\".format(len(val_acc), required_epochs))\n            return False\n            \n        # Get the last few validation accuracies\n        recent_acc = val_acc[-self.swap_thresholds['consecutive_epochs']:]\n        previous_acc = val_acc[-(self.swap_thresholds['consecutive_epochs'] + 1)]\n        \n        # Check if all recent accuracies show minimal improvement\n        stagnant = all(acc - previous_acc < self.swap_thresholds['accuracy_stagnation'] \n                     for acc in recent_acc)\n        \n        if stagnant:\n            logger.warning(\"Detected accuracy stagnation over {} epochs\".format(self.swap_thresholds['consecutive_epochs']))\n            # Convert f-string in list comprehension to standard format\n            acc_strings = [\"{:.2f}%\".format(acc) for acc in recent_acc]\n            logger.info(\"Recent accuracies: {}\".format(\", \".join(acc_strings)))\n            logger.info(\"Previous accuracy: {:.2f}%\".format(previous_acc))\n            \n            # Decide which model to swap to based on current model and training progress\n            current_model_size = self._get_model_size_category(self.current_model_type)\n            \n            # Determine whether to go larger or smaller\n            current_acc = recent_acc[-1]\n            if current_acc < 70.0:\n                # For lower accuracy, prefer a larger model for more capacity\n                new_model_type = self._get_next_model_size(current_model_size, larger=True)\n            else:\n                # For good accuracy but stalled, try a different architecture family\n                if 'resnet' in self.current_model_type:\n                    if current_acc > 85.0:\n                        # Try more efficient architecture if accuracy is already good\n                        new_model_type = \"mobilenet_v2\"\n                    else:\n                        # Try more powerful architecture\n                        new_model_type = \"vgg16\"\n                elif 'vgg' in self.current_model_type:\n                    # VGG is large, try a more efficient architecture\n                    new_model_type = \"resnet34\"\n                elif 'mobilenet' in self.current_model_type:\n                    # MobileNet is efficient but may lack capacity\n                    new_model_type = \"resnet34\"\n                else:\n                    # Default fallback\n                    new_model_type = \"resnet50\"\n            \n            # Log the decision\n            logger.info(\"Current accuracy: {:.2f}%, proposing model swap: {} \u2192 {}\".format(\n                current_acc, self.current_model_type, new_model_type))\n            \n            # Perform the swap\n            return self.swap_model(new_model_type)\n        \n        return False\n        \n    def _get_model_size_category(self, model_type):\n        \"\"\"Determine the size category of a model.\"\"\"\n        if 'resnet18' in model_type or 'mobilenet' in model_type:"
        },
        "_get_model_size_category": {
          "start_line": 691,
          "end_line": 702,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "model_type"
            }
          ],
          "return_type": null,
          "calls": [],
          "docstring": "Determine the size category of a model.",
          "code_snippet": "        return False\n        \n    def _get_model_size_category(self, model_type):\n        \"\"\"Determine the size category of a model.\"\"\"\n        if 'resnet18' in model_type or 'mobilenet' in model_type:\n            return 'small'\n        elif 'resnet34' in model_type or 'resnet50' in model_type:\n            return 'medium'\n        elif 'vgg' in model_type or 'resnet101' in model_type or 'resnet152' in model_type:\n            return 'large'\n        else:\n            return 'medium'  # Default\n            \n    def _get_next_model_size(self, current_size, larger=True):\n        \"\"\"Get the next model size based on the current size.\"\"\"\n        size_progression = {"
        },
        "_get_next_model_size": {
          "start_line": 702,
          "end_line": 731,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "current_size"
            },
            {
              "name": "larger"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "....get",
              "line": 720
            },
            {
              "name": "size_to_model.get",
              "line": 729
            },
            {
              "name": "size_progression.get",
              "line": 720
            }
          ],
          "docstring": "Get the next model size based on the current size.",
          "code_snippet": "            return 'medium'  # Default\n            \n    def _get_next_model_size(self, current_size, larger=True):\n        \"\"\"Get the next model size based on the current size.\"\"\"\n        size_progression = {\n            'small': {\n                'larger': 'medium',\n                'smaller': 'small'  # Can't go smaller than small\n            },\n            'medium': {\n                'larger': 'large',\n                'smaller': 'small'\n            },\n            'large': {\n                'larger': 'large',  # Can't go larger than large\n                'smaller': 'medium'\n            }\n        }\n        \n        direction = 'larger' if larger else 'smaller'\n        next_size = size_progression.get(current_size, {}).get(direction, current_size)\n        \n        # Map size to actual model\n        size_to_model = {\n            'small': 'resnet18',\n            'medium': 'resnet34',\n            'large': 'resnet50'\n        }\n        \n        return size_to_model.get(next_size, 'resnet34')\n            \n    def _train_epoch(self, dataset, batch_size, mini_val_interval=50):\n        \"\"\"\n        Train for one epoch with mini-validation for continuous feedback."
        },
        "_train_epoch": {
          "start_line": 731,
          "end_line": 1026,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "dataset"
            },
            {
              "name": "batch_size"
            },
            {
              "name": "mini_val_interval"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.model.train",
              "line": 743
            },
            {
              "name": "enumerate",
              "line": 879
            },
            {
              "name": "hasattr",
              "line": 752
            },
            {
              "name": "hasattr",
              "line": 752
            },
            {
              "name": "logger.info",
              "line": 753
            },
            {
              "name": "hasattr",
              "line": 781
            },
            {
              "name": "self._apply_adaptive_augmentation",
              "line": 783
            },
            {
              "name": "hasattr",
              "line": 788
            },
            {
              "name": "self.create_optimized_dataloader",
              "line": 790
            },
            {
              "name": "hasattr",
              "line": 875
            },
            {
              "name": "logger.info",
              "line": 877
            },
            {
              "name": "list",
              "line": 885
            },
            {
              "name": "all_batch_indices.extend",
              "line": 886
            },
            {
              "name": "self.optimizer.zero_grad",
              "line": 889
            },
            {
              "name": "self.model",
              "line": 892
            },
            {
              "name": "self.criterion",
              "line": 893
            },
            {
              "name": "outputs.max",
              "line": 896
            },
            {
              "name": "predicted.eq",
              "line": 897
            },
            {
              "name": "list",
              "line": 900
            },
            {
              "name": "isinstance",
              "line": 903
            },
            {
              "name": "loss.item",
              "line": 930
            },
            {
              "name": "....item",
              "line": 931
            },
            {
              "name": "targets.size",
              "line": 933
            },
            {
              "name": "loss.backward",
              "line": 936
            },
            {
              "name": "hasattr",
              "line": 939
            },
            {
              "name": "len",
              "line": 1014
            },
            {
              "name": "hasattr",
              "line": 1019
            },
            {
              "name": "self.pattern_mediator.end_epoch",
              "line": 1020
            },
            {
              "name": "logger.debug",
              "line": 1021
            },
            {
              "name": "dataset.add_augmentations",
              "line": 761
            },
            {
              "name": "multiprocessing.cpu_count",
              "line": 804
            },
            {
              "name": "max",
              "line": 817
            },
            {
              "name": "torch.cuda.is_available",
              "line": 819
            },
            {
              "name": "logger.info",
              "line": 823
            },
            {
              "name": "torch.utils.data.DataLoader",
              "line": 855
            },
            {
              "name": "len",
              "line": 876
            },
            {
              "name": "inputs.to",
              "line": 881
            },
            {
              "name": "targets.to",
              "line": 881
            },
            {
              "name": "range",
              "line": 885
            },
            {
              "name": "range",
              "line": 900
            },
            {
              "name": "hasattr",
              "line": 908
            },
            {
              "name": "self.pattern_mediator.update_from_batch",
              "line": 910
            },
            {
              "name": "logger.debug",
              "line": 915
            },
            {
              "name": "isinstance",
              "line": 918
            },
            {
              "name": "self.pattern_service.get_batch_pattern_states",
              "line": 941
            },
            {
              "name": "self.optimizer.step",
              "line": 942
            },
            {
              "name": "self.optimizer.step",
              "line": 945
            },
            {
              "name": "hasattr",
              "line": 948
            },
            {
              "name": "torch.utils.data.Subset",
              "line": 950
            },
            {
              "name": "self.model.eval",
              "line": 992
            },
            {
              "name": "self.model.train",
              "line": 1011
            },
            {
              "name": "hasattr",
              "line": 764
            },
            {
              "name": "logger.info",
              "line": 767
            },
            {
              "name": "getattr",
              "line": 784
            },
            {
              "name": "getattr",
              "line": 785
            },
            {
              "name": "min",
              "line": 817
            },
            {
              "name": "min",
              "line": 821
            },
            {
              "name": "....format",
              "line": 823
            },
            {
              "name": "logger.warning",
              "line": 862
            },
            {
              "name": "torch.utils.data.DataLoader",
              "line": 863
            },
            {
              "name": "min",
              "line": 885
            },
            {
              "name": "min",
              "line": 900
            },
            {
              "name": "self.optimizer.update_accuracy_metrics_with_epoch",
              "line": 922
            },
            {
              "name": "correct_mask.sum",
              "line": 931
            },
            {
              "name": "list",
              "line": 950
            },
            {
              "name": "hasattr",
              "line": 953
            },
            {
              "name": "self.create_optimized_dataloader",
              "line": 955
            },
            {
              "name": "torch.no_grad",
              "line": 996
            },
            {
              "name": "isinstance",
              "line": 1008
            },
            {
              "name": "isinstance",
              "line": 1008
            },
            {
              "name": "self.optimizer.update_accuracy_metrics",
              "line": 1009
            },
            {
              "name": "len",
              "line": 753
            },
            {
              "name": "....append",
              "line": 766
            },
            {
              "name": "hasattr",
              "line": 773
            },
            {
              "name": "....append",
              "line": 774
            },
            {
              "name": "logger.info",
              "line": 775
            },
            {
              "name": "psutil.cpu_percent",
              "line": 809
            },
            {
              "name": "int",
              "line": 817
            },
            {
              "name": "torch.utils.data.default_collate",
              "line": 831
            },
            {
              "name": "torch.cuda.is_available",
              "line": 858
            },
            {
              "name": "....format",
              "line": 862
            },
            {
              "name": "len",
              "line": 877
            },
            {
              "name": "len",
              "line": 885
            },
            {
              "name": "len",
              "line": 900
            },
            {
              "name": "getattr",
              "line": 924
            },
            {
              "name": "self.batch_optimizer_kwargs.get",
              "line": 926
            },
            {
              "name": "range",
              "line": 950
            },
            {
              "name": "multiprocessing.cpu_count",
              "line": 970
            },
            {
              "name": "max",
              "line": 971
            },
            {
              "name": "torch.cuda.is_available",
              "line": 973
            },
            {
              "name": "torch.utils.data.DataLoader",
              "line": 978
            },
            {
              "name": "self.model",
              "line": 999
            },
            {
              "name": "val_outputs.max",
              "line": 1000
            },
            {
              "name": "....item",
              "line": 1001
            },
            {
              "name": "val_targets.size",
              "line": 1002
            },
            {
              "name": "len",
              "line": 774
            },
            {
              "name": "str",
              "line": 862
            },
            {
              "name": "min",
              "line": 950
            },
            {
              "name": "min",
              "line": 971
            },
            {
              "name": "min",
              "line": 975
            },
            {
              "name": "logger.warning",
              "line": 985
            },
            {
              "name": "torch.utils.data.DataLoader",
              "line": 987
            },
            {
              "name": "val_inputs.to",
              "line": 998
            },
            {
              "name": "val_targets.to",
              "line": 998
            },
            {
              "name": "data.append",
              "line": 838
            },
            {
              "name": "isinstance",
              "line": 841
            },
            {
              "name": "len",
              "line": 950
            },
            {
              "name": "int",
              "line": 971
            },
            {
              "name": "torch.cuda.is_available",
              "line": 982
            },
            {
              "name": "....format",
              "line": 985
            },
            {
              "name": "....sum",
              "line": 1001
            },
            {
              "name": "len",
              "line": 775
            },
            {
              "name": "targets.append",
              "line": 842
            },
            {
              "name": "targets.append",
              "line": 844
            },
            {
              "name": "torch.stack",
              "line": 848
            },
            {
              "name": "torch.stack",
              "line": 848
            },
            {
              "name": "logger.warning",
              "line": 851
            },
            {
              "name": "str",
              "line": 985
            },
            {
              "name": "torch.tensor",
              "line": 842
            },
            {
              "name": "....unsqueeze",
              "line": 852
            },
            {
              "name": "....unsqueeze",
              "line": 852
            },
            {
              "name": "val_predicted.eq",
              "line": 1001
            }
          ],
          "docstring": "\n        Train for one epoch with mini-validation for continuous feedback.\n        \n        Args:\n            dataset: Dataset to train on\n            batch_size: Batch size\n            mini_val_interval: Interval for mini-validation during training\n            \n        Returns:\n            Dictionary with training metrics\n        ",
          "code_snippet": "        return size_to_model.get(next_size, 'resnet34')\n            \n    def _train_epoch(self, dataset, batch_size, mini_val_interval=50):\n        \"\"\"\n        Train for one epoch with mini-validation for continuous feedback.\n        \n        Args:\n            dataset: Dataset to train on\n            batch_size: Batch size\n            mini_val_interval: Interval for mini-validation during training\n            \n        Returns:\n            Dictionary with training metrics\n        \"\"\"\n        self.model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        # We're using lazy_augmentation instead of AugmentationMediator\n        # No need to initialize anything here since it's handled by lazy_augmentation.py\n        \n        # Apply any pending augmentations from validation-based decisions\n        if hasattr(self, 'pending_augmentations') and self.pending_augmentations and hasattr(dataset, 'add_augmentations'):\n            logger.info(f\"Applying {len(self.pending_augmentations)} pending augmentations from previous validation\")\n            \n            for aug_plan in self.pending_augmentations:\n                pattern_type = aug_plan['pattern_type']\n                percentage = aug_plan['percentage']\n                level = aug_plan['level']\n                \n                # Apply the planned augmentation\n                added_count = dataset.add_augmentations(pattern_type, percentage)\n                \n                # Record the addition\n                if added_count > 0 and hasattr(self, 'added_augmentation_levels'):\n                    if pattern_type in self.added_augmentation_levels and level not in self.added_augmentation_levels[pattern_type]:\n                        self.added_augmentation_levels[pattern_type].append(level)\n                    logger.info(f\"Applied pending {level} augmentation: Added {added_count} examples for {pattern_type}\")\n                    \n                    # Track when we last added augmentation to prevent OOM\n                    self.last_augmentation_epoch = self.current_epoch\n                    \n                    # Record dataset size change for visualization\n                    if hasattr(self, 'history') and 'dataset_sizes' in self.history:\n                        self.history['dataset_sizes'].append(len(dataset))\n                        logger.info(f\"Dataset size updated to {len(dataset)} examples\")\n            \n            # Clear pending augmentations\n            self.pending_augmentations = []\n            \n        # Also apply validation-based augmentation directly if performance indicates it's needed\n        if hasattr(self, 'using_lazy_augmentation') and self.using_lazy_augmentation:\n            # Try immediate application with the current dataset\n            self._apply_adaptive_augmentation(train_dataset=dataset, \n                                            val_loss=getattr(self, 'last_val_loss', None),\n                                            val_acc=getattr(self, 'last_val_acc', None))\n        \n        # Create dataloader\n        if hasattr(self, 'using_lazy_augmentation') and self.using_lazy_augmentation:\n            # Use optimized dataloader from lazy augmentation module\n            dataloader = self.create_optimized_dataloader(\n                dataset=dataset,\n                batch_size=batch_size,\n                shuffle=True\n            )\n        else:\n            # Fallback to standard DataLoader with conservative settings\n            try:\n                # Calculate optimal number of workers based on system capabilities\n                import multiprocessing\n                import psutil\n                import math\n                \n                # Get number of CPU cores\n                cpu_count = multiprocessing.cpu_count()\n                \n                # Get system load\n                system_load = 0.5  # Default value if psutil fails\n                try:\n                    system_load = psutil.cpu_percent(interval=0.1) / 100.0\n                except:\n                    pass\n                    \n                # Calculate resource factor based on cognitive efficiency formula\n                resource_factor = (1 - system_load * 0.8)\n                \n                # Calculate optimal workers - apply parallel processing penalty\n                optimal_workers = max(0, min(int(cpu_count * resource_factor * 0.75), 4))\n                \n                if torch.cuda.is_available():\n                    # When using GPU, we want fewer workers\n                    optimal_workers = min(optimal_workers, 2)\n                    \n                logger.info(\"Using {} DataLoader workers (cores: {}, load: {:.2f})\".format(\n                    optimal_workers, cpu_count, system_load))\n                \n                # Define custom collate function to handle tensor type inconsistencies\n                def custom_collate_fn(batch):\n                    \"\"\"Custom collate function to ensure consistent tensor types\"\"\"\n                    try:\n                        # Default collation if everything is already properly formatted\n                        return torch.utils.data.default_collate(batch)\n                    except (TypeError, RuntimeError) as e:\n                        # If default collation fails, handle it manually\n                        data = []\n                        targets = []\n                        for item in batch:\n                            # Handle data\n                            data.append(item[0])\n                            \n                            # Handle target - ensure it's a tensor\n                            if isinstance(item[1], (int, float)):\n                                targets.append(torch.tensor(item[1]))\n                            else:\n                                targets.append(item[1])\n                        \n                        # Stack the data and targets\n                        try:\n                            return torch.stack(data), torch.stack(targets)\n                        except:\n                            # Last resort fallback\n                            logger.warning(\"Custom collation failed, returning limited batch\")\n                            return data[0].unsqueeze(0), targets[0].unsqueeze(0)\n                \n                # Use optimal workers for DataLoader with custom collate function\n                dataloader = torch.utils.data.DataLoader(\n                    dataset, batch_size=batch_size, shuffle=True, \n                    num_workers=optimal_workers,\n                    pin_memory=torch.cuda.is_available(),\n                    collate_fn=custom_collate_fn)\n                    \n            except Exception as e:\n                logger.warning(\"Error creating DataLoader: {}\".format(str(e)))\n                dataloader = torch.utils.data.DataLoader(\n                    dataset, batch_size=batch_size, shuffle=True, \n                    num_workers=0,\n                    collate_fn=custom_collate_fn)\n                    \n        # For batch-level risk assessment\n        all_batch_indices = []\n        \n        # Track batch numbers for mini-validation\n        batch_count = 0\n        \n        # Initialize dataset sizes tracking if not present\n        if hasattr(self, 'history') and 'dataset_sizes' not in self.history:\n            self.history['dataset_sizes'] = [len(dataset)]\n            logger.info(f\"Initialized dataset size tracking with {len(dataset)} examples\")\n        \n        for i, (inputs, targets) in enumerate(dataloader):\n            batch_count += 1\n            inputs, targets = inputs.to(self.device), targets.to(self.device)\n            \n            # Get batch indices for pattern tracking\n            batch_start = i * batch_size\n            batch_indices = list(range(batch_start, min(batch_start + batch_size, len(dataset))))\n            all_batch_indices.extend(batch_indices)\n            \n            # Zero the parameter gradients\n            self.optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = self.model(inputs)\n            loss = self.criterion(outputs, targets)\n            \n            # Calculate per-example correctness for pattern recognition\n            _, predicted = outputs.max(1)\n            correct_mask = predicted.eq(targets)\n            \n            # Batch indices for pattern tracking\n            batch_indices_list = list(range(batch_start, min(batch_start + batch_size, len(dataset))))\n            \n            # Store batch data in the optimizer for pattern tracking\n            if isinstance(self.optimizer, EVEUnifiedRatio):\n                self.optimizer.last_batch_indices = batch_indices_list\n                self.optimizer.last_correct_mask = correct_mask\n            \n            # Update the pattern mediator with batch recognition data\n            if self.pattern_mediator and hasattr(self.pattern_mediator, 'update_from_batch'):\n                # Direct update from batch for internal mediator\n                self.pattern_mediator.update_from_batch(\n                    batch_indices_list, \n                    correct_mask, \n                    self.current_epoch\n                )\n                logger.debug(f\"Updated pattern mediator with batch data for epoch {self.current_epoch}\")\n                \n                # Now update the optimizer with epoch accuracy info if it's an EVE optimizer\n                if isinstance(self.optimizer, EVEUnifiedRatio):\n                    # Calculate training accuracy safely - avoid division by zero\n                    train_accuracy = 0.0 if total == 0 else (correct / total * 100)\n                    \n                    self.optimizer.update_accuracy_metrics_with_epoch(\n                        train_accuracy,  # Current train accuracy as percentage\n                        getattr(self, 'last_val_acc', 0),  # Last validation accuracy if available\n                        self.current_epoch,\n                        self.batch_optimizer_kwargs.get('total_epochs', 100)\n                    )\n            \n            # Update metrics\n            running_loss += loss.item()\n            batch_correct = correct_mask.sum().item()\n            correct += batch_correct\n            total += targets.size(0)\n            \n            # Backward pass and optimize\n            loss.backward()\n            \n            # Special handling for EVE optimizers - pass pattern states\n            if hasattr(self, 'pattern_service'):\n                # Get pattern states for this batch\n                pattern_states = self.pattern_service.get_batch_pattern_states(batch_indices_list)\n                self.optimizer.step(pattern_states=pattern_states)\n            else:\n                # Normal optimization step\n                self.optimizer.step()\n            \n            # Perform mini-validation to get more frequent feedback\n            if batch_count % mini_val_interval == 0 and hasattr(self, 'val_dataset') and self.val_dataset is not None:\n                # Do quick validation on subset of validation data\n                validation_subset = torch.utils.data.Subset(self.val_dataset, list(range(min(1000, len(self.val_dataset)))))\n                \n                # Use optimized dataloader if available\n                if hasattr(self, 'using_lazy_augmentation') and self.using_lazy_augmentation:\n                    # Use the optimized dataloader from lazy augmentation module\n                    quick_val_loader = self.create_optimized_dataloader(\n                        dataset=validation_subset,\n                        batch_size=batch_size,\n                        shuffle=False,\n                        num_workers=1  # Use minimal workers for quick validation\n                    )\n                else:\n                    # Use the same optimal worker calculation approach\n                    try:\n                        # Calculate optimal number of workers based on system capabilities\n                        import multiprocessing\n                        import psutil\n                        import math\n                        \n                        # Get number of CPU cores and apply a lighter workload for mini-validation\n                        cpu_count = multiprocessing.cpu_count()\n                        optimal_workers = max(0, min(int(cpu_count * 0.5), 2))  # Lighter worker count for mini-val\n                        \n                        if torch.cuda.is_available():\n                            # When using GPU, prefer fewer workers for mini-val\n                            optimal_workers = min(optimal_workers, 1)\n                        \n                        # Create DataLoader with calculated workers\n                        quick_val_loader = torch.utils.data.DataLoader(\n                            validation_subset,\n                            batch_size=batch_size, shuffle=False, \n                            num_workers=optimal_workers,\n                            pin_memory=torch.cuda.is_available()\n                        )\n                    except Exception as e:\n                        logger.warning(\"Error creating mini-validation DataLoader: {}\".format(str(e)))\n                        # Fallback to basic loader\n                        quick_val_loader = torch.utils.data.DataLoader(\n                            validation_subset,\n                            batch_size=batch_size, shuffle=False, num_workers=0\n                        )\n                \n                self.model.eval()\n                val_correct = 0\n                val_total = 0\n                \n                with torch.no_grad():\n                    for val_inputs, val_targets in quick_val_loader:\n                        val_inputs, val_targets = val_inputs.to(self.device), val_targets.to(self.device)\n                        val_outputs = self.model(val_inputs)\n                        _, val_predicted = val_outputs.max(1)\n                        val_correct += val_predicted.eq(val_targets).sum().item()\n                        val_total += val_targets.size(0)\n                \n                quick_val_acc = 100. * val_correct / val_total\n                current_train_acc = 100. * correct / total\n                \n                # Update optimizer with more frequent feedback\n                if isinstance(self.optimizer, EVENaturalWeights) or isinstance(self.optimizer, EVEUnifiedRatio):\n                    self.optimizer.update_accuracy_metrics(current_train_acc, quick_val_acc)\n                \n                self.model.train()\n        \n        # Calculate epoch metrics\n        epoch_loss = running_loss / len(dataloader)\n        epoch_acc = 100. * correct / total\n        self.last_train_acc = epoch_acc  # Store training accuracy for reference\n        \n        # Signal end of epoch to pattern mediator\n        if self.pattern_mediator and hasattr(self.pattern_mediator, 'end_epoch'):\n            self.pattern_mediator.end_epoch(self.current_epoch)\n            logger.debug(f\"Signaled end of epoch {self.current_epoch} to pattern mediator\")\n        \n        # Return as separate values to match AdaptiveTrainer's expected format\n        return epoch_loss, epoch_acc\n    \n    def _validate(self, dataset, batch_size=128):\n        \"\"\"\n        Validate the model on a dataset."
        },
        "_validate": {
          "start_line": 1026,
          "end_line": 1166,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "dataset"
            },
            {
              "name": "batch_size"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.model.eval",
              "line": 1037
            },
            {
              "name": "hasattr",
              "line": 1043
            },
            {
              "name": "hasattr",
              "line": 1045
            },
            {
              "name": "isinstance",
              "line": 1154
            },
            {
              "name": "hasattr",
              "line": 1049
            },
            {
              "name": "self.create_optimized_dataloader",
              "line": 1051
            },
            {
              "name": "torch.no_grad",
              "line": 1130
            },
            {
              "name": "len",
              "line": 1145
            },
            {
              "name": "hasattr",
              "line": 1149
            },
            {
              "name": "self.augmentation_mediator.update_validation_accuracy",
              "line": 1150
            },
            {
              "name": "logger.debug",
              "line": 1151
            },
            {
              "name": "getattr",
              "line": 1156
            },
            {
              "name": "self._apply_adaptive_augmentation",
              "line": 1161
            },
            {
              "name": "multiprocessing.cpu_count",
              "line": 1065
            },
            {
              "name": "max",
              "line": 1078
            },
            {
              "name": "torch.cuda.is_available",
              "line": 1080
            },
            {
              "name": "logger.info",
              "line": 1084
            },
            {
              "name": "torch.utils.data.DataLoader",
              "line": 1116
            },
            {
              "name": "self.model",
              "line": 1135
            },
            {
              "name": "self.criterion",
              "line": 1136
            },
            {
              "name": "loss.item",
              "line": 1139
            },
            {
              "name": "outputs.max",
              "line": 1140
            },
            {
              "name": "....item",
              "line": 1141
            },
            {
              "name": "targets.size",
              "line": 1142
            },
            {
              "name": "min",
              "line": 1078
            },
            {
              "name": "min",
              "line": 1082
            },
            {
              "name": "....format",
              "line": 1084
            },
            {
              "name": "logger.warning",
              "line": 1123
            },
            {
              "name": "torch.utils.data.DataLoader",
              "line": 1125
            },
            {
              "name": "inputs.to",
              "line": 1132
            },
            {
              "name": "targets.to",
              "line": 1132
            },
            {
              "name": "psutil.cpu_percent",
              "line": 1070
            },
            {
              "name": "int",
              "line": 1078
            },
            {
              "name": "torch.utils.data.default_collate",
              "line": 1092
            },
            {
              "name": "torch.cuda.is_available",
              "line": 1119
            },
            {
              "name": "....format",
              "line": 1123
            },
            {
              "name": "....sum",
              "line": 1141
            },
            {
              "name": "str",
              "line": 1123
            },
            {
              "name": "data.append",
              "line": 1099
            },
            {
              "name": "isinstance",
              "line": 1102
            },
            {
              "name": "predicted.eq",
              "line": 1141
            },
            {
              "name": "targets.append",
              "line": 1103
            },
            {
              "name": "targets.append",
              "line": 1105
            },
            {
              "name": "torch.stack",
              "line": 1109
            },
            {
              "name": "torch.stack",
              "line": 1109
            },
            {
              "name": "logger.warning",
              "line": 1112
            },
            {
              "name": "torch.tensor",
              "line": 1103
            },
            {
              "name": "....unsqueeze",
              "line": 1113
            },
            {
              "name": "....unsqueeze",
              "line": 1113
            }
          ],
          "docstring": "\n        Validate the model on a dataset.\n        \n        Args:\n            dataset: Validation dataset\n            batch_size: Batch size\n            \n        Returns:\n            Dictionary with validation metrics\n        ",
          "code_snippet": "        return epoch_loss, epoch_acc\n    \n    def _validate(self, dataset, batch_size=128):\n        \"\"\"\n        Validate the model on a dataset.\n        \n        Args:\n            dataset: Validation dataset\n            batch_size: Batch size\n            \n        Returns:\n            Dictionary with validation metrics\n        \"\"\"\n        self.model.eval()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        # Store previous validation metrics before updating\n        if hasattr(self, 'last_val_loss'):\n            self.prev_val_loss = self.last_val_loss\n        if hasattr(self, 'last_val_acc'):\n            self.prev_val_acc = self.last_val_acc\n        \n        # Create dataloader\n        if hasattr(self, 'using_lazy_augmentation') and self.using_lazy_augmentation:\n            # Use optimized dataloader from lazy augmentation module\n            dataloader = self.create_optimized_dataloader(\n                dataset=dataset,\n                batch_size=batch_size,\n                shuffle=False\n            )\n        else:\n            # Fallback to standard DataLoader with conservative settings\n            try:\n                # Calculate optimal number of workers based on system capabilities\n                import multiprocessing\n                import psutil\n                import math\n                \n                # Get number of CPU cores\n                cpu_count = multiprocessing.cpu_count()\n                \n                # Get system load\n                system_load = 0.5  # Default value if psutil fails\n                try:\n                    system_load = psutil.cpu_percent(interval=0.1) / 100.0\n                except:\n                    pass\n                    \n                # Calculate resource factor based on cognitive efficiency formula\n                resource_factor = (1 - system_load * 0.8)\n                \n                # Calculate optimal workers - apply parallel processing penalty\n                optimal_workers = max(0, min(int(cpu_count * resource_factor * 0.75), 4))\n                \n                if torch.cuda.is_available():\n                    # When using GPU, we want fewer workers\n                    optimal_workers = min(optimal_workers, 2)\n                    \n                logger.info(\"Using {} DataLoader workers for validation (cores: {}, load: {:.2f})\".format(\n                    optimal_workers, cpu_count, system_load))\n                \n                # Define custom collate function for validation\n                def custom_collate_fn(batch):\n                    \"\"\"Custom collate function to ensure consistent tensor types\"\"\"\n                    try:\n                        # Default collation if everything is already properly formatted\n                        return torch.utils.data.default_collate(batch)\n                    except (TypeError, RuntimeError) as e:\n                        # If default collation fails, handle it manually\n                        data = []\n                        targets = []\n                        for item in batch:\n                            # Handle data\n                            data.append(item[0])\n                            \n                            # Handle target - ensure it's a tensor\n                            if isinstance(item[1], (int, float)):\n                                targets.append(torch.tensor(item[1]))\n                            else:\n                                targets.append(item[1])\n                        \n                        # Stack the data and targets\n                        try:\n                            return torch.stack(data), torch.stack(targets)\n                        except:\n                            # Last resort fallback\n                            logger.warning(\"Custom collation failed, returning limited batch\")\n                            return data[0].unsqueeze(0), targets[0].unsqueeze(0)\n                \n                # Use optimal workers for DataLoader with custom collate\n                dataloader = torch.utils.data.DataLoader(\n                    dataset, batch_size=batch_size, shuffle=False, \n                    num_workers=optimal_workers,\n                    pin_memory=torch.cuda.is_available(),\n                    collate_fn=custom_collate_fn)\n                    \n            except Exception as e:\n                logger.warning(\"Error creating optimized validation DataLoader: {}\".format(str(e)))\n                # Fallback to single-process data loading with custom collate\n                dataloader = torch.utils.data.DataLoader(\n                    dataset, batch_size=batch_size, shuffle=False, num_workers=0,\n                    collate_fn=custom_collate_fn)\n        \n        # Disable gradient calculation\n        with torch.no_grad():\n            for inputs, targets in dataloader:\n                inputs, targets = inputs.to(self.device), targets.to(self.device)\n                \n                # Forward pass\n                outputs = self.model(inputs)\n                loss = self.criterion(outputs, targets)\n                \n                # Update metrics\n                running_loss += loss.item()\n                _, predicted = outputs.max(1)\n                correct += predicted.eq(targets).sum().item()\n                total += targets.size(0)\n        \n        # Calculate metrics\n        val_loss = running_loss / len(dataloader)\n        val_acc = 100. * correct / total\n        \n        # Update augmentation mediator with validation accuracy\n        if self.augmentation_mediator and hasattr(self.augmentation_mediator, 'update_validation_accuracy'):\n            self.augmentation_mediator.update_validation_accuracy(val_acc)\n            logger.debug(f\"Updated augmentation mediator with validation accuracy: {val_acc:.2f}%\")\n        \n        # Also update optimizer with validation info\n        if isinstance(self.optimizer, EVEUnifiedRatio):\n            self.optimizer.test_acc = val_acc\n            self.optimizer.train_acc = getattr(self, 'last_train_acc', 0)  # Last training accuracy if available\n            self.last_val_acc = val_acc   # Store for future reference\n            self.last_val_loss = val_loss # Store for future reference\n            \n            # Adaptive augmentation based on validation performance\n            self._apply_adaptive_augmentation(train_dataset=None, val_loss=val_loss, val_acc=val_acc)\n        \n        # Return as separate values to match AdaptiveTrainer's expected format\n        return val_loss, val_acc\n    \n    def _apply_adaptive_augmentation(self, train_dataset=None, val_loss=None, val_acc=None):\n        \"\"\"\n        Apply adaptive augmentation based on validation performance."
        },
        "_apply_adaptive_augmentation": {
          "start_line": 1166,
          "end_line": 1313,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "train_dataset"
            },
            {
              "name": "val_loss"
            },
            {
              "name": "val_acc"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "hasattr",
              "line": 1256
            },
            {
              "name": "hasattr",
              "line": 1192
            },
            {
              "name": "self.optimizer.equilibrium_tracker.get_patterns_below_min",
              "line": 1194
            },
            {
              "name": "hasattr",
              "line": 1202
            },
            {
              "name": "self.optimizer.pattern_mediator.get_pattern_accuracies",
              "line": 1204
            },
            {
              "name": "self.optimizer.pattern_mediator.get_pattern_risks",
              "line": 1205
            },
            {
              "name": "pattern_accs.items",
              "line": 1210
            },
            {
              "name": "sorted",
              "line": 1216
            },
            {
              "name": "hasattr",
              "line": 1220
            },
            {
              "name": "self.added_augmentation_levels.items",
              "line": 1222
            },
            {
              "name": "hasattr",
              "line": 1229
            },
            {
              "name": "hasattr",
              "line": 1229
            },
            {
              "name": "logger.info",
              "line": 1264
            },
            {
              "name": "hasattr",
              "line": 1179
            },
            {
              "name": "hasattr",
              "line": 1182
            },
            {
              "name": "hasattr",
              "line": 1182
            },
            {
              "name": "logger.info",
              "line": 1197
            },
            {
              "name": "pattern_types_to_augment.extend",
              "line": 1199
            },
            {
              "name": "pattern_performances.keys",
              "line": 1216
            },
            {
              "name": "abs",
              "line": 1238
            },
            {
              "name": "abs",
              "line": 1239
            },
            {
              "name": "logger.info",
              "line": 1246
            },
            {
              "name": "logger.info",
              "line": 1260
            },
            {
              "name": "self.added_augmentation_levels.get",
              "line": 1271
            },
            {
              "name": "logger.debug",
              "line": 1309
            },
            {
              "name": "logger.info",
              "line": 1213
            },
            {
              "name": "len",
              "line": 1224
            },
            {
              "name": "len",
              "line": 1224
            },
            {
              "name": "pattern_types_to_augment.append",
              "line": 1225
            },
            {
              "name": "self.batch_optimizer_kwargs.get",
              "line": 1252
            },
            {
              "name": "hasattr",
              "line": 1268
            },
            {
              "name": "pattern_performances.get",
              "line": 1217
            },
            {
              "name": "....join",
              "line": 1264
            },
            {
              "name": "self.preloaded_augmentation_levels.keys",
              "line": 1274
            },
            {
              "name": "hasattr",
              "line": 1282
            },
            {
              "name": "logger.info",
              "line": 1283
            },
            {
              "name": "train_dataset.add_augmentations",
              "line": 1284
            },
            {
              "name": "self.pending_augmentations.append",
              "line": 1302
            },
            {
              "name": "logger.info",
              "line": 1307
            },
            {
              "name": "....join",
              "line": 1197
            },
            {
              "name": "....append",
              "line": 1288
            },
            {
              "name": "logger.info",
              "line": 1289
            },
            {
              "name": "hasattr",
              "line": 1299
            },
            {
              "name": "hasattr",
              "line": 1295
            },
            {
              "name": "....append",
              "line": 1296
            },
            {
              "name": "len",
              "line": 1296
            }
          ],
          "docstring": "\n        Apply adaptive augmentation based on validation performance.\n        \n        This method adds preloaded augmentation levels for pattern types that will benefit\n        from additional training data based on validation metrics.\n        \n        Args:\n            train_dataset: Training dataset (if None, assumed to be applied later)\n            val_loss: Current validation loss\n            val_acc: Current validation accuracy\n        ",
          "code_snippet": "        return val_loss, val_acc\n    \n    def _apply_adaptive_augmentation(self, train_dataset=None, val_loss=None, val_acc=None):\n        \"\"\"\n        Apply adaptive augmentation based on validation performance.\n        \n        This method adds preloaded augmentation levels for pattern types that will benefit\n        from additional training data based on validation metrics.\n        \n        Args:\n            train_dataset: Training dataset (if None, assumed to be applied later)\n            val_loss: Current validation loss\n            val_acc: Current validation accuracy\n        \"\"\"\n        # Skip if not using lazy augmentation or not enough data\n        if not hasattr(self, 'using_lazy_augmentation') or not self.using_lazy_augmentation:\n            return\n            \n        if not hasattr(self, 'prev_val_loss') or not hasattr(self, 'preloaded_augmentation_levels'):\n            return\n            \n        # Determine which pattern types need more augmentation\n        pattern_types_to_augment = []\n        \n        # Get pattern performance metrics if available\n        pattern_performances = {}\n        \n        # Try to get pattern-specific metrics first - including equilibrium bounds\n        if hasattr(self.optimizer, 'equilibrium_tracker') and self.optimizer.use_equilibrium_bounds:\n            # Check for patterns that are below their minimum bounds (underfitting)\n            below_min_patterns = self.optimizer.equilibrium_tracker.get_patterns_below_min()\n            \n            if below_min_patterns:\n                logger.info(f\"Patterns below equilibrium bounds: {', '.join(below_min_patterns)}\")\n                # Add these patterns to our augmentation targets\n                pattern_types_to_augment.extend(below_min_patterns)\n                \n        # If no patterns below bounds, check for general underperformance\n        if not pattern_types_to_augment and hasattr(self.optimizer, 'pattern_mediator') and self.optimizer.pattern_mediator:\n            # Get accuracies and risks from the pattern mediator\n            pattern_accs = self.optimizer.pattern_mediator.get_pattern_accuracies()\n            pattern_risks = self.optimizer.pattern_mediator.get_pattern_risks()\n            \n            # Find pattern types with lower accuracy/higher risk\n            threshold_acc = val_acc * 0.9  # 90% of overall accuracy\n            \n            for pattern_type, acc in pattern_accs.items():\n                if acc < threshold_acc:\n                    pattern_performances[pattern_type] = acc\n                    logger.info(f\"Pattern {pattern_type} underperforming: {acc:.2f}% vs overall {val_acc:.2f}%\")\n            \n            # Sort patterns by performance (worst first)\n            pattern_types_to_augment = sorted(pattern_performances.keys(), \n                                            key=lambda x: pattern_performances.get(x, 100))\n        \n        # If no specific patterns identified or pattern mediator not available\n        if not pattern_types_to_augment and hasattr(self, 'added_augmentation_levels'):\n            # Check which pattern types can still have augmentation added\n            for pattern_type, added_levels in self.added_augmentation_levels.items():\n                # If we haven't added all augmentation levels yet\n                if len(added_levels) < len(self.preloaded_augmentation_levels):\n                    pattern_types_to_augment.append(pattern_type)\n        \n        # If we have patterns to augment and validation performance degraded or plateau\n        performance_degraded = False\n        if hasattr(self, 'prev_val_loss') and hasattr(self, 'prev_val_acc'):\n            # Define small epsilon to avoid floating point comparison issues\n            epsilon = 1e-4\n            \n            # Check if validation loss increased or accuracy decreased\n            loss_increased = val_loss > self.prev_val_loss + epsilon\n            acc_decreased = val_acc < self.prev_val_acc - epsilon\n            \n            # Also check for plateaus (no significant improvement)\n            loss_plateau = abs(val_loss - self.prev_val_loss) < epsilon\n            acc_plateau = abs(val_acc - self.prev_val_acc) < 0.1  # 0.1% change threshold\n            \n            # Triggers: performance worsened or plateaued after multiple epochs\n            performance_degraded = (loss_increased or acc_decreased or \n                                  (loss_plateau and acc_plateau and self.current_epoch > 3))\n            \n            if performance_degraded:\n                logger.info(f\"Validation performance needs improvement: \"\n                         f\"loss {self.prev_val_loss:.4f}->{val_loss:.4f}, \"\n                         f\"acc {self.prev_val_acc:.2f}%->{val_acc:.2f}%\")\n        \n        # Apply augmentation if we have targets and performance needs improvement\n        # But only in certain epochs range (not too early, not too late)\n        suitable_epoch = self.current_epoch > 2 and self.current_epoch < self.batch_optimizer_kwargs.get('total_epochs', 100) * 0.8\n        \n        # Memory safety check - don't augment too frequently to avoid OOM\n        safe_to_augment = True\n        if hasattr(self, 'last_augmentation_epoch'):\n            # Only allow augmentation every 2 epochs minimum to prevent memory issues\n            if self.current_epoch - self.last_augmentation_epoch < 2:\n                safe_to_augment = False\n                logger.info(f\"Skipping augmentation to prevent memory issues (last augmentation at epoch {self.last_augmentation_epoch})\")\n        \n        if pattern_types_to_augment and (performance_degraded or self.current_epoch % 3 == 0) and suitable_epoch and safe_to_augment:\n            # Apply to the dataset if provided, otherwise save for later\n            logger.info(f\"Planning adaptive augmentation for: {', '.join(pattern_types_to_augment)}\")\n            \n            # Determine which level of augmentation to add next\n            for pattern_type in pattern_types_to_augment[:1]:  # Limit to top 1 pattern to avoid memory issues\n                if not hasattr(self, 'added_augmentation_levels'):\n                    continue\n                    \n                added_levels = self.added_augmentation_levels.get(pattern_type, [])\n                \n                # Determine the next level to add\n                remaining_levels = [level for level in self.preloaded_augmentation_levels.keys() \n                                   if level not in added_levels]\n                \n                if remaining_levels:\n                    next_level = remaining_levels[0]\n                    augmentation_pct = self.preloaded_augmentation_levels[next_level]\n                    \n                    # Apply it now if we have the dataset\n                    if train_dataset and hasattr(train_dataset, 'add_augmentations'):\n                        logger.info(f\"Adding {next_level} augmentation ({augmentation_pct:.1%}) for {pattern_type}\")\n                        added_count = train_dataset.add_augmentations(pattern_type, augmentation_pct)\n                        \n                        # Record the addition\n                        if added_count > 0:\n                            self.added_augmentation_levels[pattern_type].append(next_level)\n                            logger.info(f\"Added {added_count} examples for {pattern_type} ({next_level} level)\")\n                            \n                            # Track when we last added augmentation to prevent OOM\n                            self.last_augmentation_epoch = self.current_epoch\n                            \n                            # Update dataset size tracking\n                            if hasattr(self, 'history') and 'dataset_sizes' in self.history:\n                                self.history['dataset_sizes'].append(len(train_dataset))\n                    else:\n                        # Store the plan for execution in next epoch\n                        if not hasattr(self, 'pending_augmentations'):\n                            self.pending_augmentations = []\n                        \n                        self.pending_augmentations.append({\n                            'pattern_type': pattern_type,\n                            'level': next_level,\n                            'percentage': augmentation_pct\n                        })\n                        logger.info(f\"Queued {next_level} augmentation ({augmentation_pct:.1%}) for {pattern_type}\")\n        elif suitable_epoch:\n            logger.debug(\"No adaptive augmentation needed this epoch\")\n            \n        return\n\ndef calculate_optimal_workers():\n    \"\"\"\n    Calculate the optimal number of workers based on system capabilities."
        }
      },
      "class_variables": [],
      "bases": [
        "AdaptiveTrainer"
      ],
      "docstring": "\n    Enhanced adaptive trainer that uses the unified risk/accuracy ratio approach\n    for batch size adaptation and learning rate adjustments with model swapping.\n    "
    }
  },
  "functions": {
    "load_cifar10_data": {
      "start_line": 52,
      "end_line": 87,
      "parameters": [
        {
          "name": "input_size"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "transforms.Compose",
          "line": 65
        },
        {
          "name": "transforms.Compose",
          "line": 72
        },
        {
          "name": "torchvision.datasets.CIFAR10",
          "line": 79
        },
        {
          "name": "torchvision.datasets.CIFAR10",
          "line": 82
        },
        {
          "name": "transforms.RandomHorizontalFlip",
          "line": 67
        },
        {
          "name": "transforms.ToTensor",
          "line": 68
        },
        {
          "name": "transforms.Normalize",
          "line": 69
        },
        {
          "name": "transforms.Normalize",
          "line": 75
        },
        {
          "name": "transforms.Resize",
          "line": 66
        },
        {
          "name": "transforms.RandomCrop",
          "line": 66
        },
        {
          "name": "transforms.Resize",
          "line": 73
        },
        {
          "name": "transforms.ToTensor",
          "line": 73
        },
        {
          "name": "transforms.ToTensor",
          "line": 74
        }
      ],
      "docstring": "\n    Load and prepare CIFAR-10 dataset.\n    \n    Args:\n        input_size: Input image size\n        \n    Returns:\n        Train dataset, test dataset\n    ",
      "code_snippet": "logging.getLogger('lazy_augmentation').setLevel(logging.WARNING)\n\ndef load_cifar10_data(input_size=32):\n    \"\"\"\n    Load and prepare CIFAR-10 dataset.\n    \n    Args:\n        input_size: Input image size\n        \n    Returns:\n        Train dataset, test dataset\n    \"\"\"\n    # Data transforms\n    need_resize = input_size != 32\n    \n    transform_train = transforms.Compose([\n        transforms.Resize(input_size) if need_resize else transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n\n    transform_test = transforms.Compose([\n        transforms.Resize(input_size) if need_resize else transforms.ToTensor(),\n        transforms.ToTensor() if need_resize else lambda x: x,  # Add ToTensor only if we're resizing\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n\n    # Load datasets\n    trainset = torchvision.datasets.CIFAR10(\n        root='./data', train=True, download=True, transform=transform_train)\n\n    testset = torchvision.datasets.CIFAR10(\n        root='./data', train=False, download=True, transform=transform_test)\n\n    return trainset, testset\n\n# Pattern Data Mediator for coordinating between isekaiZen and EVE\nclass PatternDataMediator:\n    \"\"\""
    },
    "get_fibonacci_check_intervals": {
      "start_line": 256,
      "end_line": 287,
      "parameters": [
        {
          "name": "total_epochs"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "range",
          "line": 276
        },
        {
          "name": "fib_sequence.append",
          "line": 269
        },
        {
          "name": "intervals.append",
          "line": 282
        },
        {
          "name": "len",
          "line": 278
        }
      ],
      "docstring": "\n    Pre-calculate Fibonacci-based check intervals for the entire training.\n    \n    Args:\n        total_epochs: Total number of training epochs\n        \n    Returns:\n        List of check intervals for each epoch\n    ",
      "code_snippet": "from optimizer_configs import get_optimizer_config, ALL_CONFIGS\n\ndef get_fibonacci_check_intervals(total_epochs):\n    \"\"\"\n    Pre-calculate Fibonacci-based check intervals for the entire training.\n    \n    Args:\n        total_epochs: Total number of training epochs\n        \n    Returns:\n        List of check intervals for each epoch\n    \"\"\"\n    # Generate Fibonacci sequence\n    fib_sequence = [1, 1]\n    while fib_sequence[-1] < total_epochs // 2:  # Generate enough Fibonacci numbers\n        fib_sequence.append(fib_sequence[-1] + fib_sequence[-2])\n    \n    # Create interval mapping for each epoch\n    intervals = []\n    current_fib_index = 0\n    epoch_counter = 0\n    \n    for epoch in range(total_epochs):\n        # Move to next Fibonacci number if we've used current one enough times\n        if epoch_counter >= fib_sequence[current_fib_index] and current_fib_index < len(fib_sequence) - 1:\n            current_fib_index += 1\n            epoch_counter = 0\n        \n        intervals.append(fib_sequence[current_fib_index])\n        epoch_counter += 1\n    \n    return intervals\n\n# Register the unified ratio EVE optimizer\ndef register_unified_ratio_optimizer(config_params=None):\n    \"\"\""
    },
    "register_unified_ratio_optimizer": {
      "start_line": 288,
      "end_line": 359,
      "parameters": [
        {
          "name": "config_params"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "logger.info",
          "line": 351
        },
        {
          "name": "logger.info",
          "line": 352
        },
        {
          "name": "logger.info",
          "line": 353
        },
        {
          "name": "logger.info",
          "line": 354
        },
        {
          "name": "logger.info",
          "line": 357
        },
        {
          "name": "config_params.get",
          "line": 308
        },
        {
          "name": "config_params.get",
          "line": 309
        },
        {
          "name": "config_params.get",
          "line": 310
        },
        {
          "name": "config_params.get",
          "line": 311
        },
        {
          "name": "config_params.get",
          "line": 312
        },
        {
          "name": "config_params.get",
          "line": 313
        },
        {
          "name": "config_params.get",
          "line": 314
        },
        {
          "name": "config_params.get",
          "line": 315
        },
        {
          "name": "config_params.get",
          "line": 316
        },
        {
          "name": "....format",
          "line": 352
        },
        {
          "name": "....format",
          "line": 353
        },
        {
          "name": "....format",
          "line": 354
        },
        {
          "name": "logger.info",
          "line": 356
        },
        {
          "name": "....format",
          "line": 357
        }
      ],
      "docstring": "\n    Register the EVEUnifiedRatio optimizer in the available optimizers.\n    \n    Args:\n        config_params: Optional dictionary with configuration parameters\n    ",
      "code_snippet": "\n# Register the unified ratio EVE optimizer\ndef register_unified_ratio_optimizer(config_params=None):\n    \"\"\"\n    Register the EVEUnifiedRatio optimizer in the available optimizers.\n    \n    Args:\n        config_params: Optional dictionary with configuration parameters\n    \"\"\"\n    # Set default configuration values\n    debug_ratios = False\n    debug_bounds = False\n    weight_adjustment_range = \"default\"  # Default value (0.85-1.15)\n    weight_range_iris = False  # Default to disabled iris feature\n    lr_check_interval = 10  # Default value\n    lr_change_threshold = 0.005  # Default value\n    lr_log_threshold = 0.05  # Default value\n    use_equilibrium_bounds = True  # Default to use equilibrium bounds\n    fibonacci_intervals = None  # For Fibonacci-based sensitivity\n    \n    # Override with provided parameters if any\n    if config_params:\n        debug_ratios = config_params.get('debug_ratios', debug_ratios)\n        debug_bounds = config_params.get('debug_bounds', debug_bounds)\n        weight_adjustment_range = config_params.get('weight_adjustment_range', weight_adjustment_range)\n        weight_range_iris = config_params.get('weight_range_iris', weight_range_iris)\n        lr_check_interval = config_params.get('lr_check_interval', lr_check_interval)\n        lr_change_threshold = config_params.get('lr_change_threshold', lr_change_threshold)\n        lr_log_threshold = config_params.get('lr_log_threshold', lr_log_threshold)\n        use_equilibrium_bounds = config_params.get('use_equilibrium_bounds', use_equilibrium_bounds)\n        fibonacci_intervals = config_params.get('fibonacci_intervals', None)\n    \n    # Create configuration for the unified ratio optimizer\n    unified_config = {\n        'default': {\n            'optimizer_class': EVEUnifiedRatio,\n            'optimizer_kwargs': {\n                'lr': 0.01,\n                'eps': 1e-8,\n                'base_confidence_threshold': 0.7,\n                'weight_decay': 0.0001,\n                'debug_ratios': debug_ratios,\n                'debug_bounds': debug_bounds,\n                'warmup_epochs': 1,  # Faster warmup\n                'weight_adjustment_range': weight_adjustment_range,\n                'weight_range_iris': weight_range_iris,\n                'lr_check_interval': lr_check_interval,\n                'lr_change_threshold': lr_change_threshold,\n                'lr_log_threshold': lr_log_threshold,\n                'use_equilibrium_bounds': use_equilibrium_bounds\n            },\n            'scheduler_class': optim.lr_scheduler.CosineAnnealingLR,\n            'scheduler_kwargs': {\n                'T_max': 200\n            }\n        }\n    }\n    \n    # If using Fibonacci intervals, we'll set them as an attribute on the optimizer after creation\n    if fibonacci_intervals:\n        unified_config['default']['optimizer_kwargs']['fibonacci_intervals'] = fibonacci_intervals\n    \n    # Update ALL_CONFIGS with the new optimizer\n    ALL_CONFIGS['eve_unified'] = unified_config\n    \n    logger.info(\"EVEUnifiedRatio optimizer registered successfully\")\n    logger.info(\"  Weight adjustment range: {}\".format(weight_adjustment_range))\n    logger.info(\"  Weight range iris: {}\".format(\"Enabled\" if weight_range_iris else \"Disabled\"))\n    logger.info(\"  LR sensitivity: check interval={}, threshold={:.4f}\".format(lr_check_interval, lr_change_threshold))\n    if fibonacci_intervals:\n        logger.info(\"  Using Fibonacci intervals for dynamic sensitivity\")\n    logger.info(\"  Using equilibrium bounds: {}\".format(use_equilibrium_bounds))\n\nclass UnifiedRatioTrainer(AdaptiveTrainer):\n    \"\"\"\n    Enhanced adaptive trainer that uses the unified risk/accuracy ratio approach"
    },
    "calculate_optimal_workers": {
      "start_line": 1313,
      "end_line": 1366,
      "parameters": [],
      "return_type": null,
      "calls": [
        {
          "name": "multiprocessing.cpu_count",
          "line": 1330
        },
        {
          "name": "math.ceil",
          "line": 1343
        },
        {
          "name": "torch.cuda.is_available",
          "line": 1346
        },
        {
          "name": "min",
          "line": 1355
        },
        {
          "name": "max",
          "line": 1356
        },
        {
          "name": "logger.info",
          "line": 1358
        },
        {
          "name": "psutil.cpu_percent",
          "line": 1333
        },
        {
          "name": "torch.cuda.device_count",
          "line": 1348
        },
        {
          "name": "min",
          "line": 1349
        },
        {
          "name": "max",
          "line": 1355
        },
        {
          "name": "min",
          "line": 1356
        },
        {
          "name": "....format",
          "line": 1358
        },
        {
          "name": "logger.warning",
          "line": 1363
        },
        {
          "name": "psutil.virtual_memory",
          "line": 1336
        },
        {
          "name": "max",
          "line": 1349
        },
        {
          "name": "....format",
          "line": 1363
        },
        {
          "name": "str",
          "line": 1363
        }
      ],
      "docstring": "\n    Calculate the optimal number of workers based on system capabilities.\n    \n    Following isekaiZen mathematical foundation, this function integrates\n    cognitive efficiency principles to determine the optimal number of\n    DataLoader workers that balances performance and stability.\n    \n    Returns:\n        Integer number of workers\n    ",
      "code_snippet": "        return\n\ndef calculate_optimal_workers():\n    \"\"\"\n    Calculate the optimal number of workers based on system capabilities.\n    \n    Following isekaiZen mathematical foundation, this function integrates\n    cognitive efficiency principles to determine the optimal number of\n    DataLoader workers that balances performance and stability.\n    \n    Returns:\n        Integer number of workers\n    \"\"\"\n    try:\n        import multiprocessing\n        import psutil\n        import math\n        \n        # Get number of CPU cores\n        cpu_count = multiprocessing.cpu_count()\n        \n        # Get current system load\n        system_load = psutil.cpu_percent(interval=0.1) / 100.0\n        \n        # Get memory usage as a factor\n        memory_usage = psutil.virtual_memory().percent / 100.0\n        \n        # Calculate resource factor using polynomial function\n        # This follows the cognitive efficiency function patterns from the isekaiZen mathematical foundation\n        resource_factor = (1 - system_load) * (1 - 0.8 * memory_usage)\n        \n        # Calculate base workers using the parallel processing penalty formula pattern\n        base_workers = math.ceil(cpu_count * resource_factor)\n        \n        # Add GPU factor if available\n        if torch.cuda.is_available():\n            # When using GPU, we need fewer workers to avoid bottlenecks\n            gpu_count = torch.cuda.device_count()\n            workers = min(base_workers, max(1, 2 * gpu_count))\n        else:\n            # For CPU-only, use calculated base workers\n            workers = base_workers\n        \n        # Apply bounds based on system capabilities\n        workers = min(workers, max(1, cpu_count - 1))  # Leave at least one core free\n        workers = max(1, min(workers, 4))  # Cap at 4 workers for stability\n        \n        logger.info(\"Calculated optimal DataLoader workers: {} (from {} cores, load: {:.2f})\".format(\n            workers, cpu_count, system_load))\n        return workers\n        \n    except Exception as e:\n        logger.warning(\"Error calculating optimal workers: {}. Using 0 workers for safety.\".format(str(e)))\n        return 0  # Safe fallback\n\ndef create_model(model_type=\"resnet18\", use_pretrained=False, num_classes=10, input_channels=3, input_size=32):\n    \"\"\"\n    Create a model for the specified architecture, optionally with pre-trained weights."
    },
    "create_model": {
      "start_line": 2030,
      "end_line": 2229,
      "parameters": [
        {
          "name": "model_type"
        },
        {
          "name": "use_pretrained"
        },
        {
          "name": "num_classes"
        },
        {
          "name": "input_channels"
        },
        {
          "name": "input_size"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "logger.info",
          "line": 2044
        },
        {
          "name": "model_type.startswith",
          "line": 2047
        },
        {
          "name": "nn.Linear",
          "line": 2092
        },
        {
          "name": "model_type.startswith",
          "line": 2094
        },
        {
          "name": "models.resnet18",
          "line": 2050
        },
        {
          "name": "model.conv1.weight.clone",
          "line": 2064
        },
        {
          "name": "logger.info",
          "line": 2065
        },
        {
          "name": "model_type.startswith",
          "line": 2151
        },
        {
          "name": "models.resnet34",
          "line": 2052
        },
        {
          "name": "logger.info",
          "line": 2071
        },
        {
          "name": "nn.Conv2d",
          "line": 2072
        },
        {
          "name": "nn.Identity",
          "line": 2074
        },
        {
          "name": "nn.Conv2d",
          "line": 2077
        },
        {
          "name": "logger.info",
          "line": 2089
        },
        {
          "name": "models.vgg11",
          "line": 2097
        },
        {
          "name": "max",
          "line": 2111
        },
        {
          "name": "int",
          "line": 2115
        },
        {
          "name": "logger.info",
          "line": 2117
        },
        {
          "name": "nn.Sequential",
          "line": 2120
        },
        {
          "name": "nn.Linear",
          "line": 2132
        },
        {
          "name": "logger.warning",
          "line": 2136
        },
        {
          "name": "nn.Conv2d",
          "line": 2139
        },
        {
          "name": "hasattr",
          "line": 2163
        },
        {
          "name": "model_type.startswith",
          "line": 2204
        },
        {
          "name": "models.resnet50",
          "line": 2054
        },
        {
          "name": "torch.no_grad",
          "line": 2081
        },
        {
          "name": "models.vgg13",
          "line": 2099
        },
        {
          "name": "nn.Linear",
          "line": 2121
        },
        {
          "name": "nn.ReLU",
          "line": 2122
        },
        {
          "name": "nn.Dropout",
          "line": 2123
        },
        {
          "name": "nn.Linear",
          "line": 2124
        },
        {
          "name": "nn.ReLU",
          "line": 2125
        },
        {
          "name": "nn.Dropout",
          "line": 2126
        },
        {
          "name": "nn.Linear",
          "line": 2127
        },
        {
          "name": "models.mobilenet_v2",
          "line": 2154
        },
        {
          "name": "nn.Linear",
          "line": 2165
        },
        {
          "name": "nn.Linear",
          "line": 2169
        },
        {
          "name": "nn.Linear",
          "line": 2219
        },
        {
          "name": "ValueError",
          "line": 2225
        },
        {
          "name": "models.resnet101",
          "line": 2056
        },
        {
          "name": "model.conv1.weight.copy_",
          "line": 2084
        },
        {
          "name": "model.conv1.weight.copy_",
          "line": 2087
        },
        {
          "name": "models.vgg16",
          "line": 2101
        },
        {
          "name": "torch.no_grad",
          "line": 2146
        },
        {
          "name": "models.mobilenet_v3_small",
          "line": 2156
        },
        {
          "name": "hasattr",
          "line": 2173
        },
        {
          "name": "hasattr",
          "line": 2173
        },
        {
          "name": "nn.Conv2d",
          "line": 2176
        },
        {
          "name": "nn.Conv2d",
          "line": 2191
        },
        {
          "name": "models.efficientnet_b0",
          "line": 2207
        },
        {
          "name": "logger.info",
          "line": 2223
        },
        {
          "name": "models.resnet152",
          "line": 2058
        },
        {
          "name": "ValueError",
          "line": 2060
        },
        {
          "name": "models.vgg19",
          "line": 2103
        },
        {
          "name": "ValueError",
          "line": 2105
        },
        {
          "name": "models.mobilenet_v3_large",
          "line": 2158
        },
        {
          "name": "ValueError",
          "line": 2160
        },
        {
          "name": "models.efficientnet_b1",
          "line": 2209
        },
        {
          "name": "torch.no_grad",
          "line": 2184
        },
        {
          "name": "torch.no_grad",
          "line": 2199
        },
        {
          "name": "models.efficientnet_b2",
          "line": 2211
        },
        {
          "name": "min",
          "line": 2147
        },
        {
          "name": "min",
          "line": 2147
        },
        {
          "name": "models.efficientnet_b3",
          "line": 2213
        },
        {
          "name": "ValueError",
          "line": 2215
        },
        {
          "name": "min",
          "line": 2185
        },
        {
          "name": "min",
          "line": 2185
        },
        {
          "name": "min",
          "line": 2200
        },
        {
          "name": "min",
          "line": 2200
        }
      ],
      "docstring": "\n    Create a model for the specified architecture, optionally with pre-trained weights.\n    \n    Args:\n        model_type: Type of model to create (resnet18, resnet34, vgg16, etc.)\n        use_pretrained: Whether to use pre-trained weights from ImageNet\n        num_classes: Number of output classes\n        input_channels: Number of input channels\n        input_size: Input image size\n        \n    Returns:\n        Model instance\n    ",
      "code_snippet": "    logger.info(\"=\" * width)\n\ndef create_model(model_type=\"resnet18\", use_pretrained=False, num_classes=10, input_channels=3, input_size=32):\n    \"\"\"\n    Create a model for the specified architecture, optionally with pre-trained weights.\n    \n    Args:\n        model_type: Type of model to create (resnet18, resnet34, vgg16, etc.)\n        use_pretrained: Whether to use pre-trained weights from ImageNet\n        num_classes: Number of output classes\n        input_channels: Number of input channels\n        input_size: Input image size\n        \n    Returns:\n        Model instance\n    \"\"\"\n    logger.info(f\"Creating {model_type} model for {num_classes} classes with input size {input_size}x{input_size}\")\n    \n    # Handle different model architectures\n    if model_type.startswith(\"resnet\"):\n        # Choose the appropriate ResNet model\n        if model_type == \"resnet18\":\n            model = models.resnet18(pretrained=use_pretrained)\n        elif model_type == \"resnet34\":\n            model = models.resnet34(pretrained=use_pretrained)\n        elif model_type == \"resnet50\":\n            model = models.resnet50(pretrained=use_pretrained)\n        elif model_type == \"resnet101\":\n            model = models.resnet101(pretrained=use_pretrained)\n        elif model_type == \"resnet152\":\n            model = models.resnet152(pretrained=use_pretrained)\n        else:\n            raise ValueError(f\"Unsupported ResNet model: {model_type}\")\n        \n        # Save the pre-trained conv1 weights if using pre-trained model\n        if use_pretrained:\n            pretrained_conv1_weight = model.conv1.weight.clone()\n            logger.info(f\"Using pre-trained {model_type} with weights from ImageNet\")\n        \n        # Check if we need to adapt the first conv layer\n        if input_size < 64 or input_channels != 3:\n            # For small image sizes like CIFAR-10 (32x32), we need to adapt the first conv layer\n            if input_size < 64:\n                logger.info(f\"Adapting first conv layer for small input size: {input_size}x{input_size}\")\n                model.conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, stride=1, padding=1, bias=False)\n                # Remove aggressive pooling for small images\n                model.maxpool = nn.Identity()\n            else:\n                # Just adapt channels if input size is large enough\n                model.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n            \n            # Adapt the pre-trained weights to the new conv layer if possible\n            if use_pretrained and input_channels == 3:\n                with torch.no_grad():\n                    if input_size < 64:\n                        # Take the center 3x3 section of the 7x7 kernels for small images\n                        model.conv1.weight.copy_(pretrained_conv1_weight[:, :, 2:5, 2:5])\n                    else:\n                        # For different number of channels but same kernel size\n                        model.conv1.weight.copy_(pretrained_conv1_weight)\n                        \n                logger.info(\"Adapted pre-trained conv1 weights for the new input size\")\n        \n        # Adapt the final fc layer for the specified number of classes\n        model.fc = nn.Linear(model.fc.in_features, num_classes)\n        \n    elif model_type.startswith(\"vgg\"):\n        # Choose the appropriate VGG model\n        if model_type == \"vgg11\":\n            model = models.vgg11(pretrained=use_pretrained)\n        elif model_type == \"vgg13\":\n            model = models.vgg13(pretrained=use_pretrained)\n        elif model_type == \"vgg16\":\n            model = models.vgg16(pretrained=use_pretrained)\n        elif model_type == \"vgg19\":\n            model = models.vgg19(pretrained=use_pretrained)\n        else:\n            raise ValueError(f\"Unsupported VGG model: {model_type}\")\n        \n        # Adapt the classifier for different input sizes and number of classes\n        if input_size != 224:\n            # Calculate the feature size after the convolutional layers\n            feature_size = input_size // 32  # VGG has 5 max pooling layers (2^5 = 32)\n            feature_size = max(1, feature_size)  # Ensure at least 1x1 feature size\n            \n            # Adjust the first FC layer to accommodate the new feature size\n            in_features = model.classifier[0].in_features // (49 / (feature_size * feature_size))\n            in_features = int(in_features)\n            \n            logger.info(f\"Adapting VGG classifier for input size {input_size}x{input_size}, features: {in_features}\")\n            \n            # Create a new classifier with adjusted input size\n            model.classifier = nn.Sequential(\n                nn.Linear(in_features, 4096),\n                nn.ReLU(True),\n                nn.Dropout(),\n                nn.Linear(4096, 4096),\n                nn.ReLU(True),\n                nn.Dropout(),\n                nn.Linear(4096, num_classes),\n            )\n        else:\n            # Just adapt the final layer for the number of classes\n            num_features = model.classifier[6].in_features\n            model.classifier[6] = nn.Linear(num_features, num_classes)\n        \n        # Handle different input channels if needed\n        if input_channels != 3 and use_pretrained:\n            logger.warning(f\"Pre-trained VGG models expect 3 input channels, but {input_channels} were specified.\")\n            # Adapt the first conv layer for different number of channels\n            first_conv = model.features[0]\n            new_conv = nn.Conv2d(input_channels, first_conv.out_channels, \n                               kernel_size=first_conv.kernel_size, \n                               stride=first_conv.stride,\n                               padding=first_conv.padding)\n            \n            # Copy weights for shared channels if possible\n            if input_channels > 0:\n                with torch.no_grad():\n                    new_conv.weight[:, :min(3, input_channels)] = first_conv.weight[:, :min(3, input_channels)]\n            \n            model.features[0] = new_conv\n    \n    elif model_type.startswith(\"mobilenet\"):\n        # MobileNet models\n        if model_type == \"mobilenet_v2\":\n            model = models.mobilenet_v2(pretrained=use_pretrained)\n        elif model_type == \"mobilenet_v3_small\":\n            model = models.mobilenet_v3_small(pretrained=use_pretrained)\n        elif model_type == \"mobilenet_v3_large\":\n            model = models.mobilenet_v3_large(pretrained=use_pretrained)\n        else:\n            raise ValueError(f\"Unsupported MobileNet model: {model_type}\")\n        \n        # Adapt the classifier for the number of classes\n        if hasattr(model, 'classifier'):\n            in_features = model.classifier[-1].in_features\n            model.classifier[-1] = nn.Linear(in_features, num_classes)\n        else:\n            # For older versions of torchvision\n            in_features = model.last_channel\n            model.classifier = nn.Linear(in_features, num_classes)\n        \n        # Adapt the first conv layer for different input channels if needed\n        if input_channels != 3:\n            if hasattr(model, 'features') and hasattr(model.features[0], 'conv'):\n                # MobileNetV3\n                first_conv = model.features[0].conv\n                new_conv = nn.Conv2d(input_channels, first_conv.out_channels,\n                                   kernel_size=first_conv.kernel_size,\n                                   stride=first_conv.stride,\n                                   padding=first_conv.padding,\n                                   bias=False if first_conv.bias is None else True)\n                \n                # Copy weights for shared channels if possible and using pre-trained\n                if use_pretrained and input_channels > 0:\n                    with torch.no_grad():\n                        new_conv.weight[:, :min(3, input_channels)] = first_conv.weight[:, :min(3, input_channels)]\n                \n                model.features[0].conv = new_conv\n            else:\n                # MobileNetV2\n                first_conv = model.features[0][0]\n                new_conv = nn.Conv2d(input_channels, first_conv.out_channels,\n                                  kernel_size=first_conv.kernel_size,\n                                  stride=first_conv.stride,\n                                  padding=first_conv.padding,\n                                  bias=False if first_conv.bias is None else True)\n                \n                # Copy weights for shared channels if possible and using pre-trained\n                if use_pretrained and input_channels > 0:\n                    with torch.no_grad():\n                        new_conv.weight[:, :min(3, input_channels)] = first_conv.weight[:, :min(3, input_channels)]\n                \n                model.features[0][0] = new_conv\n    \n    elif model_type.startswith(\"efficientnet\"):\n        # EfficientNet models\n        if model_type == \"efficientnet_b0\":\n            model = models.efficientnet_b0(pretrained=use_pretrained)\n        elif model_type == \"efficientnet_b1\":\n            model = models.efficientnet_b1(pretrained=use_pretrained)\n        elif model_type == \"efficientnet_b2\":\n            model = models.efficientnet_b2(pretrained=use_pretrained)\n        elif model_type == \"efficientnet_b3\":\n            model = models.efficientnet_b3(pretrained=use_pretrained)\n        else:\n            raise ValueError(f\"Unsupported EfficientNet model: {model_type}\")\n        \n        # Modify the classifier for the target number of classes\n        in_features = model.classifier[1].in_features\n        model.classifier[1] = nn.Linear(in_features, num_classes)\n        \n        # Adapt for smaller input sizes if needed\n        if input_size < 224:\n            logger.info(f\"Adapting EfficientNet for smaller input size: {input_size}x{input_size}\")\n    else:\n        raise ValueError(f\"Unsupported model type: {model_type}\")\n    \n    return model\n\ndef patch_eve_unified_ratio():\n    \"\"\"\n    Apply runtime patch to EVEUnifiedRatio to support the weight_range_iris option"
    },
    "track_unified_ratio_callback": {
      "start_line": 1579,
      "end_line": 1640,
      "parameters": [
        {
          "name": "epoch"
        },
        {
          "name": "history"
        },
        {
          "name": "model"
        },
        {
          "name": "optimizer"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "....append",
          "line": 1633
        },
        {
          "name": "hasattr",
          "line": 1582
        },
        {
          "name": "isinstance",
          "line": 1588
        },
        {
          "name": "hasattr",
          "line": 1588
        },
        {
          "name": "optimizer.calculate_risk_accuracy_ratios",
          "line": 1589
        },
        {
          "name": "isinstance",
          "line": 1606
        },
        {
          "name": "hasattr",
          "line": 1606
        },
        {
          "name": "optimizer.get_bound_status_history",
          "line": 1607
        },
        {
          "name": "hasattr",
          "line": 1617
        },
        {
          "name": "len",
          "line": 1583
        },
        {
          "name": "logger.debug",
          "line": 1585
        },
        {
          "name": "logger.info",
          "line": 1599
        },
        {
          "name": "....join",
          "line": 1602
        },
        {
          "name": "logger.info",
          "line": 1603
        },
        {
          "name": "bound_history.items",
          "line": 1613
        },
        {
          "name": "optimizer.get_bound_adjustment_history",
          "line": 1618
        },
        {
          "name": "sum",
          "line": 1598
        },
        {
          "name": "len",
          "line": 1598
        },
        {
          "name": "adjustment_history.items",
          "line": 1624
        },
        {
          "name": "ratios.values",
          "line": 1598
        },
        {
          "name": "ratios.items",
          "line": 1602
        }
      ],
      "docstring": "Callback function to track unified ratio metrics and bounds status.",
      "code_snippet": "\n# Callback for tracking unified ratio metrics\ndef track_unified_ratio_callback(epoch, history, model, optimizer):\n    \"\"\"Callback function to track unified ratio metrics and bounds status.\"\"\"\n    # Update Fibonacci interval if using Fibonacci sensitivity\n    if hasattr(optimizer, 'fibonacci_intervals') and optimizer.fibonacci_intervals:\n        if epoch < len(optimizer.fibonacci_intervals):\n            optimizer.lr_check_interval = optimizer.fibonacci_intervals[epoch]\n            logger.debug(f\"Updated LR check interval to {optimizer.lr_check_interval} for epoch {epoch}\")\n    \n    # Track risk/accuracy ratios if available\n    if isinstance(optimizer, EVEUnifiedRatio) and hasattr(optimizer, 'calculate_risk_accuracy_ratios'):\n        ratios = optimizer.calculate_risk_accuracy_ratios()\n        \n        if ratios:\n            if 'risk_accuracy_ratios' not in history:\n                history['risk_accuracy_ratios'] = {}\n            \n            history['risk_accuracy_ratios'][epoch] = ratios\n            \n            # Calculate and log average ratio\n            avg_ratio = sum(ratios.values()) / len(ratios)\n            logger.info(f\"Epoch {epoch} - Average Risk/Accuracy ratio: {avg_ratio:.3f}\")\n            \n            # Store the detailed ratio information\n            ratio_str = \", \".join([f\"{k}: {v:.2f}\" for k, v in ratios.items()])\n            logger.info(f\"Risk/Accuracy ratios: {ratio_str}\")\n    \n    # Track equilibrium bounds status if available\n    if isinstance(optimizer, EVEUnifiedRatio) and hasattr(optimizer, 'get_bound_status_history'):\n        bound_history = optimizer.get_bound_status_history()\n        \n        if bound_history:\n            if 'equilibrium_bounds_history' not in history:\n                history['equilibrium_bounds_history'] = {}\n            \n            for e, statuses in bound_history.items():\n                history['equilibrium_bounds_history'][e] = statuses\n                \n        # Track bound adjustments if available\n        if hasattr(optimizer, 'get_bound_adjustment_history'):\n            adjustment_history = optimizer.get_bound_adjustment_history()\n            \n            if adjustment_history:\n                if 'bound_adjustments' not in history:\n                    history['bound_adjustments'] = {}\n                \n                for e, ratio in adjustment_history.items():\n                    history['bound_adjustments'][e] = ratio\n    \n    # Track learning rate\n    if 'learning_rates' not in history:\n        history['learning_rates'] = []\n    \n    # Get current learning rate\n    current_lr = optimizer.param_groups[0]['lr']\n    history['learning_rates'].append(current_lr)\n    \n    # If the trainer is passed directly to this callback, we could check model swapping here\n    # We expect this to be handled in the train method instead, using check_and_swap_model\n    \n    return False  # Continue training\n\ndef modify_pattern_risk_accuracy_tracker():\n    \"\"\"\n    Modify the PatternRiskAccuracyTracker to initialize risk from complexity."
    },
    "modify_pattern_risk_accuracy_tracker": {
      "start_line": 1640,
      "end_line": 1696,
      "parameters": [],
      "return_type": null,
      "calls": [
        {
          "name": "logger.info",
          "line": 1694
        },
        {
          "name": "original_init",
          "line": 1653
        },
        {
          "name": "logger.info",
          "line": 1666
        },
        {
          "name": "complexities.items",
          "line": 1669
        },
        {
          "name": "self._initialize_risks_from_complexities",
          "line": 1657
        },
        {
          "name": "logger.warning",
          "line": 1663
        },
        {
          "name": "min",
          "line": 1684
        },
        {
          "name": "logger.info",
          "line": 1686
        },
        {
          "name": "isinstance",
          "line": 1672
        },
        {
          "name": "max",
          "line": 1684
        },
        {
          "name": "isinstance",
          "line": 1675
        }
      ],
      "docstring": "\n    Modify the PatternRiskAccuracyTracker to initialize risk from complexity.\n    This ensures compatibility with the streamlined pattern map format.\n    ",
      "code_snippet": "    return False  # Continue training\n\ndef modify_pattern_risk_accuracy_tracker():\n    \"\"\"\n    Modify the PatternRiskAccuracyTracker to initialize risk from complexity.\n    This ensures compatibility with the streamlined pattern map format.\n    \"\"\"\n    from isekaizen.core.optimizer.pattern_risk_accuracy_tracker import PatternRiskAccuracyTracker\n    \n    # Store the original __init__ method\n    original_init = PatternRiskAccuracyTracker.__init__\n    \n    # Define a new initialization method that handles complexity information\n    def new_init(self, pattern_map=None):\n        # Call the original initialization\n        original_init(self, pattern_map)\n        \n        # Initialize risks from complexity if available\n        if pattern_map and 'pattern_complexities' in pattern_map:\n            self._initialize_risks_from_complexities()\n    \n    # Define the new method to initialize risks from complexities\n    def initialize_risks_from_complexities(self):\n        \"\"\"Initialize risks from complexity scores if available.\"\"\"\n        if not self.pattern_map or 'pattern_complexities' not in self.pattern_map:\n            logger.warning(\"No pattern complexities found for risk initialization\")\n            return\n            \n        logger.info(\"Initializing pattern risks from complexity scores\")\n        complexities = self.pattern_map['pattern_complexities']\n        \n        for pattern_type, complexity_info in complexities.items():\n            if pattern_type in self.pattern_stats:\n                # Use complexity score to initialize risk\n                if isinstance(complexity_info, dict) and 'avg_complexity' in complexity_info:\n                    avg_complexity = complexity_info['avg_complexity']\n                else:\n                    avg_complexity = complexity_info if isinstance(complexity_info, (int, float)) else 2.5\n                    \n                # Since the complexity in the pattern map is very low (~0.1), scale it up\n                if avg_complexity < 0.2:\n                    # Scale up low complexities to get meaningful risk values\n                    scaled_complexity = avg_complexity * 2.5\n                else:\n                    scaled_complexity = avg_complexity / 5.0  # Keep original scale for higher values\n                    \n                initial_risk = min(1.0, max(0.1, scaled_complexity))  # Minimum risk of 0.1\n                self.pattern_stats[pattern_type]['risk'] = initial_risk\n                logger.info(f\"Initialized {pattern_type} risk to {initial_risk:.2f} based on complexity {avg_complexity}\")\n    \n    # Add the new method to the class\n    PatternRiskAccuracyTracker._initialize_risks_from_complexities = initialize_risks_from_complexities\n    \n    # Replace the __init__ method\n    PatternRiskAccuracyTracker.__init__ = new_init\n    \n    logger.info(\"Enhanced PatternRiskAccuracyTracker to initialize risk from complexity scores\")\n\ndef visualize_training_results(history, output_path=None):\n    \"\"\"\n    Visualize training results with unified risk/accuracy ratio information."
    },
    "visualize_training_results": {
      "start_line": 1696,
      "end_line": 1999,
      "parameters": [
        {
          "name": "history"
        },
        {
          "name": "output_path"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "plt.figure",
          "line": 1705
        },
        {
          "name": "range",
          "line": 1708
        },
        {
          "name": "plt.subplot",
          "line": 1716
        },
        {
          "name": "plt.plot",
          "line": 1718
        },
        {
          "name": "plt.title",
          "line": 1721
        },
        {
          "name": "plt.xlabel",
          "line": 1722
        },
        {
          "name": "plt.ylabel",
          "line": 1723
        },
        {
          "name": "plt.legend",
          "line": 1724
        },
        {
          "name": "plt.grid",
          "line": 1725
        },
        {
          "name": "plt.subplot",
          "line": 1728
        },
        {
          "name": "plt.plot",
          "line": 1730
        },
        {
          "name": "plt.title",
          "line": 1733
        },
        {
          "name": "plt.xlabel",
          "line": 1734
        },
        {
          "name": "plt.ylabel",
          "line": 1735
        },
        {
          "name": "plt.legend",
          "line": 1736
        },
        {
          "name": "plt.grid",
          "line": 1737
        },
        {
          "name": "plt.subplot",
          "line": 1740
        },
        {
          "name": "plt.subplot",
          "line": 1757
        },
        {
          "name": "plt.plot",
          "line": 1759
        },
        {
          "name": "plt.title",
          "line": 1760
        },
        {
          "name": "plt.xlabel",
          "line": 1761
        },
        {
          "name": "plt.ylabel",
          "line": 1762
        },
        {
          "name": "plt.grid",
          "line": 1763
        },
        {
          "name": "plt.subplot",
          "line": 1766
        },
        {
          "name": "plt.grid",
          "line": 1794
        },
        {
          "name": "plt.subplot",
          "line": 1797
        },
        {
          "name": "plt.grid",
          "line": 1825
        },
        {
          "name": "plt.subplot",
          "line": 1828
        },
        {
          "name": "plt.subplot",
          "line": 1863
        },
        {
          "name": "plt.subplot",
          "line": 1879
        },
        {
          "name": "plt.subplot",
          "line": 1911
        },
        {
          "name": "plt.subplot",
          "line": 1961
        },
        {
          "name": "plt.subplot",
          "line": 1976
        },
        {
          "name": "plt.tight_layout",
          "line": 1990
        },
        {
          "name": "plt.close",
          "line": 1997
        },
        {
          "name": "plt.plot",
          "line": 1720
        },
        {
          "name": "plt.plot",
          "line": 1732
        },
        {
          "name": "plt.plot",
          "line": 1744
        },
        {
          "name": "plt.axhline",
          "line": 1745
        },
        {
          "name": "plt.title",
          "line": 1746
        },
        {
          "name": "plt.xlabel",
          "line": 1747
        },
        {
          "name": "plt.ylabel",
          "line": 1748
        },
        {
          "name": "plt.legend",
          "line": 1749
        },
        {
          "name": "plt.grid",
          "line": 1750
        },
        {
          "name": "plt.text",
          "line": 1752
        },
        {
          "name": "plt.title",
          "line": 1754
        },
        {
          "name": "set",
          "line": 1770
        },
        {
          "name": "plt.title",
          "line": 1784
        },
        {
          "name": "plt.xlabel",
          "line": 1785
        },
        {
          "name": "plt.ylabel",
          "line": 1786
        },
        {
          "name": "plt.legend",
          "line": 1787
        },
        {
          "name": "plt.text",
          "line": 1790
        },
        {
          "name": "plt.title",
          "line": 1792
        },
        {
          "name": "set",
          "line": 1801
        },
        {
          "name": "plt.title",
          "line": 1815
        },
        {
          "name": "plt.xlabel",
          "line": 1816
        },
        {
          "name": "plt.ylabel",
          "line": 1817
        },
        {
          "name": "plt.legend",
          "line": 1818
        },
        {
          "name": "plt.text",
          "line": 1821
        },
        {
          "name": "plt.title",
          "line": 1823
        },
        {
          "name": "sorted",
          "line": 1832
        },
        {
          "name": "set",
          "line": 1835
        },
        {
          "name": "....values",
          "line": 1836
        },
        {
          "name": "plt.title",
          "line": 1851
        },
        {
          "name": "plt.xlabel",
          "line": 1852
        },
        {
          "name": "plt.ylabel",
          "line": 1853
        },
        {
          "name": "plt.axhline",
          "line": 1854
        },
        {
          "name": "plt.legend",
          "line": 1855
        },
        {
          "name": "plt.grid",
          "line": 1856
        },
        {
          "name": "plt.text",
          "line": 1858
        },
        {
          "name": "plt.title",
          "line": 1860
        },
        {
          "name": "plt.plot",
          "line": 1866
        },
        {
          "name": "plt.title",
          "line": 1867
        },
        {
          "name": "plt.xlabel",
          "line": 1868
        },
        {
          "name": "plt.ylabel",
          "line": 1869
        },
        {
          "name": "plt.yscale",
          "line": 1870
        },
        {
          "name": "plt.grid",
          "line": 1871
        },
        {
          "name": "plt.legend",
          "line": 1872
        },
        {
          "name": "plt.text",
          "line": 1874
        },
        {
          "name": "plt.title",
          "line": 1876
        },
        {
          "name": "set",
          "line": 1883
        },
        {
          "name": "plt.title",
          "line": 1899
        },
        {
          "name": "plt.xlabel",
          "line": 1900
        },
        {
          "name": "plt.ylabel",
          "line": 1901
        },
        {
          "name": "plt.axhline",
          "line": 1902
        },
        {
          "name": "plt.legend",
          "line": 1903
        },
        {
          "name": "plt.grid",
          "line": 1904
        },
        {
          "name": "plt.text",
          "line": 1906
        },
        {
          "name": "plt.title",
          "line": 1908
        },
        {
          "name": "sorted",
          "line": 1915
        },
        {
          "name": "set",
          "line": 1918
        },
        {
          "name": "....values",
          "line": 1919
        },
        {
          "name": "plt.yticks",
          "line": 1948
        },
        {
          "name": "plt.title",
          "line": 1949
        },
        {
          "name": "plt.xlabel",
          "line": 1950
        },
        {
          "name": "plt.ylabel",
          "line": 1951
        },
        {
          "name": "plt.grid",
          "line": 1952
        },
        {
          "name": "plt.legend",
          "line": 1953
        },
        {
          "name": "plt.text",
          "line": 1956
        },
        {
          "name": "plt.title",
          "line": 1958
        },
        {
          "name": "plt.plot",
          "line": 1964
        },
        {
          "name": "plt.title",
          "line": 1965
        },
        {
          "name": "plt.xlabel",
          "line": 1966
        },
        {
          "name": "plt.ylabel",
          "line": 1967
        },
        {
          "name": "plt.grid",
          "line": 1968
        },
        {
          "name": "plt.legend",
          "line": 1969
        },
        {
          "name": "plt.text",
          "line": 1971
        },
        {
          "name": "plt.title",
          "line": 1973
        },
        {
          "name": "plt.plot",
          "line": 1979
        },
        {
          "name": "plt.title",
          "line": 1980
        },
        {
          "name": "plt.xlabel",
          "line": 1981
        },
        {
          "name": "plt.ylabel",
          "line": 1982
        },
        {
          "name": "plt.grid",
          "line": 1983
        },
        {
          "name": "plt.legend",
          "line": 1984
        },
        {
          "name": "plt.text",
          "line": 1986
        },
        {
          "name": "plt.title",
          "line": 1988
        },
        {
          "name": "plt.savefig",
          "line": 1994
        },
        {
          "name": "logger.info",
          "line": 1995
        },
        {
          "name": "len",
          "line": 1708
        },
        {
          "name": "pattern_types.update",
          "line": 1772
        },
        {
          "name": "pattern_types.update",
          "line": 1803
        },
        {
          "name": "....keys",
          "line": 1832
        },
        {
          "name": "pattern_types.update",
          "line": 1837
        },
        {
          "name": "pattern_types.update",
          "line": 1885
        },
        {
          "name": "enumerate",
          "line": 1891
        },
        {
          "name": "....keys",
          "line": 1915
        },
        {
          "name": "pattern_types.update",
          "line": 1920
        },
        {
          "name": "len",
          "line": 1963
        },
        {
          "name": "range",
          "line": 1964
        },
        {
          "name": "max",
          "line": 1743
        },
        {
          "name": "zip",
          "line": 1743
        },
        {
          "name": "rates.keys",
          "line": 1772
        },
        {
          "name": "epoch_rates.get",
          "line": 1775
        },
        {
          "name": "len",
          "line": 1777
        },
        {
          "name": "len",
          "line": 1777
        },
        {
          "name": "plt.plot",
          "line": 1778
        },
        {
          "name": "plt.plot",
          "line": 1782
        },
        {
          "name": "risks.keys",
          "line": 1803
        },
        {
          "name": "epoch_risks.get",
          "line": 1806
        },
        {
          "name": "len",
          "line": 1808
        },
        {
          "name": "len",
          "line": 1808
        },
        {
          "name": "plt.plot",
          "line": 1809
        },
        {
          "name": "plt.plot",
          "line": 1813
        },
        {
          "name": "ratios.keys",
          "line": 1837
        },
        {
          "name": "plt.plot",
          "line": 1849
        },
        {
          "name": "weights.keys",
          "line": 1885
        },
        {
          "name": "plt.plot",
          "line": 1897
        },
        {
          "name": "statuses.keys",
          "line": 1920
        },
        {
          "name": "len",
          "line": 1964
        },
        {
          "name": "ratios.append",
          "line": 1845
        },
        {
          "name": "epochs_with_data.append",
          "line": 1846
        },
        {
          "name": "weights.append",
          "line": 1893
        },
        {
          "name": "epochs_with_data.append",
          "line": 1894
        },
        {
          "name": "plt.scatter",
          "line": 1945
        },
        {
          "name": "len",
          "line": 1778
        },
        {
          "name": "len",
          "line": 1781
        },
        {
          "name": "len",
          "line": 1781
        },
        {
          "name": "len",
          "line": 1809
        },
        {
          "name": "len",
          "line": 1812
        },
        {
          "name": "len",
          "line": 1812
        },
        {
          "name": "x_pts.append",
          "line": 1939
        },
        {
          "name": "y_pts.append",
          "line": 1942
        },
        {
          "name": "....index",
          "line": 1941
        },
        {
          "name": "list",
          "line": 1941
        }
      ],
      "docstring": "\n    Visualize training results with unified risk/accuracy ratio information.\n    \n    Args:\n        history: Training history dictionary\n        output_path: Optional output file path\n    ",
      "code_snippet": "    logger.info(\"Enhanced PatternRiskAccuracyTracker to initialize risk from complexity scores\")\n\ndef visualize_training_results(history, output_path=None):\n    \"\"\"\n    Visualize training results with unified risk/accuracy ratio information.\n    \n    Args:\n        history: Training history dictionary\n        output_path: Optional output file path\n    \"\"\"\n    # Create figure with more subplots for EVE-specific metrics\n    plt.figure(figsize=(15, 28))  # Increased figure height for more plots\n    \n    # Create epochs range\n    epochs = range(1, len(history['train_loss']) + 1)\n    \n    # Plot arrangement\n    num_rows = 8  # Increased from 7 to 8 rows to accommodate the augmentation plot\n    num_cols = 2\n    plot_idx = 1\n    \n    # 1. Training metrics: Loss\n    plt.subplot(num_rows, num_cols, plot_idx)\n    plot_idx += 1\n    plt.plot(epochs, history['train_loss'], 'b-', label='Training Loss')\n    if 'val_loss' in history and history['val_loss']:\n        plt.plot(epochs, history['val_loss'], 'r-', label='Validation Loss')\n    plt.title('Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n    \n    # 2. Training metrics: Accuracy\n    plt.subplot(num_rows, num_cols, plot_idx)\n    plot_idx += 1\n    plt.plot(epochs, history['train_acc'], 'b-', label='Training Accuracy')\n    if 'val_acc' in history and history['val_acc']:\n        plt.plot(epochs, history['val_acc'], 'r-', label='Validation Accuracy')\n    plt.title('Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy (%)')\n    plt.legend()\n    plt.grid(True)\n    \n    # 3. Train/Test Accuracy Ratio\n    plt.subplot(num_rows, num_cols, plot_idx)\n    plot_idx += 1\n    if 'train_acc' in history and 'val_acc' in history:\n        ratios = [train/max(0.1, val) for train, val in zip(history['train_acc'], history['val_acc'])]\n        plt.plot(epochs, ratios, 'g-o', label='Train/Test Ratio')\n        plt.axhline(y=1.0, color='r', linestyle='--', label='Ideal Ratio (1.0)')\n        plt.title('Train/Test Accuracy Ratio')\n        plt.xlabel('Epoch')\n        plt.ylabel('Ratio')\n        plt.legend()\n        plt.grid(True)\n    else:\n        plt.text(0.5, 0.5, 'No accuracy data available',\n                horizontalalignment='center', verticalalignment='center')\n        plt.title('Train/Test Accuracy Ratio')\n    \n    # 4. Batch sizes\n    plt.subplot(num_rows, num_cols, plot_idx)\n    plot_idx += 1\n    plt.plot(epochs, history['batch_sizes'], 'g-o', label='Batch Size')\n    plt.title('Batch Size')\n    plt.xlabel('Epoch')\n    plt.ylabel('Batch Size')\n    plt.grid(True)\n    \n    # 5. Pattern recognition rates\n    plt.subplot(num_rows, num_cols, plot_idx)\n    plot_idx += 1\n    if 'pattern_recognition_rates' in history and history['pattern_recognition_rates']:\n        # Extract pattern types and create separate lines for each\n        pattern_types = set()\n        for rates in history['pattern_recognition_rates']:\n            pattern_types.update(rates.keys())\n        \n        for pattern_type in pattern_types:\n            rates = [epoch_rates.get(pattern_type, 0) for epoch_rates in history['pattern_recognition_rates']]\n            # Only plot if we have enough data points\n            if len(rates) >= len(epochs):\n                plt.plot(epochs[:len(rates)], rates, marker='o', alpha=0.7, label=pattern_type)\n            else:\n                # Pad with zeros if needed\n                padded_rates = [0] * (len(epochs) - len(rates)) + rates\n                plt.plot(epochs, padded_rates, marker='o', alpha=0.7, label=pattern_type)\n        \n        plt.title('Pattern Recognition Rates')\n        plt.xlabel('Epoch')\n        plt.ylabel('Recognition Rate')\n        plt.legend(loc='lower right')\n    else:\n        # Fallback\n        plt.text(0.5, 0.5, 'No pattern recognition data available', \n                horizontalalignment='center', verticalalignment='center')\n        plt.title('Pattern Recognition Rates')\n    \n    plt.grid(True)\n    \n    # 6. Pattern risks (derived from accuracy)\n    plt.subplot(num_rows, num_cols, plot_idx)\n    plot_idx += 1\n    if 'pattern_risks' in history and history['pattern_risks']:\n        # Extract pattern types and create separate lines for each\n        pattern_types = set()\n        for risks in history['pattern_risks']:\n            pattern_types.update(risks.keys())\n        \n        for pattern_type in pattern_types:\n            risks = [epoch_risks.get(pattern_type, 0.5) for epoch_risks in history['pattern_risks']]\n            # Only plot if we have enough data points\n            if len(risks) >= len(epochs):\n                plt.plot(epochs[:len(risks)], risks, marker='o', alpha=0.7, label=pattern_type)\n            else:\n                # Pad with default value (0.5) if needed\n                padded_risks = [0.5] * (len(epochs) - len(risks)) + risks\n                plt.plot(epochs, padded_risks, marker='o', alpha=0.7, label=pattern_type)\n        \n        plt.title('Pattern Risk Levels (Derived from Accuracy)')\n        plt.xlabel('Epoch')\n        plt.ylabel('Risk Level')\n        plt.legend(loc='upper right')\n    else:\n        # Fallback\n        plt.text(0.5, 0.5, 'No pattern risk data available', \n                horizontalalignment='center', verticalalignment='center')\n        plt.title('Pattern Risk Levels')\n    \n    plt.grid(True)\n    \n    # 7. Unified Risk/Accuracy Ratios\n    plt.subplot(num_rows, num_cols, plot_idx)\n    plot_idx += 1\n    if 'risk_accuracy_ratios' in history and history['risk_accuracy_ratios']:\n        # Sort epochs\n        epochs_list = sorted(history['risk_accuracy_ratios'].keys())\n        \n        # Extract pattern types\n        pattern_types = set()\n        for ratios in history['risk_accuracy_ratios'].values():\n            pattern_types.update(ratios.keys())\n        \n        for pattern_type in pattern_types:\n            ratios = []\n            epochs_with_data = []\n            \n            for epoch in epochs_list:\n                if pattern_type in history['risk_accuracy_ratios'][epoch]:\n                    ratios.append(history['risk_accuracy_ratios'][epoch][pattern_type])\n                    epochs_with_data.append(epoch + 1)  # 1-indexed epochs\n            \n            if ratios:\n                plt.plot(epochs_with_data, ratios, marker='o', alpha=0.7, label=pattern_type)\n        \n        plt.title('Unified Risk/Accuracy Ratios')\n        plt.xlabel('Epoch')\n        plt.ylabel('Risk/Accuracy Ratio')\n        plt.axhline(y=1.0, color='k', linestyle='--', alpha=0.5, label='Balanced (1.0)')\n        plt.legend(loc='upper right')\n        plt.grid(True)\n    else:\n        plt.text(0.5, 0.5, 'No unified ratio data available', \n                horizontalalignment='center', verticalalignment='center')\n        plt.title('Unified Risk/Accuracy Ratios')\n    \n    # 8. Learning Rate\n    plt.subplot(num_rows, num_cols, plot_idx)\n    plot_idx += 1\n    if 'learning_rates' in history and history['learning_rates']:\n        plt.plot(epochs, history['learning_rates'], 'r-o', label='Learning Rate')\n        plt.title('Learning Rate Adaptation')\n        plt.xlabel('Epoch')\n        plt.ylabel('Learning Rate')\n        plt.yscale('log')  # Log scale often better for LR\n        plt.grid(True)\n        plt.legend()\n    else:\n        plt.text(0.5, 0.5, 'No learning rate data available', \n                horizontalalignment='center', verticalalignment='center')\n        plt.title('Learning Rate Adaptation')\n    \n    # 9. Weight Adjustments\n    plt.subplot(num_rows, num_cols, plot_idx)\n    plot_idx += 1\n    if 'weight_adjustments' in history and history['weight_adjustments']:\n        # Extract pattern types for weight adjustments\n        pattern_types = set()\n        for weights in history['weight_adjustments']:\n            pattern_types.update(weights.keys())\n        \n        for pattern_type in pattern_types:\n            weights = []\n            epochs_with_data = []\n            \n            for i, weights_data in enumerate(history['weight_adjustments']):\n                if pattern_type in weights_data:\n                    weights.append(weights_data[pattern_type])\n                    epochs_with_data.append(i + 1)  # 1-indexed epochs\n            \n            if weights:\n                plt.plot(epochs_with_data, weights, marker='o', alpha=0.7, label=pattern_type)\n        \n        plt.title('EVE Weight Adjustments by Pattern')\n        plt.xlabel('Epoch')\n        plt.ylabel('Weight Adjustment Factor')\n        plt.axhline(y=1.0, color='k', linestyle='--', alpha=0.5, label='No Adjustment')\n        plt.legend(loc='upper right')\n        plt.grid(True)\n    else:\n        plt.text(0.5, 0.5, 'No weight adjustment data available', \n                horizontalalignment='center', verticalalignment='center')\n        plt.title('EVE Weight Adjustments by Pattern')\n    \n    # 10. Pattern Equilibrium Bounds Visualization\n    plt.subplot(num_rows, num_cols, plot_idx)\n    plot_idx += 1\n    if 'equilibrium_bounds_history' in history and history['equilibrium_bounds_history']:\n        # Extract data for plotting\n        epochs_list = sorted(history['equilibrium_bounds_history'].keys())\n        \n        # Get all pattern types\n        pattern_types = set()\n        for statuses in history['equilibrium_bounds_history'].values():\n            pattern_types.update(statuses.keys())\n        \n        # Set up colors and markers\n        colors = {'min': 'red', 'max': 'blue'}\n        labels = {'min': 'Below Min', 'max': 'Above Max'}\n        \n        # Plot min and max bound violations separately\n        for bound_type in ['min', 'max']:\n            for pattern_type in pattern_types:\n                # Create lists to hold the data points\n                x_pts = []\n                y_pts = []\n                \n                for epoch in epochs_list:\n                    if pattern_type in history['equilibrium_bounds_history'][epoch]:\n                        status = history['equilibrium_bounds_history'][epoch][pattern_type]\n                        \n                        # If it violates the bound (value is False), add a point\n                        if bound_type in status and not status[bound_type]:\n                            x_pts.append(epoch + 1)  # 1-indexed for display\n                            # Offset points slightly based on pattern type for visibility\n                            y_offset = list(pattern_types).index(pattern_type) * 0.1\n                            y_pts.append(y_offset + (0 if bound_type == 'min' else 0.5))\n                \n                if x_pts:\n                    plt.scatter(x_pts, y_pts, label=f\"{pattern_type} {labels[bound_type]}\", \n                               color=colors[bound_type], alpha=0.7, marker='o')\n        \n        plt.yticks([0.1, 0.6], ['Min Bound', 'Max Bound'])\n        plt.title('Pattern Bound Violations')\n        plt.xlabel('Epoch')\n        plt.ylabel('Bound Type')\n        plt.grid(True, axis='x')\n        plt.legend(loc='best', fontsize='small')\n    else:\n        # Leave empty if not using bounds\n        plt.text(0.5, 0.5, 'No equilibrium bounds data available',\n                horizontalalignment='center', verticalalignment='center')\n        plt.title('Pattern Bound Violations')\n    \n    # 11. Dataset Size\n    plt.subplot(num_rows, num_cols, plot_idx)\n    plot_idx += 1\n    if 'dataset_sizes' in history and len(history['dataset_sizes']) > 1:\n        plt.plot(range(len(history['dataset_sizes'])), history['dataset_sizes'], 'b-o', label='Dataset Size')\n        plt.title('Dataset Size')\n        plt.xlabel('Epoch')\n        plt.ylabel('Number of Examples')\n        plt.grid(True)\n        plt.legend()\n    else:\n        plt.text(0.5, 0.5, 'No dataset size data available',\n                horizontalalignment='center', verticalalignment='center')\n        plt.title('Dataset Size')\n    \n    # 12. Memory Usage\n    plt.subplot(num_rows, num_cols, plot_idx)\n    plot_idx += 1\n    if 'memory_usage' in history and history['memory_usage']:\n        plt.plot(epochs, history['memory_usage'], 'm-o', label='Memory Usage')\n        plt.title('GPU Memory Usage')\n        plt.xlabel('Epoch')\n        plt.ylabel('Memory (GB)')\n        plt.grid(True)\n        plt.legend()\n    else:\n        plt.text(0.5, 0.5, 'No memory usage data available',\n                horizontalalignment='center', verticalalignment='center')\n        plt.title('GPU Memory Usage')\n    \n    plt.tight_layout()\n    \n    # Save figure if output path provided\n    if output_path:\n        plt.savefig(output_path)\n        logger.info(f\"Training visualization saved to: {output_path}\")\n    \n    plt.close()\n\ndef format_metrics_table(metrics_dict, title=\"Metrics\", min_width=15):\n    \"\"\"Format metrics as a readable table with aligned columns.\"\"\"\n    # Calculate column widths"
    },
    "format_metrics_table": {
      "start_line": 1999,
      "end_line": 2024,
      "parameters": [
        {
          "name": "metrics_dict"
        },
        {
          "name": "title"
        },
        {
          "name": "min_width"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "max",
          "line": 2002
        },
        {
          "name": "max",
          "line": 2003
        },
        {
          "name": "max",
          "line": 2006
        },
        {
          "name": "max",
          "line": 2007
        },
        {
          "name": "metrics_dict.items",
          "line": 2015
        },
        {
          "name": "....join",
          "line": 2022
        },
        {
          "name": "isinstance",
          "line": 2016
        },
        {
          "name": "lines.append",
          "line": 2020
        },
        {
          "name": "len",
          "line": 2002
        },
        {
          "name": "len",
          "line": 2003
        },
        {
          "name": "str",
          "line": 2019
        },
        {
          "name": "str",
          "line": 2002
        },
        {
          "name": "metrics_dict.keys",
          "line": 2002
        },
        {
          "name": "metrics_dict.values",
          "line": 2003
        },
        {
          "name": "isinstance",
          "line": 2003
        },
        {
          "name": "str",
          "line": 2003
        },
        {
          "name": "str",
          "line": 2020
        }
      ],
      "docstring": "Format metrics as a readable table with aligned columns.",
      "code_snippet": "    plt.close()\n\ndef format_metrics_table(metrics_dict, title=\"Metrics\", min_width=15):\n    \"\"\"Format metrics as a readable table with aligned columns.\"\"\"\n    # Calculate column widths\n    key_width = max(len(str(k)) for k in metrics_dict.keys())\n    val_width = max(len(f\"{v:.4f}\" if isinstance(v, float) else str(v)) for v in metrics_dict.values())\n    \n    # Ensure minimum width\n    key_width = max(key_width, min_width)\n    val_width = max(val_width, min_width)\n    \n    # Create header\n    total_width = key_width + val_width + 3  # 3 for spacing and separator\n    header = f\"\\n{title}\\n\" + \"-\" * total_width\n    \n    # Format each line\n    lines = [header]\n    for key, value in metrics_dict.items():\n        if isinstance(value, float):\n            val_str = f\"{value:.4f}\"\n        else:\n            val_str = str(value)\n        lines.append(f\"{str(key):<{key_width}} | {val_str:>{val_width}}\")\n    \n    return \"\\n\".join(lines)\n\ndef log_section(title, width=80):\n    \"\"\"Print a clearly defined section header.\"\"\"\n    logger.info(\"\\n\" + \"=\" * width)"
    },
    "log_section": {
      "start_line": 2024,
      "end_line": 2030,
      "parameters": [
        {
          "name": "title"
        },
        {
          "name": "width"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "logger.info",
          "line": 2026
        },
        {
          "name": "logger.info",
          "line": 2027
        },
        {
          "name": "logger.info",
          "line": 2028
        },
        {
          "name": "....format",
          "line": 2027
        },
        {
          "name": "title.center",
          "line": 2027
        }
      ],
      "docstring": "Print a clearly defined section header.",
      "code_snippet": "    return \"\\n\".join(lines)\n\ndef log_section(title, width=80):\n    \"\"\"Print a clearly defined section header.\"\"\"\n    logger.info(\"\\n\" + \"=\" * width)\n    logger.info(\"{}\".format(title.center(width)))\n    logger.info(\"=\" * width)\n\ndef create_model(model_type=\"resnet18\", use_pretrained=False, num_classes=10, input_channels=3, input_size=32):\n    \"\"\"\n    Create a model for the specified architecture, optionally with pre-trained weights."
    },
    "patch_eve_unified_ratio": {
      "start_line": 2229,
      "end_line": 2316,
      "parameters": [],
      "return_type": null,
      "calls": [
        {
          "name": "logger.info",
          "line": 2312
        },
        {
          "name": "logger.info",
          "line": 2313
        },
        {
          "name": "logger.info",
          "line": 2314
        },
        {
          "name": "original_calculate_natural_weight_adjustment",
          "line": 2277
        },
        {
          "name": "original_step",
          "line": 2306
        },
        {
          "name": "hasattr",
          "line": 2246
        },
        {
          "name": "hasattr",
          "line": 2249
        },
        {
          "name": "self.pattern_mediator.get_pattern_accuracies",
          "line": 2250
        },
        {
          "name": "pattern_accuracies.get",
          "line": 2251
        },
        {
          "name": "logger.error",
          "line": 2254
        },
        {
          "name": "RuntimeError",
          "line": 2255
        },
        {
          "name": "hasattr",
          "line": 2258
        },
        {
          "name": "self.equilibrium_tracker.get_patterns_above_max",
          "line": 2259
        },
        {
          "name": "hasattr",
          "line": 2284
        },
        {
          "name": "hasattr",
          "line": 2284
        },
        {
          "name": "hasattr",
          "line": 2284
        },
        {
          "name": "self.pattern_mediator.update_from_batch",
          "line": 2285
        },
        {
          "name": "getattr",
          "line": 2288
        },
        {
          "name": "hasattr",
          "line": 2292
        },
        {
          "name": "self.pattern_mediator.get_pattern_accuracies",
          "line": 2294
        },
        {
          "name": "getattr",
          "line": 2297
        },
        {
          "name": "getattr",
          "line": 2298
        },
        {
          "name": "logger.info",
          "line": 2271
        },
        {
          "name": "logger.info",
          "line": 2272
        },
        {
          "name": "self.equilibrium_tracker.update_pattern_statuses",
          "line": 2302
        }
      ],
      "docstring": "\n    Apply runtime patch to EVEUnifiedRatio to support the weight_range_iris option\n    and fix the equilibrium bounds tracking issue.\n    ",
      "code_snippet": "    return model\n\ndef patch_eve_unified_ratio():\n    \"\"\"\n    Apply runtime patch to EVEUnifiedRatio to support the weight_range_iris option\n    and fix the equilibrium bounds tracking issue.\n    \"\"\"\n    from isekaizen.optimizers.eve_unified_ratio import EVEUnifiedRatio\n    \n    # Store the original methods\n    original_calculate_natural_weight_adjustment = EVEUnifiedRatio.calculate_natural_weight_adjustment\n    original_step = EVEUnifiedRatio.step\n    \n    def patched_calculate_natural_weight_adjustment(self, pattern_type):\n        \"\"\"\n        Calculate weight adjustment with iris feature that dynamically adjusts range\n        when patterns violate their max bounds.\n        \"\"\"\n        # Check if iris feature is enabled\n        has_iris = hasattr(self, 'weight_range_iris') and self.weight_range_iris\n        \n        # Get pattern accuracy from mediator\n        if hasattr(self, 'pattern_mediator') and self.pattern_mediator is not None:\n            pattern_accuracies = self.pattern_mediator.get_pattern_accuracies()\n            pattern_accuracy = pattern_accuracies.get(pattern_type, 0.5)\n        else:\n            # This should never happen with the self-initializing mediator\n            logger.error(f\"Pattern mediator not available when calculating weight adjustment for {pattern_type}\")\n            raise RuntimeError(\"Pattern mediator not available. This should never happen with self-initializing mediator.\")\n        \n        # If iris is enabled, check for max bound violations to adjust range\n        if has_iris and self.use_equilibrium_bounds and hasattr(self, 'equilibrium_tracker'):\n            above_max_patterns = self.equilibrium_tracker.get_patterns_above_max()\n            is_above_max = pattern_type in above_max_patterns\n            \n            if is_above_max:\n                # Use wide range for more aggressive correction\n                min_adjust = 0.8\n                max_adjust = 1.2\n                \n                # Apply the more aggressive adjustment\n                adjustment = min_adjust\n                \n                if self.debug_bounds:\n                    logger.info(f\"  {pattern_type} has max bound violation - using iris mode with wider range\")\n                    logger.info(f\"  {pattern_type} weight adjustment: {adjustment:.3f} (with iris, above max bound)\")\n                \n                return adjustment\n        \n        # Continue with the original method for all other cases\n        return original_calculate_natural_weight_adjustment(self, pattern_type)\n    \n    def patched_step(self, closure=None, pattern_states=None):\n        \"\"\"\n        Enhanced step method that ensures equilibrium bounds are updated throughout training.\n        \"\"\"\n        # If pattern states are provided, update the internal mediator\n        if pattern_states and hasattr(self, 'pattern_mediator') and hasattr(self, 'last_batch_indices') and hasattr(self, 'last_correct_mask'):\n            self.pattern_mediator.update_from_batch(\n                self.last_batch_indices,\n                self.last_correct_mask,\n                getattr(self, 'epoch', 0)\n            )\n            \n            # FIX: Update equilibrium bounds on every step to ensure they're maintained throughout training\n            if self.use_equilibrium_bounds and hasattr(self, 'epoch'):\n                # Get pattern accuracies from mediator \n                pattern_accuracies = self.pattern_mediator.get_pattern_accuracies()\n                \n                # Use current train/test accuracy if available\n                train_acc = getattr(self, 'train_acc', None)\n                test_acc = getattr(self, 'test_acc', None)\n                \n                # Update bounds\n                if pattern_accuracies:\n                    self.equilibrium_tracker.update_pattern_statuses(\n                        self.epoch, pattern_accuracies, train_acc, test_acc)\n        \n        # Call the original step method\n        return original_step(self, closure=closure, pattern_states=pattern_states)\n    \n    # Apply the patches\n    EVEUnifiedRatio.calculate_natural_weight_adjustment = patched_calculate_natural_weight_adjustment\n    EVEUnifiedRatio.step = patched_step\n    \n    logger.info(\"Applied patches to EVEUnifiedRatio:\")\n    logger.info(\"  - Iris feature for dynamic weight range adjustment\")\n    logger.info(\"  - Fixed equilibrium bounds tracking throughout training\")\n\ndef main():\n    \"\"\"\n    Run streamlined pattern-responsive training with unified risk/accuracy ratio approach"
    },
    "main": {
      "start_line": 2316,
      "end_line": 2755,
      "parameters": [],
      "return_type": null,
      "calls": [
        {
          "name": "argparse.ArgumentParser",
          "line": 2322
        },
        {
          "name": "early_parser.add_argument",
          "line": 2323
        },
        {
          "name": "early_parser.parse_known_args",
          "line": 2324
        },
        {
          "name": "register_unified_ratio_optimizer",
          "line": 2343
        },
        {
          "name": "modify_pattern_risk_accuracy_tracker",
          "line": 2346
        },
        {
          "name": "argparse.ArgumentParser",
          "line": 2349
        },
        {
          "name": "parser.add_argument",
          "line": 2356
        },
        {
          "name": "parser.add_argument",
          "line": 2357
        },
        {
          "name": "parser.add_argument",
          "line": 2359
        },
        {
          "name": "parser.add_argument",
          "line": 2362
        },
        {
          "name": "parser.add_argument",
          "line": 2366
        },
        {
          "name": "parser.add_argument",
          "line": 2372
        },
        {
          "name": "parser.add_argument",
          "line": 2374
        },
        {
          "name": "parser.add_argument",
          "line": 2376
        },
        {
          "name": "parser.add_argument",
          "line": 2380
        },
        {
          "name": "parser.add_argument",
          "line": 2382
        },
        {
          "name": "parser.add_argument",
          "line": 2384
        },
        {
          "name": "parser.add_argument",
          "line": 2386
        },
        {
          "name": "parser.add_argument",
          "line": 2388
        },
        {
          "name": "parser.add_argument",
          "line": 2393
        },
        {
          "name": "parser.add_argument",
          "line": 2395
        },
        {
          "name": "parser.parse_args",
          "line": 2398
        },
        {
          "name": "args.lr_sensitivity.startswith",
          "line": 2401
        },
        {
          "name": "torch.device",
          "line": 2417
        },
        {
          "name": "logger.info",
          "line": 2418
        },
        {
          "name": "logger.info",
          "line": 2421
        },
        {
          "name": "logger.info",
          "line": 2455
        },
        {
          "name": "load_cifar10_data",
          "line": 2456
        },
        {
          "name": "logger.info",
          "line": 2457
        },
        {
          "name": "logger.info",
          "line": 2543
        },
        {
          "name": "create_model",
          "line": 2544
        },
        {
          "name": "model.to",
          "line": 2551
        },
        {
          "name": "register_unified_ratio_optimizer",
          "line": 2600
        },
        {
          "name": "configure_optimizer",
          "line": 2608
        },
        {
          "name": "logger.info",
          "line": 2632
        },
        {
          "name": "UnifiedRatioTrainer",
          "line": 2634
        },
        {
          "name": "os.path.join",
          "line": 2655
        },
        {
          "name": "os.makedirs",
          "line": 2656
        },
        {
          "name": "os.path.join",
          "line": 2659
        },
        {
          "name": "logger.setLevel",
          "line": 2328
        },
        {
          "name": "....setLevel",
          "line": 2329
        },
        {
          "name": "....setLevel",
          "line": 2330
        },
        {
          "name": "logger.info",
          "line": 2331
        },
        {
          "name": "logger.setLevel",
          "line": 2334
        },
        {
          "name": "....setLevel",
          "line": 2335
        },
        {
          "name": "....setLevel",
          "line": 2336
        },
        {
          "name": "....setLevel",
          "line": 2337
        },
        {
          "name": "....setLevel",
          "line": 2338
        },
        {
          "name": "....setLevel",
          "line": 2339
        },
        {
          "name": "....setLevel",
          "line": 2340
        },
        {
          "name": "....format",
          "line": 2418
        },
        {
          "name": "logger.info",
          "line": 2423
        },
        {
          "name": "logger.info",
          "line": 2433
        },
        {
          "name": "load_latest_pattern_map",
          "line": 2434
        },
        {
          "name": "logger.info",
          "line": 2437
        },
        {
          "name": "logger.warning",
          "line": 2451
        },
        {
          "name": "....format",
          "line": 2455
        },
        {
          "name": "....format",
          "line": 2457
        },
        {
          "name": "....format",
          "line": 2543
        },
        {
          "name": "logger.info",
          "line": 2555
        },
        {
          "name": "logger.info",
          "line": 2556
        },
        {
          "name": "logger.info",
          "line": 2557
        },
        {
          "name": "logger.info",
          "line": 2558
        },
        {
          "name": "logger.info",
          "line": 2560
        },
        {
          "name": "get_fibonacci_check_intervals",
          "line": 2568
        },
        {
          "name": "logger.info",
          "line": 2573
        },
        {
          "name": "logger.info",
          "line": 2574
        },
        {
          "name": "patch_eve_unified_ratio",
          "line": 2604
        },
        {
          "name": "logger.info",
          "line": 2605
        },
        {
          "name": "logger.info",
          "line": 2622
        },
        {
          "name": "....format",
          "line": 2632
        },
        {
          "name": "logger.info",
          "line": 2669
        },
        {
          "name": "trainer.train",
          "line": 2670
        },
        {
          "name": "....strftime",
          "line": 2679
        },
        {
          "name": "os.path.join",
          "line": 2680
        },
        {
          "name": "logger.info",
          "line": 2695
        },
        {
          "name": "os.path.join",
          "line": 2698
        },
        {
          "name": "visualize_training_results",
          "line": 2699
        },
        {
          "name": "logger.info",
          "line": 2412
        },
        {
          "name": "torch.cuda.is_available",
          "line": 2417
        },
        {
          "name": "....format",
          "line": 2423
        },
        {
          "name": "logger.info",
          "line": 2427
        },
        {
          "name": "logger.info",
          "line": 2442
        },
        {
          "name": "translate_pattern_map_to_standard_format",
          "line": 2443
        },
        {
          "name": "logger.info",
          "line": 2444
        },
        {
          "name": "len",
          "line": 2457
        },
        {
          "name": "len",
          "line": 2457
        },
        {
          "name": "logger.info",
          "line": 2462
        },
        {
          "name": "create_lazy_dataset",
          "line": 2521
        },
        {
          "name": "hasattr",
          "line": 2529
        },
        {
          "name": "logger.info",
          "line": 2533
        },
        {
          "name": "logger.info",
          "line": 2534
        },
        {
          "name": "logger.info",
          "line": 2535
        },
        {
          "name": "....format",
          "line": 2555
        },
        {
          "name": "....format",
          "line": 2556
        },
        {
          "name": "....format",
          "line": 2557
        },
        {
          "name": "....format",
          "line": 2560
        },
        {
          "name": "....format",
          "line": 2574
        },
        {
          "name": "nn.CrossEntropyLoss",
          "line": 2636
        },
        {
          "name": "trainer.check_and_swap_model",
          "line": 2664
        },
        {
          "name": "open",
          "line": 2681
        },
        {
          "name": "history.items",
          "line": 2684
        },
        {
          "name": "json.dump",
          "line": 2693
        },
        {
          "name": "max",
          "line": 2705
        },
        {
          "name": "logger.info",
          "line": 2707
        },
        {
          "name": "logger.info",
          "line": 2708
        },
        {
          "name": "logger.info",
          "line": 2709
        },
        {
          "name": "logger.info",
          "line": 2710
        },
        {
          "name": "logger.info",
          "line": 2711
        },
        {
          "name": "logger.info",
          "line": 2712
        },
        {
          "name": "logger.info",
          "line": 2737
        },
        {
          "name": "logger.warning",
          "line": 2740
        },
        {
          "name": "logger.error",
          "line": 2750
        },
        {
          "name": "logger.error",
          "line": 2752
        },
        {
          "name": "sys.exit",
          "line": 2753
        },
        {
          "name": "logging.getLogger",
          "line": 2329
        },
        {
          "name": "logging.getLogger",
          "line": 2330
        },
        {
          "name": "logging.getLogger",
          "line": 2335
        },
        {
          "name": "logging.getLogger",
          "line": 2336
        },
        {
          "name": "logging.getLogger",
          "line": 2337
        },
        {
          "name": "logging.getLogger",
          "line": 2338
        },
        {
          "name": "logging.getLogger",
          "line": 2339
        },
        {
          "name": "logging.getLogger",
          "line": 2340
        },
        {
          "name": "open",
          "line": 2425
        },
        {
          "name": "json.load",
          "line": 2426
        },
        {
          "name": "logger.error",
          "line": 2429
        },
        {
          "name": "logger.info",
          "line": 2430
        },
        {
          "name": "load_latest_pattern_map",
          "line": 2431
        },
        {
          "name": "logger.warning",
          "line": 2448
        },
        {
          "name": "logger.warning",
          "line": 2449
        },
        {
          "name": "logger.info",
          "line": 2469
        },
        {
          "name": "....keys",
          "line": 2482
        },
        {
          "name": "logger.info",
          "line": 2532
        },
        {
          "name": "logger.error",
          "line": 2537
        },
        {
          "name": "logger.error",
          "line": 2539
        },
        {
          "name": "logger.warning",
          "line": 2540
        },
        {
          "name": "args.model.upper",
          "line": 2555
        },
        {
          "name": "datetime.now",
          "line": 2679
        },
        {
          "name": "isinstance",
          "line": 2685
        },
        {
          "name": "logger.info",
          "line": 2716
        },
        {
          "name": "logger.info",
          "line": 2722
        },
        {
          "name": "....values",
          "line": 2725
        },
        {
          "name": "violations.items",
          "line": 2734
        },
        {
          "name": "logger.info",
          "line": 2742
        },
        {
          "name": "....strftime",
          "line": 2743
        },
        {
          "name": "os.path.join",
          "line": 2744
        },
        {
          "name": "logger.info",
          "line": 2747
        },
        {
          "name": "traceback.format_exc",
          "line": 2752
        },
        {
          "name": "....format",
          "line": 2429
        },
        {
          "name": "....format",
          "line": 2448
        },
        {
          "name": "sys.path.append",
          "line": 2472
        },
        {
          "name": "logger.info",
          "line": 2474
        },
        {
          "name": "data.append",
          "line": 2507
        },
        {
          "name": "isinstance",
          "line": 2509
        },
        {
          "name": "torch.stack",
          "line": 2515
        },
        {
          "name": "torch.stack",
          "line": 2515
        },
        {
          "name": "traceback.format_exc",
          "line": 2539
        },
        {
          "name": "value.tolist",
          "line": 2686
        },
        {
          "name": "isinstance",
          "line": 2687
        },
        {
          "name": "logger.info",
          "line": 2718
        },
        {
          "name": "epoch_data.items",
          "line": 2726
        },
        {
          "name": "logger.info",
          "line": 2735
        },
        {
          "name": "open",
          "line": 2745
        },
        {
          "name": "json.dump",
          "line": 2746
        },
        {
          "name": "str",
          "line": 2429
        },
        {
          "name": "str",
          "line": 2448
        },
        {
          "name": "os.path.dirname",
          "line": 2472
        },
        {
          "name": "targets.append",
          "line": 2510
        },
        {
          "name": "targets.append",
          "line": 2512
        },
        {
          "name": "len",
          "line": 2534
        },
        {
          "name": "float",
          "line": 2688
        },
        {
          "name": "isinstance",
          "line": 2689
        },
        {
          "name": "datetime.now",
          "line": 2743
        },
        {
          "name": "str",
          "line": 2750
        },
        {
          "name": "torch.tensor",
          "line": 2510
        },
        {
          "name": "str",
          "line": 2537
        },
        {
          "name": "int",
          "line": 2690
        }
      ],
      "docstring": "\n    Run streamlined pattern-responsive training with unified risk/accuracy ratio approach\n    and dynamic model swapping capabilities.\n    ",
      "code_snippet": "    logger.info(\"  - Fixed equilibrium bounds tracking throughout training\")\n\ndef main():\n    \"\"\"\n    Run streamlined pattern-responsive training with unified risk/accuracy ratio approach\n    and dynamic model swapping capabilities.\n    \"\"\"\n    # Parse command line arguments early to set logging level\n    early_parser = argparse.ArgumentParser(add_help=False)\n    early_parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Enable verbose logging output\")\n    early_args, remaining_argv = early_parser.parse_known_args()\n    \n    # Adjust logging based on verbosity\n    if early_args.verbose:\n        logger.setLevel(logging.DEBUG)\n        logging.getLogger('isekaizen').setLevel(logging.DEBUG)\n        logging.getLogger('lazy_augmentation').setLevel(logging.DEBUG)\n        logger.info(\"Verbose logging enabled\")\n    else:\n        # Keep main logger at INFO but reduce subcomponent verbosity\n        logger.setLevel(logging.INFO)\n        logging.getLogger('isekaizen.pattern').setLevel(logging.WARNING)\n        logging.getLogger('isekaizen.optimizers').setLevel(logging.WARNING)\n        logging.getLogger('isekaizen.core.optimizer').setLevel(logging.WARNING)\n        logging.getLogger('isekaizen.mediators').setLevel(logging.WARNING)\n        logging.getLogger('lazy_augmentation').setLevel(logging.WARNING)\n        logging.getLogger('isekaizen.hardware.analyzer').setLevel(logging.WARNING)\n    \n    # Register the unified ratio optimizer\n    register_unified_ratio_optimizer()\n    \n    # Modify the PatternRiskAccuracyTracker to handle complexity information\n    modify_pattern_risk_accuracy_tracker()\n    \n    # Parse command line arguments\n    parser = argparse.ArgumentParser(\n        description=\"Run streamlined pattern-responsive training with unified risk/accuracy ratio\",\n        formatter_class=argparse.RawTextHelpFormatter,\n        parents=[early_parser]\n    )\n    \n    # Basic training arguments\n    parser.add_argument(\"--epochs\", type=int, default=15, help=\"Number of epochs for training (default: 15)\")\n    parser.add_argument(\"--target-accuracy\", type=float, default=None, \n                      help=\"Target accuracy to dynamically adjust training behavior\")\n    parser.add_argument(\"--optimizer\", type=str, default=\"eve_unified\", \n                      choices=[\"eve\", \"eve_unified\"],\n                      help=\"Optimizer to use (default: eve_unified)\")\n    parser.add_argument(\"--pretrained\", action=\"store_true\",\n                      help=\"Use pre-trained model with ImageNet weights (default: False)\")\n    \n    # Model-related arguments\n    parser.add_argument(\"--model\", type=str, default=\"resnet18\", \n                      choices=[\"resnet18\", \"resnet34\", \"resnet50\", \"resnet101\", \"resnet152\", \n                              \"vgg11\", \"vgg13\", \"vgg16\", \"vgg19\", \n                              \"mobilenet_v2\", \"mobilenet_v3_small\", \"mobilenet_v3_large\",\n                              \"efficientnet_b0\", \"efficientnet_b1\", \"efficientnet_b2\", \"efficientnet_b3\"],\n                      help=\"Model architecture to use (default: resnet18)\")\n    parser.add_argument(\"--input-size\", type=int, default=32,\n                      help=\"Input image size (default: 32 for CIFAR-10)\")\n    parser.add_argument(\"--num-classes\", type=int, default=10,\n                      help=\"Number of output classes (default: 10 for CIFAR-10)\")\n    parser.add_argument(\"--input-channels\", type=int, default=3,\n                      help=\"Number of input channels (default: 3 for RGB images)\")\n    \n    # Pattern and optimization arguments\n    parser.add_argument(\"--pattern-map-path\", type=str, default=None, \n                      help=\"Path to a specific pattern map file (default: use latest)\")\n    parser.add_argument(\"--debug-ratios\", action=\"store_true\",\n                      help=\"Enable detailed logging of ratio calculations\")\n    parser.add_argument(\"--weight-range\", type=str, default=\"wide\", choices=[\"narrow\", \"wide\", \"default\"],\n                      help=\"Weight adjustment range: narrow (0.9-1.1), default (0.85-1.15), or wide (0.8-1.2)\")\n    parser.add_argument(\"--weight-range-iris\", action=\"store_true\",\n                      help=\"Enable dynamic weight range adjustment when patterns exceed their maximum bounds\")\n    parser.add_argument(\"--lr-sensitivity\", type=str, default=\"high\", \n                      choices=[\"low\", \"medium\", \"high\", \"fibonacci\", \"fibonacci-quick\", \"fibonacci-standard\", \"fibonacci-thorough\", \"fibonacci-extended\"],\n                      help=\"Learning rate adaptation sensitivity: low, medium, high, fibonacci (custom epochs), \" \n                           \"fibonacci-quick (20 epochs), fibonacci-standard (54 epochs), \"\n                           \"fibonacci-thorough (143 epochs), or fibonacci-extended (232 epochs)\")\n    parser.add_argument(\"--debug-bounds\", action=\"store_true\",\n                      help=\"Enable detailed logging of equilibrium bounds calculations\")\n    parser.add_argument(\"--use-augmentation\", action=\"store_true\",\n                      help=\"Enable the augmentation mediator for data augmentation\")\n    \n    args = parser.parse_args()\n    \n    # Override epochs based on Fibonacci sensitivity variant\n    if args.lr_sensitivity.startswith(\"fibonacci-\"):\n        fibonacci_epochs = {\n            \"fibonacci-quick\": 20,     # Completes first 3 phases\n            \"fibonacci-standard\": 54,  # Completes first 4 phases\n            \"fibonacci-thorough\": 143, # Completes first 5 phases\n            \"fibonacci-extended\": 232  # Completes first 6 phases\n        }\n        \n        if args.lr_sensitivity in fibonacci_epochs:\n            original_epochs = args.epochs\n            args.epochs = fibonacci_epochs[args.lr_sensitivity]\n            logger.info(f\"Using {args.lr_sensitivity} preset: Setting epochs to {args.epochs} (overriding {original_epochs})\")\n            # Treat these variants as regular fibonacci for processing\n            args.lr_sensitivity = \"fibonacci\"\n    \n    # Set device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    logger.info(\"Using device: {}\".format(device))\n    \n    # Load pattern map\n    logger.info(\"Loading pattern map...\")\n    if args.pattern_map_path:\n        logger.info(\"Loading pattern map from specified path: {}\".format(args.pattern_map_path))\n        try:\n            with open(args.pattern_map_path, 'r') as f:\n                pattern_map = json.load(f)\n            logger.info(\"Pattern map loaded successfully from specified path\")\n        except Exception as e:\n            logger.error(\"Error loading pattern map from {}: {}\".format(args.pattern_map_path, str(e)))\n            logger.info(\"Falling back to latest pattern map\")\n            pattern_map = load_latest_pattern_map()\n    else:\n        logger.info(\"Loading latest pattern map from default location\")\n        pattern_map = load_latest_pattern_map()\n    \n    if pattern_map:\n        logger.info(\"Pattern map loaded successfully\")\n        \n        # Convert to standardized format if available\n        try:\n            from isekaizen.utils.pattern_map_utils import translate_pattern_map_to_standard_format\n            logger.info(\"Converting pattern map to standardized format...\")\n            standardized_pattern_map = translate_pattern_map_to_standard_format(pattern_map)\n            logger.info(\"Pattern map successfully converted to standardized format\")\n            \n            pattern_map = standardized_pattern_map\n        except Exception as e:\n            logger.warning(\"Unable to convert pattern map to standardized format: {}\".format(str(e)))\n            logger.warning(\"Proceeding with original pattern map\")\n    else:\n        logger.warning(\"No pattern map found. Creating a new map will be required.\")\n        pattern_map = None\n    \n    # Load dataset with appropriate input size\n    logger.info(\"Loading CIFAR-10 dataset with input size {}x{}...\".format(args.input_size, args.input_size))\n    trainset, testset = load_cifar10_data(input_size=args.input_size)\n    logger.info(\"Dataset loaded: {} training samples, {} test samples\".format(len(trainset), len(testset)))\n    \n    # Apply lazy augmentation if requested\n    if args.use_augmentation and pattern_map:\n        try:\n            logger.info(\"Setting up lazy pattern augmentation...\")\n            # Import the lazy augmentation module\n            import sys\n            # Try to import directly first\n            try:\n                import lazy_augmentation\n                create_lazy_dataset = lazy_augmentation.create_lazy_augmented_dataset\n                logger.info(\"Imported lazy_augmentation module\")\n            except ImportError:\n                # Try relative import\n                sys.path.append(os.path.dirname(__file__))\n                from lazy_augmentation import create_lazy_augmented_dataset as create_lazy_dataset\n                logger.info(\"Imported lazy_augmentation module via relative path\")\n            \n            # Preload multiple augmentation levels for each pattern type\n            # We'll create different percentages (1%, 3%, 5%) to add during training\n            # based on validation performance\n            augmentation_percentages = {}\n            if 'pattern_distribution' in pattern_map:\n                # Start with small augmentation - we'll add more based on validation\n                for pattern_type in pattern_map['pattern_distribution'].keys():\n                    augmentation_percentages[pattern_type] = 0.01  # Start with 1% augmentation per pattern type\n            else:\n                # Default augmentation percentages for basic pattern types\n                augmentation_percentages = {\n                    'structural': 0.01,\n                    'statistical': 0.01,\n                    'temporal': 0.01\n                }\n            \n            # Store preloaded augmentation percentages for later use - IMPORTANT: keep these small\n            self.preloaded_augmentation_levels = {\n                'small': 0.01,  # Already loaded in initial dataset\n                'medium': 0.02, # Will add when needed (reduced from 0.03 to prevent OOM)\n                'large': 0.03   # Will add when needed (reduced from 0.05 to prevent OOM)\n            }\n            \n            # Track which augmentation levels have been added for each pattern type\n            self.added_augmentation_levels = {pattern_type: ['small'] for pattern_type in augmentation_percentages}\n            \n            # Create a custom collate function to ensure all labels are tensors\n            def custom_collate_fn(batch):\n                data = []\n                targets = []\n                for item in batch:\n                    data.append(item[0])\n                    # Ensure target is a tensor\n                    if isinstance(item[1], int):\n                        targets.append(torch.tensor(item[1]))\n                    else:\n                        targets.append(item[1])\n                \n                # Stack the data and targets\n                return torch.stack(data), torch.stack(targets)\n            \n            # IMPORTANT: Use a smaller cache size to prevent OOM errors\n            cache_size = 500  # Reduced from default 1000 to avoid OOM\n            \n            # Create the lazy augmented dataset with consistent label types and reduced memory usage\n            trainset = create_lazy_dataset(\n                base_dataset=trainset,\n                pattern_map=pattern_map,\n                augmentation_percentages=augmentation_percentages,\n                device=device\n            )\n            \n            # Set a smaller cache size to avoid OOM errors\n            if hasattr(trainset, 'cache_size'):\n                original_cache = trainset.cache_size\n                trainset.cache_size = cache_size\n                logger.info(f\"Reduced augmentation cache size from {original_cache} to {cache_size} to prevent OOM errors\")\n            logger.info(\"Created lazy augmented training dataset:\")\n            logger.info(f\"  Total size: {len(trainset)} examples\")\n            logger.info(f\"  Augmentation percentages: {augmentation_percentages}\")\n        except Exception as e:\n            logger.error(f\"Error setting up lazy augmentation: {str(e)}\")\n            import traceback\n            logger.error(traceback.format_exc())\n            logger.warning(\"Proceeding with original dataset (no augmentation)\")\n    \n    # Create model with specified architecture\n    logger.info(\"Creating {} model...\".format(args.model))\n    model = create_model(\n        model_type=args.model,\n        use_pretrained=args.pretrained,\n        num_classes=args.num_classes,\n        input_channels=args.input_channels,\n        input_size=args.input_size\n    )\n    model = model.to(device)\n    \n    # Log model configuration\n    if args.pretrained:\n        logger.info(\"======= USING PRE-TRAINED {} =======\".format(args.model.upper()))\n        logger.info(\"Using pre-trained {} with weights from ImageNet\".format(args.model))\n        logger.info(\"Adapted for {} classes and input size {}x{}\".format(args.num_classes, args.input_size, args.input_size))\n        logger.info(\"===========================================\")\n    else:\n        logger.info(\"Using {} initialized with random weights (training from scratch)\".format(args.model))\n    \n    # Make sure the unified ratio optimizer is registered with appropriate parameters\n    config_params = {}\n    \n    # Set learning rate sensitivity parameters\n    if args.lr_sensitivity == \"fibonacci\":\n        # Pre-calculate Fibonacci intervals for the entire training\n        fibonacci_intervals = get_fibonacci_check_intervals(args.epochs)\n        config_params['lr_check_interval'] = fibonacci_intervals[0]  # Start with first interval\n        config_params['lr_change_threshold'] = 0.003\n        config_params['lr_log_threshold'] = 0.03\n        config_params['fibonacci_intervals'] = fibonacci_intervals  # Pass all intervals\n        logger.info(\"Using Fibonacci-based learning rate sensitivity\")\n        logger.info(\"First 10 intervals: {}...\".format(fibonacci_intervals[:10]))\n    elif args.lr_sensitivity == \"high\":\n        config_params['lr_check_interval'] = 3\n        config_params['lr_change_threshold'] = 0.003\n        config_params['lr_log_threshold'] = 0.03\n    elif args.lr_sensitivity == \"medium\":\n        config_params['lr_check_interval'] = 5\n        config_params['lr_change_threshold'] = 0.004\n        config_params['lr_log_threshold'] = 0.04\n    elif args.lr_sensitivity == \"low\":\n        config_params['lr_check_interval'] = 9\n        config_params['lr_change_threshold'] = 0.005\n        config_params['lr_log_threshold'] = 0.05\n    # Low sensitivity uses defaults\n    \n    # Set weight adjustment range\n    config_params['weight_adjustment_range'] = args.weight_range\n    \n    # Set weight range iris feature if enabled\n    config_params['weight_range_iris'] = args.weight_range_iris\n    \n    # Set debug flags\n    config_params['debug_ratios'] = args.debug_ratios\n    config_params['debug_bounds'] = args.debug_bounds\n    \n    # Register the optimizer\n    register_unified_ratio_optimizer(config_params)\n    \n    # Apply the iris patch if enabled\n    if args.weight_range_iris:\n        patch_eve_unified_ratio()\n        logger.info(\"Weight Range Iris feature enabled - optimizer will use more aggressive weight adjustment (0.8) for patterns exceeding max bounds\")\n    \n    # Create optimizer with pattern map\n    optimizer, scheduler = configure_optimizer(\n        model, \n        optimizer_type=args.optimizer,\n        custom_params={\n            \"pattern_map\": pattern_map\n        }\n    )\n    \n    # Note: The optimizer already has its internal pattern mediator\n    # We'll use that instead of creating a separate one\n    \n    # If using Fibonacci intervals, set them on the optimizer\n    if args.lr_sensitivity == \"fibonacci\" and 'fibonacci_intervals' in config_params:\n        optimizer.fibonacci_intervals = config_params['fibonacci_intervals']\n        logger.info(\"Set Fibonacci intervals on optimizer for dynamic LR sensitivity\")\n    \n    # Create trainer\n    batch_optimizer_kwargs = {\n        \"pattern_map\": pattern_map,\n        \"target_accuracy\": args.target_accuracy,\n        \"run_diagnostics\": True,\n        \"total_epochs\": args.epochs\n    }\n    \n    logger.info(\"Initializing batch optimizer with kwargs: {}\".format(batch_optimizer_kwargs))\n    \n    trainer = UnifiedRatioTrainer(\n        model=model,\n        criterion=nn.CrossEntropyLoss(),\n        optimizer_class=optimizer.__class__,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        device=device,\n        pattern_map=pattern_map,\n        val_dataset=testset,\n        batch_optimizer_class=EnhancedPatternResponsiveOptimizer,\n        batch_optimizer_kwargs=batch_optimizer_kwargs,\n        use_augmentation=args.use_augmentation,\n        # Add model configuration for swapping\n        model_type=args.model,\n        use_pretrained=args.pretrained,\n        num_classes=args.num_classes,\n        input_channels=args.input_channels,\n        input_size=args.input_size\n    )\n    \n    # Create output directory\n    output_dir = os.path.join(\"examples\", \"output\")\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Define checkpoint path with model name\n    checkpoint_path = os.path.join(output_dir, f\"{args.model}_unified_ratio_model\")\n    \n    # Set up callbacks including unified ratio tracking and model swapping\n    callbacks = [\n        track_unified_ratio_callback,\n        lambda epoch, history, model, optimizer: trainer.check_and_swap_model(history)\n    ]\n    \n    try:\n        # Train the model\n        logger.info(\"Starting training...\")\n        history = trainer.train(\n            train_dataset=trainset,\n            val_dataset=testset,\n            epochs=args.epochs,\n            checkpoint_path=checkpoint_path,\n            callbacks=callbacks\n        )\n        \n        # Save final metrics\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        results_path = os.path.join(output_dir, f'{args.model}_unified_ratio_results_{timestamp}.json')\n        with open(results_path, 'w') as f:\n            # Convert NumPy types to Python types for JSON serialization\n            json_history = {}\n            for key, value in history.items():\n                if isinstance(value, np.ndarray):\n                    json_history[key] = value.tolist()\n                elif isinstance(value, (np.float32, np.float64)):\n                    json_history[key] = float(value)\n                elif isinstance(value, (np.int32, np.int64)):\n                    json_history[key] = int(value)\n                else:\n                    json_history[key] = value\n            json.dump(json_history, f, indent=4)\n        \n        logger.info(f\"Training results saved to: {results_path}\")\n        \n        # Visualize results\n        viz_path = os.path.join(output_dir, f'{args.model}_unified_ratio_visualization_{timestamp}.png')\n        visualize_training_results(history, viz_path)\n        \n        # Print final metrics\n        if history['val_acc']:\n            final_train_acc = history['train_acc'][-1]\n            final_val_acc = history['val_acc'][-1]\n            best_val_acc = max(history['val_acc'])\n            \n            logger.info(\"\\n\" + \"=\"*50)\n            logger.info(\"TRAINING COMPLETE\")\n            logger.info(\"=\"*50)\n            logger.info(f\"Final training accuracy: {final_train_acc:.2f}%\")\n            logger.info(f\"Final validation accuracy: {final_val_acc:.2f}%\")\n            logger.info(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n            \n            # Log model swapping summary if any swaps occurred\n            if trainer.model_swaps:\n                logger.info(\"\\nModel Swapping Summary:\")\n                for swap in trainer.model_swaps:\n                    logger.info(f\"  Epoch {swap['epoch']}: {swap['old_model']} \u2192 {swap['new_model']} (transferred {swap['transfer_percentage']:.1f}% of parameters)\")\n                \n            # Check if using equilibrium bounds\n            if 'equilibrium_bounds_history' in history and history['equilibrium_bounds_history']:\n                logger.info(\"\\nPattern Equilibrium Summary:\")\n                # Count bound violations per pattern\n                violations = {}\n                for epoch_data in history['equilibrium_bounds_history'].values():\n                    for pattern, status in epoch_data.items():\n                        if pattern not in violations:\n                            violations[pattern] = {'min': 0, 'max': 0}\n                        if not status['min']:\n                            violations[pattern]['min'] += 1\n                        if not status['max']:\n                            violations[pattern]['max'] += 1\n                \n                for pattern, counts in violations.items():\n                    logger.info(f\"  {pattern}: {counts['min']} min bound violations, {counts['max']} max bound violations\")\n            \n            logger.info(\"=\"*50)\n            \n    except KeyboardInterrupt:\n        logger.warning(\"\\nTraining interrupted by user\")\n        if trainer.history:\n            logger.info(\"Saving partial results...\")\n            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n            results_path = os.path.join(output_dir, f'{args.model}_unified_ratio_partial_results_{timestamp}.json')\n            with open(results_path, 'w') as f:\n                json.dump(trainer.history, f, indent=4)\n            logger.info(f\"Partial results saved to: {results_path}\")\n            \n    except Exception as e:\n        logger.error(f\"Error during execution: {str(e)}\")\n        import traceback\n        logger.error(traceback.format_exc())\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    try:\n        main()"
    }
  },
  "constants": {}
}