{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\tests\\test_pre_augment_optimizer.py",
  "imports": [
    {
      "name": "unittest",
      "line": 8
    },
    {
      "name": "torch",
      "line": 9
    },
    {
      "name": "torch.nn",
      "line": 10
    },
    {
      "name": "numpy",
      "line": 11
    },
    {
      "name": "collections.defaultdict",
      "line": 12
    },
    {
      "name": "sys",
      "line": 13
    },
    {
      "name": "os",
      "line": 14
    },
    {
      "name": "isekaizen.core.optimizer.pre_augment_optimizer.PreAugmentOptimizer",
      "line": 19
    },
    {
      "name": "isekaizen.core.optimizer.risk_aware_optimizer.RiskAwarePatternIsekaiZen",
      "line": 20
    }
  ],
  "classes": {
    "SimpleModel": {
      "start_line": 22,
      "end_line": 37,
      "methods": {
        "__init__": {
          "start_line": 25,
          "end_line": 31,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "input_dim"
            },
            {
              "name": "hidden_dim"
            },
            {
              "name": "output_dim"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "....__init__",
              "line": 26
            },
            {
              "name": "nn.Linear",
              "line": 27
            },
            {
              "name": "nn.ReLU",
              "line": 28
            },
            {
              "name": "nn.Linear",
              "line": 29
            },
            {
              "name": "super",
              "line": 26
            }
          ],
          "code_snippet": "    \"\"\"Simple MLP model for testing.\"\"\"\n    \n    def __init__(self, input_dim=10, hidden_dim=20, output_dim=2):\n        super().__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)"
        },
        "forward": {
          "start_line": 31,
          "end_line": 37,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "x"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.fc1",
              "line": 32
            },
            {
              "name": "self.relu",
              "line": 33
            },
            {
              "name": "self.fc2",
              "line": 34
            }
          ],
          "code_snippet": "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n\nclass DummyDataset(torch.utils.data.Dataset):\n    \"\"\"Dummy dataset for testing.\"\"\"\n    "
        }
      },
      "class_variables": [],
      "bases": [
        "..."
      ],
      "docstring": "Simple MLP model for testing."
    },
    "DummyDataset": {
      "start_line": 37,
      "end_line": 55,
      "methods": {
        "__init__": {
          "start_line": 40,
          "end_line": 49,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "size"
            },
            {
              "name": "dim"
            },
            {
              "name": "num_classes"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "torch.randn",
              "line": 46
            },
            {
              "name": "torch.randint",
              "line": 47
            }
          ],
          "code_snippet": "    \"\"\"Dummy dataset for testing.\"\"\"\n    \n    def __init__(self, size=100, dim=10, num_classes=2):\n        self.size = size\n        self.dim = dim\n        self.num_classes = num_classes\n        \n        # Generate random data\n        self.data = torch.randn(size, dim)\n        self.targets = torch.randint(0, num_classes, (size,))\n        \n    def __len__(self):\n        return self.size\n        "
        },
        "__len__": {
          "start_line": 49,
          "end_line": 52,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [],
          "code_snippet": "        self.targets = torch.randint(0, num_classes, (size,))\n        \n    def __len__(self):\n        return self.size\n        \n    def __getitem__(self, idx):\n        return self.data[idx], self.targets[idx]\n"
        },
        "__getitem__": {
          "start_line": 52,
          "end_line": 55,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "idx"
            }
          ],
          "return_type": null,
          "calls": [],
          "code_snippet": "        return self.size\n        \n    def __getitem__(self, idx):\n        return self.data[idx], self.targets[idx]\n\nclass MockPatternMap:\n    \"\"\"Mock pattern map for testing.\"\"\"\n    "
        }
      },
      "class_variables": [],
      "bases": [
        "..."
      ],
      "docstring": "Dummy dataset for testing."
    },
    "MockPatternMap": {
      "start_line": 55,
      "end_line": 81,
      "methods": {
        "__init__": {
          "start_line": 58,
          "end_line": 72,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [],
          "code_snippet": "    \"\"\"Mock pattern map for testing.\"\"\"\n    \n    def __init__(self):\n        # Create a simple pattern map structure similar to the real one\n        self.pattern_map = {\n            'pattern_map': {\n                'pattern_map': {\n                    '0': {'pattern_type': 'structural', 'difficulty': 1.0},\n                    '1': {'pattern_type': 'statistical', 'difficulty': 1.5},\n                    '2': {'pattern_type': 'temporal', 'difficulty': 2.0}\n                },\n                'pattern_preferences': {\n                    'pattern_difficulty_ranking': ['structural', 'statistical', 'temporal'],\n                    'preferred_patterns': ['structural'],\n                    'challenging_patterns': ['temporal']\n                }\n            }\n        }\n    "
        },
        "__getitem__": {
          "start_line": 75,
          "end_line": 78,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "key"
            }
          ],
          "return_type": null,
          "calls": [],
          "code_snippet": "        }\n    \n    def __getitem__(self, key):\n        return self.pattern_map[key]\n        \n    def get(self, key, default=None):\n        return self.pattern_map.get(key, default)\n"
        },
        "get": {
          "start_line": 78,
          "end_line": 81,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "key"
            },
            {
              "name": "default"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.pattern_map.get",
              "line": 79
            }
          ],
          "code_snippet": "        return self.pattern_map[key]\n        \n    def get(self, key, default=None):\n        return self.pattern_map.get(key, default)\n\nclass TestPreAugmentOptimizer(unittest.TestCase):\n    \"\"\"Test cases for PreAugmentOptimizer.\"\"\"\n    "
        }
      },
      "class_variables": [],
      "bases": [],
      "docstring": "Mock pattern map for testing."
    },
    "TestPreAugmentOptimizer": {
      "start_line": 81,
      "end_line": 252,
      "methods": {
        "setUp": {
          "start_line": 84,
          "end_line": 103,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "SimpleModel",
              "line": 87
            },
            {
              "name": "DummyDataset",
              "line": 88
            },
            {
              "name": "torch.device",
              "line": 89
            },
            {
              "name": "MockPatternMap",
              "line": 92
            },
            {
              "name": "torch.cuda.is_available",
              "line": 89
            }
          ],
          "docstring": "Set up test environment before each test method.",
          "code_snippet": "    \"\"\"Test cases for PreAugmentOptimizer.\"\"\"\n    \n    def setUp(self):\n        \"\"\"Set up test environment before each test method.\"\"\"\n        # Create model and dataset\n        self.model = SimpleModel()\n        self.dataset = DummyDataset()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        # Mock pattern map\n        self.pattern_map = MockPatternMap()\n        \n        # Common parameters for optimizers\n        self.common_params = {\n            'model': self.model,\n            'device': self.device,\n            'pattern_map': self.pattern_map,\n            'total_epochs': 10,\n            'min_batch_size': 4,\n            'max_batch_size': 32\n        }\n        \n    def test_initialization(self):\n        \"\"\"Test correct initialization of PreAugmentOptimizer.\"\"\""
        },
        "test_initialization": {
          "start_line": 104,
          "end_line": 119,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "PreAugmentOptimizer",
              "line": 107
            },
            {
              "name": "self.assertEqual",
              "line": 110
            },
            {
              "name": "self.assertEqual",
              "line": 114
            },
            {
              "name": "self.assertEqual",
              "line": 116
            }
          ],
          "docstring": "Test correct initialization of PreAugmentOptimizer.",
          "code_snippet": "        }\n        \n    def test_initialization(self):\n        \"\"\"Test correct initialization of PreAugmentOptimizer.\"\"\"\n        # Create optimizer\n        optimizer = PreAugmentOptimizer(**self.common_params)\n        \n        # Check that exploration rate is 0\n        self.assertEqual(optimizer.exploration_rate, 0.0, \n                        \"PreAugmentOptimizer should initialize with exploration_rate=0.0\")\n        \n        # Check batch size range\n        self.assertEqual(optimizer.min_batch, self.common_params['min_batch_size'],\n                        \"Min batch size not set correctly\")\n        self.assertEqual(optimizer.max_batch, self.common_params['max_batch_size'],\n                        \"Max batch size not set correctly\")\n    \n    def test_get_optimal_batch_size_first_epoch(self):\n        \"\"\"Test batch size calculation for the first epoch.\"\"\"\n        # Create optimizer"
        },
        "test_get_optimal_batch_size_first_epoch": {
          "start_line": 119,
          "end_line": 133,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "PreAugmentOptimizer",
              "line": 122
            },
            {
              "name": "optimizer.get_optimal_batch_size",
              "line": 125
            },
            {
              "name": "self.assertEqual",
              "line": 130
            }
          ],
          "docstring": "Test batch size calculation for the first epoch.",
          "code_snippet": "                        \"Max batch size not set correctly\")\n    \n    def test_get_optimal_batch_size_first_epoch(self):\n        \"\"\"Test batch size calculation for the first epoch.\"\"\"\n        # Create optimizer\n        optimizer = PreAugmentOptimizer(**self.common_params)\n        \n        # Get batch size for first epoch\n        batch_size = optimizer.get_optimal_batch_size()\n        \n        # Should be the midpoint of min and max batch size\n        expected_batch_size = (self.common_params['min_batch_size'] + \n                             self.common_params['max_batch_size']) // 2\n        self.assertEqual(batch_size, expected_batch_size,\n                        f\"First epoch batch size should be {expected_batch_size}, got {batch_size}\")\n    \n    def test_get_optimal_batch_size_subsequent_epochs(self):\n        \"\"\"Test batch size calculation for subsequent epochs.\"\"\"\n        # Create optimizer"
        },
        "test_get_optimal_batch_size_subsequent_epochs": {
          "start_line": 133,
          "end_line": 148,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "PreAugmentOptimizer",
              "line": 136
            },
            {
              "name": "optimizer.get_optimal_batch_size",
              "line": 139
            },
            {
              "name": "optimizer.get_optimal_batch_size",
              "line": 142
            },
            {
              "name": "self.assertEqual",
              "line": 145
            }
          ],
          "docstring": "Test batch size calculation for subsequent epochs.",
          "code_snippet": "                        f\"First epoch batch size should be {expected_batch_size}, got {batch_size}\")\n    \n    def test_get_optimal_batch_size_subsequent_epochs(self):\n        \"\"\"Test batch size calculation for subsequent epochs.\"\"\"\n        # Create optimizer\n        optimizer = PreAugmentOptimizer(**self.common_params)\n        \n        # First call to set up history\n        first_batch = optimizer.get_optimal_batch_size()\n        \n        # Second call should return the same batch size\n        second_batch = optimizer.get_optimal_batch_size()\n        \n        # Should be the same as first batch\n        self.assertEqual(second_batch, first_batch,\n                        \"Subsequent epoch batch size should be the same as first epoch\")\n    \n    def test_adjustment_methods_disabled(self):\n        \"\"\"Test that pattern/risk/stability adjustment methods are disabled.\"\"\"\n        # Create optimizer"
        },
        "test_adjustment_methods_disabled": {
          "start_line": 148,
          "end_line": 169,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "PreAugmentOptimizer",
              "line": 151
            },
            {
              "name": "optimizer.adjust_batch_size_for_patterns",
              "line": 157
            },
            {
              "name": "self.assertEqual",
              "line": 158
            },
            {
              "name": "optimizer.adjust_batch_size_for_risk",
              "line": 161
            },
            {
              "name": "self.assertEqual",
              "line": 162
            },
            {
              "name": "optimizer.adjust_batch_size_for_stability",
              "line": 165
            },
            {
              "name": "self.assertEqual",
              "line": 166
            }
          ],
          "docstring": "Test that pattern/risk/stability adjustment methods are disabled.",
          "code_snippet": "                        \"Subsequent epoch batch size should be the same as first epoch\")\n    \n    def test_adjustment_methods_disabled(self):\n        \"\"\"Test that pattern/risk/stability adjustment methods are disabled.\"\"\"\n        # Create optimizer\n        optimizer = PreAugmentOptimizer(**self.common_params)\n        \n        # Try each adjustment method with a test batch size\n        test_batch = 16\n        \n        # Each method should return the input unchanged\n        pattern_adjusted = optimizer.adjust_batch_size_for_patterns(test_batch)\n        self.assertEqual(pattern_adjusted, test_batch,\n                        \"Pattern adjustment should return the input unchanged\")\n        \n        risk_adjusted = optimizer.adjust_batch_size_for_risk(test_batch)\n        self.assertEqual(risk_adjusted, test_batch,\n                        \"Risk adjustment should return the input unchanged\")\n        \n        stability_adjusted = optimizer.adjust_batch_size_for_stability(test_batch)\n        self.assertEqual(stability_adjusted, test_batch,\n                        \"Stability adjustment should return the input unchanged\")\n    \n    def test_compare_with_risk_aware_optimizer(self):\n        \"\"\"Compare PreAugmentOptimizer with RiskAwarePatternIsekaiZen.\"\"\"\n        # Create both optimizers with same base parameters"
        },
        "test_compare_with_risk_aware_optimizer": {
          "start_line": 169,
          "end_line": 197,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "PreAugmentOptimizer",
              "line": 172
            },
            {
              "name": "RiskAwarePatternIsekaiZen",
              "line": 173
            },
            {
              "name": "pre_augment_optimizer.get_optimal_batch_size",
              "line": 179
            },
            {
              "name": "risk_aware_optimizer.get_optimal_batch_size",
              "line": 180
            },
            {
              "name": "pre_augment_optimizer.adjust_batch_size_for_patterns",
              "line": 189
            },
            {
              "name": "risk_aware_optimizer.adjust_batch_size_for_patterns",
              "line": 190
            },
            {
              "name": "self.assertEqual",
              "line": 194
            }
          ],
          "docstring": "Compare PreAugmentOptimizer with RiskAwarePatternIsekaiZen.",
          "code_snippet": "                        \"Stability adjustment should return the input unchanged\")\n    \n    def test_compare_with_risk_aware_optimizer(self):\n        \"\"\"Compare PreAugmentOptimizer with RiskAwarePatternIsekaiZen.\"\"\"\n        # Create both optimizers with same base parameters\n        pre_augment_optimizer = PreAugmentOptimizer(**self.common_params)\n        risk_aware_optimizer = RiskAwarePatternIsekaiZen(\n            **self.common_params,\n            exploration_rate=0.1  # Regular optimizer has exploration enabled\n        )\n        \n        # Get initial batch sizes\n        pre_augment_batch = pre_augment_optimizer.get_optimal_batch_size()\n        risk_aware_batch = risk_aware_optimizer.get_optimal_batch_size()\n        \n        # Initial batch sizes might be the same or different (due to random exploration)\n        # What matters is the adjustment behavior\n        \n        # Create fake pattern metrics to trigger adjustments\n        test_batch = 16\n        \n        # PreAugmentOptimizer should not adjust for patterns\n        pre_adjust_pattern = pre_augment_optimizer.adjust_batch_size_for_patterns(test_batch)\n        risk_adjust_pattern = risk_aware_optimizer.adjust_batch_size_for_patterns(test_batch)\n        \n        # Check if risk-aware optimizer might adjust (we can't guarantee it will, but we know\n        # pre_augment should not)\n        self.assertEqual(pre_adjust_pattern, test_batch,\n                       \"PreAugmentOptimizer should not adjust batch size for patterns\")\n    \n    def test_pattern_tracking_preserved(self):\n        \"\"\"Test that pattern tracking is still preserved for EVE's risk awareness.\"\"\"\n        # Create optimizer"
        },
        "test_pattern_tracking_preserved": {
          "start_line": 197,
          "end_line": 215,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "PreAugmentOptimizer",
              "line": 200
            },
            {
              "name": "list",
              "line": 203
            },
            {
              "name": "optimizer.update_with_pattern_recognition",
              "line": 207
            },
            {
              "name": "optimizer.pattern_recognition_tracker.get_current_recognition_rates",
              "line": 210
            },
            {
              "name": "self.assertIsNotNone",
              "line": 213
            },
            {
              "name": "range",
              "line": 203
            }
          ],
          "docstring": "Test that pattern tracking is still preserved for EVE's risk awareness.",
          "code_snippet": "                       \"PreAugmentOptimizer should not adjust batch size for patterns\")\n    \n    def test_pattern_tracking_preserved(self):\n        \"\"\"Test that pattern tracking is still preserved for EVE's risk awareness.\"\"\"\n        # Create optimizer\n        optimizer = PreAugmentOptimizer(**self.common_params)\n        \n        # Simulate batch processing with fake results\n        batch_indices = list(range(10))\n        correct_mask = [True, False, True, True, False, True, False, True, True, False]\n        \n        # Update pattern recognition tracking\n        optimizer.update_with_pattern_recognition(batch_indices, correct_mask)\n        \n        # Get recognition rates - we should still be tracking this\n        recognition_rates = optimizer.pattern_recognition_tracker.get_current_recognition_rates()\n        \n        # We should have some tracking data, though the exact values will depend on the mock pattern map\n        self.assertIsNotNone(recognition_rates, \"Pattern recognition rates should be tracked\")\n    \n    def test_with_overfitting_underfitting_scenarios(self):\n        \"\"\"Test behavior with simulated overfitting/underfitting scenarios.\"\"\"\n        # Create optimizer"
        },
        "test_with_overfitting_underfitting_scenarios": {
          "start_line": 215,
          "end_line": 252,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "PreAugmentOptimizer",
              "line": 218
            },
            {
              "name": "optimizer.get_optimal_batch_size",
              "line": 221
            },
            {
              "name": "optimizer.adjust_batch_size_for_patterns",
              "line": 230
            },
            {
              "name": "self.assertEqual",
              "line": 233
            },
            {
              "name": "optimizer.adjust_batch_size_for_patterns",
              "line": 238
            },
            {
              "name": "self.assertEqual",
              "line": 241
            },
            {
              "name": "optimizer.get_optimal_batch_size",
              "line": 246
            },
            {
              "name": "self.assertEqual",
              "line": 249
            }
          ],
          "docstring": "Test behavior with simulated overfitting/underfitting scenarios.",
          "code_snippet": "        self.assertIsNotNone(recognition_rates, \"Pattern recognition rates should be tracked\")\n    \n    def test_with_overfitting_underfitting_scenarios(self):\n        \"\"\"Test behavior with simulated overfitting/underfitting scenarios.\"\"\"\n        # Create optimizer\n        optimizer = PreAugmentOptimizer(**self.common_params)\n        \n        # Initialize batch and history\n        optimizer.get_optimal_batch_size()  # First epoch\n        optimizer.batch_history[-1] = 16  # Override for testing\n        \n        # Simulate underfitting scenario (val_acc > train_acc)\n        optimizer.train_test_gap = -5.0  # Negative means underfitting\n        \n        # In underfitting, smaller batch would normally be prohibited\n        # But our adjustment methods are disabled, so size is unchanged\n        current_batch = optimizer.batch_history[-1]\n        adjusted_batch = optimizer.adjust_batch_size_for_patterns(current_batch // 2)\n        \n        # Should return input unchanged regardless of underfitting\n        self.assertEqual(adjusted_batch, current_batch // 2,\n                       \"Adjustment should return input unchanged despite underfitting\")\n        \n        # Simulated overfitting\n        optimizer.train_test_gap = 10.0  # Positive means overfitting\n        adjusted_batch = optimizer.adjust_batch_size_for_patterns(current_batch * 2)\n        \n        # Should still return input unchanged\n        self.assertEqual(adjusted_batch, current_batch * 2,\n                       \"Adjustment should return input unchanged despite overfitting\")\n        \n        # Adjust based on get_optimal_batch_size for next epoch\n        optimizer.train_test_gap = -5.0  # Underfitting\n        next_batch = optimizer.get_optimal_batch_size()\n        \n        # Should largely maintain the same batch size from history\n        self.assertEqual(next_batch, optimizer.batch_history[-2],\n                       \"Batch size should be maintained between epochs\")\n\nif __name__ == '__main__':\n    unittest.main()"
        }
      },
      "class_variables": [],
      "bases": [
        "..."
      ],
      "docstring": "Test cases for PreAugmentOptimizer."
    }
  },
  "functions": {},
  "constants": {}
}