{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\isekaizen\\mediators\\augmentation\\specialized\\vision_mediator.py",
  "imports": [
    {
      "name": "logging",
      "line": 8
    },
    {
      "name": "torch",
      "line": 9
    },
    {
      "name": "numpy",
      "line": 10
    },
    {
      "name": "typing.Dict",
      "line": 11
    },
    {
      "name": "typing.Any",
      "line": 11
    },
    {
      "name": "typing.Optional",
      "line": 11
    },
    {
      "name": "typing.List",
      "line": 11
    },
    {
      "name": "typing.Set",
      "line": 11
    },
    {
      "name": "typing.Tuple",
      "line": 11
    },
    {
      "name": "isekaizen.mediators.augmentation.template_mediator.AugmentationMediator",
      "line": 13
    },
    {
      "name": "isekaizen.mediators.base.Mediator",
      "line": 14
    },
    {
      "name": "isekaizen.mediators.base.MediatorInitializationError",
      "line": 14
    },
    {
      "name": "torchvision.transforms",
      "line": 127
    }
  ],
  "classes": {
    "VisionAugmentationMediator": {
      "start_line": 18,
      "end_line": 238,
      "methods": {
        "__init__": {
          "start_line": 28,
          "end_line": 66,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "dataset"
            },
            {
              "name": "pattern_map"
            },
            {
              "name": "device"
            },
            {
              "name": "image_size"
            },
            {
              "name": "channels"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "....__init__",
              "line": 48
            },
            {
              "name": "kwargs.get",
              "line": 58
            },
            {
              "name": "self.metrics.update",
              "line": 61
            },
            {
              "name": "super",
              "line": 48
            },
            {
              "name": "set",
              "line": 64
            }
          ],
          "docstring": "\n        Initialize the vision augmentation mediator.\n        \n        Args:\n            dataset: Dataset to augment\n            pattern_map: Pattern map containing pattern information\n            device: Computation device (CPU/GPU)\n            image_size: Tuple of (height, width) for images\n            channels: Number of image channels (default: 3 for RGB)\n            **kwargs: Additional arguments\n        ",
          "code_snippet": "    \"\"\"\n    \n    def __init__(\n        self, \n        dataset=None, \n        pattern_map=None, \n        device=None, \n        image_size=None,\n        channels=3,\n        **kwargs\n    ):\n        \"\"\"\n        Initialize the vision augmentation mediator.\n        \n        Args:\n            dataset: Dataset to augment\n            pattern_map: Pattern map containing pattern information\n            device: Computation device (CPU/GPU)\n            image_size: Tuple of (height, width) for images\n            channels: Number of image channels (default: 3 for RGB)\n            **kwargs: Additional arguments\n        \"\"\"\n        super().__init__(dataset=dataset, pattern_map=pattern_map, device=device, **kwargs)\n        \n        # Vision-specific parameters\n        self.image_size = image_size\n        self.channels = channels\n        \n        # Cache for augmentation transformations\n        self.transform_cache = {}\n        \n        # Flag for efficient processing\n        self.use_efficient_processing = kwargs.get('use_efficient_processing', True)\n        \n        # Additional metrics for vision augmentation\n        self.metrics.update({\n            'augmentation_times_ms': [],\n            'per_channel_stats': {},\n            'augmentation_types_used': set()\n        })\n    \n    def initialize(self):\n        \"\"\"Initialize vision-specific augmentation capabilities.\"\"\""
        },
        "initialize": {
          "start_line": 67,
          "end_line": 82,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "....initialize",
              "line": 69
            },
            {
              "name": "self._detect_image_properties",
              "line": 74
            },
            {
              "name": "self._init_vision_transformations",
              "line": 77
            },
            {
              "name": "logger.info",
              "line": 79
            },
            {
              "name": "super",
              "line": 69
            }
          ],
          "docstring": "Initialize vision-specific augmentation capabilities.",
          "code_snippet": "        })\n    \n    def initialize(self):\n        \"\"\"Initialize vision-specific augmentation capabilities.\"\"\"\n        super().initialize()\n        \n        # Initialize vision-specific augmentation capabilities\n        if self.dataset is not None:\n            # Try to automatically detect image dimensions\n            self._detect_image_properties()\n            \n            # Initialize vision-specific transformations\n            self._init_vision_transformations()\n            \n            logger.info(f\"Initialized VisionAugmentationMediator with images: \"\n                      f\"{self.image_size}, {self.channels} channels\")\n    \n    def _detect_image_properties(self):\n        \"\"\"Auto-detect image properties from the dataset.\"\"\"\n        if self.dataset is None:"
        },
        "_detect_image_properties": {
          "start_line": 82,
          "end_line": 123,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "hasattr",
              "line": 89
            },
            {
              "name": "isinstance",
              "line": 93
            },
            {
              "name": "isinstance",
              "line": 101
            },
            {
              "name": "logger.info",
              "line": 118
            },
            {
              "name": "logger.warning",
              "line": 121
            },
            {
              "name": "hasattr",
              "line": 106
            },
            {
              "name": "len",
              "line": 102
            },
            {
              "name": "len",
              "line": 108
            },
            {
              "name": "str",
              "line": 121
            }
          ],
          "docstring": "Auto-detect image properties from the dataset.",
          "code_snippet": "                      f\"{self.image_size}, {self.channels} channels\")\n    \n    def _detect_image_properties(self):\n        \"\"\"Auto-detect image properties from the dataset.\"\"\"\n        if self.dataset is None:\n            return\n            \n        # Try to get a sample from the dataset\n        try:\n            if hasattr(self.dataset, '__getitem__'):\n                sample = self.dataset[0]\n                \n                # Handle different dataset formats\n                if isinstance(sample, tuple):\n                    # Assume (image, label) format\n                    image = sample[0]\n                else:\n                    # Assume direct image format\n                    image = sample\n                \n                # Detect image dimensions\n                if isinstance(image, torch.Tensor):\n                    if len(image.shape) == 3:\n                        self.channels = image.shape[0]  # Assume CHW format\n                        if not self.image_size:\n                            self.image_size = (image.shape[1], image.shape[2])\n                elif hasattr(image, 'shape'):\n                    # Numpy array or similar\n                    if len(image.shape) == 3:\n                        if image.shape[0] <= 4:  # Assume CHW format\n                            self.channels = image.shape[0]\n                            if not self.image_size:\n                                self.image_size = (image.shape[1], image.shape[2])\n                        else:  # Assume HWC format\n                            self.channels = image.shape[2]\n                            if not self.image_size:\n                                self.image_size = (image.shape[0], image.shape[1])\n                \n                logger.info(f\"Auto-detected image properties: size={self.image_size}, channels={self.channels}\")\n                \n        except Exception as e:\n            logger.warning(f\"Could not auto-detect image properties: {str(e)}\")\n    \n    def _init_vision_transformations(self):\n        \"\"\"Initialize vision-specific transformation pipelines.\"\"\"\n        # Import here to avoid adding dependencies unless needed"
        },
        "_init_vision_transformations": {
          "start_line": 123,
          "end_line": 165,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "transforms.Compose",
              "line": 130
            },
            {
              "name": "logger.info",
              "line": 160
            },
            {
              "name": "....keys",
              "line": 138
            },
            {
              "name": "logger.warning",
              "line": 163
            },
            {
              "name": "transforms.RandomHorizontalFlip",
              "line": 131
            },
            {
              "name": "transforms.RandomRotation",
              "line": 132
            },
            {
              "name": "transforms.ColorJitter",
              "line": 133
            },
            {
              "name": "....add",
              "line": 158
            },
            {
              "name": "transforms.Compose",
              "line": 141
            },
            {
              "name": "len",
              "line": 160
            },
            {
              "name": "transforms.Compose",
              "line": 147
            },
            {
              "name": "transforms.RandomAffine",
              "line": 142
            },
            {
              "name": "transforms.RandomHorizontalFlip",
              "line": 144
            },
            {
              "name": "transforms.Compose",
              "line": 153
            },
            {
              "name": "transforms.ColorJitter",
              "line": 148
            },
            {
              "name": "transforms.RandomGrayscale",
              "line": 150
            },
            {
              "name": "transforms.RandomErasing",
              "line": 154
            },
            {
              "name": "transforms.RandomHorizontalFlip",
              "line": 155
            }
          ],
          "docstring": "Initialize vision-specific transformation pipelines.",
          "code_snippet": "            logger.warning(f\"Could not auto-detect image properties: {str(e)}\")\n    \n    def _init_vision_transformations(self):\n        \"\"\"Initialize vision-specific transformation pipelines.\"\"\"\n        # Import here to avoid adding dependencies unless needed\n        try:\n            import torchvision.transforms as transforms\n            \n            # Create standard augmentation pipelines for vision data\n            self.transform_cache['standard'] = transforms.Compose([\n                transforms.RandomHorizontalFlip(),\n                transforms.RandomRotation(10),\n                transforms.ColorJitter(brightness=0.1, contrast=0.1)\n            ])\n            \n            # Create pattern-specific transformation pipelines\n            if self.pattern_map and 'pattern_distribution' in self.pattern_map:\n                for pattern_type in self.pattern_map['pattern_distribution'].keys():\n                    # Create custom pipeline for each pattern\n                    if pattern_type == 'structural':\n                        self.transform_cache[pattern_type] = transforms.Compose([\n                            transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), \n                                                   scale=(0.9, 1.1), shear=10),\n                            transforms.RandomHorizontalFlip(),\n                        ])\n                    elif pattern_type == 'statistical':\n                        self.transform_cache[pattern_type] = transforms.Compose([\n                            transforms.ColorJitter(brightness=0.2, contrast=0.2, \n                                                  saturation=0.2, hue=0.1),\n                            transforms.RandomGrayscale(p=0.05),\n                        ])\n                    elif pattern_type == 'temporal':\n                        self.transform_cache[pattern_type] = transforms.Compose([\n                            transforms.RandomErasing(p=0.2, scale=(0.02, 0.1)),\n                            transforms.RandomHorizontalFlip(),\n                        ])\n                    # Add to metrics\n                    self.metrics['augmentation_types_used'].add(pattern_type)\n            \n            logger.info(f\"Initialized {len(self.transform_cache)} vision transformation pipelines\")\n            \n        except ImportError:\n            logger.warning(\"Could not import torchvision for vision transformations\")\n    \n    def get_augmentation_for_pattern(self, pattern_type, sample):\n        \"\"\"\n        Get augmentation for a specific pattern and sample."
        },
        "get_augmentation_for_pattern": {
          "start_line": 165,
          "end_line": 217,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "pattern_type"
            },
            {
              "name": "sample"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.transform_cache.get",
              "line": 183
            },
            {
              "name": "isinstance",
              "line": 190
            },
            {
              "name": "torch.cuda.is_available",
              "line": 176
            },
            {
              "name": "torch.cuda.Event",
              "line": 176
            },
            {
              "name": "torch.cuda.is_available",
              "line": 177
            },
            {
              "name": "torch.cuda.Event",
              "line": 177
            },
            {
              "name": "start_time.record",
              "line": 180
            },
            {
              "name": "self.transform_cache.get",
              "line": 183
            },
            {
              "name": "end_time.record",
              "line": 210
            },
            {
              "name": "torch.cuda.synchronize",
              "line": 211
            },
            {
              "name": "start_time.elapsed_time",
              "line": 212
            },
            {
              "name": "....append",
              "line": 213
            },
            {
              "name": "transform",
              "line": 195
            },
            {
              "name": "transform",
              "line": 203
            },
            {
              "name": "logger.warning",
              "line": 198
            },
            {
              "name": "logger.warning",
              "line": 205
            },
            {
              "name": "str",
              "line": 198
            },
            {
              "name": "str",
              "line": 205
            }
          ],
          "docstring": "\n        Get augmentation for a specific pattern and sample.\n        \n        Args:\n            pattern_type: Type of pattern to augment for\n            sample: Sample to augment\n            \n        Returns:\n            Augmented sample\n        ",
          "code_snippet": "            logger.warning(\"Could not import torchvision for vision transformations\")\n    \n    def get_augmentation_for_pattern(self, pattern_type, sample):\n        \"\"\"\n        Get augmentation for a specific pattern and sample.\n        \n        Args:\n            pattern_type: Type of pattern to augment for\n            sample: Sample to augment\n            \n        Returns:\n            Augmented sample\n        \"\"\"\n        start_time = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None\n        end_time = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None\n        \n        if start_time:\n            start_time.record()\n        \n        # Use pattern-specific transform if available\n        transform = self.transform_cache.get(pattern_type, self.transform_cache.get('standard'))\n        \n        if transform is None:\n            # No transform available, return original\n            return sample\n            \n        # Apply transformation\n        if isinstance(sample, tuple):\n            # Assume (image, label) format\n            image, label = sample\n            # Apply transform to image only\n            try:\n                augmented_image = transform(image)\n                result = (augmented_image, label)\n            except Exception as e:\n                logger.warning(f\"Error applying transform to image: {str(e)}\")\n                result = sample\n        else:\n            # Apply transform to whole sample\n            try:\n                result = transform(sample)\n            except Exception as e:\n                logger.warning(f\"Error applying transform to sample: {str(e)}\")\n                result = sample\n        \n        # Record timing\n        if end_time:\n            end_time.record()\n            torch.cuda.synchronize()\n            elapsed_ms = start_time.elapsed_time(end_time)\n            self.metrics['augmentation_times_ms'].append(elapsed_ms)\n        \n        return result\n    \n    def clear_cache(self):\n        \"\"\"Clear cached data and transformations.\"\"\"\n        super().clear_cache()"
        },
        "clear_cache": {
          "start_line": 217,
          "end_line": 223,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "....clear_cache",
              "line": 219
            },
            {
              "name": "self.transform_cache.clear",
              "line": 221
            },
            {
              "name": "super",
              "line": 219
            }
          ],
          "docstring": "Clear cached data and transformations.",
          "code_snippet": "        return result\n    \n    def clear_cache(self):\n        \"\"\"Clear cached data and transformations.\"\"\"\n        super().clear_cache()\n        # Clear vision-specific caches\n        self.transform_cache.clear()\n        \n    def get_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get vision-specific augmentation metrics.\"\"\"\n        metrics = super().get_metrics()"
        },
        "get_metrics": {
          "start_line": 223,
          "end_line": 238,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": "complex_type",
          "calls": [
            {
              "name": "....get_metrics",
              "line": 225
            },
            {
              "name": "self.metrics.get",
              "line": 228
            },
            {
              "name": "max",
              "line": 230
            },
            {
              "name": "list",
              "line": 234
            },
            {
              "name": "super",
              "line": 225
            },
            {
              "name": "sum",
              "line": 229
            },
            {
              "name": "len",
              "line": 229
            }
          ],
          "docstring": "Get vision-specific augmentation metrics.",
          "code_snippet": "        self.transform_cache.clear()\n        \n    def get_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get vision-specific augmentation metrics.\"\"\"\n        metrics = super().get_metrics()\n        \n        # Add vision-specific metrics\n        if self.metrics.get('augmentation_times_ms'):\n            metrics['avg_augmentation_time_ms'] = sum(self.metrics['augmentation_times_ms']) / len(self.metrics['augmentation_times_ms'])\n            metrics['max_augmentation_time_ms'] = max(self.metrics['augmentation_times_ms'])\n        \n        # Convert set to list for serialization\n        if 'augmentation_types_used' in self.metrics:\n            metrics['augmentation_types_used'] = list(self.metrics['augmentation_types_used'])\n            \n        return metrics\n\n\ndef create_augmentation_mediator_vision(dataset=None, pattern_map=None, device=None, **kwargs):\n    \"\"\""
        }
      },
      "class_variables": [],
      "bases": [
        "AugmentationMediator"
      ],
      "docstring": "\n    Specialized augmentation mediator for vision models and image datasets.\n    \n    This mediator implements vision-specific augmentation techniques like:\n    - Efficient batch processing for images\n    - Custom augmentation pipelines for different image types\n    - GPU-accelerated image transformations\n    "
    }
  },
  "functions": {
    "create_augmentation_mediator_vision": {
      "start_line": 239,
      "end_line": 265,
      "parameters": [
        {
          "name": "dataset"
        },
        {
          "name": "pattern_map"
        },
        {
          "name": "device"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "VisionAugmentationMediator",
          "line": 253
        },
        {
          "name": "mediator.initialize",
          "line": 259
        },
        {
          "name": "logger.error",
          "line": 262
        },
        {
          "name": "MediatorInitializationError",
          "line": 263
        },
        {
          "name": "str",
          "line": 262
        },
        {
          "name": "str",
          "line": 263
        }
      ],
      "docstring": "\n    Factory function to create a vision-specific augmentation mediator.\n    \n    Args:\n        dataset: Dataset to augment\n        pattern_map: Pattern map containing pattern information\n        device: Computation device (CPU/GPU)\n        **kwargs: Additional arguments for the mediator\n        \n    Returns:\n        VisionAugmentationMediator instance\n    ",
      "code_snippet": "\n\ndef create_augmentation_mediator_vision(dataset=None, pattern_map=None, device=None, **kwargs):\n    \"\"\"\n    Factory function to create a vision-specific augmentation mediator.\n    \n    Args:\n        dataset: Dataset to augment\n        pattern_map: Pattern map containing pattern information\n        device: Computation device (CPU/GPU)\n        **kwargs: Additional arguments for the mediator\n        \n    Returns:\n        VisionAugmentationMediator instance\n    \"\"\"\n    try:\n        mediator = VisionAugmentationMediator(\n            dataset=dataset,\n            pattern_map=pattern_map,\n            device=device,\n            **kwargs\n        )\n        mediator.initialize()\n        return mediator\n    except Exception as e:\n        logger.error(f\"Failed to create vision augmentation mediator: {str(e)}\")\n        raise MediatorInitializationError(f\"Failed to create vision augmentation mediator: {str(e)}\") from e"
    }
  },
  "constants": {}
}