{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\isekaizen\\core\\optimizer\\risk_accuracy\\risk_accuracy_optimizer.py",
  "imports": [
    {
      "name": "torch",
      "line": 8
    },
    {
      "name": "logging",
      "line": 9
    },
    {
      "name": "time",
      "line": 10
    },
    {
      "name": "random",
      "line": 11
    },
    {
      "name": "math",
      "line": 12
    },
    {
      "name": "typing.Dict",
      "line": 13
    },
    {
      "name": "typing.List",
      "line": 13
    },
    {
      "name": "typing.Any",
      "line": 13
    },
    {
      "name": "typing.Optional",
      "line": 13
    },
    {
      "name": "typing.Tuple",
      "line": 13
    },
    {
      "name": "typing.Set",
      "line": 13
    },
    {
      "name": "typing.Union",
      "line": 13
    },
    {
      "name": "isekaizen.core.optimizer.base_optimizer.IsekaiZenOptimizer",
      "line": 15
    },
    {
      "name": "pattern_risk_accuracy_tracker.PatternRiskAccuracyTracker",
      "line": 16
    },
    {
      "name": "pattern_adapter.PatternRiskAccuracyAdapter",
      "line": 17
    }
  ],
  "classes": {
    "RiskAccuracyOptimizer": {
      "start_line": 21,
      "end_line": 279,
      "methods": {
        "__init__": {
          "start_line": 29,
          "end_line": 94,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "model"
            },
            {
              "name": "device"
            },
            {
              "name": "total_epochs"
            },
            {
              "name": "max_epoch_time"
            },
            {
              "name": "run_diagnostics"
            },
            {
              "name": "pattern_map"
            },
            {
              "name": "risk_scale_factor"
            },
            {
              "name": "adaptation_start_epoch"
            },
            {
              "name": "adaptation_frequency"
            },
            {
              "name": "augmentation_strength"
            },
            {
              "name": "batch_risk_sensitivity"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "....__init__",
              "line": 62
            },
            {
              "name": "min",
              "line": 74
            },
            {
              "name": "PatternRiskAccuracyAdapter",
              "line": 77
            },
            {
              "name": "logger.info",
              "line": 89
            },
            {
              "name": "logger.info",
              "line": 90
            },
            {
              "name": "logger.info",
              "line": 91
            },
            {
              "name": "logger.info",
              "line": 92
            },
            {
              "name": "max",
              "line": 74
            },
            {
              "name": "super",
              "line": 62
            }
          ],
          "docstring": "\n        Initialize the risk-accuracy optimizer.\n        \n        Args:\n            model: PyTorch model to optimize\n            device: Computation device\n            total_epochs: Total number of epochs for training\n            max_epoch_time: Maximum time per epoch in seconds (None = no limit)\n            run_diagnostics: Whether to run initial diagnostics\n            pattern_map: Pattern map containing pattern information\n            risk_scale_factor: Factor to scale risk values (higher = more cautious)\n            adaptation_start_epoch: Epoch to start pattern adaptation\n            adaptation_frequency: How often to adapt patterns\n            augmentation_strength: Strength of augmentation (0.0-1.0)\n            batch_risk_sensitivity: How sensitive batch size is to risk (0.0-2.0)\n            **kwargs: Additional parameters\n        ",
          "code_snippet": "    \"\"\"\n    \n    def __init__(\n        self,\n        model,\n        device=None,\n        total_epochs=50,\n        max_epoch_time=None,\n        run_diagnostics=True,\n        pattern_map=None,\n        risk_scale_factor=1.0,\n        adaptation_start_epoch=5,\n        adaptation_frequency=3,\n        augmentation_strength=0.5,\n        batch_risk_sensitivity=1.0,\n        **kwargs\n    ):\n        \"\"\"\n        Initialize the risk-accuracy optimizer.\n        \n        Args:\n            model: PyTorch model to optimize\n            device: Computation device\n            total_epochs: Total number of epochs for training\n            max_epoch_time: Maximum time per epoch in seconds (None = no limit)\n            run_diagnostics: Whether to run initial diagnostics\n            pattern_map: Pattern map containing pattern information\n            risk_scale_factor: Factor to scale risk values (higher = more cautious)\n            adaptation_start_epoch: Epoch to start pattern adaptation\n            adaptation_frequency: How often to adapt patterns\n            augmentation_strength: Strength of augmentation (0.0-1.0)\n            batch_risk_sensitivity: How sensitive batch size is to risk (0.0-2.0)\n            **kwargs: Additional parameters\n        \"\"\"\n        # Initialize base optimizer\n        super().__init__(\n            model=model,\n            device=device,\n            total_epochs=total_epochs,\n            max_epoch_time=max_epoch_time,\n            run_diagnostics=run_diagnostics,\n            **kwargs\n        )\n        \n        # Set specific parameters\n        self.pattern_map = pattern_map\n        self.risk_scale_factor = risk_scale_factor\n        self.batch_risk_sensitivity = min(2.0, max(0.1, batch_risk_sensitivity))\n        \n        # Initialize pattern adapter\n        self.pattern_adapter = PatternRiskAccuracyAdapter(\n            pattern_map=pattern_map,\n            device=device,\n            augmentation_strength=augmentation_strength,\n            adaptation_start_epoch=adaptation_start_epoch,\n            adaptation_frequency=adaptation_frequency,\n            risk_scale_factor=risk_scale_factor\n        )\n        \n        # Track epoch metrics\n        self.epoch_metrics = {}\n        \n        logger.info(\"Risk-Accuracy Optimizer initialized\")\n        logger.info(f\"Batch size range: {self.min_batch} - {self.max_batch}\")\n        logger.info(f\"Risk scale factor: {self.risk_scale_factor}\")\n        logger.info(f\"Batch risk sensitivity: {self.batch_risk_sensitivity}\")\n    \n    def update_with_batch_results(self, batch_indices, correct_mask):\n        \"\"\"\n        Update risk-accuracy tracking with batch results."
        },
        "update_with_batch_results": {
          "start_line": 94,
          "end_line": 104,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "batch_indices"
            },
            {
              "name": "correct_mask"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.pattern_adapter.update_with_batch_results",
              "line": 102
            }
          ],
          "docstring": "\n        Update risk-accuracy tracking with batch results.\n        \n        Args:\n            batch_indices: Indices of examples in the batch\n            correct_mask: Boolean mask of whether each prediction was correct\n        ",
          "code_snippet": "        logger.info(f\"Batch risk sensitivity: {self.batch_risk_sensitivity}\")\n    \n    def update_with_batch_results(self, batch_indices, correct_mask):\n        \"\"\"\n        Update risk-accuracy tracking with batch results.\n        \n        Args:\n            batch_indices: Indices of examples in the batch\n            correct_mask: Boolean mask of whether each prediction was correct\n        \"\"\"\n        self.pattern_adapter.update_with_batch_results(batch_indices, correct_mask)\n    \n    def update_with_epoch_metrics(self, epoch_metrics):\n        \"\"\"\n        Update optimizer with epoch-level metrics."
        },
        "update_with_epoch_metrics": {
          "start_line": 104,
          "end_line": 123,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "epoch_metrics"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.pattern_adapter.update_with_epoch_results",
              "line": 115
            },
            {
              "name": "logger.info",
              "line": 118
            },
            {
              "name": "epoch_metrics.get",
              "line": 119
            },
            {
              "name": "epoch_metrics.get",
              "line": 120
            },
            {
              "name": "self.pattern_adapter.pattern_tracker.get_overall_risk",
              "line": 121
            }
          ],
          "docstring": "\n        Update optimizer with epoch-level metrics.\n        \n        Args:\n            epoch_metrics: Dictionary of epoch metrics (loss, accuracy, time, etc.)\n        ",
          "code_snippet": "        self.pattern_adapter.update_with_batch_results(batch_indices, correct_mask)\n    \n    def update_with_epoch_metrics(self, epoch_metrics):\n        \"\"\"\n        Update optimizer with epoch-level metrics.\n        \n        Args:\n            epoch_metrics: Dictionary of epoch metrics (loss, accuracy, time, etc.)\n        \"\"\"\n        # Store epoch metrics\n        self.epoch_metrics = epoch_metrics\n        \n        # Update risk-accuracy tracking\n        self.pattern_adapter.update_with_epoch_results(self.epoch)\n        \n        # Log training progress\n        logger.info(f\"Epoch {self.epoch} metrics - \"\n                  f\"Loss: {epoch_metrics.get('loss', 0.0):.4f}, \"\n                  f\"Accuracy: {epoch_metrics.get('accuracy', 0.0):.2f}%, \"\n                  f\"Overall risk: {self.pattern_adapter.pattern_tracker.get_overall_risk():.2f}\")\n    \n    def should_adapt_patterns(self):\n        \"\"\"\n        Determine if patterns should be adapted in the current epoch."
        },
        "should_adapt_patterns": {
          "start_line": 123,
          "end_line": 132,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.pattern_adapter.should_adapt_dataset",
              "line": 130
            }
          ],
          "docstring": "\n        Determine if patterns should be adapted in the current epoch.\n        \n        Returns:\n            True if patterns should be adapted, False otherwise\n        ",
          "code_snippet": "                  f\"Overall risk: {self.pattern_adapter.pattern_tracker.get_overall_risk():.2f}\")\n    \n    def should_adapt_patterns(self):\n        \"\"\"\n        Determine if patterns should be adapted in the current epoch.\n        \n        Returns:\n            True if patterns should be adapted, False otherwise\n        \"\"\"\n        return self.pattern_adapter.should_adapt_dataset()\n    \n    def adapt_dataset(self, dataset):\n        \"\"\"\n        Adapt the dataset based on pattern risks and accuracies."
        },
        "adapt_dataset": {
          "start_line": 132,
          "end_line": 144,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "dataset"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.pattern_adapter.adapt_dataset",
              "line": 142
            }
          ],
          "docstring": "\n        Adapt the dataset based on pattern risks and accuracies.\n        \n        Args:\n            dataset: Dataset to adapt\n            \n        Returns:\n            Tuple of (adapted dataset, adaptation metrics)\n        ",
          "code_snippet": "        return self.pattern_adapter.should_adapt_dataset()\n    \n    def adapt_dataset(self, dataset):\n        \"\"\"\n        Adapt the dataset based on pattern risks and accuracies.\n        \n        Args:\n            dataset: Dataset to adapt\n            \n        Returns:\n            Tuple of (adapted dataset, adaptation metrics)\n        \"\"\"\n        return self.pattern_adapter.adapt_dataset(dataset)\n    \n    def adjust_batch_size_for_risk(self, base_batch):\n        \"\"\"\n        Adjust batch size based on pattern risks."
        },
        "adjust_batch_size_for_risk": {
          "start_line": 144,
          "end_line": 184,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "base_batch"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.pattern_adapter.pattern_tracker.get_overall_risk",
              "line": 155
            },
            {
              "name": "int",
              "line": 172
            },
            {
              "name": "max",
              "line": 175
            },
            {
              "name": "max",
              "line": 164
            },
            {
              "name": "min",
              "line": 169
            },
            {
              "name": "min",
              "line": 175
            },
            {
              "name": "abs",
              "line": 178
            },
            {
              "name": "logger.info",
              "line": 179
            }
          ],
          "docstring": "\n        Adjust batch size based on pattern risks.\n        \n        Args:\n            base_batch: Base batch size to adjust\n            \n        Returns:\n            Adjusted batch size\n        ",
          "code_snippet": "        return self.pattern_adapter.adapt_dataset(dataset)\n    \n    def adjust_batch_size_for_risk(self, base_batch):\n        \"\"\"\n        Adjust batch size based on pattern risks.\n        \n        Args:\n            base_batch: Base batch size to adjust\n            \n        Returns:\n            Adjusted batch size\n        \"\"\"\n        # Get overall risk from pattern tracker\n        overall_risk = self.pattern_adapter.pattern_tracker.get_overall_risk()\n        \n        # Calculate adjustment factor based on risk\n        # Higher risk = smaller batches, lower risk = larger batches\n        # Scale adjustment by batch_risk_sensitivity\n        if overall_risk > 0.5:  # Higher than medium risk\n            # Calculate reduction factor - more significant for higher risks\n            # At maximum risk (1.0) and max sensitivity (2.0), could reduce batch by up to 80%\n            reduction_factor = ((overall_risk - 0.5) * 2) * self.batch_risk_sensitivity\n            adjustment = max(0.2, 1.0 - reduction_factor)\n        else:  # Lower than medium risk\n            # Calculate increase factor - more significant for lower risks\n            # At minimum risk (0.0) and max sensitivity (2.0), could increase batch by up to 60%\n            increase_factor = ((0.5 - overall_risk) * 2) * (self.batch_risk_sensitivity * 0.6)\n            adjustment = min(1.6, 1.0 + increase_factor)\n        \n        # Apply adjustment\n        adjusted_batch = int(base_batch * adjustment)\n        \n        # Ensure within bounds\n        adjusted_batch = max(self.min_batch, min(self.max_batch, adjusted_batch))\n        \n        # Log significant adjustments\n        if abs(adjusted_batch - base_batch) > base_batch * 0.1:  # More than 10% change\n            logger.info(f\"Risk-based batch adjustment: {base_batch} \u2192 {adjusted_batch} \"\n                      f\"(overall risk: {overall_risk:.2f}, adjustment: {adjustment:.2f}x)\")\n        \n        return adjusted_batch\n    \n    def adjust_batch_size_for_stability(self, base_batch):\n        \"\"\"\n        Adjust batch size based on training stability."
        },
        "adjust_batch_size_for_stability": {
          "start_line": 184,
          "end_line": 214,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "base_batch"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "int",
              "line": 202
            },
            {
              "name": "max",
              "line": 205
            },
            {
              "name": "logger.info",
              "line": 207
            },
            {
              "name": "min",
              "line": 205
            }
          ],
          "docstring": "\n        Adjust batch size based on training stability.\n        \n        Args:\n            base_batch: Base batch size to adjust\n            \n        Returns:\n            Adjusted batch size\n        ",
          "code_snippet": "        return adjusted_batch\n    \n    def adjust_batch_size_for_stability(self, base_batch):\n        \"\"\"\n        Adjust batch size based on training stability.\n        \n        Args:\n            base_batch: Base batch size to adjust\n            \n        Returns:\n            Adjusted batch size\n        \"\"\"\n        # Use stability metrics from base optimizer\n        stability = self.stability_metrics[\"stability_score\"]\n        \n        # Only adjust if stability is concerning\n        if stability < 0.6:\n            # Calculate adjustment - lower stability means smaller batches\n            # At 0 stability, reduce by up to 50%\n            adjustment = 0.5 + (stability * 0.5)\n            adjusted_batch = int(base_batch * adjustment)\n            \n            # Ensure within bounds\n            adjusted_batch = max(self.min_batch, min(self.max_batch, adjusted_batch))\n            \n            logger.info(f\"Stability-based batch adjustment: {base_batch} \u2192 {adjusted_batch} \"\n                      f\"(stability: {stability:.2f}, adjustment: {adjustment:.2f}x)\")\n            \n            return adjusted_batch\n        \n        return base_batch\n    \n    def get_optimal_batch_size(self):\n        \"\"\"\n        Get the optimal batch size for the current training state."
        },
        "get_optimal_batch_size": {
          "start_line": 214,
          "end_line": 253,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.adjust_batch_size_for_risk",
              "line": 240
            },
            {
              "name": "self.adjust_batch_size_for_stability",
              "line": 243
            },
            {
              "name": "max",
              "line": 246
            },
            {
              "name": "self.batch_history.append",
              "line": 249
            },
            {
              "name": "int",
              "line": 226
            },
            {
              "name": "logger.info",
              "line": 227
            },
            {
              "name": "self.batch_history.append",
              "line": 228
            },
            {
              "name": "min",
              "line": 246
            }
          ],
          "docstring": "\n        Get the optimal batch size for the current training state.\n        \n        Returns:\n            Optimal batch size\n        ",
          "code_snippet": "        return base_batch\n    \n    def get_optimal_batch_size(self):\n        \"\"\"\n        Get the optimal batch size for the current training state.\n        \n        Returns:\n            Optimal batch size\n        \"\"\"\n        # Increment epoch counter\n        self.epoch += 1\n        \n        # For the first epoch, use a conservative batch size\n        if self.epoch == 1:\n            base_batch = int(self.max_batch * 0.6)\n            logger.info(f\"First epoch: Using 60% of max batch size: {base_batch}\")\n            self.batch_history.append(base_batch)\n            return base_batch\n        \n        # For subsequent epochs, use last batch size as base if available\n        if self.batch_history:\n            base_batch = self.batch_history[-1]\n        else:\n            # Default to middle of the range\n            base_batch = (self.min_batch + self.max_batch) // 2\n        \n        # Apply adjustments in sequence\n        # 1. Adjust based on risk assessment\n        risk_adjusted_batch = self.adjust_batch_size_for_risk(base_batch)\n        \n        # 2. Adjust for stability if needed\n        stability_adjusted_batch = self.adjust_batch_size_for_stability(risk_adjusted_batch)\n        \n        # Final check - ensure batch size is within bounds\n        final_batch = max(self.min_batch, min(self.max_batch, stability_adjusted_batch))\n        \n        # Store batch for history\n        self.batch_history.append(final_batch)\n        \n        return final_batch\n    \n    def get_status(self):\n        \"\"\"\n        Get comprehensive status of the optimizer."
        },
        "get_status": {
          "start_line": 253,
          "end_line": 279,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "....get_current_state",
              "line": 261
            },
            {
              "name": "self.pattern_adapter.get_status",
              "line": 264
            },
            {
              "name": "state.update",
              "line": 267
            },
            {
              "name": "super",
              "line": 261
            },
            {
              "name": "self.pattern_adapter.pattern_tracker.get_overall_risk",
              "line": 270
            },
            {
              "name": "self.pattern_adapter.pattern_tracker.get_overall_accuracy",
              "line": 271
            },
            {
              "name": "self.pattern_adapter.pattern_tracker.get_pattern_risks",
              "line": 272
            },
            {
              "name": "self.pattern_adapter.pattern_tracker.get_pattern_accuracies",
              "line": 273
            }
          ],
          "docstring": "\n        Get comprehensive status of the optimizer.\n        \n        Returns:\n            Dictionary with status information\n        ",
          "code_snippet": "        return final_batch\n    \n    def get_status(self):\n        \"\"\"\n        Get comprehensive status of the optimizer.\n        \n        Returns:\n            Dictionary with status information\n        \"\"\"\n        # Get base state\n        state = super().get_current_state()\n        \n        # Add risk-accuracy specific state\n        pattern_adapter_status = self.pattern_adapter.get_status()\n        \n        # Add to state\n        state.update({\n            \"risk_scale_factor\": self.risk_scale_factor,\n            \"batch_risk_sensitivity\": self.batch_risk_sensitivity,\n            \"overall_risk\": self.pattern_adapter.pattern_tracker.get_overall_risk(),\n            \"overall_accuracy\": self.pattern_adapter.pattern_tracker.get_overall_accuracy(),\n            \"pattern_risks\": self.pattern_adapter.pattern_tracker.get_pattern_risks(),\n            \"pattern_accuracies\": self.pattern_adapter.pattern_tracker.get_pattern_accuracies(),\n            \"adaptation_history\": pattern_adapter_status[\"adaptation_history\"]\n        })\n        \n        return state"
        }
      },
      "class_variables": [],
      "bases": [
        "IsekaiZenOptimizer"
      ],
      "docstring": "\n    Optimizer that uses the risk-accuracy relationship for batch size optimization.\n    \n    This optimizer directly calculates risk from accuracy and uses this relationship\n    to make more intuitive batch size and pattern adaptation decisions.\n    "
    }
  },
  "functions": {},
  "constants": {}
}