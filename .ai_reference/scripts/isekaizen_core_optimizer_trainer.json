{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\isekaizen\\core\\optimizer\\trainer.py",
  "imports": [
    {
      "name": "torch",
      "line": 8
    },
    {
      "name": "logging",
      "line": 9
    },
    {
      "name": "time",
      "line": 10
    },
    {
      "name": "typing.Dict",
      "line": 11
    },
    {
      "name": "typing.Any",
      "line": 11
    },
    {
      "name": "typing.Optional",
      "line": 11
    },
    {
      "name": "typing.Type",
      "line": 11
    },
    {
      "name": "typing.Union",
      "line": 11
    },
    {
      "name": "typing.Callable",
      "line": 11
    }
  ],
  "classes": {
    "IsekaiZenTrainer": {
      "start_line": 15,
      "end_line": 435,
      "methods": {
        "__init__": {
          "start_line": 23,
          "end_line": 72,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "model"
            },
            {
              "name": "criterion"
            },
            {
              "name": "optimizer_class"
            },
            {
              "name": "optimizer_kwargs"
            },
            {
              "name": "device"
            },
            {
              "name": "scheduler_class"
            },
            {
              "name": "scheduler_kwargs"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.model.to",
              "line": 50
            },
            {
              "name": "optimizer_class",
              "line": 53
            },
            {
              "name": "torch.device",
              "line": 47
            },
            {
              "name": "model.parameters",
              "line": 53
            },
            {
              "name": "scheduler_class",
              "line": 58
            },
            {
              "name": "torch.cuda.is_available",
              "line": 47
            }
          ],
          "docstring": "\n        Initialize the trainer.\n        \n        Args:\n            model: PyTorch model to train\n            criterion: Loss function\n            optimizer_class: PyTorch optimizer class\n            optimizer_kwargs: Optimizer parameters\n            device: Computation device (auto-detected if None)\n            scheduler_class: Optional learning rate scheduler class\n            scheduler_kwargs: Optional scheduler parameters\n        ",
          "code_snippet": "    \"\"\"\n    \n    def __init__(\n        self,\n        model: torch.nn.Module,\n        criterion: torch.nn.Module,\n        optimizer_class: Type[torch.optim.Optimizer],\n        optimizer_kwargs: Dict[str, Any],\n        device: Optional[torch.device] = None,\n        scheduler_class: Optional[Type] = None,\n        scheduler_kwargs: Optional[Dict[str, Any]] = None\n    ):\n        \"\"\"\n        Initialize the trainer.\n        \n        Args:\n            model: PyTorch model to train\n            criterion: Loss function\n            optimizer_class: PyTorch optimizer class\n            optimizer_kwargs: Optimizer parameters\n            device: Computation device (auto-detected if None)\n            scheduler_class: Optional learning rate scheduler class\n            scheduler_kwargs: Optional scheduler parameters\n        \"\"\"\n        self.model = model\n        self.criterion = criterion\n        self.device = device if device is not None else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        # Move model to device\n        self.model.to(self.device)\n        \n        # Create optimizer\n        self.optimizer = optimizer_class(model.parameters(), **optimizer_kwargs)\n        \n        # Create scheduler if specified\n        self.scheduler = None\n        if scheduler_class is not None:\n            self.scheduler = scheduler_class(self.optimizer, **scheduler_kwargs)\n        \n        # Batch size optimizer\n        self.batch_optimizer = None\n        \n        # Training metrics\n        self.training_history = []\n        self.current_epoch = 0\n        \n        # For external access during training\n        self.model.trainer = self\n        self.model.optimizer = self.optimizer\n        self.model.criterion = self.criterion\n    \n    def set_batch_optimizer(self, optimizer):\n        \"\"\"\n        Set the batch size optimizer."
        },
        "set_batch_optimizer": {
          "start_line": 72,
          "end_line": 81,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "optimizer"
            }
          ],
          "return_type": null,
          "calls": [],
          "docstring": "\n        Set the batch size optimizer.\n        \n        Args:\n            optimizer: IsekaiZen optimizer instance\n        ",
          "code_snippet": "        self.model.criterion = self.criterion\n    \n    def set_batch_optimizer(self, optimizer):\n        \"\"\"\n        Set the batch size optimizer.\n        \n        Args:\n            optimizer: IsekaiZen optimizer instance\n        \"\"\"\n        self.batch_optimizer = optimizer\n    \n    def train_epoch(\n        self,\n        dataset,"
        },
        "train_epoch": {
          "start_line": 81,
          "end_line": 248,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "dataset"
            },
            {
              "name": "batch_size"
            },
            {
              "name": "shuffle",
              "type": "bool"
            },
            {
              "name": "max_epoch_time"
            },
            {
              "name": "progress_callback"
            }
          ],
          "return_type": "complex_type",
          "calls": [
            {
              "name": "torch.utils.data.DataLoader",
              "line": 111
            },
            {
              "name": "time.time",
              "line": 115
            },
            {
              "name": "torch.cuda.is_available",
              "line": 122
            },
            {
              "name": "self.model.train",
              "line": 126
            },
            {
              "name": "enumerate",
              "line": 129
            },
            {
              "name": "torch.cuda.is_available",
              "line": 203
            },
            {
              "name": "self.training_history.append",
              "line": 244
            },
            {
              "name": "self.batch_optimizer.get_optimal_batch_size",
              "line": 104
            },
            {
              "name": "torch.cuda.reset_peak_memory_stats",
              "line": 123
            },
            {
              "name": "self.optimizer.zero_grad",
              "line": 138
            },
            {
              "name": "self.model",
              "line": 141
            },
            {
              "name": "self.criterion",
              "line": 144
            },
            {
              "name": "outputs.max",
              "line": 147
            },
            {
              "name": "....item",
              "line": 148
            },
            {
              "name": "targets.size",
              "line": 149
            },
            {
              "name": "loss.item",
              "line": 152
            },
            {
              "name": "loss.backward",
              "line": 157
            },
            {
              "name": "self.optimizer.step",
              "line": 168
            },
            {
              "name": "self.scheduler.step",
              "line": 208
            },
            {
              "name": "len",
              "line": 211
            },
            {
              "name": "time.time",
              "line": 213
            },
            {
              "name": "hasattr",
              "line": 228
            },
            {
              "name": "self.batch_optimizer.get_status",
              "line": 229
            },
            {
              "name": "logger.warning",
              "line": 108
            },
            {
              "name": "logger.info",
              "line": 132
            },
            {
              "name": "inputs.to",
              "line": 135
            },
            {
              "name": "targets.to",
              "line": 135
            },
            {
              "name": "self.model.parameters",
              "line": 162
            },
            {
              "name": "list",
              "line": 172
            },
            {
              "name": "self.batch_optimizer.update_training_state",
              "line": 176
            },
            {
              "name": "hasattr",
              "line": 179
            },
            {
              "name": "hasattr",
              "line": 184
            },
            {
              "name": "progress_callback",
              "line": 200
            },
            {
              "name": "torch.cuda.max_memory_allocated",
              "line": 204
            },
            {
              "name": "....sum",
              "line": 148
            },
            {
              "name": "range",
              "line": 172
            },
            {
              "name": "loss.item",
              "line": 176
            },
            {
              "name": "predicted.eq",
              "line": 180
            },
            {
              "name": "self.batch_optimizer.update_with_pattern_recognition",
              "line": 181
            },
            {
              "name": "self.batch_optimizer.update_with_batch_results",
              "line": 189
            },
            {
              "name": "len",
              "line": 195
            },
            {
              "name": "loss.item",
              "line": 196
            },
            {
              "name": "time.time",
              "line": 131
            },
            {
              "name": "p.grad.data.norm",
              "line": 164
            },
            {
              "name": "min",
              "line": 172
            },
            {
              "name": "max",
              "line": 186
            },
            {
              "name": "loss.item",
              "line": 187
            },
            {
              "name": "time.time",
              "line": 198
            },
            {
              "name": "predicted.eq",
              "line": 148
            },
            {
              "name": "param_norm.item",
              "line": 165
            },
            {
              "name": "len",
              "line": 172
            }
          ],
          "docstring": "\n        Train one epoch.\n        \n        Args:\n            dataset: PyTorch dataset\n            batch_size: Batch size to use (auto-selected if None)\n            shuffle: Whether to shuffle the data\n            max_epoch_time: Maximum time per epoch in seconds\n            progress_callback: Optional callback for progress updates\n            \n        Returns:\n            Dictionary with training metrics\n        ",
          "code_snippet": "        self.batch_optimizer = optimizer\n    \n    def train_epoch(\n        self,\n        dataset,\n        batch_size: Optional[int] = None,\n        shuffle: bool = True,\n        max_epoch_time: Optional[float] = None,\n        progress_callback: Optional[Callable] = None\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Train one epoch.\n        \n        Args:\n            dataset: PyTorch dataset\n            batch_size: Batch size to use (auto-selected if None)\n            shuffle: Whether to shuffle the data\n            max_epoch_time: Maximum time per epoch in seconds\n            progress_callback: Optional callback for progress updates\n            \n        Returns:\n            Dictionary with training metrics\n        \"\"\"\n        # Auto-select batch size if needed\n        if batch_size is None and self.batch_optimizer is not None:\n            batch_size = self.batch_optimizer.get_optimal_batch_size()\n        elif batch_size is None:\n            # Default to a reasonable batch size\n            batch_size = 64\n            logger.warning(\"No batch optimizer set. Using default batch size 64.\")\n        \n        # Create DataLoader\n        dataloader = torch.utils.data.DataLoader(\n            dataset, batch_size=batch_size, shuffle=shuffle, num_workers=2)\n\n        # Tracking metrics\n        start_time = time.time()\n        total_loss = 0.0\n        correct = 0\n        total = 0\n        \n        # Get current memory usage\n        memory_usage = 0.0\n        if torch.cuda.is_available():\n            torch.cuda.reset_peak_memory_stats(self.device)\n\n        # Set model to training mode\n        self.model.train()\n\n        # Training loop\n        for i, (inputs, targets) in enumerate(dataloader):\n            # Check for time limit\n            if max_epoch_time and (time.time() - start_time) > max_epoch_time:\n                logger.info(f\"Epoch time limit reached ({max_epoch_time}s). Stopping early.\")\n                break\n                \n            inputs, targets = inputs.to(self.device), targets.to(self.device)\n\n            # Zero the parameter gradients\n            self.optimizer.zero_grad()\n\n            # Forward pass\n            outputs = self.model(inputs)\n            \n            # Calculate batch loss\n            loss = self.criterion(outputs, targets)\n            \n            # Calculate accuracy for this batch\n            _, predicted = outputs.max(1)\n            batch_correct = predicted.eq(targets).sum().item()\n            batch_total = targets.size(0)\n            \n            # Update metrics\n            total_loss += loss.item()\n            correct += batch_correct\n            total += batch_total\n\n            # Backward pass and optimize\n            loss.backward()\n            \n            # Calculate gradient norm for batch optimizer\n            if self.batch_optimizer is not None:\n                total_norm = 0.0\n                for p in self.model.parameters():\n                    if p.grad is not None:\n                        param_norm = p.grad.data.norm(2)\n                        total_norm += param_norm.item() ** 2\n                total_norm = total_norm ** 0.5\n            \n            self.optimizer.step()\n            \n            # Update batch optimizer with results if available\n            if self.batch_optimizer is not None:\n                batch_indices = list(range(i * batch_size, min((i + 1) * batch_size, len(dataset))))\n                batch_accuracy = 100 * batch_correct / batch_total\n                \n                # Update training state\n                self.batch_optimizer.update_training_state(loss.item(), total_norm, batch_accuracy)\n                \n                # Update pattern recognition if the optimizer supports it\n                if hasattr(self.batch_optimizer, 'update_with_pattern_recognition'):\n                    correct_mask = predicted.eq(targets)\n                    self.batch_optimizer.update_with_pattern_recognition(batch_indices, correct_mask)\n                \n                # Update risk assessment if the optimizer supports it\n                if hasattr(self.batch_optimizer, 'update_with_batch_results'):\n                    # Check if loss decreased\n                    prev_avg_loss = total_loss / max(1, i)\n                    loss_decreased = loss.item() <= prev_avg_loss\n                    \n                    self.batch_optimizer.update_with_batch_results(batch_indices, batch_size, loss_decreased)\n            \n            # Call progress callback if provided\n            if progress_callback and i % 10 == 0:\n                progress = {\n                    'batch': i + 1,\n                    'total_batches': len(dataloader),\n                    'loss': loss.item(),\n                    'batch_accuracy': 100 * batch_correct / batch_total,\n                    'elapsed_time': time.time() - start_time\n                }\n                progress_callback(progress)\n\n        # Get peak memory usage\n        if torch.cuda.is_available():\n            memory_usage = torch.cuda.max_memory_allocated(self.device) / (1024**3)  # GB\n\n        # Step the scheduler if present\n        if self.scheduler is not None:\n            self.scheduler.step()\n\n        # Calculate final metrics\n        avg_loss = total_loss / len(dataloader)\n        accuracy = 100. * correct / total\n        epoch_time = time.time() - start_time\n        \n        # Increment epoch counter\n        self.current_epoch += 1\n        \n        # Create metrics dict\n        metrics = {\n            'loss': avg_loss,\n            'accuracy': accuracy,\n            'time': epoch_time,\n            'memory': memory_usage,\n            'batch_size': batch_size\n        }\n        \n        # Add batch optimizer metrics if available\n        if self.batch_optimizer is not None and hasattr(self.batch_optimizer, 'get_status'):\n            optimizer_status = self.batch_optimizer.get_status()\n            \n            if 'preferred_pattern_recognition' in optimizer_status:\n                metrics['preferred_pattern_recognition'] = optimizer_status['preferred_pattern_recognition']\n            \n            if 'challenging_pattern_recognition' in optimizer_status:\n                metrics['challenging_pattern_recognition'] = optimizer_status['challenging_pattern_recognition']\n                \n            if 'current_risk_level' in optimizer_status:\n                metrics['risk_level'] = optimizer_status['current_risk_level']\n                \n            if 'stability' in optimizer_status:\n                metrics['stability_score'] = optimizer_status['stability']['score']\n        \n        # Store metrics in history\n        self.training_history.append(metrics)\n        \n        return metrics\n    \n    def evaluate(self, dataset, batch_size=128) -> Dict[str, Any]:\n        \"\"\"\n        Evaluate model on a dataset."
        },
        "evaluate": {
          "start_line": 248,
          "end_line": 297,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "dataset"
            },
            {
              "name": "batch_size"
            }
          ],
          "return_type": "complex_type",
          "calls": [
            {
              "name": "torch.utils.data.DataLoader",
              "line": 260
            },
            {
              "name": "time.time",
              "line": 267
            },
            {
              "name": "self.model.eval",
              "line": 270
            },
            {
              "name": "torch.no_grad",
              "line": 273
            },
            {
              "name": "len",
              "line": 288
            },
            {
              "name": "time.time",
              "line": 290
            },
            {
              "name": "self.model",
              "line": 278
            },
            {
              "name": "self.criterion",
              "line": 279
            },
            {
              "name": "loss.item",
              "line": 282
            },
            {
              "name": "outputs.max",
              "line": 283
            },
            {
              "name": "targets.size",
              "line": 284
            },
            {
              "name": "....item",
              "line": 285
            },
            {
              "name": "inputs.to",
              "line": 275
            },
            {
              "name": "targets.to",
              "line": 275
            },
            {
              "name": "....sum",
              "line": 285
            },
            {
              "name": "predicted.eq",
              "line": 285
            }
          ],
          "docstring": "\n        Evaluate model on a dataset.\n        \n        Args:\n            dataset: PyTorch dataset\n            batch_size: Batch size for evaluation\n            \n        Returns:\n            Dictionary with evaluation metrics\n        ",
          "code_snippet": "        return metrics\n    \n    def evaluate(self, dataset, batch_size=128) -> Dict[str, Any]:\n        \"\"\"\n        Evaluate model on a dataset.\n        \n        Args:\n            dataset: PyTorch dataset\n            batch_size: Batch size for evaluation\n            \n        Returns:\n            Dictionary with evaluation metrics\n        \"\"\"\n        # Create DataLoader\n        dataloader = torch.utils.data.DataLoader(\n            dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n        \n        # Tracking metrics\n        total_loss = 0.0\n        correct = 0\n        total = 0\n        start_time = time.time()\n        \n        # Set model to evaluation mode\n        self.model.eval()\n        \n        # Disable gradient calculation\n        with torch.no_grad():\n            for inputs, targets in dataloader:\n                inputs, targets = inputs.to(self.device), targets.to(self.device)\n                \n                # Forward pass\n                outputs = self.model(inputs)\n                loss = self.criterion(outputs, targets)\n                \n                # Update metrics\n                total_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += targets.size(0)\n                correct += predicted.eq(targets).sum().item()\n        \n        # Calculate final metrics\n        avg_loss = total_loss / len(dataloader)\n        accuracy = 100. * correct / total\n        eval_time = time.time() - start_time\n        \n        return {\n            'loss': avg_loss,\n            'accuracy': accuracy,\n            'time': eval_time\n        }\n    \n    def train(\n        self,"
        },
        "train": {
          "start_line": 298,
          "end_line": 403,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "train_dataset"
            },
            {
              "name": "val_dataset"
            },
            {
              "name": "epochs"
            },
            {
              "name": "early_stopping"
            },
            {
              "name": "patience"
            },
            {
              "name": "target_accuracy"
            },
            {
              "name": "progress_callback"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logger.info",
              "line": 323
            },
            {
              "name": "range",
              "line": 341
            },
            {
              "name": "logger.info",
              "line": 397
            },
            {
              "name": "logger.info",
              "line": 342
            },
            {
              "name": "self.train_epoch",
              "line": 351
            },
            {
              "name": "logger.info",
              "line": 358
            },
            {
              "name": "....append",
              "line": 361
            },
            {
              "name": "....append",
              "line": 362
            },
            {
              "name": "....append",
              "line": 363
            },
            {
              "name": "....append",
              "line": 364
            },
            {
              "name": "logger.info",
              "line": 399
            },
            {
              "name": "self.batch_optimizer.get_optimal_batch_size",
              "line": 347
            },
            {
              "name": "logger.info",
              "line": 348
            },
            {
              "name": "train_metrics.get",
              "line": 363
            },
            {
              "name": "train_metrics.get",
              "line": 364
            },
            {
              "name": "self.evaluate",
              "line": 368
            },
            {
              "name": "logger.info",
              "line": 369
            },
            {
              "name": "....append",
              "line": 372
            },
            {
              "name": "....append",
              "line": 373
            },
            {
              "name": "logger.info",
              "line": 394
            },
            {
              "name": "len",
              "line": 397
            },
            {
              "name": "logger.info",
              "line": 383
            },
            {
              "name": "logger.info",
              "line": 386
            },
            {
              "name": "logger.info",
              "line": 389
            }
          ],
          "docstring": "\n        Train for multiple epochs with validation and early stopping.\n        \n        Args:\n            train_dataset: Training dataset\n            val_dataset: Validation dataset (optional)\n            epochs: Number of epochs to train\n            early_stopping: Whether to use early stopping\n            patience: Number of epochs with no improvement before stopping\n            target_accuracy: Target accuracy to stop at\n            progress_callback: Optional callback for progress updates\n            \n        Returns:\n            Dictionary with training history\n        ",
          "code_snippet": "        }\n    \n    def train(\n        self,\n        train_dataset,\n        val_dataset=None,\n        epochs=10,\n        early_stopping=False,\n        patience=5,\n        target_accuracy=None,\n        progress_callback=None\n    ):\n        \"\"\"\n        Train for multiple epochs with validation and early stopping.\n        \n        Args:\n            train_dataset: Training dataset\n            val_dataset: Validation dataset (optional)\n            epochs: Number of epochs to train\n            early_stopping: Whether to use early stopping\n            patience: Number of epochs with no improvement before stopping\n            target_accuracy: Target accuracy to stop at\n            progress_callback: Optional callback for progress updates\n            \n        Returns:\n            Dictionary with training history\n        \"\"\"\n        logger.info(f\"Starting training for {epochs} epochs\")\n        \n        # Early stopping variables\n        best_accuracy = 0.0\n        best_epoch = -1\n        no_improvement = 0\n        \n        # Training history\n        history = {\n            'train_loss': [],\n            'train_acc': [],\n            'val_loss': [],\n            'val_acc': [],\n            'batch_sizes': [],\n            'epoch_times': []\n        }\n        \n        # Train for the specified number of epochs\n        for epoch in range(epochs):\n            logger.info(f\"Epoch {epoch+1}/{epochs}\")\n            \n            # Get batch size from optimizer if available\n            batch_size = None\n            if self.batch_optimizer is not None:\n                batch_size = self.batch_optimizer.get_optimal_batch_size()\n                logger.info(f\"Using batch size: {batch_size}\")\n            \n            # Train one epoch\n            train_metrics = self.train_epoch(\n                train_dataset,\n                batch_size=batch_size,\n                progress_callback=progress_callback\n            )\n            \n            # Log training metrics\n            logger.info(f\"Train Loss: {train_metrics['loss']:.4f} | Train Acc: {train_metrics['accuracy']:.2f}%\")\n            \n            # Update history\n            history['train_loss'].append(train_metrics['loss'])\n            history['train_acc'].append(train_metrics['accuracy'])\n            history['batch_sizes'].append(train_metrics.get('batch_size', 0))\n            history['epoch_times'].append(train_metrics.get('time', 0))\n            \n            # Evaluate on validation set if provided\n            if val_dataset is not None:\n                val_metrics = self.evaluate(val_dataset)\n                logger.info(f\"Val Loss: {val_metrics['loss']:.4f} | Val Acc: {val_metrics['accuracy']:.2f}%\")\n                \n                # Update history\n                history['val_loss'].append(val_metrics['loss'])\n                history['val_acc'].append(val_metrics['accuracy'])\n                \n                # Check for early stopping\n                if early_stopping:\n                    current_accuracy = val_metrics['accuracy']\n                    \n                    if current_accuracy > best_accuracy:\n                        best_accuracy = current_accuracy\n                        best_epoch = epoch\n                        no_improvement = 0\n                        logger.info(f\"New best accuracy: {best_accuracy:.2f}%\")\n                    else:\n                        no_improvement += 1\n                        logger.info(f\"No improvement for {no_improvement} epochs. Best: {best_accuracy:.2f}% at epoch {best_epoch+1}\")\n                        \n                        if no_improvement >= patience:\n                            logger.info(f\"Early stopping after {epoch+1} epochs\")\n                            break\n                \n                # Check for target accuracy\n                if target_accuracy is not None and val_metrics['accuracy'] >= target_accuracy:\n                    logger.info(f\"Target accuracy {target_accuracy:.2f}% reached after {epoch+1} epochs\")\n                    break\n        \n        logger.info(f\"Training completed after {len(history['train_loss'])} epochs\")\n        if val_dataset is not None:\n            logger.info(f\"Best validation accuracy: {best_accuracy:.2f}% at epoch {best_epoch+1}\")\n        \n        return history\n\n    def save_model(self, path):\n        \"\"\"\n        Save model to a file."
        },
        "save_model": {
          "start_line": 403,
          "end_line": 418,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "path"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "torch.save",
              "line": 410
            },
            {
              "name": "logger.info",
              "line": 416
            },
            {
              "name": "self.model.state_dict",
              "line": 411
            },
            {
              "name": "self.optimizer.state_dict",
              "line": 412
            },
            {
              "name": "self.scheduler.state_dict",
              "line": 413
            }
          ],
          "docstring": "\n        Save model to a file.\n        \n        Args:\n            path: File path to save the model\n        ",
          "code_snippet": "        return history\n\n    def save_model(self, path):\n        \"\"\"\n        Save model to a file.\n        \n        Args:\n            path: File path to save the model\n        \"\"\"\n        torch.save({\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,\n            'epoch': self.current_epoch\n        }, path)\n        logger.info(f\"Model saved to {path}\")\n    \n    def load_model(self, path):\n        \"\"\"\n        Load model from a file."
        },
        "load_model": {
          "start_line": 418,
          "end_line": 435,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "path"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "torch.load",
              "line": 425
            },
            {
              "name": "self.model.load_state_dict",
              "line": 426
            },
            {
              "name": "self.optimizer.load_state_dict",
              "line": 427
            },
            {
              "name": "checkpoint.get",
              "line": 432
            },
            {
              "name": "logger.info",
              "line": 433
            },
            {
              "name": "self.scheduler.load_state_dict",
              "line": 430
            }
          ],
          "docstring": "\n        Load model from a file.\n        \n        Args:\n            path: File path to load the model from\n        ",
          "code_snippet": "        logger.info(f\"Model saved to {path}\")\n    \n    def load_model(self, path):\n        \"\"\"\n        Load model from a file.\n        \n        Args:\n            path: File path to load the model from\n        \"\"\"\n        checkpoint = torch.load(path, map_location=self.device)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        \n        if self.scheduler and 'scheduler_state_dict' in checkpoint and checkpoint['scheduler_state_dict']:\n            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n            \n        self.current_epoch = checkpoint.get('epoch', 0)\n        logger.info(f\"Model loaded from {path} (epoch {self.current_epoch})\")"
        }
      },
      "class_variables": [],
      "bases": [],
      "docstring": "\n    Model trainer with IsekaiZen optimizer integration.\n    \n    This trainer handles the training loop with batch size optimization\n    and comprehensive progress tracking.\n    "
    }
  },
  "functions": {},
  "constants": {}
}