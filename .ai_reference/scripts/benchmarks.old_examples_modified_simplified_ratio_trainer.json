{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\benchmarks.old\\examples\\modified\\simplified_ratio_trainer.py",
  "imports": [
    {
      "name": "os",
      "line": 21
    },
    {
      "name": "sys",
      "line": 22
    },
    {
      "name": "time",
      "line": 23
    },
    {
      "name": "logging",
      "line": 24
    },
    {
      "name": "argparse",
      "line": 25
    },
    {
      "name": "json",
      "line": 26
    },
    {
      "name": "torch",
      "line": 27
    },
    {
      "name": "torch.nn",
      "line": 28
    },
    {
      "name": "torch.optim",
      "line": 29
    },
    {
      "name": "torchvision",
      "line": 30
    },
    {
      "name": "torchvision.transforms",
      "line": 31
    },
    {
      "name": "torchvision.models",
      "line": 32
    },
    {
      "name": "matplotlib.pyplot",
      "line": 33
    },
    {
      "name": "numpy",
      "line": 34
    },
    {
      "name": "datetime.datetime",
      "line": 35
    },
    {
      "name": "math",
      "line": 36
    },
    {
      "name": "isekaizen.trainer.adaptive_trainer.AdaptiveTrainer",
      "line": 42
    },
    {
      "name": "isekaizen.pattern.data_loading.load_latest_pattern_map",
      "line": 43
    },
    {
      "name": "isekaizen.core.optimizer.simplified_responsive.SimplifiedPatternResponsiveOptimizer",
      "line": 44
    },
    {
      "name": "isekaizen.optimizers.eve_simplified.EVESimplifiedRatio",
      "line": 45
    },
    {
      "name": "isekaizen.core.optimizer.simplified_ratio_tracker.SimplifiedRatioTracker",
      "line": 46
    },
    {
      "name": "optimizer_utils.configure_optimizer",
      "line": 50
    },
    {
      "name": "optimizer_utils.print_available_optimizers",
      "line": 50
    },
    {
      "name": "optimizer_utils.AVAILABLE_OPTIMIZERS",
      "line": 630
    },
    {
      "name": "isekaizen.pattern.detection.PatternRecognitionService",
      "line": 101
    },
    {
      "name": "traceback",
      "line": 759
    }
  ],
  "classes": {
    "SimplifiedRatioTrainer": {
      "start_line": 59,
      "end_line": 426,
      "methods": {
        "__init__": {
          "start_line": 64,
          "end_line": 114,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "model"
            },
            {
              "name": "criterion"
            },
            {
              "name": "optimizer_class"
            },
            {
              "name": "optimizer"
            },
            {
              "name": "optimizer_kwargs"
            },
            {
              "name": "scheduler_class"
            },
            {
              "name": "scheduler_kwargs"
            },
            {
              "name": "device"
            },
            {
              "name": "pattern_map"
            },
            {
              "name": "batch_optimizer_class"
            },
            {
              "name": "batch_optimizer_kwargs"
            },
            {
              "name": "val_dataset"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "....__init__",
              "line": 84
            },
            {
              "name": "SimplifiedRatioTracker",
              "line": 106
            },
            {
              "name": "PatternRecognitionService",
              "line": 102
            },
            {
              "name": "logger.info",
              "line": 103
            },
            {
              "name": "super",
              "line": 84
            }
          ],
          "docstring": "\n        Initialize the trainer with simplified ratio tracking.\n        ",
          "code_snippet": "    \"\"\"\n    \n    def __init__(\n        self,\n        model,\n        criterion,\n        optimizer_class,\n        optimizer=None,\n        optimizer_kwargs=None,\n        scheduler_class=None,\n        scheduler_kwargs=None,\n        device=None,\n        pattern_map=None,\n        batch_optimizer_class=None,\n        batch_optimizer_kwargs=None,\n        val_dataset=None,\n        **kwargs\n    ):\n        \"\"\"\n        Initialize the trainer with simplified ratio tracking.\n        \"\"\"\n        # Initialize base trainer\n        super().__init__(\n            model=model,\n            criterion=criterion,\n            optimizer_class=optimizer_class,\n            optimizer=optimizer,\n            optimizer_kwargs=optimizer_kwargs, \n            scheduler_class=scheduler_class,\n            scheduler_kwargs=scheduler_kwargs,\n            device=device,\n            pattern_map=pattern_map,\n            batch_optimizer_class=batch_optimizer_class,\n            batch_optimizer_kwargs=batch_optimizer_kwargs,\n            **kwargs\n        )\n        \n        # Initialize pattern service if pattern map available\n        if pattern_map:\n            from isekaizen.pattern.detection import PatternRecognitionService\n            self.pattern_service = PatternRecognitionService(pattern_map)\n            logger.info(\"PatternRecognitionService initialized with pattern map\")\n        \n        # Initialize simplified ratio tracker\n        self.ratio_tracker = SimplifiedRatioTracker()\n        \n        # Store validation dataset reference\n        self.val_dataset = val_dataset\n        \n        # Track the previous batch size for change detection\n        self.prev_batch_size = None\n    \n    def train(\n        self,\n        train_dataset,"
        },
        "train": {
          "start_line": 114,
          "end_line": 316,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "train_dataset"
            },
            {
              "name": "val_dataset"
            },
            {
              "name": "epochs"
            },
            {
              "name": "early_stopping"
            },
            {
              "name": "patience"
            },
            {
              "name": "test_interval"
            },
            {
              "name": "checkpoint_interval"
            },
            {
              "name": "checkpoint_path"
            },
            {
              "name": "callbacks"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "float",
              "line": 152
            },
            {
              "name": "....append",
              "line": 155
            },
            {
              "name": "range",
              "line": 160
            },
            {
              "name": "logger.info",
              "line": 313
            },
            {
              "name": "len",
              "line": 155
            },
            {
              "name": "logger.info",
              "line": 161
            },
            {
              "name": "self.batch_optimizer.get_optimal_batch_size",
              "line": 187
            },
            {
              "name": "....append",
              "line": 188
            },
            {
              "name": "....append",
              "line": 191
            },
            {
              "name": "logger.info",
              "line": 194
            },
            {
              "name": "logger.info",
              "line": 195
            },
            {
              "name": "time.time",
              "line": 198
            },
            {
              "name": "self._train_epoch",
              "line": 199
            },
            {
              "name": "....append",
              "line": 204
            },
            {
              "name": "logger.info",
              "line": 207
            },
            {
              "name": "logger.info",
              "line": 208
            },
            {
              "name": "logger.info",
              "line": 209
            },
            {
              "name": "....append",
              "line": 212
            },
            {
              "name": "....append",
              "line": 213
            },
            {
              "name": "....append",
              "line": 214
            },
            {
              "name": "torch.cuda.is_available",
              "line": 298
            },
            {
              "name": "hasattr",
              "line": 164
            },
            {
              "name": "self.batch_optimizer.should_adapt_patterns",
              "line": 164
            },
            {
              "name": "logger.info",
              "line": 165
            },
            {
              "name": "self.batch_optimizer.adapt_dataset",
              "line": 166
            },
            {
              "name": "adaptation_metrics.get",
              "line": 168
            },
            {
              "name": "len",
              "line": 191
            },
            {
              "name": "time.time",
              "line": 200
            },
            {
              "name": "torch.cuda.is_available",
              "line": 219
            },
            {
              "name": "self._validate",
              "line": 222
            },
            {
              "name": "....append",
              "line": 223
            },
            {
              "name": "....append",
              "line": 224
            },
            {
              "name": "self.ratio_tracker.update_train_test_ratio",
              "line": 227
            },
            {
              "name": "....append",
              "line": 228
            },
            {
              "name": "hasattr",
              "line": 232
            },
            {
              "name": "self.ratio_tracker.update_pattern_accuracies",
              "line": 236
            },
            {
              "name": "....append",
              "line": 237
            },
            {
              "name": "isinstance",
              "line": 240
            },
            {
              "name": "logger.info",
              "line": 254
            },
            {
              "name": "logger.info",
              "line": 255
            },
            {
              "name": "hasattr",
              "line": 281
            },
            {
              "name": "self.save_model",
              "line": 291
            },
            {
              "name": "self.scheduler.step",
              "line": 295
            },
            {
              "name": "torch.cuda.empty_cache",
              "line": 299
            },
            {
              "name": "logger.info",
              "line": 174
            },
            {
              "name": "logger.info",
              "line": 175
            },
            {
              "name": "logger.info",
              "line": 179
            },
            {
              "name": "logger.info",
              "line": 182
            },
            {
              "name": "torch.cuda.empty_cache",
              "line": 220
            },
            {
              "name": "self.optimizer.update_with_ratios",
              "line": 242
            },
            {
              "name": "decision.get",
              "line": 245
            },
            {
              "name": "len",
              "line": 258
            },
            {
              "name": "self.batch_optimizer.update_with_epoch_metrics",
              "line": 287
            },
            {
              "name": "callback",
              "line": 305
            },
            {
              "name": "logger.info",
              "line": 310
            },
            {
              "name": "logger.info",
              "line": 178
            },
            {
              "name": "len",
              "line": 195
            },
            {
              "name": "logger.info",
              "line": 247
            },
            {
              "name": "logger.info",
              "line": 263
            },
            {
              "name": "logger.info",
              "line": 268
            },
            {
              "name": "self.save_model",
              "line": 277
            },
            {
              "name": "logger.info",
              "line": 278
            },
            {
              "name": "self.optimizer.apply_decision",
              "line": 251
            },
            {
              "name": "logger.info",
              "line": 265
            },
            {
              "name": "logger.info",
              "line": 270
            },
            {
              "name": "adaptation_metrics.get",
              "line": 182
            },
            {
              "name": "....join",
              "line": 178
            },
            {
              "name": "abs",
              "line": 265
            },
            {
              "name": "abs",
              "line": 270
            },
            {
              "name": "examples.items",
              "line": 178
            }
          ],
          "docstring": "\n        Train the model with simplified ratio-based optimization.\n        \n        Args:\n            train_dataset: Training dataset\n            val_dataset: Validation dataset\n            epochs: Number of epochs\n            early_stopping: Whether to use early stopping\n            patience: Early stopping patience\n            test_interval: Interval for validation\n            checkpoint_interval: Interval for saving checkpoints\n            checkpoint_path: Path to save checkpoints\n            callbacks: List of callback functions\n            \n        Returns:\n            Training history\n        ",
          "code_snippet": "        self.prev_batch_size = None\n    \n    def train(\n        self,\n        train_dataset,\n        val_dataset=None,\n        epochs=10,\n        early_stopping=None,\n        patience=None,\n        test_interval=1,\n        checkpoint_interval=None,\n        checkpoint_path=None,\n        callbacks=None\n    ):\n        \"\"\"\n        Train the model with simplified ratio-based optimization.\n        \n        Args:\n            train_dataset: Training dataset\n            val_dataset: Validation dataset\n            epochs: Number of epochs\n            early_stopping: Whether to use early stopping\n            patience: Early stopping patience\n            test_interval: Interval for validation\n            checkpoint_interval: Interval for saving checkpoints\n            checkpoint_path: Path to save checkpoints\n            callbacks: List of callback functions\n            \n        Returns:\n            Training history\n        \"\"\"\n        # Initialize variables\n        history = {\n            'train_loss': [], 'train_acc': [],\n            'val_loss': [], 'val_acc': [],\n            'batch_sizes': [], 'epoch_times': [],\n            'dataset_sizes': [], 'learning_rates': [],\n            'tt_ratios': [], 'ra_ratios': []\n        }\n        \n        best_val_loss = float('inf')\n        \n        # Initial dataset size\n        history['dataset_sizes'].append(len(train_dataset))\n        \n        # Store reference to train dataset for potential adaptation\n        self.train_dataset = train_dataset\n        \n        for epoch in range(epochs):\n            logger.info(f\"Epoch {epoch+1}/{epochs}\")\n            \n            # Check if we should adapt the dataset\n            if hasattr(self.batch_optimizer, 'should_adapt_patterns') and self.batch_optimizer.should_adapt_patterns():\n                logger.info(\"Adapting dataset based on pattern performance...\")\n                adapted_dataset, adaptation_metrics = self.batch_optimizer.adapt_dataset(self.train_dataset)\n                \n                if adaptation_metrics.get('adapted', False):\n                    # Use the adapted dataset for this epoch\n                    current_dataset = adapted_dataset\n                    # Update the reference\n                    self.train_dataset = adapted_dataset\n                    \n                    logger.info(f\"Dataset adapted successfully:\")\n                    logger.info(f\"  Added {adaptation_metrics['examples_added']} new examples\")\n                    if 'examples_per_pattern' in adaptation_metrics:\n                        examples = adaptation_metrics['examples_per_pattern']\n                        logger.info(f\"  Examples per pattern: {', '.join([f'{k}:{v}' for k, v in examples.items()])}\")\n                    logger.info(f\"  New dataset size: {adaptation_metrics['total_size']} examples\")\n                else:\n                    current_dataset = self.train_dataset\n                    logger.info(f\"Dataset adaptation skipped: {adaptation_metrics.get('reason', 'unknown')}\")\n            else:\n                current_dataset = self.train_dataset\n            \n            # Get optimal batch size\n            batch_size = self.batch_optimizer.get_optimal_batch_size()\n            history['batch_sizes'].append(batch_size)\n            \n            # Track dataset size\n            history['dataset_sizes'].append(len(current_dataset))\n            \n            # Log epoch start information\n            logger.info(f\"Starting epoch {epoch+1}/{epochs} with batch size {batch_size}\")\n            logger.info(f\"Dataset size: {len(current_dataset)} examples\")\n            \n            # Train for one epoch\n            start_time = time.time()\n            train_metrics = self._train_epoch(current_dataset, batch_size)\n            epoch_time = time.time() - start_time\n            \n            # Store current learning rate\n            current_lr = self.optimizer.param_groups[0]['lr']\n            history['learning_rates'].append(current_lr)\n            \n            # Log epoch results\n            logger.info(f\"Epoch {epoch+1}/{epochs} completed in {epoch_time:.2f} seconds\")\n            logger.info(f\"Training - Loss: {train_metrics['loss']:.4f}, Accuracy: {train_metrics['accuracy']:.2f}%\")\n            logger.info(f\"Current learning rate: {current_lr:.6f}\")\n            \n            # Store metrics in history\n            history['train_loss'].append(train_metrics['loss'])\n            history['train_acc'].append(train_metrics['accuracy'])\n            history['epoch_times'].append(epoch_time)\n            \n            # Validate if a validation set is provided\n            if val_dataset is not None and (epoch + 1) % test_interval == 0:\n                # Clean up memory before validation\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n                    \n                val_metrics = self._validate(val_dataset)\n                history['val_loss'].append(val_metrics['loss'])\n                history['val_acc'].append(val_metrics['accuracy'])\n                \n                # Calculate and store ratios\n                tt_ratio = self.ratio_tracker.update_train_test_ratio(epoch, train_metrics['accuracy'], val_metrics['accuracy'])\n                history['tt_ratios'].append(tt_ratio)\n                \n                # Extract pattern accuracies from batch optimizer if available\n                pattern_accuracies = {}\n                if hasattr(self.batch_optimizer, 'pattern_accuracies'):\n                    pattern_accuracies = self.batch_optimizer.pattern_accuracies\n                \n                # Update pattern accuracies and get Risk/Accuracy ratio\n                ra_ratio = self.ratio_tracker.update_pattern_accuracies(epoch, pattern_accuracies)\n                history['ra_ratios'].append(ra_ratio)\n                \n                # Update the EVE optimizer with ratios if it's our simplified version\n                if isinstance(self.optimizer, EVESimplifiedRatio):\n                    # Make and apply decision based on ratios\n                    decision = self.optimizer.update_with_ratios(tt_ratio, ra_ratio)\n                    \n                    # Log the decision\n                    action = decision.get('action', 'none')\n                    if action != 'none':\n                        logger.info(f\"Ratio-based decision: {action}\")\n                        \n                        # Apply any immediate optimizer changes\n                        if action in ['adjust_lr', 'adjust_both']:\n                            self.optimizer.apply_decision()\n                \n                # Log validation results\n                logger.info(f\"Validation - Loss: {val_metrics['loss']:.4f}, Accuracy: {val_metrics['accuracy']:.2f}%\")\n                logger.info(f\"Ratios - Train/Test: {tt_ratio:.3f}, Risk/Accuracy: {ra_ratio:.3f}\")\n                \n                # Display improvement/decline compared to previous validation\n                if len(history['val_acc']) > 1:\n                    acc_diff = val_metrics['accuracy'] - history['val_acc'][-2]\n                    loss_diff = history['val_loss'][-2] - val_metrics['loss']  # Loss should decrease\n                    \n                    if acc_diff > 0:\n                        logger.info(f\"Validation accuracy improved by {acc_diff:.2f}%\")\n                    elif acc_diff < 0:\n                        logger.info(f\"Validation accuracy decreased by {abs(acc_diff):.2f}%\")\n                    \n                    if loss_diff > 0:\n                        logger.info(f\"Validation loss improved by {loss_diff:.4f}\")\n                    elif loss_diff < 0:\n                        logger.info(f\"Validation loss worsened by {abs(loss_diff):.4f}\")\n                \n                # Save best model when validation improves\n                if val_metrics['loss'] < best_val_loss:\n                    best_val_loss = val_metrics['loss']\n                    # Save best model\n                    if checkpoint_path:\n                        self.save_model(f\"{checkpoint_path}_best.pth\")\n                        logger.info(f\"New best model saved at epoch {epoch+1}\")\n                \n                # Update optimizer with validation results\n                if hasattr(self.batch_optimizer, 'update_with_epoch_metrics'):\n                    # Add validation metrics to train metrics\n                    train_metrics['val_accuracy'] = val_metrics['accuracy']\n                    train_metrics['val_loss'] = val_metrics['loss']\n                    \n                    # Update batch optimizer\n                    self.batch_optimizer.update_with_epoch_metrics(train_metrics)\n            \n            # Save checkpoint if interval is specified\n            if checkpoint_interval and (epoch + 1) % checkpoint_interval == 0 and checkpoint_path:\n                self.save_model(f\"{checkpoint_path}_epoch{epoch+1}.pth\")\n            \n            # Step the scheduler if it exists\n            if self.scheduler:\n                self.scheduler.step()\n            \n            # Clean up memory after epoch\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n            \n            # Execute callbacks if provided\n            if callbacks:\n                stop_training = False\n                for callback in callbacks:\n                    result = callback(epoch, history, self.model, self.optimizer)\n                    if result:  # If callback returns True, stop training\n                        stop_training = True\n                        \n                if stop_training:\n                    logger.info(f\"Training stopped by callback after epoch {epoch+1}\")\n                    break\n        \n        logger.info(\"Training complete\")\n        return history\n        \n    def _train_epoch(self, dataset, batch_size):\n        \"\"\"\n        Train for one epoch."
        },
        "_train_epoch": {
          "start_line": 316,
          "end_line": 385,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "dataset"
            },
            {
              "name": "batch_size"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.model.train",
              "line": 327
            },
            {
              "name": "torch.utils.data.DataLoader",
              "line": 333
            },
            {
              "name": "enumerate",
              "line": 336
            },
            {
              "name": "list",
              "line": 341
            },
            {
              "name": "self.optimizer.zero_grad",
              "line": 344
            },
            {
              "name": "self.model",
              "line": 347
            },
            {
              "name": "self.criterion",
              "line": 348
            },
            {
              "name": "outputs.max",
              "line": 351
            },
            {
              "name": "predicted.eq",
              "line": 352
            },
            {
              "name": "hasattr",
              "line": 355
            },
            {
              "name": "loss.item",
              "line": 359
            },
            {
              "name": "....item",
              "line": 360
            },
            {
              "name": "targets.size",
              "line": 362
            },
            {
              "name": "loss.backward",
              "line": 365
            },
            {
              "name": "hasattr",
              "line": 368
            },
            {
              "name": "len",
              "line": 380
            },
            {
              "name": "inputs.to",
              "line": 337
            },
            {
              "name": "targets.to",
              "line": 337
            },
            {
              "name": "range",
              "line": 341
            },
            {
              "name": "self.batch_optimizer.update_with_pattern_recognition",
              "line": 356
            },
            {
              "name": "self.optimizer.step",
              "line": 377
            },
            {
              "name": "min",
              "line": 341
            },
            {
              "name": "....sum",
              "line": 360
            },
            {
              "name": "hasattr",
              "line": 370
            },
            {
              "name": "hasattr",
              "line": 370
            },
            {
              "name": "self.pattern_service.get_batch_pattern_states",
              "line": 371
            },
            {
              "name": "self.optimizer.step",
              "line": 372
            },
            {
              "name": "self.optimizer.step",
              "line": 374
            },
            {
              "name": "len",
              "line": 341
            },
            {
              "name": "predicted.eq",
              "line": 360
            }
          ],
          "docstring": "\n        Train for one epoch.\n        \n        Args:\n            dataset: Dataset to train on\n            batch_size: Batch size\n            \n        Returns:\n            Dictionary with training metrics\n        ",
          "code_snippet": "        return history\n        \n    def _train_epoch(self, dataset, batch_size):\n        \"\"\"\n        Train for one epoch.\n        \n        Args:\n            dataset: Dataset to train on\n            batch_size: Batch size\n            \n        Returns:\n            Dictionary with training metrics\n        \"\"\"\n        self.model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        # Create data loader\n        dataloader = torch.utils.data.DataLoader(\n            dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        \n        for i, (inputs, targets) in enumerate(dataloader):\n            inputs, targets = inputs.to(self.device), targets.to(self.device)\n            \n            # Get batch indices for pattern tracking\n            batch_start = i * batch_size\n            batch_indices = list(range(batch_start, min(batch_start + batch_size, len(dataset))))\n            \n            # Zero the parameter gradients\n            self.optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = self.model(inputs)\n            loss = self.criterion(outputs, targets)\n            \n            # Calculate per-example correctness for pattern recognition\n            _, predicted = outputs.max(1)\n            correct_mask = predicted.eq(targets)\n            \n            # Update pattern recognition tracking\n            if hasattr(self.batch_optimizer, 'update_with_pattern_recognition'):\n                self.batch_optimizer.update_with_pattern_recognition(batch_indices, correct_mask)\n            \n            # Update metrics\n            running_loss += loss.item()\n            batch_correct = predicted.eq(targets).sum().item()\n            correct += batch_correct\n            total += targets.size(0)\n            \n            # Backward pass and optimize\n            loss.backward()\n            \n            # Optimization step\n            if hasattr(self.optimizer, 'step'):\n                # Get pattern states if needed (for EVE optimizers)\n                if hasattr(self, 'pattern_service') and hasattr(self.optimizer, 'step') and 'pattern_states' in self.optimizer.step.__code__.co_varnames:\n                    pattern_states = self.pattern_service.get_batch_pattern_states(batch_indices)\n                    self.optimizer.step(pattern_states=pattern_states)\n                else:\n                    self.optimizer.step()\n            else:\n                # Fallback to standard optimizer step\n                self.optimizer.step()\n        \n        # Calculate epoch metrics\n        epoch_loss = running_loss / len(dataloader)\n        epoch_acc = 100. * correct / total\n        \n        return {'loss': epoch_loss, 'accuracy': epoch_acc}\n    \n    def _validate(self, dataset, batch_size=128):\n        \"\"\"\n        Validate the model on a dataset."
        },
        "_validate": {
          "start_line": 385,
          "end_line": 426,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "dataset"
            },
            {
              "name": "batch_size"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.model.eval",
              "line": 396
            },
            {
              "name": "torch.utils.data.DataLoader",
              "line": 402
            },
            {
              "name": "torch.no_grad",
              "line": 406
            },
            {
              "name": "len",
              "line": 421
            },
            {
              "name": "self.model",
              "line": 411
            },
            {
              "name": "self.criterion",
              "line": 412
            },
            {
              "name": "loss.item",
              "line": 415
            },
            {
              "name": "outputs.max",
              "line": 416
            },
            {
              "name": "....item",
              "line": 417
            },
            {
              "name": "targets.size",
              "line": 418
            },
            {
              "name": "inputs.to",
              "line": 408
            },
            {
              "name": "targets.to",
              "line": 408
            },
            {
              "name": "....sum",
              "line": 417
            },
            {
              "name": "predicted.eq",
              "line": 417
            }
          ],
          "docstring": "\n        Validate the model on a dataset.\n        \n        Args:\n            dataset: Validation dataset\n            batch_size: Batch size\n            \n        Returns:\n            Dictionary with validation metrics\n        ",
          "code_snippet": "        return {'loss': epoch_loss, 'accuracy': epoch_acc}\n    \n    def _validate(self, dataset, batch_size=128):\n        \"\"\"\n        Validate the model on a dataset.\n        \n        Args:\n            dataset: Validation dataset\n            batch_size: Batch size\n            \n        Returns:\n            Dictionary with validation metrics\n        \"\"\"\n        self.model.eval()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        # Create data loader\n        dataloader = torch.utils.data.DataLoader(\n            dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n        \n        # Disable gradient calculation\n        with torch.no_grad():\n            for inputs, targets in dataloader:\n                inputs, targets = inputs.to(self.device), targets.to(self.device)\n                \n                # Forward pass\n                outputs = self.model(inputs)\n                loss = self.criterion(outputs, targets)\n                \n                # Update metrics\n                running_loss += loss.item()\n                _, predicted = outputs.max(1)\n                correct += predicted.eq(targets).sum().item()\n                total += targets.size(0)\n        \n        # Calculate metrics\n        val_loss = running_loss / len(dataloader)\n        val_acc = 100. * correct / total\n        \n        return {'loss': val_loss, 'accuracy': val_acc}\n\ndef load_cifar10_data():\n    \"\"\"\n    Load CIFAR-10 dataset."
        }
      },
      "class_variables": [],
      "bases": [
        "AdaptiveTrainer"
      ],
      "docstring": "\n    Trainer that uses simplified ratio-based decision making for efficient training.\n    "
    }
  },
  "functions": {
    "load_cifar10_data": {
      "start_line": 426,
      "end_line": 452,
      "parameters": [],
      "return_type": null,
      "calls": [
        {
          "name": "transforms.Compose",
          "line": 433
        },
        {
          "name": "transforms.Compose",
          "line": 440
        },
        {
          "name": "torchvision.datasets.CIFAR10",
          "line": 445
        },
        {
          "name": "torchvision.datasets.CIFAR10",
          "line": 447
        },
        {
          "name": "transforms.RandomCrop",
          "line": 434
        },
        {
          "name": "transforms.RandomHorizontalFlip",
          "line": 435
        },
        {
          "name": "transforms.ToTensor",
          "line": 436
        },
        {
          "name": "transforms.Normalize",
          "line": 437
        },
        {
          "name": "transforms.ToTensor",
          "line": 441
        },
        {
          "name": "transforms.Normalize",
          "line": 442
        }
      ],
      "docstring": "\n    Load CIFAR-10 dataset.\n    \n    Returns:\n        Tuple of (train_dataset, test_dataset)\n    ",
      "code_snippet": "        return {'loss': val_loss, 'accuracy': val_acc}\n\ndef load_cifar10_data():\n    \"\"\"\n    Load CIFAR-10 dataset.\n    \n    Returns:\n        Tuple of (train_dataset, test_dataset)\n    \"\"\"\n    transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n    \n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n    \n    trainset = torchvision.datasets.CIFAR10(\n        root='./data', train=True, download=True, transform=transform_train)\n    testset = torchvision.datasets.CIFAR10(\n        root='./data', train=False, download=True, transform=transform_test)\n    \n    return trainset, testset\n\ndef create_model(use_pretrained=False):\n    \"\"\"\n    Create a ResNet-18 model for CIFAR-10, optionally with pre-trained weights."
    },
    "create_model": {
      "start_line": 452,
      "end_line": 489,
      "parameters": [
        {
          "name": "use_pretrained"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "models.resnet18",
          "line": 463
        },
        {
          "name": "nn.Conv2d",
          "line": 472
        },
        {
          "name": "nn.Identity",
          "line": 481
        },
        {
          "name": "nn.Linear",
          "line": 485
        },
        {
          "name": "model.conv1.weight.clone",
          "line": 467
        },
        {
          "name": "logger.info",
          "line": 468
        },
        {
          "name": "torch.no_grad",
          "line": 476
        },
        {
          "name": "model.conv1.weight.copy_",
          "line": 478
        },
        {
          "name": "logger.info",
          "line": 479
        }
      ],
      "docstring": "\n    Create a ResNet-18 model for CIFAR-10, optionally with pre-trained weights.\n    \n    Args:\n        use_pretrained: Whether to use pre-trained weights from ImageNet\n    \n    Returns:\n        A ResNet-18 model adapted for CIFAR-10\n    ",
      "code_snippet": "    return trainset, testset\n\ndef create_model(use_pretrained=False):\n    \"\"\"\n    Create a ResNet-18 model for CIFAR-10, optionally with pre-trained weights.\n    \n    Args:\n        use_pretrained: Whether to use pre-trained weights from ImageNet\n    \n    Returns:\n        A ResNet-18 model adapted for CIFAR-10\n    \"\"\"\n    # Load model with or without pre-trained weights\n    model = models.resnet18(pretrained=use_pretrained)\n    \n    # Save the pre-trained conv1 weights if using pre-trained model\n    if use_pretrained:\n        pretrained_conv1_weight = model.conv1.weight.clone()\n        logger.info(\"Using pre-trained ResNet-18 with weights from ImageNet\")\n    \n    # Modify for CIFAR-10 (32x32 images)\n    original_conv1 = model.conv1\n    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n    \n    # Adapt the pre-trained weights to the new conv layer if using pre-trained model\n    if use_pretrained:\n        with torch.no_grad():\n            # Take the center 3x3 section of the 7x7 kernels\n            model.conv1.weight.copy_(pretrained_conv1_weight[:, :, 2:5, 2:5])\n            logger.info(\"Adapted pre-trained conv1 weights for CIFAR-10\")\n    \n    model.maxpool = nn.Identity()  # Remove pooling for smaller images\n    \n    # Adjust final layer for 10 classes\n    num_features = model.fc.in_features\n    model.fc = nn.Linear(num_features, 10)\n    \n    return model\n\ndef visualize_training_results(history, output_path=None):\n    \"\"\"\n    Visualize training results with ratio information."
    },
    "visualize_training_results": {
      "start_line": 489,
      "end_line": 566,
      "parameters": [
        {
          "name": "history"
        },
        {
          "name": "output_path"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "plt.figure",
          "line": 497
        },
        {
          "name": "plt.subplot",
          "line": 500
        },
        {
          "name": "plt.plot",
          "line": 501
        },
        {
          "name": "plt.title",
          "line": 504
        },
        {
          "name": "plt.xlabel",
          "line": 505
        },
        {
          "name": "plt.ylabel",
          "line": 506
        },
        {
          "name": "plt.legend",
          "line": 507
        },
        {
          "name": "plt.grid",
          "line": 508
        },
        {
          "name": "plt.subplot",
          "line": 511
        },
        {
          "name": "plt.plot",
          "line": 512
        },
        {
          "name": "plt.title",
          "line": 515
        },
        {
          "name": "plt.xlabel",
          "line": 516
        },
        {
          "name": "plt.ylabel",
          "line": 517
        },
        {
          "name": "plt.legend",
          "line": 518
        },
        {
          "name": "plt.grid",
          "line": 519
        },
        {
          "name": "plt.subplot",
          "line": 522
        },
        {
          "name": "plt.gca",
          "line": 523
        },
        {
          "name": "ax1.set_xlabel",
          "line": 524
        },
        {
          "name": "ax1.set_ylabel",
          "line": 525
        },
        {
          "name": "ax1.plot",
          "line": 526
        },
        {
          "name": "ax1.tick_params",
          "line": 527
        },
        {
          "name": "ax1.twinx",
          "line": 529
        },
        {
          "name": "ax2.set_ylabel",
          "line": 530
        },
        {
          "name": "ax2.tick_params",
          "line": 533
        },
        {
          "name": "plt.title",
          "line": 535
        },
        {
          "name": "ax1.get_legend_handles_labels",
          "line": 537
        },
        {
          "name": "ax2.get_legend_handles_labels",
          "line": 538
        },
        {
          "name": "ax1.legend",
          "line": 539
        },
        {
          "name": "plt.subplot",
          "line": 542
        },
        {
          "name": "plt.tight_layout",
          "line": 557
        },
        {
          "name": "plt.close",
          "line": 564
        },
        {
          "name": "plt.plot",
          "line": 503
        },
        {
          "name": "plt.plot",
          "line": 514
        },
        {
          "name": "ax2.plot",
          "line": 532
        },
        {
          "name": "plt.plot",
          "line": 544
        },
        {
          "name": "plt.plot",
          "line": 545
        },
        {
          "name": "plt.axhline",
          "line": 546
        },
        {
          "name": "plt.title",
          "line": 547
        },
        {
          "name": "plt.xlabel",
          "line": 548
        },
        {
          "name": "plt.ylabel",
          "line": 549
        },
        {
          "name": "plt.legend",
          "line": 550
        },
        {
          "name": "plt.grid",
          "line": 551
        },
        {
          "name": "plt.text",
          "line": 553
        },
        {
          "name": "plt.title",
          "line": 555
        },
        {
          "name": "plt.savefig",
          "line": 561
        },
        {
          "name": "logger.info",
          "line": 562
        }
      ],
      "docstring": "\n    Visualize training results with ratio information.\n    \n    Args:\n        history: Training history dictionary\n        output_path: Path to save the visualization\n    ",
      "code_snippet": "    return model\n\ndef visualize_training_results(history, output_path=None):\n    \"\"\"\n    Visualize training results with ratio information.\n    \n    Args:\n        history: Training history dictionary\n        output_path: Path to save the visualization\n    \"\"\"\n    plt.figure(figsize=(15, 10))\n    \n    # 1. Training and validation accuracy\n    plt.subplot(2, 2, 1)\n    plt.plot(history['train_acc'], label='Train')\n    if 'val_acc' in history:\n        plt.plot(history['val_acc'], label='Validation')\n    plt.title('Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy (%)')\n    plt.legend()\n    plt.grid(True)\n    \n    # 2. Training and validation loss\n    plt.subplot(2, 2, 2)\n    plt.plot(history['train_loss'], label='Train')\n    if 'val_loss' in history:\n        plt.plot(history['val_loss'], label='Validation')\n    plt.title('Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n    \n    # 3. Batch size and learning rate\n    plt.subplot(2, 2, 3)\n    ax1 = plt.gca()\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Batch Size', color='b')\n    ax1.plot(history['batch_sizes'], 'b-', label='Batch Size')\n    ax1.tick_params(axis='y', labelcolor='b')\n    \n    ax2 = ax1.twinx()\n    ax2.set_ylabel('Learning Rate', color='r')\n    if 'learning_rates' in history:\n        ax2.plot(history['learning_rates'], 'r-', label='Learning Rate')\n    ax2.tick_params(axis='y', labelcolor='r')\n    \n    plt.title('Batch Size and Learning Rate')\n    \n    lines1, labels1 = ax1.get_legend_handles_labels()\n    lines2, labels2 = ax2.get_legend_handles_labels()\n    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n    \n    # 4. Train/Test and Risk/Accuracy ratios\n    plt.subplot(2, 2, 4)\n    if 'tt_ratios' in history and 'ra_ratios' in history:\n        plt.plot(history['tt_ratios'], 'b-', label='Train/Test Ratio')\n        plt.plot(history['ra_ratios'], 'r-', label='Risk/Accuracy Ratio')\n        plt.axhline(y=1.0, color='k', linestyle='--', alpha=0.5, label='Target')\n        plt.title('Training Ratios')\n        plt.xlabel('Epoch')\n        plt.ylabel('Ratio')\n        plt.legend()\n        plt.grid(True)\n    else:\n        plt.text(0.5, 0.5, 'No ratio data available',\n                horizontalalignment='center', verticalalignment='center')\n        plt.title('Training Ratios')\n    \n    plt.tight_layout()\n    \n    # Save figure if output path provided\n    if output_path:\n        plt.savefig(output_path)\n        logger.info(f\"Training results visualization saved to: {output_path}\")\n    \n    plt.close()\n\ndef track_learning_rate_callback(epoch, history, model, optimizer):\n    \"\"\"Callback to track learning rate changes.\"\"\"\n    if 'learning_rates' not in history:"
    },
    "track_learning_rate_callback": {
      "start_line": 566,
      "end_line": 580,
      "parameters": [
        {
          "name": "epoch"
        },
        {
          "name": "history"
        },
        {
          "name": "model"
        },
        {
          "name": "optimizer"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "len",
          "line": 575
        },
        {
          "name": "....append",
          "line": 576
        }
      ],
      "docstring": "Callback to track learning rate changes.",
      "code_snippet": "    plt.close()\n\ndef track_learning_rate_callback(epoch, history, model, optimizer):\n    \"\"\"Callback to track learning rate changes.\"\"\"\n    if 'learning_rates' not in history:\n        history['learning_rates'] = []\n    \n    # Get current learning rate\n    current_lr = optimizer.param_groups[0]['lr']\n    \n    # Store in history\n    if len(history['learning_rates']) <= epoch:\n        history['learning_rates'].append(current_lr)\n    \n    return False  # Continue training\n\ndef main():\n    \"\"\"\n    Run simplified ratio-based training."
    },
    "main": {
      "start_line": 580,
      "end_line": 752,
      "parameters": [],
      "return_type": null,
      "calls": [
        {
          "name": "argparse.ArgumentParser",
          "line": 585
        },
        {
          "name": "parser.add_argument",
          "line": 589
        },
        {
          "name": "parser.add_argument",
          "line": 590
        },
        {
          "name": "parser.add_argument",
          "line": 592
        },
        {
          "name": "parser.add_argument",
          "line": 595
        },
        {
          "name": "parser.add_argument",
          "line": 597
        },
        {
          "name": "parser.parse_args",
          "line": 599
        },
        {
          "name": "torch.device",
          "line": 614
        },
        {
          "name": "logger.info",
          "line": 615
        },
        {
          "name": "logger.info",
          "line": 618
        },
        {
          "name": "load_cifar10_data",
          "line": 619
        },
        {
          "name": "logger.info",
          "line": 620
        },
        {
          "name": "logger.info",
          "line": 623
        },
        {
          "name": "create_model",
          "line": 624
        },
        {
          "name": "model.to",
          "line": 625
        },
        {
          "name": "logger.info",
          "line": 646
        },
        {
          "name": "configure_optimizer",
          "line": 647
        },
        {
          "name": "SimplifiedRatioTrainer",
          "line": 667
        },
        {
          "name": "os.path.join",
          "line": 681
        },
        {
          "name": "os.makedirs",
          "line": 682
        },
        {
          "name": "os.path.join",
          "line": 685
        },
        {
          "name": "logger.info",
          "line": 688
        },
        {
          "name": "logger.info",
          "line": 689
        },
        {
          "name": "logger.info",
          "line": 690
        },
        {
          "name": "logger.info",
          "line": 691
        },
        {
          "name": "logger.info",
          "line": 692
        },
        {
          "name": "logger.info",
          "line": 693
        },
        {
          "name": "logger.info",
          "line": 694
        },
        {
          "name": "logger.info",
          "line": 695
        },
        {
          "name": "torch.cuda.is_available",
          "line": 698
        },
        {
          "name": "trainer.train",
          "line": 702
        },
        {
          "name": "os.path.join",
          "line": 711
        },
        {
          "name": "trainer.save_model",
          "line": 712
        },
        {
          "name": "logger.info",
          "line": 713
        },
        {
          "name": "....strftime",
          "line": 716
        },
        {
          "name": "os.path.join",
          "line": 717
        },
        {
          "name": "visualize_training_results",
          "line": 718
        },
        {
          "name": "os.path.join",
          "line": 721
        },
        {
          "name": "logger.info",
          "line": 730
        },
        {
          "name": "logger.info",
          "line": 733
        },
        {
          "name": "logger.info",
          "line": 734
        },
        {
          "name": "logger.info",
          "line": 735
        },
        {
          "name": "logger.info",
          "line": 736
        },
        {
          "name": "logger.info",
          "line": 746
        },
        {
          "name": "logger.info",
          "line": 747
        },
        {
          "name": "logger.info",
          "line": 748
        },
        {
          "name": "logger.info",
          "line": 604
        },
        {
          "name": "load_latest_pattern_map",
          "line": 605
        },
        {
          "name": "torch.cuda.empty_cache",
          "line": 699
        },
        {
          "name": "history.get",
          "line": 723
        },
        {
          "name": "history.get",
          "line": 724
        },
        {
          "name": "open",
          "line": 728
        },
        {
          "name": "json.dump",
          "line": 729
        },
        {
          "name": "logger.info",
          "line": 738
        },
        {
          "name": "logger.info",
          "line": 742
        },
        {
          "name": "logger.info",
          "line": 744
        },
        {
          "name": "logger.info",
          "line": 609
        },
        {
          "name": "logger.warning",
          "line": 611
        },
        {
          "name": "torch.cuda.is_available",
          "line": 614
        },
        {
          "name": "nn.CrossEntropyLoss",
          "line": 669
        },
        {
          "name": "datetime.now",
          "line": 716
        },
        {
          "name": "history.get",
          "line": 725
        },
        {
          "name": "len",
          "line": 620
        },
        {
          "name": "len",
          "line": 620
        },
        {
          "name": "len",
          "line": 609
        },
        {
          "name": "pattern_map.keys",
          "line": 609
        }
      ],
      "docstring": "\n    Run simplified ratio-based training.\n    ",
      "code_snippet": "    return False  # Continue training\n\ndef main():\n    \"\"\"\n    Run simplified ratio-based training.\n    \"\"\"\n    # Parse command line arguments\n    parser = argparse.ArgumentParser(\n        description=\"Run simplified ratio-based training\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    parser.add_argument(\"--epochs\", type=int, default=15, help=\"Number of epochs for training\")\n    parser.add_argument(\"--target-accuracy\", type=float, default=None, \n                      help=\"Target accuracy to stop training\")\n    parser.add_argument(\"--optimizer\", type=str, default=\"eve_simplified\", \n                      choices=[\"sgd\", \"adam\", \"rmsprop\", \"eve\", \"eve_simplified\"],\n                      help=\"Optimizer to use\")\n    parser.add_argument(\"--pretrained\", action=\"store_true\",\n                      help=\"Use pre-trained ResNet-18 model with ImageNet weights\")\n    parser.add_argument(\"--skip-pattern-map\", action=\"store_true\",\n                      help=\"Skip pattern map loading\")\n    args = parser.parse_args()\n    \n    # Load pattern map (unless explicitly skipped)\n    pattern_map = None\n    if not args.skip_pattern_map:\n        logger.info(\"Loading pattern map...\")\n        pattern_map = load_latest_pattern_map()\n        \n        # Check if pattern map was found\n        if pattern_map:\n            logger.info(f\"Pattern map loaded with {len(pattern_map.keys())} top-level keys\")\n        else:\n            logger.warning(\"No pattern map found - using simplified pattern tracking\")\n    \n    # Set device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    logger.info(f\"Using device: {device}\")\n    \n    # Load dataset\n    logger.info(\"Loading CIFAR-10 dataset...\")\n    trainset, testset = load_cifar10_data()\n    logger.info(f\"Dataset loaded with {len(trainset)} training and {len(testset)} test samples\")\n    \n    # Create model\n    logger.info(\"Creating model...\")\n    model = create_model(use_pretrained=args.pretrained)\n    model = model.to(device)\n    \n    # Register the EVE simplified optimizer if needed\n    if args.optimizer == \"eve_simplified\":\n        # Add simplified EVE to available optimizers\n        from optimizer_utils import AVAILABLE_OPTIMIZERS\n        AVAILABLE_OPTIMIZERS['eve_simplified'] = {\n            'optimizer_class': EVESimplifiedRatio,\n            'optimizer_kwargs': {\n                'lr': 0.01,\n                'eps': 1e-8,\n                'base_confidence_threshold': 0.7,\n                'weight_decay': 0.0001\n            },\n            'scheduler_class': optim.lr_scheduler.CosineAnnealingLR,\n            'scheduler_kwargs': {\n                'T_max': 200\n            }\n        }\n    \n    # Configure optimizer\n    logger.info(f\"Configuring optimizer: {args.optimizer}\")\n    optimizer, scheduler = configure_optimizer(\n        model, \n        optimizer_type=args.optimizer,\n        custom_params={\n            \"pattern_map\": pattern_map\n        }\n    )\n    \n    # Create batch optimizer\n    batch_optimizer_class = SimplifiedPatternResponsiveOptimizer\n    batch_optimizer_kwargs = {\n        \"model\": model,\n        \"pattern_map\": pattern_map,\n        \"min_batch\": 32,\n        \"max_batch\": 256,\n        \"device\": device,\n        \"total_epochs\": args.epochs\n    }\n    \n    # Create trainer\n    trainer = SimplifiedRatioTrainer(\n        model=model,\n        criterion=nn.CrossEntropyLoss(),\n        optimizer_class=optimizer.__class__,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        device=device,\n        pattern_map=pattern_map,\n        batch_optimizer_class=batch_optimizer_class,\n        batch_optimizer_kwargs=batch_optimizer_kwargs,\n        val_dataset=testset\n    )\n    \n    # Create output directory\n    output_dir = os.path.join(\"examples\", \"output\")\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Define checkpoint path\n    checkpoint_path = os.path.join(output_dir, \"simplified_ratio_model\")\n    \n    # Train the model\n    logger.info(\"====================================================\")\n    logger.info(f\"STARTING SIMPLIFIED RATIO-BASED TRAINING\")\n    logger.info(\"====================================================\")\n    logger.info(f\"Epochs: {args.epochs}\")\n    logger.info(f\"Target accuracy: {args.target_accuracy if args.target_accuracy else 'None'}\")\n    logger.info(f\"Optimizer: {args.optimizer}\")\n    logger.info(f\"Model: ResNet-18 for CIFAR-10 ({('pre-trained with ImageNet weights' if args.pretrained else 'from scratch')})\")\n    logger.info(\"====================================================\")\n    \n    # Clean up any residual memory before starting\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    # Train the model\n    history = trainer.train(\n        train_dataset=trainset,\n        val_dataset=testset,\n        epochs=args.epochs,\n        callbacks=[track_learning_rate_callback],\n        checkpoint_path=checkpoint_path\n    )\n    \n    # Save final model\n    final_model_path = os.path.join(output_dir, \"simplified_ratio_model_final.pth\")\n    trainer.save_model(final_model_path)\n    logger.info(f\"Final model saved to {final_model_path}\")\n    \n    # Visualize training results\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    output_path = os.path.join(output_dir, f\"simplified_ratio_training_{timestamp}.png\")\n    visualize_training_results(history, output_path)\n    \n    # Export ratio metrics\n    metrics_path = os.path.join(output_dir, f\"simplified_ratio_metrics_{timestamp}.json\")\n    metrics = {\n        'train_test_ratios': history.get('tt_ratios', []),\n        'risk_accuracy_ratios': history.get('ra_ratios', []),\n        'final_accuracy': history['val_acc'][-1] if history.get('val_acc') else None,\n    }\n    \n    with open(metrics_path, 'w') as f:\n        json.dump(metrics, f, indent=2)\n    logger.info(f\"Ratio metrics saved to {metrics_path}\")\n    \n    # Print summary\n    logger.info(\"\\n\" + \"=\" * 50)\n    logger.info(\"TRAINING SUMMARY\")\n    logger.info(\"=\" * 50)\n    logger.info(f\"Final Training Accuracy: {history['train_acc'][-1]:.2f}%\")\n    if 'val_acc' in history and history['val_acc']:\n        logger.info(f\"Final Test Accuracy:     {history['val_acc'][-1]:.2f}%\")\n    \n    # Print ratio metrics\n    if 'tt_ratios' in history and history['tt_ratios']:\n        logger.info(f\"Final Train/Test Ratio:     {history['tt_ratios'][-1]:.3f}\")\n    if 'ra_ratios' in history and history['ra_ratios']:\n        logger.info(f\"Final Risk/Accuracy Ratio:  {history['ra_ratios'][-1]:.3f}\")\n    \n    logger.info(\"=\" * 50)\n    logger.info(f\"Results visualization saved to: {output_path}\")\n    logger.info(\"Simplified ratio-based training completed successfully\")\n    \n    return history\n\nif __name__ == \"__main__\":\n    try:\n        main()"
    }
  },
  "constants": {}
}