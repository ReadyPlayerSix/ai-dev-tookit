{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\isekaizen\\core\\optimizer\\simplified_responsive.py",
  "imports": [
    {
      "name": "torch",
      "line": 8
    },
    {
      "name": "logging",
      "line": 9
    },
    {
      "name": "time",
      "line": 10
    },
    {
      "name": "random",
      "line": 11
    },
    {
      "name": "math",
      "line": 12
    },
    {
      "name": "numpy",
      "line": 13
    },
    {
      "name": "typing.Dict",
      "line": 14
    },
    {
      "name": "typing.List",
      "line": 14
    },
    {
      "name": "typing.Any",
      "line": 14
    },
    {
      "name": "typing.Optional",
      "line": 14
    },
    {
      "name": "typing.Tuple",
      "line": 14
    },
    {
      "name": "typing.Set",
      "line": 14
    },
    {
      "name": "isekaizen.core.optimizer.enhanced_pattern_responsive.EnhancedPatternResponsiveOptimizer",
      "line": 16
    },
    {
      "name": "isekaizen.core.optimizer.simplified_ratio_tracker.SimplifiedRatioTracker",
      "line": 17
    },
    {
      "name": "isekaizen.optimizers.eve_simplified.EVESimplifiedRatio",
      "line": 18
    },
    {
      "name": "torch.utils.data.ConcatDataset",
      "line": 201
    },
    {
      "name": "torch.utils.data.TensorDataset",
      "line": 201
    }
  ],
  "classes": {
    "SimplifiedPatternResponsiveOptimizer": {
      "start_line": 22,
      "end_line": 358,
      "methods": {
        "__init__": {
          "start_line": 32,
          "end_line": 87,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "model"
            },
            {
              "name": "device"
            },
            {
              "name": "total_epochs"
            },
            {
              "name": "max_epoch_time"
            },
            {
              "name": "run_diagnostics"
            },
            {
              "name": "pattern_map"
            },
            {
              "name": "min_batch"
            },
            {
              "name": "max_batch"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "....__init__",
              "line": 59
            },
            {
              "name": "SimplifiedRatioTracker",
              "line": 74
            },
            {
              "name": "logger.info",
              "line": 83
            },
            {
              "name": "logger.info",
              "line": 84
            },
            {
              "name": "logger.info",
              "line": 85
            },
            {
              "name": "super",
              "line": 59
            }
          ],
          "docstring": "\n        Initialize the simplified pattern-responsive optimizer.\n        \n        Args:\n            model: PyTorch model to optimize\n            device: Computation device\n            total_epochs: Total number of epochs for training\n            max_epoch_time: Maximum time per epoch in seconds (None = no limit)\n            run_diagnostics: Whether to run initial diagnostics\n            pattern_map: Pattern map containing pattern information\n            min_batch: Minimum batch size\n            max_batch: Maximum batch size\n            **kwargs: Additional parameters\n        ",
          "code_snippet": "    \"\"\"\n    \n    def __init__(\n        self, \n        model,\n        device=None,\n        total_epochs=50,\n        max_epoch_time=None,\n        run_diagnostics=True,\n        pattern_map=None,\n        min_batch=32,\n        max_batch=256,\n        **kwargs\n    ):\n        \"\"\"\n        Initialize the simplified pattern-responsive optimizer.\n        \n        Args:\n            model: PyTorch model to optimize\n            device: Computation device\n            total_epochs: Total number of epochs for training\n            max_epoch_time: Maximum time per epoch in seconds (None = no limit)\n            run_diagnostics: Whether to run initial diagnostics\n            pattern_map: Pattern map containing pattern information\n            min_batch: Minimum batch size\n            max_batch: Maximum batch size\n            **kwargs: Additional parameters\n        \"\"\"\n        # Initialize base optimizer\n        super().__init__(\n            model=model,\n            device=device,\n            total_epochs=total_epochs,\n            max_epoch_time=max_epoch_time,\n            run_diagnostics=run_diagnostics,\n            pattern_map=pattern_map,\n            **kwargs\n        )\n        \n        # Override batch size limits\n        self.min_batch = min_batch\n        self.max_batch = max_batch\n        \n        # Initialize the simplified ratio tracker\n        self.simplified_ratio_tracker = SimplifiedRatioTracker()\n        \n        # Track when we last adapted\n        self.last_adaptation_epoch = 0\n        self.min_epochs_between_adaptations = 2\n        \n        # Track underperforming patterns\n        self.underperforming_patterns = []\n        \n        logger.info(\"Simplified Pattern-Responsive Optimizer initialized\")\n        logger.info(f\"Batch size range: {self.min_batch} - {self.max_batch}\")\n        logger.info(f\"Total epochs: {total_epochs}\")\n    \n    def get_optimizer(self):\n        \"\"\"Get the underlying optimizer instance.\"\"\"\n        optimizer = None"
        },
        "get_optimizer": {
          "start_line": 87,
          "end_line": 102,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "getattr",
              "line": 100
            },
            {
              "name": "hasattr",
              "line": 92
            },
            {
              "name": "hasattr",
              "line": 92
            },
            {
              "name": "logger.info",
              "line": 98
            },
            {
              "name": "hasattr",
              "line": 96
            },
            {
              "name": "type",
              "line": 98
            }
          ],
          "docstring": "Get the underlying optimizer instance.",
          "code_snippet": "        logger.info(f\"Total epochs: {total_epochs}\")\n    \n    def get_optimizer(self):\n        \"\"\"Get the underlying optimizer instance.\"\"\"\n        optimizer = None\n        \n        # Try to find an instance of EVESimplifiedRatio\n        if hasattr(self, 'trainer') and hasattr(self.trainer, 'optimizer'):\n            optimizer = self.trainer.optimizer\n            \n        # Set first time we find the optimizer\n        if optimizer and not hasattr(self, '_optimizer'):\n            self._optimizer = optimizer\n            logger.info(f\"Simplified optimizer found: {type(optimizer).__name__}\")\n            \n        return getattr(self, '_optimizer', None)\n    \n    def should_adapt_patterns(self) -> bool:\n        \"\"\"\n        Determine if dataset adaptation is needed based on ratio decision logic."
        },
        "should_adapt_patterns": {
          "start_line": 102,
          "end_line": 140,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": "bool",
          "calls": [
            {
              "name": "self.get_optimizer",
              "line": 123
            },
            {
              "name": "hasattr",
              "line": 110
            },
            {
              "name": "hasattr",
              "line": 114
            },
            {
              "name": "logger.info",
              "line": 119
            },
            {
              "name": "hasattr",
              "line": 126
            },
            {
              "name": "decision.get",
              "line": 130
            },
            {
              "name": "logger.info",
              "line": 131
            },
            {
              "name": "decision.get",
              "line": 131
            }
          ],
          "docstring": "\n        Determine if dataset adaptation is needed based on ratio decision logic.\n        \n        Returns:\n            True if patterns should be adapted, False otherwise\n        ",
          "code_snippet": "        return getattr(self, '_optimizer', None)\n    \n    def should_adapt_patterns(self) -> bool:\n        \"\"\"\n        Determine if dataset adaptation is needed based on ratio decision logic.\n        \n        Returns:\n            True if patterns should be adapted, False otherwise\n        \"\"\"\n        # Check if we've already adapted in this epoch\n        if hasattr(self, 'last_adaptation_epoch') and self.last_adaptation_epoch == self.epoch:\n            return False\n        \n        # Check if enough epochs have passed since last adaptation\n        if hasattr(self, 'last_adaptation_epoch') and self.epoch - self.last_adaptation_epoch < self.min_epochs_between_adaptations:\n            return False\n        \n        # Need at least one epoch to have meaningful comparison\n        if self.epoch < 1:\n            logger.info(f\"Not adapting patterns yet - need at least one completed epoch (current: {self.epoch})\")\n            return False\n            \n        # Get optimizer\n        optimizer = self.get_optimizer()\n        \n        # Check if we're using the simplified ratio system\n        if optimizer and hasattr(optimizer, 'ratio_decision'):\n            decision = optimizer.ratio_decision\n            \n            # Only augment data when specifically indicated by the ratio decision\n            if decision.get('action') == 'augment_data':\n                logger.info(f\"Data augmentation triggered by ratio decision (action: {decision.get('action')})\")\n                \n                # Record that we adapted in this epoch\n                self.last_adaptation_epoch = self.epoch\n                return True\n        \n        # Default to no adaptation\n        return False\n    \n    def adapt_dataset(self, dataset) -> Tuple[Any, Dict[str, Any]]:\n        \"\"\"\n        Adapt dataset based on underperforming patterns."
        },
        "adapt_dataset": {
          "start_line": 140,
          "end_line": 221,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "dataset"
            }
          ],
          "return_type": "complex_type",
          "calls": [
            {
              "name": "self.get_optimizer",
              "line": 151
            },
            {
              "name": "hasattr",
              "line": 157
            },
            {
              "name": "torch.stack",
              "line": 204
            },
            {
              "name": "torch.tensor",
              "line": 205
            },
            {
              "name": "TensorDataset",
              "line": 206
            },
            {
              "name": "ConcatDataset",
              "line": 209
            },
            {
              "name": "logger.info",
              "line": 212
            },
            {
              "name": "logger.info",
              "line": 213
            },
            {
              "name": "logger.warning",
              "line": 153
            },
            {
              "name": "self.simplified_ratio_tracker.get_underperforming_patterns",
              "line": 158
            },
            {
              "name": "logger.info",
              "line": 164
            },
            {
              "name": "logger.info",
              "line": 168
            },
            {
              "name": "hasattr",
              "line": 174
            },
            {
              "name": "min",
              "line": 176
            },
            {
              "name": "logger.warning",
              "line": 197
            },
            {
              "name": "hasattr",
              "line": 152
            },
            {
              "name": "logger.info",
              "line": 161
            },
            {
              "name": "int",
              "line": 176
            },
            {
              "name": "len",
              "line": 177
            },
            {
              "name": "hasattr",
              "line": 183
            },
            {
              "name": "len",
              "line": 218
            },
            {
              "name": "len",
              "line": 219
            },
            {
              "name": "logger.info",
              "line": 184
            },
            {
              "name": "self.augmenter.augment_pattern",
              "line": 185
            },
            {
              "name": "len",
              "line": 212
            },
            {
              "name": "len",
              "line": 213
            },
            {
              "name": "len",
              "line": 213
            },
            {
              "name": "....join",
              "line": 164
            },
            {
              "name": "len",
              "line": 176
            },
            {
              "name": "augmented_examples.extend",
              "line": 192
            },
            {
              "name": "logger.info",
              "line": 193
            },
            {
              "name": "len",
              "line": 193
            }
          ],
          "docstring": "\n        Adapt dataset based on underperforming patterns.\n        \n        Args:\n            dataset: Dataset to adapt\n            \n        Returns:\n            Tuple of (adapted dataset, adaptation metrics)\n        ",
          "code_snippet": "        return False\n    \n    def adapt_dataset(self, dataset) -> Tuple[Any, Dict[str, Any]]:\n        \"\"\"\n        Adapt dataset based on underperforming patterns.\n        \n        Args:\n            dataset: Dataset to adapt\n            \n        Returns:\n            Tuple of (adapted dataset, adaptation metrics)\n        \"\"\"\n        # Use the underperforming patterns identified by the ratio tracker\n        optimizer = self.get_optimizer()\n        if not optimizer or not hasattr(optimizer, 'ratio_tracker'):\n            logger.warning(\"Cannot adapt dataset: No ratio tracker available\")\n            return dataset, {\"adapted\": False, \"reason\": \"no_ratio_tracker\"}\n            \n        # Get underperforming patterns\n        if hasattr(self, 'simplified_ratio_tracker'):\n            underperforming = self.simplified_ratio_tracker.get_underperforming_patterns()\n            \n            if not underperforming:\n                logger.info(\"No underperforming patterns identified for adaptation\")\n                return dataset, {\"adapted\": False, \"reason\": \"no_underperforming_patterns\"}\n                \n            logger.info(f\"Adapting dataset for underperforming patterns: {', '.join(underperforming)}\")\n            self.underperforming_patterns = underperforming\n        else:\n            # Fallback to basic adaptation if no specific patterns identified\n            logger.info(\"No specific pattern information available - using general adaptation\")\n        \n        # Create augmented examples for each pattern type\n        augmented_examples = []\n        examples_per_pattern = {}\n        \n        if hasattr(self, 'underperforming_patterns') and self.underperforming_patterns:\n            # Calculate examples to add per pattern\n            total_to_add = min(5000, int(len(dataset) * 0.2))  # Add up to 20% more examples, max 5000\n            examples_per_pattern_count = total_to_add // len(self.underperforming_patterns)\n            \n            for pattern_type in self.underperforming_patterns:\n                examples_per_pattern[pattern_type] = examples_per_pattern_count\n                \n                # Augment for this pattern\n                if hasattr(self, 'augmenter'):\n                    logger.info(f\"Creating {examples_per_pattern_count} examples for pattern {pattern_type}\")\n                    pattern_examples = self.augmenter.augment_pattern(\n                        dataset,\n                        pattern_type,\n                        count=examples_per_pattern_count\n                    )\n                    \n                    if pattern_examples:\n                        augmented_examples.extend(pattern_examples)\n                        logger.info(f\"Added {len(pattern_examples)} examples for pattern {pattern_type}\")\n        \n        # Return if no augmentations were created\n        if not augmented_examples:\n            logger.warning(\"No augmentations were created\")\n            return dataset, {\"adapted\": False, \"reason\": \"no_augmentations_created\"}\n        \n        # Create a combined dataset\n        from torch.utils.data import ConcatDataset, TensorDataset\n        \n        # Create a dataset from the augmented examples\n        features = torch.stack([item[0] for item in augmented_examples])\n        labels = torch.tensor([item[1] for item in augmented_examples])\n        augmented_dataset = TensorDataset(features, labels)\n        \n        # Combine with original dataset\n        combined_dataset = ConcatDataset([dataset, augmented_dataset])\n        \n        # Log augmentation\n        logger.info(f\"Dataset adapted with {len(augmented_examples)} new examples\")\n        logger.info(f\"New dataset size: {len(combined_dataset)} (original: {len(dataset)})\")\n        \n        return combined_dataset, {\n            \"adapted\": True,\n            \"examples_per_pattern\": examples_per_pattern,\n            \"examples_added\": len(augmented_examples),\n            \"total_size\": len(combined_dataset)\n        }\n    \n    def get_optimal_batch_size(self):\n        \"\"\""
        },
        "get_optimal_batch_size": {
          "start_line": 222,
          "end_line": 269,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.get_optimizer",
              "line": 240
            },
            {
              "name": "self.batch_history.append",
              "line": 266
            },
            {
              "name": "self.batch_history.append",
              "line": 235
            },
            {
              "name": "logger.info",
              "line": 236
            },
            {
              "name": "hasattr",
              "line": 249
            },
            {
              "name": "decision.get",
              "line": 253
            },
            {
              "name": "decision.get",
              "line": 254
            },
            {
              "name": "int",
              "line": 255
            },
            {
              "name": "max",
              "line": 258
            },
            {
              "name": "min",
              "line": 258
            },
            {
              "name": "logger.info",
              "line": 261
            },
            {
              "name": "self.batch_history.append",
              "line": 262
            }
          ],
          "docstring": "\n        Get the optimal batch size for the current training state.\n        \n        Returns:\n            Optimal batch size\n        ",
          "code_snippet": "        }\n    \n    def get_optimal_batch_size(self):\n        \"\"\"\n        Get the optimal batch size for the current training state.\n        \n        Returns:\n            Optimal batch size\n        \"\"\"\n        # Increment epoch counter\n        self.epoch += 1\n        \n        # For the first epoch, use the middle of the range\n        if self.epoch == 1:\n            initial_batch = (self.min_batch + self.max_batch) // 2\n            self.batch_history.append(initial_batch)\n            logger.info(f\"First epoch: Using initial batch size {initial_batch}\")\n            return initial_batch\n        \n        # Get optimizer for ratio decisions\n        optimizer = self.get_optimizer()\n        \n        # Base batch size (current or default)\n        if self.batch_history:\n            base_batch = self.batch_history[-1]\n        else:\n            base_batch = (self.min_batch + self.max_batch) // 2\n        \n        # Check if we should adjust batch size based on ratio decision\n        if optimizer and hasattr(optimizer, 'ratio_decision'):\n            decision = optimizer.ratio_decision\n            \n            # If the decision was to adjust batch size\n            if decision.get('action') == 'adjust_both':\n                batch_factor = decision.get('batch_factor', 1.0)\n                adjusted_batch = int(base_batch * batch_factor)\n                \n                # Ensure within bounds\n                adjusted_batch = max(self.min_batch, min(self.max_batch, adjusted_batch))\n                \n                if adjusted_batch != base_batch:\n                    logger.info(f\"Adjusting batch size from {base_batch} to {adjusted_batch} based on ratio decision\")\n                    self.batch_history.append(adjusted_batch)\n                    return adjusted_batch\n        \n        # No adjustment needed, continue with current batch size\n        self.batch_history.append(base_batch)\n        return base_batch\n    \n    def update_with_epoch_metrics(self, epoch_metrics):\n        \"\"\"\n        Update optimizer with epoch-level metrics."
        },
        "update_with_epoch_metrics": {
          "start_line": 269,
          "end_line": 307,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "epoch_metrics"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "epoch_metrics.get",
              "line": 280
            },
            {
              "name": "epoch_metrics.get",
              "line": 281
            },
            {
              "name": "logger.info",
              "line": 305
            },
            {
              "name": "self.simplified_ratio_tracker.update_train_test_ratio",
              "line": 285
            },
            {
              "name": "self.get_optimizer",
              "line": 288
            },
            {
              "name": "isinstance",
              "line": 291
            },
            {
              "name": "hasattr",
              "line": 291
            },
            {
              "name": "self.simplified_ratio_tracker.update_pattern_accuracies",
              "line": 293
            },
            {
              "name": "self.simplified_ratio_tracker.get_current_train_test_ratio",
              "line": 296
            },
            {
              "name": "optimizer.update_with_ratios",
              "line": 299
            },
            {
              "name": "optimizer.apply_decision",
              "line": 302
            }
          ],
          "docstring": "\n        Update optimizer with epoch-level metrics.\n        \n        Args:\n            epoch_metrics: Dictionary of epoch metrics (loss, accuracy, etc.)\n        ",
          "code_snippet": "        return base_batch\n    \n    def update_with_epoch_metrics(self, epoch_metrics):\n        \"\"\"\n        Update optimizer with epoch-level metrics.\n        \n        Args:\n            epoch_metrics: Dictionary of epoch metrics (loss, accuracy, etc.)\n        \"\"\"\n        # Store epoch metrics\n        self.epoch_metrics = epoch_metrics\n        \n        # Get train and validation accuracy\n        train_acc = epoch_metrics.get('accuracy', 0.0)\n        val_acc = epoch_metrics.get('val_accuracy', 0.0)\n        \n        # Update simplified ratio tracker if we have validation metrics\n        if val_acc > 0:\n            self.simplified_ratio_tracker.update_train_test_ratio(self.epoch, train_acc, val_acc)\n            \n            # Get optimizer\n            optimizer = self.get_optimizer()\n            \n            # Update optimizer with ratios if available\n            if optimizer and isinstance(optimizer, EVESimplifiedRatio) and hasattr(self, 'pattern_accuracies'):\n                # Update pattern accuracies and get risk/accuracy ratio\n                ra_ratio = self.simplified_ratio_tracker.update_pattern_accuracies(self.epoch, self.pattern_accuracies)\n                \n                # Get Train/Test ratio\n                tt_ratio = self.simplified_ratio_tracker.get_current_train_test_ratio()\n                \n                # Update optimizer with both ratios\n                optimizer.update_with_ratios(tt_ratio, ra_ratio)\n                \n                # Apply any immediate changes from the decision\n                optimizer.apply_decision()\n        \n        # Log the status\n        logger.info(f\"Epoch {self.epoch} metrics updated - train: {train_acc:.2f}%, val: {val_acc:.2f}%\")\n    \n    def update_with_pattern_recognition(self, batch_indices, correct_mask):\n        \"\"\"\n        Update pattern accuracies based on batch results."
        },
        "update_with_pattern_recognition": {
          "start_line": 307,
          "end_line": 358,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "batch_indices"
            },
            {
              "name": "correct_mask"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "enumerate",
              "line": 322
            },
            {
              "name": "hasattr",
              "line": 317
            },
            {
              "name": "hasattr",
              "line": 326
            },
            {
              "name": "self.pattern_service.get_pattern_type",
              "line": 327
            },
            {
              "name": "isinstance",
              "line": 345
            },
            {
              "name": "....item",
              "line": 345
            },
            {
              "name": "hasattr",
              "line": 328
            },
            {
              "name": "str",
              "line": 329
            },
            {
              "name": "....get",
              "line": 331
            },
            {
              "name": "len",
              "line": 337
            }
          ],
          "docstring": "\n        Update pattern accuracies based on batch results.\n        Simplified to just track overall accuracy per pattern.\n        \n        Args:\n            batch_indices: Indices of examples in the batch\n            correct_mask: Boolean mask of whether each prediction was correct\n        ",
          "code_snippet": "        logger.info(f\"Epoch {self.epoch} metrics updated - train: {train_acc:.2f}%, val: {val_acc:.2f}%\")\n    \n    def update_with_pattern_recognition(self, batch_indices, correct_mask):\n        \"\"\"\n        Update pattern accuracies based on batch results.\n        Simplified to just track overall accuracy per pattern.\n        \n        Args:\n            batch_indices: Indices of examples in the batch\n            correct_mask: Boolean mask of whether each prediction was correct\n        \"\"\"\n        # Initialize pattern accuracies if not exists\n        if not hasattr(self, 'pattern_accuracies'):\n            self.pattern_accuracies = {}\n            self.pattern_counts = {}\n        \n        # Get pattern for each example and update accuracy\n        for i, index in enumerate(batch_indices):\n            # Get pattern type for this example (using pattern service or pattern map)\n            pattern_type = None\n            \n            if hasattr(self, 'pattern_service'):\n                pattern_type = self.pattern_service.get_pattern_type(index)\n            elif hasattr(self, 'pattern_map') and 'pattern_map' in self.pattern_map:\n                idx_str = str(index)\n                if idx_str in self.pattern_map['pattern_map']:\n                    pattern_type = self.pattern_map['pattern_map'][idx_str].get('pattern_type')\n            \n            # Fallback to simplified taxonomy patterns if needed\n            if not pattern_type:\n                # Deterministic assignment based on index\n                simplified_patterns = [\"structural\", \"statistical\", \"temporal\"]\n                pattern_type = simplified_patterns[index % len(simplified_patterns)]\n            \n            # Initialize pattern counters if needed\n            if pattern_type not in self.pattern_accuracies:\n                self.pattern_accuracies[pattern_type] = 0.0\n                self.pattern_counts[pattern_type] = 0\n            \n            # Update accuracy counts\n            is_correct = correct_mask[i].item() if isinstance(correct_mask[i], torch.Tensor) else correct_mask[i]\n            self.pattern_counts[pattern_type] += 1\n            \n            # Incremental update of accuracy\n            if is_correct:\n                current_acc = self.pattern_accuracies[pattern_type]\n                current_count = self.pattern_counts[pattern_type]\n                self.pattern_accuracies[pattern_type] = ((current_acc * (current_count - 1)) + 1.0) / current_count\n            else:\n                current_acc = self.pattern_accuracies[pattern_type]\n                current_count = self.pattern_counts[pattern_type]\n                self.pattern_accuracies[pattern_type] = (current_acc * (current_count - 1)) / current_count"
        }
      },
      "class_variables": [],
      "bases": [
        "EnhancedPatternResponsiveOptimizer"
      ],
      "docstring": "\n    Simplified pattern-responsive optimizer that uses Train/Test and Risk/Accuracy ratios\n    to drive optimization decisions with minimal computational overhead.\n    \n    This optimizer makes decisions based on:\n    1. Train/Test ratio - to detect overfitting\n    2. Risk/Accuracy ratio - to track pattern performance regression\n    "
    }
  },
  "functions": {},
  "constants": {}
}