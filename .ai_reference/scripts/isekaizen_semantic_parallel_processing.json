{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\isekaizen\\semantic\\parallel_processing.py",
  "imports": [
    {
      "name": "os",
      "line": 8
    },
    {
      "name": "time",
      "line": 9
    },
    {
      "name": "logging",
      "line": 10
    },
    {
      "name": "multiprocessing",
      "line": 11
    },
    {
      "name": "concurrent.futures.ProcessPoolExecutor",
      "line": 12
    },
    {
      "name": "concurrent.futures.as_completed",
      "line": 12
    },
    {
      "name": "functools.partial",
      "line": 13
    },
    {
      "name": "typing.Dict",
      "line": 14
    },
    {
      "name": "typing.List",
      "line": 14
    },
    {
      "name": "typing.Any",
      "line": 14
    },
    {
      "name": "typing.Optional",
      "line": 14
    },
    {
      "name": "typing.Callable",
      "line": 14
    },
    {
      "name": "typing.Tuple",
      "line": 14
    },
    {
      "name": "torch",
      "line": 16
    },
    {
      "name": "numpy",
      "line": 17
    },
    {
      "name": "pattern_detection.extract_visual_patterns",
      "line": 24
    },
    {
      "name": "numpy",
      "line": 123
    },
    {
      "name": "collections.Counter",
      "line": 124
    },
    {
      "name": "pattern_detection.extract_visual_patterns",
      "line": 28
    },
    {
      "name": "text_patterns.extract_text_patterns",
      "line": 92
    },
    {
      "name": "scipy.signal",
      "line": 158
    },
    {
      "name": "audio_patterns.extract_audio_patterns",
      "line": 100
    }
  ],
  "classes": {},
  "functions": {
    "determine_dataset_type": {
      "start_line": 35,
      "end_line": 76,
      "parameters": [
        {
          "name": "dataset"
        }
      ],
      "return_type": "str",
      "calls": [
        {
          "name": "isinstance",
          "line": 50
        },
        {
          "name": "isinstance",
          "line": 54
        },
        {
          "name": "logger.warning",
          "line": 73
        },
        {
          "name": "len",
          "line": 50
        },
        {
          "name": "isinstance",
          "line": 66
        },
        {
          "name": "len",
          "line": 56
        },
        {
          "name": "len",
          "line": 59
        },
        {
          "name": "len",
          "line": 62
        }
      ],
      "docstring": "\n    Try to automatically determine the dataset type.\n    \n    Args:\n        dataset: PyTorch dataset or similar\n        \n    Returns:\n        String indicating dataset type ('image', 'text', 'audio', or 'unknown')\n    ",
      "code_snippet": "\n\ndef determine_dataset_type(dataset) -> str:\n    \"\"\"\n    Try to automatically determine the dataset type.\n    \n    Args:\n        dataset: PyTorch dataset or similar\n        \n    Returns:\n        String indicating dataset type ('image', 'text', 'audio', or 'unknown')\n    \"\"\"\n    # Try to get a sample\n    try:\n        sample = dataset[0]\n        \n        # Check what kind of sample we have\n        if isinstance(sample, tuple) and len(sample) >= 2:\n            data, label = sample[0], sample[1]\n            \n            # Check data type\n            if isinstance(data, torch.Tensor):\n                # Image dataset typically has shape [channels, height, width]\n                if len(data.shape) == 3 and data.shape[0] in [1, 3, 4]:\n                    return \"image\"\n                # Audio dataset typically has shape [channels, time]\n                elif len(data.shape) == 2 and data.shape[0] in [1, 2]:\n                    return \"audio\"\n                # Text might be encoded as a tensor\n                elif len(data.shape) == 1:\n                    return \"text\"\n            \n            # String data is likely text\n            elif isinstance(data, str):\n                return \"text\"\n        \n        # Unknown dataset type\n        return \"unknown\"\n    \n    except Exception as e:\n        logger.warning(f\"Error determining dataset type: {e}\")\n        return \"unknown\"\n\n\ndef get_extractor_for_dataset(dataset_type: str) -> Callable:\n    \"\"\""
    },
    "get_extractor_for_dataset": {
      "start_line": 77,
      "end_line": 109,
      "parameters": [
        {
          "name": "dataset_type",
          "type": "str"
        }
      ],
      "return_type": "Callable",
      "calls": [
        {
          "name": "logger.warning",
          "line": 95
        },
        {
          "name": "logger.warning",
          "line": 103
        }
      ],
      "docstring": "\n    Get the appropriate pattern extractor for the dataset type.\n    \n    Args:\n        dataset_type: Type of dataset ('image', 'text', 'audio', 'unknown')\n        \n    Returns:\n        Function that extracts patterns from the dataset type\n    ",
      "code_snippet": "\n\ndef get_extractor_for_dataset(dataset_type: str) -> Callable:\n    \"\"\"\n    Get the appropriate pattern extractor for the dataset type.\n    \n    Args:\n        dataset_type: Type of dataset ('image', 'text', 'audio', 'unknown')\n        \n    Returns:\n        Function that extracts patterns from the dataset type\n    \"\"\"\n    if dataset_type == \"image\":\n        return extract_visual_patterns\n    elif dataset_type == \"text\":\n        # Use a text pattern extractor if available\n        try:\n            from .text_patterns import extract_text_patterns\n            return extract_text_patterns\n        except ImportError:\n            logger.warning(\"Text pattern extractor not available, falling back to generic\")\n            return extract_generic_patterns\n    elif dataset_type == \"audio\":\n        # Use an audio pattern extractor if available\n        try:\n            from .audio_patterns import extract_audio_patterns\n            return extract_audio_patterns\n        except ImportError:\n            logger.warning(\"Audio pattern extractor not available, falling back to generic\")\n            return extract_generic_patterns\n    else:\n        # Generic pattern extractor\n        return extract_generic_patterns\n\n\ndef extract_generic_patterns(data, idx, device=None):\n    \"\"\""
    },
    "extract_generic_patterns": {
      "start_line": 110,
      "end_line": 192,
      "parameters": [
        {
          "name": "data"
        },
        {
          "name": "idx"
        },
        {
          "name": "device"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "isinstance",
          "line": 130
        },
        {
          "name": "float",
          "line": 136
        },
        {
          "name": "float",
          "line": 137
        },
        {
          "name": "float",
          "line": 138
        },
        {
          "name": "float",
          "line": 139
        },
        {
          "name": "data_np.flatten",
          "line": 143
        },
        {
          "name": "np.histogram",
          "line": 146
        },
        {
          "name": "float",
          "line": 152
        },
        {
          "name": "features.get",
          "line": 175
        },
        {
          "name": "features.get",
          "line": 176
        },
        {
          "name": "....numpy",
          "line": 131
        },
        {
          "name": "np.array",
          "line": 133
        },
        {
          "name": "np.mean",
          "line": 136
        },
        {
          "name": "np.std",
          "line": 137
        },
        {
          "name": "np.min",
          "line": 138
        },
        {
          "name": "np.max",
          "line": 139
        },
        {
          "name": "hist.astype",
          "line": 147
        },
        {
          "name": "len",
          "line": 147
        },
        {
          "name": "len",
          "line": 155
        },
        {
          "name": "features.get",
          "line": 174
        },
        {
          "name": "logger.error",
          "line": 183
        },
        {
          "name": "len",
          "line": 151
        },
        {
          "name": "np.sum",
          "line": 151
        },
        {
          "name": "signal.correlate",
          "line": 159
        },
        {
          "name": "float",
          "line": 165
        },
        {
          "name": "features.get",
          "line": 174
        },
        {
          "name": "data.cpu",
          "line": 131
        },
        {
          "name": "signal.find_peaks",
          "line": 164
        },
        {
          "name": "len",
          "line": 165
        },
        {
          "name": "float",
          "line": 166
        },
        {
          "name": "logger.debug",
          "line": 168
        },
        {
          "name": "np.log2",
          "line": 151
        },
        {
          "name": "len",
          "line": 166
        },
        {
          "name": "np.max",
          "line": 166
        },
        {
          "name": "len",
          "line": 160
        }
      ],
      "docstring": "\n    Generic pattern extractor for unknown data types.\n    Tries to extract basic statistical features.\n    \n    Args:\n        data: Input data of any type\n        idx: Sample index\n        device: Computation device (unused in generic extractor)\n        \n    Returns:\n        Dictionary with basic features\n    ",
      "code_snippet": "\n\ndef extract_generic_patterns(data, idx, device=None):\n    \"\"\"\n    Generic pattern extractor for unknown data types.\n    Tries to extract basic statistical features.\n    \n    Args:\n        data: Input data of any type\n        idx: Sample index\n        device: Computation device (unused in generic extractor)\n        \n    Returns:\n        Dictionary with basic features\n    \"\"\"\n    import numpy as np\n    from collections import Counter\n    \n    features = {}\n    \n    # Try to convert to numpy array\n    try:\n        if isinstance(data, torch.Tensor):\n            data_np = data.cpu().numpy()\n        else:\n            data_np = np.array(data)\n        \n        # Extract basic statistical features\n        features['mean'] = float(np.mean(data_np))\n        features['std'] = float(np.std(data_np))\n        features['min'] = float(np.min(data_np))\n        features['max'] = float(np.max(data_np))\n        features['range'] = features['max'] - features['min']\n        \n        # Flatten for further analysis\n        flat_data = data_np.flatten()\n        \n        # Calculate histogram-based features\n        hist, bin_edges = np.histogram(flat_data, bins=10)\n        hist = hist.astype(float) / len(flat_data)  # Normalize\n        \n        # Entropy calculation\n        non_zero_hist = hist[hist > 0]\n        entropy = -np.sum(non_zero_hist * np.log2(non_zero_hist)) if len(non_zero_hist) > 0 else 0\n        features['entropy'] = float(entropy)\n        \n        # Try to detect patterns using autocorrelation\n        if len(flat_data) > 10:\n            try:\n                # Calculate autocorrelation for pattern detection\n                from scipy import signal\n                autocorr = signal.correlate(flat_data, flat_data, mode='full')\n                autocorr = autocorr[len(autocorr)//2:]\n                # Normalize\n                autocorr = autocorr / autocorr[0]\n                # Get peak values after the first point\n                peaks = signal.find_peaks(autocorr[1:], height=0.2)[0]\n                features['periodicity'] = float(len(peaks))\n                features['autocorr_strength'] = float(np.max(autocorr[1:])) if len(autocorr) > 1 else 0\n            except Exception as e:\n                logger.debug(f\"Error calculating autocorrelation: {e}\")\n                features['periodicity'] = 0.0\n                features['autocorr_strength'] = 0.0\n        \n        # Map these generic features to our standard feature names\n        # This allows the domain-agnostic pattern detector to work\n        features['edge_density'] = features.get('std', 0) / (features.get('range', 1) + 1e-10)\n        features['texture_complexity'] = features.get('entropy', 0) \n        features['contrast'] = features.get('range', 0)\n        features['center_of_mass_x'] = 0.5  # Default center\n        features['center_of_mass_y'] = 0.5  # Default center\n        \n        return features\n    \n    except Exception as e:\n        logger.error(f\"Error in generic pattern extraction: {e}\")\n        # Return basic features with default values\n        return {\n            'edge_density': 0.2,\n            'texture_complexity': 0.2,\n            'contrast': 0.2,\n            'center_of_mass_x': 0.5,\n            'center_of_mass_y': 0.5,\n        }\n\n\ndef process_sample_worker(idx, dataset, extractor, device):"
    },
    "process_sample_worker": {
      "start_line": 194,
      "end_line": 226,
      "parameters": [
        {
          "name": "idx"
        },
        {
          "name": "dataset"
        },
        {
          "name": "extractor"
        },
        {
          "name": "device"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "extractor",
          "line": 218
        },
        {
          "name": "int",
          "line": 209
        },
        {
          "name": "isinstance",
          "line": 212
        },
        {
          "name": "logger.error",
          "line": 223
        },
        {
          "name": "len",
          "line": 212
        }
      ],
      "docstring": "\n    Worker function for processing a single sample.\n    \n    Args:\n        idx: Sample index\n        dataset: Dataset to extract from\n        extractor: Function to extract patterns\n        device: Computation device\n        \n    Returns:\n        Tuple of (idx, pattern)\n    ",
      "code_snippet": "\n\ndef process_sample_worker(idx, dataset, extractor, device):\n    \"\"\"\n    Worker function for processing a single sample.\n    \n    Args:\n        idx: Sample index\n        dataset: Dataset to extract from\n        extractor: Function to extract patterns\n        device: Computation device\n        \n    Returns:\n        Tuple of (idx, pattern)\n    \"\"\"\n    try:\n        # Get sample\n        sample = dataset[int(idx)]\n        \n        # Extract data from sample\n        if isinstance(sample, tuple) and len(sample) >= 2:\n            data, label = sample[0], sample[1]\n        else:\n            data, label = sample, None\n        \n        # Extract pattern\n        pattern = extractor(data, idx, device)\n        \n        return idx, pattern\n    \n    except Exception as e:\n        logger.error(f\"Error processing sample {idx}: {e}\")\n        return idx, None\n\n\ndef extract_patterns_parallel(samples, dataset, device=None, max_workers=None):\n    \"\"\""
    },
    "extract_patterns_parallel": {
      "start_line": 227,
      "end_line": 310,
      "parameters": [
        {
          "name": "samples"
        },
        {
          "name": "dataset"
        },
        {
          "name": "device"
        },
        {
          "name": "max_workers"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "determine_dataset_type",
          "line": 245
        },
        {
          "name": "logger.info",
          "line": 246
        },
        {
          "name": "get_extractor_for_dataset",
          "line": 249
        },
        {
          "name": "logger.info",
          "line": 250
        },
        {
          "name": "partial",
          "line": 253
        },
        {
          "name": "len",
          "line": 263
        },
        {
          "name": "time.time",
          "line": 265
        },
        {
          "name": "logger.info",
          "line": 267
        },
        {
          "name": "min",
          "line": 270
        },
        {
          "name": "range",
          "line": 273
        },
        {
          "name": "logger.info",
          "line": 304
        },
        {
          "name": "max",
          "line": 242
        },
        {
          "name": "min",
          "line": 274
        },
        {
          "name": "logger.info",
          "line": 277
        },
        {
          "name": "time.time",
          "line": 303
        },
        {
          "name": "str",
          "line": 259
        },
        {
          "name": "ProcessPoolExecutor",
          "line": 279
        },
        {
          "name": "as_completed",
          "line": 284
        },
        {
          "name": "multiprocessing.cpu_count",
          "line": 242
        },
        {
          "name": "isinstance",
          "line": 259
        },
        {
          "name": "executor.submit",
          "line": 281
        },
        {
          "name": "len",
          "line": 304
        },
        {
          "name": "int",
          "line": 281
        },
        {
          "name": "future.result",
          "line": 287
        },
        {
          "name": "logger.info",
          "line": 298
        },
        {
          "name": "logger.error",
          "line": 291
        },
        {
          "name": "time.time",
          "line": 295
        }
      ],
      "docstring": "\n    Extract patterns in parallel for any dataset type.\n    \n    Args:\n        samples: List of sample indices\n        dataset: Dataset object\n        device: Computation device\n        max_workers: Maximum number of worker processes\n        \n    Returns:\n        Dictionary mapping indices to patterns\n    ",
      "code_snippet": "\n\ndef extract_patterns_parallel(samples, dataset, device=None, max_workers=None):\n    \"\"\"\n    Extract patterns in parallel for any dataset type.\n    \n    Args:\n        samples: List of sample indices\n        dataset: Dataset object\n        device: Computation device\n        max_workers: Maximum number of worker processes\n        \n    Returns:\n        Dictionary mapping indices to patterns\n    \"\"\"\n    # Determine optimal number of workers\n    if max_workers is None:\n        max_workers = max(1, multiprocessing.cpu_count() - 1)\n    \n    # Determine dataset type\n    dataset_type = determine_dataset_type(dataset)\n    logger.info(f\"Detected dataset type: {dataset_type}\")\n    \n    # Get appropriate extractor\n    extractor = get_extractor_for_dataset(dataset_type)\n    logger.info(f\"Using extractor: {extractor.__name__}\")\n    \n    # Create worker function with fixed parameters\n    worker_fn = partial(process_sample_worker, \n                       dataset=dataset, \n                       extractor=extractor, \n                       device=device)\n    \n    # Convert all indices to strings for consistent handling\n    samples = [str(idx) if not isinstance(idx, str) else idx for idx in samples]\n    \n    # Process in parallel\n    results = {}\n    total_samples = len(samples)\n    processed = 0\n    start_time = time.time()\n    \n    logger.info(f\"Starting parallel extraction with {max_workers} workers for {total_samples} samples\")\n    \n    # Use smaller chunk sizes for better progress reporting\n    chunk_size = min(1000, total_samples // (max_workers * 2) or 1)\n    \n    # Process in chunks to avoid memory issues with very large datasets\n    for chunk_start in range(0, total_samples, chunk_size):\n        chunk_end = min(chunk_start + chunk_size, total_samples)\n        chunk = samples[chunk_start:chunk_end]\n        \n        logger.info(f\"Processing chunk {chunk_start}-{chunk_end} of {total_samples}\")\n        \n        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n            # Submit all tasks\n            future_to_idx = {executor.submit(worker_fn, int(idx)): idx for idx in chunk}\n            \n            # Process results as they complete\n            for future in as_completed(future_to_idx):\n                idx = future_to_idx[future]\n                try:\n                    sample_idx, pattern = future.result()\n                    if pattern:\n                        results[sample_idx] = pattern\n                except Exception as e:\n                    logger.error(f\"Exception processing sample {idx}: {e}\")\n                \n                processed += 1\n                if processed % 100 == 0:\n                    elapsed = time.time() - start_time\n                    rate = processed / elapsed if elapsed > 0 else 0\n                    remaining = (total_samples - processed) / rate if rate > 0 else 0\n                    logger.info(f\"Processed {processed}/{total_samples} examples | \"\n                              f\"Rate: {rate:.1f} samples/sec | \"\n                              f\"ETA: {remaining:.1f} sec\")\n    \n    # Final report\n    elapsed = time.time() - start_time\n    logger.info(f\"Parallel processing complete: {len(results)}/{total_samples} \"\n              f\"patterns extracted in {elapsed:.1f} seconds \"\n              f\"({total_samples/elapsed:.1f} samples/sec)\")\n    \n    return results"
    }
  },
  "constants": {}
}