{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\isekaizen\\pattern\\bias_testing.py",
  "imports": [
    {
      "name": "torch",
      "line": 10
    },
    {
      "name": "logging",
      "line": 11
    },
    {
      "name": "torch.nn",
      "line": 12
    },
    {
      "name": "torch.utils.data.DataLoader",
      "line": 13
    },
    {
      "name": "torch.utils.data.Subset",
      "line": 13
    },
    {
      "name": "typing.Dict",
      "line": 14
    },
    {
      "name": "typing.List",
      "line": 14
    },
    {
      "name": "typing.Optional",
      "line": 14
    },
    {
      "name": "typing.Tuple",
      "line": 14
    },
    {
      "name": "random",
      "line": 15
    },
    {
      "name": "numpy",
      "line": 16
    },
    {
      "name": "isekaizen.core.models.architecture.create_model",
      "line": 18
    },
    {
      "name": "isekaizen.pattern.data_loading.load_latest_pattern_map",
      "line": 19
    },
    {
      "name": "isekaizen.pattern.detection.PatternRecognitionService",
      "line": 48
    }
  ],
  "classes": {},
  "functions": {
    "create_pattern_test_datasets": {
      "start_line": 23,
      "end_line": 77,
      "parameters": [
        {
          "name": "full_dataset"
        },
        {
          "name": "pattern_map"
        },
        {
          "name": "samples_per_pattern"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "isinstance",
          "line": 36
        },
        {
          "name": "Subset",
          "line": 72
        },
        {
          "name": "logger.info",
          "line": 73
        },
        {
          "name": "isinstance",
          "line": 43
        },
        {
          "name": "logger.warning",
          "line": 54
        },
        {
          "name": "range",
          "line": 57
        },
        {
          "name": "logger.warning",
          "line": 68
        },
        {
          "name": "isinstance",
          "line": 46
        },
        {
          "name": "PatternRecognitionService",
          "line": 49
        },
        {
          "name": "range",
          "line": 51
        },
        {
          "name": "len",
          "line": 57
        },
        {
          "name": "random.choice",
          "line": 58
        },
        {
          "name": "len",
          "line": 51
        },
        {
          "name": "pattern_service.get_pattern_type",
          "line": 52
        },
        {
          "name": "sample_patterns.items",
          "line": 64
        },
        {
          "name": "len",
          "line": 73
        },
        {
          "name": "str",
          "line": 54
        }
      ],
      "docstring": "\n    Create test datasets for each pattern type.\n    \n    Args:\n        full_dataset: The complete dataset\n        pattern_map: Mapping of samples to pattern types\n        samples_per_pattern: Number of samples to use for each pattern test\n        \n    Returns:\n        Dictionary mapping pattern types to datasets\n    ",
      "code_snippet": "logger = logging.getLogger(__name__)\n\ndef create_pattern_test_datasets(full_dataset, pattern_map, samples_per_pattern=200):\n    \"\"\"\n    Create test datasets for each pattern type.\n    \n    Args:\n        full_dataset: The complete dataset\n        pattern_map: Mapping of samples to pattern types\n        samples_per_pattern: Number of samples to use for each pattern test\n        \n    Returns:\n        Dictionary mapping pattern types to datasets\n    \"\"\"\n    # Identify unique pattern types\n    if isinstance(pattern_map, dict) and 'pattern_types' in pattern_map:\n        pattern_types = pattern_map['pattern_types']\n    else:\n        pattern_types = ['structural', 'statistical', 'temporal']\n    \n    # Get pattern assignments for each sample\n    try:\n        if isinstance(pattern_map, dict) and 'sample_patterns' in pattern_map:\n            # Direct mapping from sample index to pattern type\n            sample_patterns = pattern_map['sample_patterns']\n        elif isinstance(pattern_map, dict) and 'pattern_distribution' in pattern_map:\n            # Need to query through a service\n            from isekaizen.pattern.detection import PatternRecognitionService\n            pattern_service = PatternRecognitionService(pattern_map)\n            sample_patterns = {}\n            for i in range(len(full_dataset)):\n                sample_patterns[i] = pattern_service.get_pattern_type(i)\n    except Exception as e:\n        logger.warning(f\"Error mapping patterns: {str(e)}. Using random assignment for testing.\")\n        # Fallback: random assignment for testing\n        sample_patterns = {}\n        for i in range(len(full_dataset)):\n            sample_patterns[i] = random.choice(pattern_types)\n    \n    # Create subsets for each pattern type\n    pattern_datasets = {}\n    for pattern_type in pattern_types:\n        # Get indices of samples with this pattern\n        indices = [i for i, p in sample_patterns.items() \n                  if p == pattern_type][:samples_per_pattern]\n        \n        if not indices:\n            logger.warning(f\"No samples found for pattern type: {pattern_type}\")\n            continue\n            \n        # Create dataset subset\n        pattern_datasets[pattern_type] = Subset(full_dataset, indices)\n        logger.info(f\"Created test dataset for {pattern_type} pattern: {len(indices)} samples\")\n        \n    return pattern_datasets\n\ndef test_model_pattern_bias(model, pattern_datasets, device=None):\n    \"\"\"\n    Evaluate a model's performance on different pattern types."
    },
    "test_model_pattern_bias": {
      "start_line": 77,
      "end_line": 139,
      "parameters": [
        {
          "name": "model"
        },
        {
          "name": "pattern_datasets"
        },
        {
          "name": "device"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "model.to",
          "line": 92
        },
        {
          "name": "model.eval",
          "line": 93
        },
        {
          "name": "pattern_datasets.items",
          "line": 97
        },
        {
          "name": "torch.device",
          "line": 90
        },
        {
          "name": "logger.info",
          "line": 98
        },
        {
          "name": "DataLoader",
          "line": 101
        },
        {
          "name": "nn.CrossEntropyLoss",
          "line": 108
        },
        {
          "name": "logger.info",
          "line": 135
        },
        {
          "name": "torch.no_grad",
          "line": 111
        },
        {
          "name": "float",
          "line": 127
        },
        {
          "name": "torch.cuda.is_available",
          "line": 90
        },
        {
          "name": "model",
          "line": 116
        },
        {
          "name": "criterion",
          "line": 117
        },
        {
          "name": "outputs.max",
          "line": 120
        },
        {
          "name": "....item",
          "line": 121
        },
        {
          "name": "targets.size",
          "line": 122
        },
        {
          "name": "inputs.to",
          "line": 113
        },
        {
          "name": "targets.to",
          "line": 113
        },
        {
          "name": "loss.item",
          "line": 123
        },
        {
          "name": "targets.size",
          "line": 123
        },
        {
          "name": "....sum",
          "line": 121
        },
        {
          "name": "predicted.eq",
          "line": 121
        }
      ],
      "docstring": "\n    Evaluate a model's performance on different pattern types.\n    \n    Args:\n        model: The model to evaluate\n        pattern_datasets: Dictionary mapping pattern types to datasets\n        device: Device to use for testing\n        \n    Returns:\n        Dictionary of performance metrics by pattern type\n    ",
      "code_snippet": "    return pattern_datasets\n\ndef test_model_pattern_bias(model, pattern_datasets, device=None):\n    \"\"\"\n    Evaluate a model's performance on different pattern types.\n    \n    Args:\n        model: The model to evaluate\n        pattern_datasets: Dictionary mapping pattern types to datasets\n        device: Device to use for testing\n        \n    Returns:\n        Dictionary of performance metrics by pattern type\n    \"\"\"\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    model.to(device)\n    model.eval()\n    \n    results = {}\n    \n    for pattern_type, dataset in pattern_datasets.items():\n        logger.info(f\"Testing model bias on {pattern_type} pattern...\")\n        \n        # Create data loader\n        loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=2)\n        \n        # Track metrics\n        correct = 0\n        total = 0\n        loss_sum = 0.0\n        \n        criterion = nn.CrossEntropyLoss()\n        \n        # Evaluate on this pattern type\n        with torch.no_grad():\n            for inputs, targets in loader:\n                inputs, targets = inputs.to(device), targets.to(device)\n                \n                # Forward pass\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n                \n                # Update metrics\n                _, predicted = outputs.max(1)\n                correct += predicted.eq(targets).sum().item()\n                total += targets.size(0)\n                loss_sum += loss.item() * targets.size(0)\n        \n        # Calculate final metrics\n        accuracy = 100.0 * correct / total if total > 0 else 0.0\n        avg_loss = loss_sum / total if total > 0 else float('inf')\n        \n        results[pattern_type] = {\n            'accuracy': accuracy,\n            'loss': avg_loss,\n            'sample_count': total\n        }\n        \n        logger.info(f\"  {pattern_type} - Accuracy: {accuracy:.2f}%, Loss: {avg_loss:.4f}\")\n    \n    return results\n\ndef normalize_bias_scores(bias_results):\n    \"\"\"\n    Convert raw bias results to normalized scores for augmentation weighting."
    },
    "normalize_bias_scores": {
      "start_line": 139,
      "end_line": 164,
      "parameters": [
        {
          "name": "bias_results"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "sum",
          "line": 153
        },
        {
          "name": "accuracies.values",
          "line": 153
        },
        {
          "name": "bias_results.items",
          "line": 150
        },
        {
          "name": "accuracies.items",
          "line": 157
        },
        {
          "name": "len",
          "line": 160
        },
        {
          "name": "accuracies.keys",
          "line": 160
        }
      ],
      "docstring": "\n    Convert raw bias results to normalized scores for augmentation weighting.\n    \n    Args:\n        bias_results: Dictionary of bias test results\n        \n    Returns:\n        Dictionary of normalized bias scores\n    ",
      "code_snippet": "    return results\n\ndef normalize_bias_scores(bias_results):\n    \"\"\"\n    Convert raw bias results to normalized scores for augmentation weighting.\n    \n    Args:\n        bias_results: Dictionary of bias test results\n        \n    Returns:\n        Dictionary of normalized bias scores\n    \"\"\"\n    # Extract accuracy values\n    accuracies = {pt: res['accuracy'] for pt, res in bias_results.items()}\n    \n    # Calculate total\n    total_accuracy = sum(accuracies.values())\n    \n    # Normalize to get weights\n    if total_accuracy > 0:\n        normalized = {pt: acc / total_accuracy for pt, acc in accuracies.items()}\n    else:\n        # Equal weights if all accuracies are 0\n        normalized = {pt: 1.0 / len(accuracies) for pt in accuracies.keys()}\n        \n    return normalized\n\ndef test_pattern_bias(model_type=\"resnet18\", pretrained=True, dataset=None, \n                     pattern_map=None, batch_size=32, device=None):\n    \"\"\""
    },
    "test_pattern_bias": {
      "start_line": 164,
      "end_line": 219,
      "parameters": [
        {
          "name": "model_type"
        },
        {
          "name": "pretrained"
        },
        {
          "name": "dataset"
        },
        {
          "name": "pattern_map"
        },
        {
          "name": "batch_size"
        },
        {
          "name": "device"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "create_model",
          "line": 194
        },
        {
          "name": "model.to",
          "line": 201
        },
        {
          "name": "create_pattern_test_datasets",
          "line": 204
        },
        {
          "name": "test_model_pattern_bias",
          "line": 207
        },
        {
          "name": "normalize_bias_scores",
          "line": 210
        },
        {
          "name": "logger.info",
          "line": 213
        },
        {
          "name": "bias_scores.items",
          "line": 214
        },
        {
          "name": "torch.device",
          "line": 182
        },
        {
          "name": "load_latest_pattern_map",
          "line": 186
        },
        {
          "name": "logger.info",
          "line": 215
        },
        {
          "name": "logger.warning",
          "line": 188
        },
        {
          "name": "torch.cuda.is_available",
          "line": 182
        }
      ],
      "docstring": "\n    Complete pipeline for testing a model's pattern bias.\n    \n    Args:\n        model_type: Type of model to test\n        pretrained: Whether to use pretrained weights\n        dataset: Dataset to use for testing\n        pattern_map: Pattern mapping for the dataset\n        batch_size: Batch size for testing\n        device: Device to use for testing\n        \n    Returns:\n        Dictionary of normalized bias scores by pattern type\n    ",
      "code_snippet": "    return normalized\n\ndef test_pattern_bias(model_type=\"resnet18\", pretrained=True, dataset=None, \n                     pattern_map=None, batch_size=32, device=None):\n    \"\"\"\n    Complete pipeline for testing a model's pattern bias.\n    \n    Args:\n        model_type: Type of model to test\n        pretrained: Whether to use pretrained weights\n        dataset: Dataset to use for testing\n        pattern_map: Pattern mapping for the dataset\n        batch_size: Batch size for testing\n        device: Device to use for testing\n        \n    Returns:\n        Dictionary of normalized bias scores by pattern type\n    \"\"\"\n    # Auto-detect device if not provided\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Load pattern map if not provided\n    if pattern_map is None:\n        pattern_map = load_latest_pattern_map()\n        if not pattern_map:\n            logger.warning(\"No pattern map found. Using default pattern types.\")\n            pattern_map = {\n                'pattern_types': ['structural', 'statistical', 'temporal']\n            }\n    \n    # Create model\n    model = create_model(\n        model_type=model_type,\n        use_pretrained=pretrained,\n        num_classes=10,  # Assuming CIFAR10, adjust if needed\n        input_channels=3,\n        input_size=32\n    )\n    model = model.to(device)\n    \n    # Create pattern test datasets\n    pattern_datasets = create_pattern_test_datasets(dataset, pattern_map)\n    \n    # Test model on each pattern type\n    bias_results = test_model_pattern_bias(model, pattern_datasets, device)\n    \n    # Normalize results to get bias scores\n    bias_scores = normalize_bias_scores(bias_results)\n    \n    # Log results\n    logger.info(\"Pattern bias test results:\")\n    for pattern_type, score in bias_scores.items():\n        logger.info(f\"  {pattern_type}: {score:.4f}\")\n    \n    return bias_scores, bias_results"
    }
  },
  "constants": {}
}