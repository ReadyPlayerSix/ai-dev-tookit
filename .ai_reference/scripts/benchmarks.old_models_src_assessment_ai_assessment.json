{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\benchmarks.old\\models\\src\\assessment\\ai_assessment.py",
  "imports": [
    {
      "name": "knowledge_probe.KnowledgeProbe",
      "line": 4
    },
    {
      "name": "typing.Dict",
      "line": 5
    },
    {
      "name": "typing.List",
      "line": 5
    },
    {
      "name": "typing.Any",
      "line": 5
    },
    {
      "name": "typing.Optional",
      "line": 5
    },
    {
      "name": "dataclasses.dataclass",
      "line": 6
    },
    {
      "name": "datetime.datetime",
      "line": 7
    },
    {
      "name": "numpy",
      "line": 8
    },
    {
      "name": "json",
      "line": 9
    }
  ],
  "classes": {
    "StudyMaterial": {
      "start_line": 12,
      "end_line": 20,
      "methods": {},
      "class_variables": [],
      "bases": [],
      "docstring": "Represents a piece of study material"
    },
    "TestQuestion": {
      "start_line": 21,
      "end_line": 31,
      "methods": {},
      "class_variables": [],
      "bases": [],
      "docstring": "Represents a test question"
    },
    "AIEducationalAssessment": {
      "start_line": 31,
      "end_line": 358,
      "methods": {
        "__init__": {
          "start_line": 34,
          "end_line": 51,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "coordinator"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "KnowledgeProbe",
              "line": 36
            }
          ],
          "code_snippet": "    \"\"\"Framework for testing AI system comprehension and learning\"\"\"\n    \n    def __init__(self, coordinator):\n        self.coordinator = coordinator\n        self.probe = KnowledgeProbe(coordinator)\n        \n        # Track learning progression\n        self.study_history = []\n        self.test_results = []\n        self.concept_mastery = {}\n        \n        # Performance metrics\n        self.metrics = {\n            \"materials_studied\": 0,\n            \"tests_taken\": 0,\n            \"average_score\": 0.0,\n            \"concept_coverage\": 0.0,\n            \"learning_curve\": []\n        }\n    \n    async def provide_study_material(self, material: StudyMaterial) -> Dict[str, Any]:\n        \"\"\"Process study material through the system\"\"\""
        },
        "_grade_response": {
          "start_line": 157,
          "end_line": 183,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "system_response"
            },
            {
              "name": "correct_response"
            }
          ],
          "return_type": "float",
          "calls": [
            {
              "name": "self._compare_patterns",
              "line": 166
            },
            {
              "name": "scores.append",
              "line": 170
            },
            {
              "name": "self._compare_semantics",
              "line": 174
            },
            {
              "name": "scores.append",
              "line": 178
            },
            {
              "name": "np.mean",
              "line": 181
            },
            {
              "name": "correct_response.get",
              "line": 168
            },
            {
              "name": "correct_response.get",
              "line": 176
            }
          ],
          "docstring": "Grade a single response (returns score 0.0-1.0)",
          "code_snippet": "        return test_results\n    \n    def _grade_response(self, \n                       system_response: Dict[str, Any],\n                       correct_response: Dict[str, Any]) -> float:\n        \"\"\"Grade a single response (returns score 0.0-1.0)\"\"\"\n        # Initialize scoring components\n        scores = []\n        \n        # Compare pattern recognition\n        if \"pattern_result\" in system_response:\n            pattern_score = self._compare_patterns(\n                system_response[\"pattern_result\"],\n                correct_response.get(\"expected_patterns\", {})\n            )\n            scores.append(pattern_score)\n        \n        # Compare semantic understanding\n        if \"semantic_result\" in system_response:\n            semantic_score = self._compare_semantics(\n                system_response[\"semantic_result\"],\n                correct_response.get(\"expected_semantics\", {})\n            )\n            scores.append(semantic_score)\n        \n        # Return average score if we have any scores\n        return np.mean(scores) if scores else 0.0\n    \n    def _compare_patterns(self, \n                         system_patterns: Dict[str, Any],\n                         expected_patterns: Dict[str, Any]) -> float:"
        },
        "_compare_patterns": {
          "start_line": 183,
          "end_line": 202,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "system_patterns"
            },
            {
              "name": "expected_patterns"
            }
          ],
          "return_type": "float",
          "calls": [
            {
              "name": "np.mean",
              "line": 200
            },
            {
              "name": "self._calculate_similarity",
              "line": 194
            },
            {
              "name": "matches.append",
              "line": 198
            }
          ],
          "docstring": "Compare pattern recognition results",
          "code_snippet": "        return np.mean(scores) if scores else 0.0\n    \n    def _compare_patterns(self, \n                         system_patterns: Dict[str, Any],\n                         expected_patterns: Dict[str, Any]) -> float:\n        \"\"\"Compare pattern recognition results\"\"\"\n        if not (system_patterns and expected_patterns):\n            return 0.0\n            \n        # Compare key pattern attributes\n        matches = []\n        for key in expected_patterns:\n            if key in system_patterns:\n                similarity = self._calculate_similarity(\n                    system_patterns[key],\n                    expected_patterns[key]\n                )\n                matches.append(similarity)\n        \n        return np.mean(matches) if matches else 0.0\n    \n    def _compare_semantics(self,\n                          system_semantics: Dict[str, Any],\n                          expected_semantics: Dict[str, Any]) -> float:"
        },
        "_compare_semantics": {
          "start_line": 202,
          "end_line": 231,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "system_semantics"
            },
            {
              "name": "expected_semantics"
            }
          ],
          "return_type": "float",
          "calls": [
            {
              "name": "scores.append",
              "line": 215
            },
            {
              "name": "abs",
              "line": 223
            },
            {
              "name": "scores.append",
              "line": 227
            },
            {
              "name": "np.mean",
              "line": 229
            },
            {
              "name": "min",
              "line": 227
            }
          ],
          "docstring": "Compare semantic understanding results",
          "code_snippet": "        return np.mean(matches) if matches else 0.0\n    \n    def _compare_semantics(self,\n                          system_semantics: Dict[str, Any],\n                          expected_semantics: Dict[str, Any]) -> float:\n        \"\"\"Compare semantic understanding results\"\"\"\n        if not (system_semantics and expected_semantics):\n            return 0.0\n            \n        # Compare semantic attributes\n        scores = []\n        \n        # Check pattern type match\n        if (\"pattern_type\" in system_semantics and \n            \"pattern_type\" in expected_semantics):\n            scores.append(\n                1.0 if system_semantics[\"pattern_type\"] == expected_semantics[\"pattern_type\"]\n                else 0.0\n            )\n        \n        # Check confidence correlation\n        if (\"confidence\" in system_semantics and \n            \"confidence\" in expected_semantics):\n            confidence_diff = abs(\n                system_semantics[\"confidence\"] - \n                expected_semantics[\"confidence\"]\n            )\n            scores.append(1.0 - min(1.0, confidence_diff))\n        \n        return np.mean(scores) if scores else 0.0\n    \n    def _calculate_similarity(self, val1: Any, val2: Any) -> float:\n        \"\"\"Calculate similarity between two values\"\"\"\n        if isinstance(val1, (int, float)) and isinstance(val2, (int, float)):"
        },
        "_calculate_similarity": {
          "start_line": 231,
          "end_line": 250,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "val1",
              "type": "Any"
            },
            {
              "name": "val2",
              "type": "Any"
            }
          ],
          "return_type": "float",
          "calls": [
            {
              "name": "isinstance",
              "line": 233
            },
            {
              "name": "isinstance",
              "line": 233
            },
            {
              "name": "max",
              "line": 235
            },
            {
              "name": "abs",
              "line": 235
            },
            {
              "name": "abs",
              "line": 235
            },
            {
              "name": "isinstance",
              "line": 237
            },
            {
              "name": "isinstance",
              "line": 237
            },
            {
              "name": "isinstance",
              "line": 240
            },
            {
              "name": "isinstance",
              "line": 240
            },
            {
              "name": "abs",
              "line": 236
            },
            {
              "name": "len",
              "line": 242
            },
            {
              "name": "len",
              "line": 242
            },
            {
              "name": "np.mean",
              "line": 247
            },
            {
              "name": "self._calculate_similarity",
              "line": 244
            },
            {
              "name": "zip",
              "line": 245
            }
          ],
          "docstring": "Calculate similarity between two values",
          "code_snippet": "        return np.mean(scores) if scores else 0.0\n    \n    def _calculate_similarity(self, val1: Any, val2: Any) -> float:\n        \"\"\"Calculate similarity between two values\"\"\"\n        if isinstance(val1, (int, float)) and isinstance(val2, (int, float)):\n            # Numeric comparison\n            max_val = max(abs(val1), abs(val2))\n            return 1.0 - (abs(val1 - val2) / max_val) if max_val > 0 else 1.0\n        elif isinstance(val1, str) and isinstance(val2, str):\n            # String comparison (exact match)\n            return 1.0 if val1 == val2 else 0.0\n        elif isinstance(val1, (list, tuple)) and isinstance(val2, (list, tuple)):\n            # Sequence comparison\n            if len(val1) == len(val2):\n                similarities = [\n                    self._calculate_similarity(v1, v2)\n                    for v1, v2 in zip(val1, val2)\n                ]\n                return np.mean(similarities)\n        return 0.0\n    \n    def _measure_learning_impact(self,\n                               start_state: Dict[str, Any],\n                               end_state: Dict[str, Any]) -> Dict[str, float]:"
        },
        "_measure_learning_impact": {
          "start_line": 250,
          "end_line": 267,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "start_state"
            },
            {
              "name": "end_state"
            }
          ],
          "return_type": "complex_type",
          "calls": [],
          "docstring": "Measure the learning impact between two states",
          "code_snippet": "        return 0.0\n    \n    def _measure_learning_impact(self,\n                               start_state: Dict[str, Any],\n                               end_state: Dict[str, Any]) -> Dict[str, float]:\n        \"\"\"Measure the learning impact between two states\"\"\"\n        return {\n            \"pattern_growth\": (\n                end_state[\"summary\"][\"total_patterns_recognized\"] -\n                start_state[\"summary\"][\"total_patterns_recognized\"]\n            ),\n            \"complexity_change\": (\n                end_state[\"complexity_stats\"][\"avg_complexity\"] -\n                start_state[\"complexity_stats\"][\"avg_complexity\"]\n            ),\n            \"confidence_change\": (\n                end_state[\"pattern_preservation\"][\"avg_preservation\"] -\n                start_state[\"pattern_preservation\"][\"avg_preservation\"]\n            )\n        }\n    \n    def _update_concept_mastery(self, "
        },
        "_update_concept_mastery": {
          "start_line": 269,
          "end_line": 287,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "concepts"
            },
            {
              "name": "learning_impact"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "np.mean",
              "line": 273
            },
            {
              "name": "self.concept_mastery.get",
              "line": 280
            },
            {
              "name": "min",
              "line": 283
            }
          ],
          "docstring": "Update concept mastery based on learning impact",
          "code_snippet": "        }\n    \n    def _update_concept_mastery(self, \n                              concepts: List[str],\n                              learning_impact: Dict[str, float]):\n        \"\"\"Update concept mastery based on learning impact\"\"\"\n        impact_score = np.mean([\n            learning_impact[\"pattern_growth\"],\n            learning_impact[\"complexity_change\"],\n            learning_impact[\"confidence_change\"]\n        ])\n        \n        for concept in concepts:\n            current_mastery = self.concept_mastery.get(concept, 0.0)\n            # Apply diminishing returns on learning\n            learning_rate = 1.0 - current_mastery\n            self.concept_mastery[concept] = min(\n                1.0,\n                current_mastery + (impact_score * learning_rate)\n            )\n    \n    def generate_progress_report(self) -> Dict[str, Any]:\n        \"\"\"Generate a comprehensive progress report\"\"\""
        },
        "generate_progress_report": {
          "start_line": 288,
          "end_line": 307,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": "complex_type",
          "calls": [
            {
              "name": "....isoformat",
              "line": 291
            },
            {
              "name": "self.probe.generate_knowledge_report",
              "line": 296
            },
            {
              "name": "datetime.now",
              "line": 291
            },
            {
              "name": "sum",
              "line": 300
            },
            {
              "name": "self.concept_mastery.items",
              "line": 305
            }
          ],
          "docstring": "Generate a comprehensive progress report",
          "code_snippet": "            )\n    \n    def generate_progress_report(self) -> Dict[str, Any]:\n        \"\"\"Generate a comprehensive progress report\"\"\"\n        return {\n            \"timestamp\": datetime.now().isoformat(),\n            \"metrics\": self.metrics,\n            \"concept_mastery\": self.concept_mastery,\n            \"recent_test_results\": self.test_results[-5:],\n            \"learning_curve\": self.metrics[\"learning_curve\"],\n            \"current_knowledge_state\": self.probe.generate_knowledge_report(),\n            \"study_effectiveness\": {\n                concept: {\n                    \"mastery\": mastery,\n                    \"study_count\": sum(\n                        1 for session in self.study_history\n                        if concept in session[\"material\"][\"concepts\"]  # Fixed: Access concepts through dictionary\n                    )\n                }\n                for concept, mastery in self.concept_mastery.items()\n            }\n        }\n    \n    async def validate_response(self, question: TestQuestion, response: dict) -> float:"
        },
        "calculate_points": {
          "start_line": 354,
          "end_line": 358,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "question",
              "type": "TestQuestion"
            },
            {
              "name": "score",
              "type": "float"
            }
          ],
          "return_type": "float",
          "calls": [],
          "docstring": "Calculate points earned based on score percentage",
          "code_snippet": "        return score\n    \n    def calculate_points(self, question: TestQuestion, score: float) -> float:\n        \"\"\"Calculate points earned based on score percentage\"\"\"\n        return question.points * score"
        }
      },
      "class_variables": [],
      "bases": [],
      "docstring": "Framework for testing AI system comprehension and learning"
    }
  },
  "functions": {},
  "constants": {}
}