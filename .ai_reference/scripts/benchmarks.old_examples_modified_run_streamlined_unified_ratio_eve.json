{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\benchmarks.old\\examples\\modified\\run_streamlined_unified_ratio_eve.py",
  "imports": [
    {
      "name": "os",
      "line": 47
    },
    {
      "name": "sys",
      "line": 48
    },
    {
      "name": "time",
      "line": 49
    },
    {
      "name": "logging",
      "line": 50
    },
    {
      "name": "argparse",
      "line": 51
    },
    {
      "name": "json",
      "line": 52
    },
    {
      "name": "torch",
      "line": 53
    },
    {
      "name": "torch.nn",
      "line": 54
    },
    {
      "name": "torch.optim",
      "line": 55
    },
    {
      "name": "torchvision",
      "line": 56
    },
    {
      "name": "torchvision.transforms",
      "line": 57
    },
    {
      "name": "torchvision.models",
      "line": 58
    },
    {
      "name": "matplotlib.pyplot",
      "line": 59
    },
    {
      "name": "numpy",
      "line": 60
    },
    {
      "name": "datetime.datetime",
      "line": 61
    },
    {
      "name": "shutil",
      "line": 62
    },
    {
      "name": "math",
      "line": 63
    },
    {
      "name": "isekaizen.trainer.adaptive_trainer.AdaptiveTrainer",
      "line": 217
    },
    {
      "name": "isekaizen.pattern.data_loading.load_latest_pattern_map",
      "line": 218
    },
    {
      "name": "isekaizen.core.optimizer.enhanced_pattern_responsive.EnhancedPatternResponsiveOptimizer",
      "line": 219
    },
    {
      "name": "isekaizen.optimizers.eve.EVENaturalWeights",
      "line": 220
    },
    {
      "name": "isekaizen.optimizers.eve_unified_ratio.EVEUnifiedRatio",
      "line": 221
    },
    {
      "name": "isekaizen.optimizers.lr_boundary.LRBoundaryCalculator",
      "line": 222
    },
    {
      "name": "isekaizen.mediators.augmentation.AugmentationMediator",
      "line": 223
    },
    {
      "name": "isekaizen.data.augmented_dataset.AugmentedDataset",
      "line": 224
    },
    {
      "name": "optimizer_utils.configure_optimizer",
      "line": 228
    },
    {
      "name": "optimizer_utils.print_available_optimizers",
      "line": 228
    },
    {
      "name": "optimizer_configs.get_optimizer_config",
      "line": 229
    },
    {
      "name": "optimizer_configs.ALL_CONFIGS",
      "line": 229
    },
    {
      "name": "isekaizen.core.optimizer.pattern_risk_accuracy_tracker.PatternRiskAccuracyTracker",
      "line": 2126
    },
    {
      "name": "multiprocessing",
      "line": 1435
    },
    {
      "name": "psutil",
      "line": 1436
    },
    {
      "name": "math",
      "line": 1437
    },
    {
      "name": "isekaizen.utils.pattern_map_utils.translate_pattern_map_to_standard_format",
      "line": 2251
    },
    {
      "name": "isekaizen.pattern.detection.PatternRecognitionService",
      "line": 347
    },
    {
      "name": "lazy_augmentation",
      "line": 393
    },
    {
      "name": "traceback",
      "line": 2583
    },
    {
      "name": "multiprocessing",
      "line": 1153
    },
    {
      "name": "psutil",
      "line": 1154
    },
    {
      "name": "math",
      "line": 1155
    },
    {
      "name": "multiprocessing",
      "line": 1364
    },
    {
      "name": "psutil",
      "line": 1365
    },
    {
      "name": "math",
      "line": 1366
    },
    {
      "name": "multiprocessing",
      "line": 1282
    },
    {
      "name": "psutil",
      "line": 1283
    },
    {
      "name": "math",
      "line": 1284
    }
  ],
  "classes": {
    "PatternDataMediator": {
      "start_line": 66,
      "end_line": 213,
      "methods": {
        "__init__": {
          "start_line": 72,
          "end_line": 83,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [],
          "code_snippet": "    \"\"\"\n    \n    def __init__(self):\n        # Raw data storage - organized by epoch for better management\n        self.batch_data_by_epoch = {}  # {epoch: [(batch_indices, correct_mask), ...]}\n        \n        # Processed metrics cache - also organized by epoch\n        self.metrics_by_epoch = {}  # {epoch: {'accuracies': {}, 'risks': {}}}\n        \n        # Current state\n        self.current_epoch = 0\n        self.pattern_service = None\n    \n    def set_pattern_service(self, pattern_service):\n        \"\"\"Set the pattern service reference for mapping indices to pattern types.\"\"\"\n        self.pattern_service = pattern_service"
        },
        "set_pattern_service": {
          "start_line": 83,
          "end_line": 87,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "pattern_service"
            }
          ],
          "return_type": null,
          "calls": [],
          "docstring": "Set the pattern service reference for mapping indices to pattern types.",
          "code_snippet": "        self.pattern_service = None\n    \n    def set_pattern_service(self, pattern_service):\n        \"\"\"Set the pattern service reference for mapping indices to pattern types.\"\"\"\n        self.pattern_service = pattern_service\n    \n    def update_with_batch_recognition(self, batch_indices, correct_mask, epoch):\n        \"\"\"Store raw batch recognition data from isekaiZen with epoch tracking.\"\"\"\n        # Update current epoch"
        },
        "update_with_batch_recognition": {
          "start_line": 87,
          "end_line": 104,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "batch_indices"
            },
            {
              "name": "correct_mask"
            },
            {
              "name": "epoch"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "....append",
              "line": 98
            },
            {
              "name": "logger.debug",
              "line": 99
            },
            {
              "name": "self._cleanup_old_epochs",
              "line": 102
            },
            {
              "name": "logger.debug",
              "line": 95
            },
            {
              "name": "len",
              "line": 99
            }
          ],
          "docstring": "Store raw batch recognition data from isekaiZen with epoch tracking.",
          "code_snippet": "        self.pattern_service = pattern_service\n    \n    def update_with_batch_recognition(self, batch_indices, correct_mask, epoch):\n        \"\"\"Store raw batch recognition data from isekaiZen with epoch tracking.\"\"\"\n        # Update current epoch\n        self.current_epoch = epoch\n        \n        # Initialize epoch data if needed\n        if epoch not in self.batch_data_by_epoch:\n            self.batch_data_by_epoch[epoch] = []\n            logger.debug(f\"PatternDataMediator: Initialized data storage for epoch {epoch}\")\n        \n        # Store the batch data\n        self.batch_data_by_epoch[epoch].append((batch_indices, correct_mask))\n        logger.debug(f\"PatternDataMediator: Added batch with {len(batch_indices)} examples for epoch {epoch}\")\n        \n        # Clean up old epochs - keep only current and previous\n        self._cleanup_old_epochs()\n    \n    def _process_epoch_data(self, epoch):\n        \"\"\"Process data for a specific epoch.\"\"\"\n        if not self.pattern_service or epoch not in self.batch_data_by_epoch:"
        },
        "_process_epoch_data": {
          "start_line": 104,
          "end_line": 159,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "epoch"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logger.info",
              "line": 110
            },
            {
              "name": "pattern_total.items",
              "line": 136
            },
            {
              "name": "logger.info",
              "line": 150
            },
            {
              "name": "accuracies.items",
              "line": 151
            },
            {
              "name": "logger.warning",
              "line": 107
            },
            {
              "name": "enumerate",
              "line": 119
            },
            {
              "name": "logger.info",
              "line": 152
            },
            {
              "name": "self.pattern_service.get_pattern_type",
              "line": 120
            },
            {
              "name": "max",
              "line": 140
            },
            {
              "name": "min",
              "line": 140
            },
            {
              "name": "risks.get",
              "line": 152
            },
            {
              "name": "pattern_total.get",
              "line": 152
            },
            {
              "name": "len",
              "line": 130
            }
          ],
          "docstring": "Process data for a specific epoch.",
          "code_snippet": "        self._cleanup_old_epochs()\n    \n    def _process_epoch_data(self, epoch):\n        \"\"\"Process data for a specific epoch.\"\"\"\n        if not self.pattern_service or epoch not in self.batch_data_by_epoch:\n            logger.warning(f\"PatternDataMediator: Cannot process epoch {epoch} - pattern service not available or no data\")\n            return False\n            \n        logger.info(f\"PatternDataMediator: Processing data for epoch {epoch}\")\n        \n        # Initialize counters\n        pattern_correct = {}\n        pattern_total = {}\n        \n        # Process all batch data for this epoch\n        for batch_indices, correct_mask in self.batch_data_by_epoch[epoch]:\n            # Map batch indices to pattern types\n            for i, idx in enumerate(batch_indices):\n                pattern_type = self.pattern_service.get_pattern_type(idx)\n                \n                if pattern_type:\n                    # Initialize counters if needed\n                    if pattern_type not in pattern_total:\n                        pattern_total[pattern_type] = 0\n                        pattern_correct[pattern_type] = 0\n                    \n                    # Update counters\n                    pattern_total[pattern_type] += 1\n                    if i < len(correct_mask) and correct_mask[i]:\n                        pattern_correct[pattern_type] += 1\n        \n        # Calculate accuracies and risks\n        accuracies = {}\n        risks = {}\n        for pattern_type, total in pattern_total.items():\n            if total > 0:\n                accuracy = pattern_correct[pattern_type] / total\n                accuracies[pattern_type] = accuracy\n                risks[pattern_type] = max(0.1, min(0.9, 1.0 - accuracy))\n        \n        # Store the processed metrics\n        self.metrics_by_epoch[epoch] = {\n            'accuracies': accuracies,\n            'risks': risks,\n            'processed': True\n        }\n        \n        # Log the results\n        logger.info(f\"PatternDataMediator: Processed epoch {epoch} data:\")\n        for pattern_type, accuracy in accuracies.items():\n            logger.info(f\"  Pattern '{pattern_type}': accuracy={accuracy:.4f}, risk={risks.get(pattern_type, 0):.4f}, count={pattern_total.get(pattern_type, 0)}\")\n        \n        # Clear the raw data to save memory\n        self.batch_data_by_epoch[epoch] = [('processed', True)]\n        \n        return True\n    \n    def _cleanup_old_epochs(self):\n        \"\"\"Remove data from epochs except current and previous.\"\"\"\n        # Keep only current and previous epoch"
        },
        "_cleanup_old_epochs": {
          "start_line": 159,
          "end_line": 174,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "list",
              "line": 165
            },
            {
              "name": "list",
              "line": 170
            },
            {
              "name": "sorted",
              "line": 162
            },
            {
              "name": "self.batch_data_by_epoch.keys",
              "line": 165
            },
            {
              "name": "self.metrics_by_epoch.keys",
              "line": 170
            },
            {
              "name": "self.batch_data_by_epoch.keys",
              "line": 162
            }
          ],
          "docstring": "Remove data from epochs except current and previous.",
          "code_snippet": "        return True\n    \n    def _cleanup_old_epochs(self):\n        \"\"\"Remove data from epochs except current and previous.\"\"\"\n        # Keep only current and previous epoch\n        epochs_to_keep = sorted(self.batch_data_by_epoch.keys(), reverse=True)[:2]\n        \n        # Remove older epochs\n        for epoch in list(self.batch_data_by_epoch.keys()):\n            if epoch not in epochs_to_keep:\n                del self.batch_data_by_epoch[epoch]\n        \n        # Also clean up metrics\n        for epoch in list(self.metrics_by_epoch.keys()):\n            if epoch not in epochs_to_keep:\n                del self.metrics_by_epoch[epoch]\n    \n    def get_pattern_accuracies(self, epoch=None, force_recalculate=False):\n        \"\"\"Get pattern accuracies for the specified epoch (defaults to current).\"\"\"\n        epoch = self.current_epoch if epoch is None else epoch"
        },
        "get_pattern_accuracies": {
          "start_line": 174,
          "end_line": 188,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "epoch"
            },
            {
              "name": "force_recalculate"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self._process_epoch_data",
              "line": 181
            }
          ],
          "docstring": "Get pattern accuracies for the specified epoch (defaults to current).",
          "code_snippet": "                del self.metrics_by_epoch[epoch]\n    \n    def get_pattern_accuracies(self, epoch=None, force_recalculate=False):\n        \"\"\"Get pattern accuracies for the specified epoch (defaults to current).\"\"\"\n        epoch = self.current_epoch if epoch is None else epoch\n        \n        # Check if we need to process the data\n        if force_recalculate or epoch not in self.metrics_by_epoch:\n            # We need to process this epoch\n            self._process_epoch_data(epoch)\n        \n        # Return the metrics (empty dict if not available)\n        if epoch in self.metrics_by_epoch:\n            return self.metrics_by_epoch[epoch]['accuracies']\n        return {}\n    \n    def get_pattern_risks(self, epoch=None, force_recalculate=False):\n        \"\"\"Get pattern risks for the specified epoch.\"\"\"\n        epoch = self.current_epoch if epoch is None else epoch"
        },
        "get_pattern_risks": {
          "start_line": 188,
          "end_line": 200,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "epoch"
            },
            {
              "name": "force_recalculate"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.get_pattern_accuracies",
              "line": 193
            }
          ],
          "docstring": "Get pattern risks for the specified epoch.",
          "code_snippet": "        return {}\n    \n    def get_pattern_risks(self, epoch=None, force_recalculate=False):\n        \"\"\"Get pattern risks for the specified epoch.\"\"\"\n        epoch = self.current_epoch if epoch is None else epoch\n        \n        # Ensure metrics are calculated\n        self.get_pattern_accuracies(epoch, force_recalculate)\n        \n        # Return the risks\n        if epoch in self.metrics_by_epoch:\n            return self.metrics_by_epoch[epoch]['risks']\n        return {}\n    \n    def end_epoch(self, epoch):\n        \"\"\"Signal the end of an epoch to ensure all data is processed.\"\"\"\n        logger.info(f\"PatternDataMediator: End of epoch {epoch} signaled, processing remaining data\")"
        },
        "end_epoch": {
          "start_line": 200,
          "end_line": 213,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "epoch"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logger.info",
              "line": 202
            },
            {
              "name": "self._process_epoch_data",
              "line": 205
            },
            {
              "name": "self._cleanup_old_epochs",
              "line": 211
            }
          ],
          "docstring": "Signal the end of an epoch to ensure all data is processed.",
          "code_snippet": "        return {}\n    \n    def end_epoch(self, epoch):\n        \"\"\"Signal the end of an epoch to ensure all data is processed.\"\"\"\n        logger.info(f\"PatternDataMediator: End of epoch {epoch} signaled, processing remaining data\")\n        \n        # Process any remaining data for this epoch\n        self._process_epoch_data(epoch)\n        \n        # Move to the next epoch\n        self.current_epoch = epoch + 1\n        \n        # Clean up old epochs\n        self._cleanup_old_epochs()\n\n# Add parent directory to path to import isekaizen package\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))\n"
        }
      },
      "class_variables": [],
      "bases": [],
      "docstring": "\n    Mediator component that handles data transfer between isekaiZen pattern tracking \n    and EVE optimizer, with efficient caching and calculation management.\n    "
    },
    "UnifiedRatioTrainer": {
      "start_line": 298,
      "end_line": 1423,
      "methods": {
        "__init__": {
          "start_line": 304,
          "end_line": 389,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "model"
            },
            {
              "name": "criterion"
            },
            {
              "name": "optimizer_class"
            },
            {
              "name": "optimizer"
            },
            {
              "name": "optimizer_kwargs"
            },
            {
              "name": "scheduler_class"
            },
            {
              "name": "scheduler_kwargs"
            },
            {
              "name": "device"
            },
            {
              "name": "pattern_map"
            },
            {
              "name": "batch_optimizer_class"
            },
            {
              "name": "batch_optimizer_kwargs"
            },
            {
              "name": "val_dataset"
            },
            {
              "name": "pattern_mediator"
            },
            {
              "name": "pattern_service"
            },
            {
              "name": "use_augmentation"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "....__init__",
              "line": 327
            },
            {
              "name": "self._initialize_lazy_augmentation",
              "line": 375
            },
            {
              "name": "PatternRecognitionService",
              "line": 348
            },
            {
              "name": "logger.info",
              "line": 349
            },
            {
              "name": "logger.info",
              "line": 366
            },
            {
              "name": "hasattr",
              "line": 380
            },
            {
              "name": "logger.info",
              "line": 385
            },
            {
              "name": "super",
              "line": 327
            },
            {
              "name": "logger.warning",
              "line": 387
            }
          ],
          "docstring": "\n        Initialize the trainer with unified ratio tracking.\n        ",
          "code_snippet": "    \"\"\"\n    \n    def __init__(\n        self,\n        model,\n        criterion,\n        optimizer_class,\n        optimizer=None,\n        optimizer_kwargs=None,\n        scheduler_class=None,\n        scheduler_kwargs=None,\n        device=None,\n        pattern_map=None,\n        batch_optimizer_class=None,\n        batch_optimizer_kwargs=None,\n        val_dataset=None,\n        pattern_mediator=None,  # Accept pre-initialized mediator\n        pattern_service=None,   # Accept pre-initialized service\n        use_augmentation=True,  # Whether to use augmentation mediator\n        **kwargs\n    ):\n        \"\"\"\n        Initialize the trainer with unified ratio tracking.\n        \"\"\"\n        # Initialize base trainer\n        super().__init__(\n            model=model,\n            criterion=criterion,\n            optimizer_class=optimizer_class,\n            optimizer=optimizer,\n            optimizer_kwargs=optimizer_kwargs, \n            scheduler_class=scheduler_class,\n            scheduler_kwargs=scheduler_kwargs,\n            device=device,\n            pattern_map=pattern_map,\n            batch_optimizer_class=batch_optimizer_class,\n            batch_optimizer_kwargs=batch_optimizer_kwargs,\n            **kwargs\n        )\n        \n        # Use the optimizer's internal pattern mediator - no need to create our own\n        self.pattern_mediator = None\n        \n        # Initialize pattern service if we need it\n        if pattern_map:\n            from isekaizen.pattern.detection import PatternRecognitionService\n            self.pattern_service = PatternRecognitionService(pattern_map)\n            logger.info(\"PatternRecognitionService initialized for trainer\")\n\n        # We don't need to connect mediator to optimizer since it has its own internal mediator\n        \n        # Initialize dataset adaptations tracking\n        self.dataset_adaptations = []\n        \n        # Store validation dataset reference for mini-validation  \n        self.val_dataset = val_dataset\n        \n        # Current epoch tracking\n        self.current_epoch = 0\n        \n        # Initialize augmentation mediator if enabled\n        self.use_augmentation = use_augmentation\n        self.augmentation_mediator = None\n        if use_augmentation:\n            logger.info(\"Initializing AugmentationMediator\")\n            # Don't initialize the mediator yet, just store the parameters\n            self.augmentation_mediator = None\n            self.augmentation_params = {\n                'pattern_map': pattern_map,\n                'device': device\n            }\n            \n        # Initialize lazy augmentation support\n        self._initialize_lazy_augmentation()\n        \n        # Flag to track if using lazy augmentation\n        self.using_lazy_augmentation = (\n            use_augmentation and \n            hasattr(self, 'lazy_augmentation_available') and\n            self.lazy_augmentation_available\n        )\n        \n        if self.using_lazy_augmentation:\n            logger.info(\"Lazy pattern augmentation is enabled\")\n        elif use_augmentation:\n            logger.warning(\"Augmentation requested but lazy pattern augmentation not available\")\n    \n    def _initialize_lazy_augmentation(self):\n        \"\"\"Initialize lazy augmentation support.\"\"\"\n        try:"
        },
        "_initialize_lazy_augmentation": {
          "start_line": 389,
          "end_line": 407,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logger.info",
              "line": 402
            },
            {
              "name": "logger.warning",
              "line": 404
            },
            {
              "name": "str",
              "line": 404
            }
          ],
          "docstring": "Initialize lazy augmentation support.",
          "code_snippet": "            logger.warning(\"Augmentation requested but lazy pattern augmentation not available\")\n    \n    def _initialize_lazy_augmentation(self):\n        \"\"\"Initialize lazy augmentation support.\"\"\"\n        try:\n            # Import the lazy augmentation module\n            import lazy_augmentation\n            \n            # Store references to required classes and functions\n            self.LazyPatternAugmentedDataset = lazy_augmentation.LazyPatternAugmentedDataset\n            self.create_optimized_dataloader = lazy_augmentation.create_optimized_dataloader\n            self.create_lazy_augmented_dataset = lazy_augmentation.create_lazy_augmented_dataset\n            \n            # Flag that lazy augmentation is available\n            self.lazy_augmentation_available = True\n            logger.info(\"Lazy pattern augmentation is available\")\n        except ImportError as e:\n            logger.warning(f\"Could not import lazy augmentation module: {str(e)}\")\n            self.lazy_augmentation_available = False\n    \n    def create_lazy_augmented_dataset(self, base_dataset, augmentation_percentages=None):\n        \"\"\"\n        Create a lazy augmented dataset with on-demand pattern augmentation."
        },
        "create_lazy_augmented_dataset": {
          "start_line": 407,
          "end_line": 433,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "base_dataset"
            },
            {
              "name": "augmentation_percentages"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logger.warning",
              "line": 419
            },
            {
              "name": "self.create_lazy_augmented_dataset",
              "line": 423
            },
            {
              "name": "hasattr",
              "line": 418
            },
            {
              "name": "logger.error",
              "line": 430
            },
            {
              "name": "str",
              "line": 430
            }
          ],
          "docstring": "\n        Create a lazy augmented dataset with on-demand pattern augmentation.\n        \n        Args:\n            base_dataset: Original dataset\n            augmentation_percentages: Dict mapping pattern types to percentages\n            \n        Returns:\n            Augmented dataset or original dataset if not available\n        ",
          "code_snippet": "            self.lazy_augmentation_available = False\n    \n    def create_lazy_augmented_dataset(self, base_dataset, augmentation_percentages=None):\n        \"\"\"\n        Create a lazy augmented dataset with on-demand pattern augmentation.\n        \n        Args:\n            base_dataset: Original dataset\n            augmentation_percentages: Dict mapping pattern types to percentages\n            \n        Returns:\n            Augmented dataset or original dataset if not available\n        \"\"\"\n        if not hasattr(self, 'lazy_augmentation_available') or not self.lazy_augmentation_available:\n            logger.warning(\"Lazy augmentation not available, using original dataset\")\n            return base_dataset\n            \n        try:\n            return self.create_lazy_augmented_dataset(\n                base_dataset=base_dataset,\n                pattern_map=self.pattern_map,\n                augmentation_percentages=augmentation_percentages,\n                device=self.device\n            )\n        except Exception as e:\n            logger.error(f\"Error creating lazy augmented dataset: {str(e)}\")\n            return base_dataset\n            \n    def adapt_dataset(self, dataset):\n        \"\"\"\n        Add the following code to the adapt_dataset method."
        },
        "adapt_dataset": {
          "start_line": 433,
          "end_line": 484,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "dataset"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "hasattr",
              "line": 444
            },
            {
              "name": "logger.info",
              "line": 445
            },
            {
              "name": "pattern_risks.items",
              "line": 456
            },
            {
              "name": "hasattr",
              "line": 461
            },
            {
              "name": "hasattr",
              "line": 448
            },
            {
              "name": "self.pattern_tracker.get_pattern_risks",
              "line": 448
            },
            {
              "name": "hasattr",
              "line": 449
            },
            {
              "name": "self.optimizer.get_pattern_risks",
              "line": 450
            },
            {
              "name": "len",
              "line": 462
            },
            {
              "name": "augmentation_percentages.items",
              "line": 465
            },
            {
              "name": "len",
              "line": 471
            },
            {
              "name": "sum",
              "line": 457
            },
            {
              "name": "sum",
              "line": 457
            },
            {
              "name": "dataset.add_augmentations",
              "line": 467
            },
            {
              "name": "logger.info",
              "line": 468
            },
            {
              "name": "len",
              "line": 477
            },
            {
              "name": "pattern_risks.values",
              "line": 457
            },
            {
              "name": "pattern_risks.values",
              "line": 457
            }
          ],
          "docstring": "\n        Add the following code to the adapt_dataset method.\n        \n        Args:\n            dataset: Dataset to adapt\n            \n        Returns:\n            Adapted dataset and metrics\n        ",
          "code_snippet": "            return base_dataset\n            \n    def adapt_dataset(self, dataset):\n        \"\"\"\n        Add the following code to the adapt_dataset method.\n        \n        Args:\n            dataset: Dataset to adapt\n            \n        Returns:\n            Adapted dataset and metrics\n        \"\"\"\n        # If using lazy augmentation, add to existing dataset\n        if hasattr(self, 'using_lazy_augmentation') and self.using_lazy_augmentation:\n            logger.info(\"Using lazy augmentation for dataset adaptation\")\n            \n            # Get pattern risks\n            pattern_risks = self.pattern_tracker.get_pattern_risks() if hasattr(self, 'pattern_tracker') else {}\n            if not pattern_risks and hasattr(self.optimizer, 'get_pattern_risks'):\n                pattern_risks = self.optimizer.get_pattern_risks()\n            \n            # Calculate percentages based on train-test gap\n            train_test_gap = self.max_train_test_gap  # From batch optimizer\n            augmentation_percentages = {}\n            \n            for pattern_type, risk in pattern_risks.items():\n                risk_proportion = risk / sum(pattern_risks.values()) if sum(pattern_risks.values()) > 0 else 1.0\n                augmentation_percentages[pattern_type] = (train_test_gap / 100.0) * risk_proportion\n            \n            # Add augmentations if dataset supports it\n            if hasattr(dataset, 'add_augmentations'):\n                before_size = len(dataset)\n                \n                # Add augmentations for each pattern type\n                for pattern_type, percentage in augmentation_percentages.items():\n                    if percentage > 0:\n                        added = dataset.add_augmentations(pattern_type, percentage)\n                        logger.info(f\"Added {added} augmentations for {pattern_type}\")\n                \n                # Calculate total added\n                added_count = len(dataset) - before_size\n                \n                # Return adapted dataset and metrics\n                return dataset, {\n                    \"adapted\": True,\n                    \"examples_added\": added_count,\n                    \"total_size\": len(dataset),\n                    \"pattern_risks\": pattern_risks\n                }\n        \n        # Fall back to base class implementation\n        return dataset, {\"adapted\": False, \"reason\": \"Not supported\"}\n            \n    def adjust_batch_size_for_risk(self, calculated_batch, epoch=0):\n        \"\"\"\n        Adjust batch size based on unified risk/accuracy ratio."
        },
        "adjust_batch_size_for_risk": {
          "start_line": 484,
          "end_line": 555,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "calculated_batch"
            },
            {
              "name": "epoch"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "hasattr",
              "line": 508
            },
            {
              "name": "getattr",
              "line": 527
            },
            {
              "name": "int",
              "line": 540
            },
            {
              "name": "int",
              "line": 541
            },
            {
              "name": "max",
              "line": 542
            },
            {
              "name": "logger.info",
              "line": 552
            },
            {
              "name": "logger.info",
              "line": 497
            },
            {
              "name": "hasattr",
              "line": 502
            },
            {
              "name": "hasattr",
              "line": 502
            },
            {
              "name": "logger.info",
              "line": 503
            },
            {
              "name": "self.optimizer.pattern_mediator.end_epoch",
              "line": 505
            },
            {
              "name": "logger.info",
              "line": 509
            },
            {
              "name": "self.optimizer.calculate_risk_accuracy_ratios",
              "line": 510
            },
            {
              "name": "logger.warning",
              "line": 523
            },
            {
              "name": "min",
              "line": 533
            },
            {
              "name": "int",
              "line": 534
            },
            {
              "name": "min",
              "line": 536
            },
            {
              "name": "int",
              "line": 537
            },
            {
              "name": "min",
              "line": 542
            },
            {
              "name": "hasattr",
              "line": 545
            },
            {
              "name": "hasattr",
              "line": 545
            },
            {
              "name": "logger.info",
              "line": 546
            },
            {
              "name": "max",
              "line": 547
            },
            {
              "name": "all",
              "line": 513
            },
            {
              "name": "ratios.items",
              "line": 515
            },
            {
              "name": "logger.info",
              "line": 518
            },
            {
              "name": "logger.warning",
              "line": 520
            },
            {
              "name": "min",
              "line": 547
            },
            {
              "name": "logger.info",
              "line": 516
            },
            {
              "name": "sum",
              "line": 517
            },
            {
              "name": "len",
              "line": 517
            },
            {
              "name": "isinstance",
              "line": 513
            },
            {
              "name": "ratios.values",
              "line": 517
            },
            {
              "name": "ratios.values",
              "line": 513
            }
          ],
          "docstring": "\n        Adjust batch size based on unified risk/accuracy ratio.\n        \n        Args:\n            calculated_batch: Calculated base batch size\n            epoch: Current epoch number\n            \n        Returns:\n            Adjusted batch size\n        ",
          "code_snippet": "        return dataset, {\"adapted\": False, \"reason\": \"Not supported\"}\n            \n    def adjust_batch_size_for_risk(self, calculated_batch, epoch=0):\n        \"\"\"\n        Adjust batch size based on unified risk/accuracy ratio.\n        \n        Args:\n            calculated_batch: Calculated base batch size\n            epoch: Current epoch number\n            \n        Returns:\n            Adjusted batch size\n        \"\"\"\n        # For the first epoch, use the calculated batch size without adjustment\n        if epoch == 0:\n            logger.info(f\"First epoch: Using initial batch size {calculated_batch} without risk adjustment\")\n            return calculated_batch\n            \n        # Get the average risk/accuracy ratio across all patterns\n        # Force an update of the internal mediator if possible to ensure fresh data\n        if hasattr(self.optimizer, 'pattern_mediator') and hasattr(self.optimizer.pattern_mediator, 'end_epoch'):\n            logger.info(f\"Ensuring optimizer's pattern mediator has current data (epoch {epoch})\")\n            # Signal the mediator to process any pending data\n            self.optimizer.pattern_mediator.end_epoch(epoch - 1)  # Process previous epoch\n\n        # Request fresh calculation of ratios with force_refresh=True\n        if hasattr(self.optimizer, 'calculate_risk_accuracy_ratios'):\n            logger.info(\"Calculating risk/accuracy ratios with forced refresh\")\n            ratios = self.optimizer.calculate_risk_accuracy_ratios(force_refresh=True)\n            \n            # Verify we got valid ratios\n            if ratios and all(isinstance(v, (int, float)) for v in ratios.values()):\n                # Log all pattern ratios for debugging\n                for pattern_type, ratio in ratios.items():\n                    logger.info(f\"  Pattern '{pattern_type}' risk/accuracy ratio: {ratio:.4f}\")\n                avg_ratio = sum(ratios.values()) / len(ratios)\n                logger.info(f\"  Average risk/accuracy ratio: {avg_ratio:.4f}\")\n            else:\n                logger.warning(\"No valid risk/accuracy ratios available, using default ratio\")\n                avg_ratio = 1.0  # Default balanced ratio\n        else:\n            logger.warning(\"Optimizer does not support risk/accuracy ratio calculation\")\n            avg_ratio = 1.0\n        \n        # Store the current batch size for calculating adjustment limits\n        prev_batch_size = getattr(self, 'last_batch_size', calculated_batch)\n        \n        # Use the average ratio to adjust batch size\n        # Higher ratio (risk > accuracy) = smaller batch size\n        # Lower ratio (risk < accuracy) = larger batch size\n        if avg_ratio > 1.0:\n            reduction_factor = min(0.7, (avg_ratio - 1.0) * 0.8)\n            adjusted_batch = int(calculated_batch * (1.0 - reduction_factor))\n        else:\n            increase_factor = min(0.5, (1.0 - avg_ratio) * 0.6)\n            adjusted_batch = int(calculated_batch * (1.0 + increase_factor))\n        \n        # Apply stability constraints - limit changes to 20% per epoch\n        max_increase = int(prev_batch_size * 1.2)\n        min_decrease = int(prev_batch_size * 0.8)  # Allow dynamic adjustment to any size\n        adjusted_batch = max(min_decrease, min(max_increase, adjusted_batch))\n        \n        # Ensure batch size is within bounds\n        if hasattr(self.batch_optimizer, 'min_batch') and hasattr(self.batch_optimizer, 'max_batch'):\n            logger.info(f\"Batch size bounds: min={self.batch_optimizer.min_batch}, max={self.batch_optimizer.max_batch}\")\n            adjusted_batch = max(self.batch_optimizer.min_batch, min(self.batch_optimizer.max_batch, adjusted_batch))\n        \n        # Store for next time\n        self.last_batch_size = adjusted_batch\n        \n        logger.info(f\"Adjusted batch size based on risk/accuracy ratio {avg_ratio:.2f}: {calculated_batch} \u2192 {adjusted_batch}\")\n        return adjusted_batch\n    \n    def train(\n        self,\n        train_dataset,"
        },
        "train": {
          "start_line": 555,
          "end_line": 1124,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "train_dataset"
            },
            {
              "name": "val_dataset"
            },
            {
              "name": "epochs"
            },
            {
              "name": "early_stopping"
            },
            {
              "name": "patience"
            },
            {
              "name": "test_interval"
            },
            {
              "name": "checkpoint_interval"
            },
            {
              "name": "checkpoint_path"
            },
            {
              "name": "callbacks"
            },
            {
              "name": "mini_val_interval"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "float",
              "line": 601
            },
            {
              "name": "....append",
              "line": 605
            },
            {
              "name": "range",
              "line": 643
            },
            {
              "name": "logger.info",
              "line": 1121
            },
            {
              "name": "len",
              "line": 605
            },
            {
              "name": "hasattr",
              "line": 611
            },
            {
              "name": "logger.info",
              "line": 612
            },
            {
              "name": "self.create_lazy_augmented_dataset",
              "line": 622
            },
            {
              "name": "logger.info",
              "line": 627
            },
            {
              "name": "logger.info",
              "line": 631
            },
            {
              "name": "logger.info",
              "line": 651
            },
            {
              "name": "self.batch_optimizer.get_optimal_batch_size",
              "line": 745
            },
            {
              "name": "self.adjust_batch_size_for_risk",
              "line": 746
            },
            {
              "name": "....append",
              "line": 748
            },
            {
              "name": "....append",
              "line": 751
            },
            {
              "name": "logger.info",
              "line": 754
            },
            {
              "name": "logger.info",
              "line": 755
            },
            {
              "name": "logger.info",
              "line": 756
            },
            {
              "name": "getattr",
              "line": 760
            },
            {
              "name": "getattr",
              "line": 761
            },
            {
              "name": "max",
              "line": 771
            },
            {
              "name": "epoch_params.items",
              "line": 773
            },
            {
              "name": "time.time",
              "line": 780
            },
            {
              "name": "self._train_epoch",
              "line": 781
            },
            {
              "name": "torch.cuda.is_available",
              "line": 785
            },
            {
              "name": "hasattr",
              "line": 840
            },
            {
              "name": "logger.info",
              "line": 844
            },
            {
              "name": "logger.info",
              "line": 845
            },
            {
              "name": "logger.info",
              "line": 846
            },
            {
              "name": "max",
              "line": 857
            },
            {
              "name": "logger.info",
              "line": 860
            },
            {
              "name": "metrics.items",
              "line": 861
            },
            {
              "name": "logger.info",
              "line": 870
            },
            {
              "name": "hasattr",
              "line": 874
            },
            {
              "name": "....append",
              "line": 913
            },
            {
              "name": "....append",
              "line": 914
            },
            {
              "name": "....append",
              "line": 915
            },
            {
              "name": "....append",
              "line": 916
            },
            {
              "name": "hasattr",
              "line": 1103
            },
            {
              "name": "hasattr",
              "line": 616
            },
            {
              "name": "....keys",
              "line": 617
            },
            {
              "name": "AugmentationMediator",
              "line": 634
            },
            {
              "name": "globals",
              "line": 645
            },
            {
              "name": "print_progress_bar",
              "line": 647
            },
            {
              "name": "hasattr",
              "line": 654
            },
            {
              "name": "self.batch_optimizer.should_adapt_patterns",
              "line": 654
            },
            {
              "name": "logger.info",
              "line": 655
            },
            {
              "name": "self.batch_optimizer.adapt_dataset",
              "line": 656
            },
            {
              "name": "adaptation_metrics.get",
              "line": 658
            },
            {
              "name": "hasattr",
              "line": 683
            },
            {
              "name": "self.optimizer.get_pattern_risks",
              "line": 684
            },
            {
              "name": "len",
              "line": 751
            },
            {
              "name": "....center",
              "line": 755
            },
            {
              "name": "len",
              "line": 767
            },
            {
              "name": "logger.info",
              "line": 774
            },
            {
              "name": "time.time",
              "line": 782
            },
            {
              "name": "isinstance",
              "line": 791
            },
            {
              "name": "hasattr",
              "line": 791
            },
            {
              "name": "logger.info",
              "line": 792
            },
            {
              "name": "self.optimizer.pattern_mediator.end_epoch",
              "line": 793
            },
            {
              "name": "self.optimizer.pattern_mediator.get_pattern_accuracies",
              "line": 796
            },
            {
              "name": "self.optimizer.pattern_mediator.get_pattern_risks",
              "line": 797
            },
            {
              "name": "hasattr",
              "line": 815
            },
            {
              "name": "self.batch_optimizer.update_with_epoch_metrics",
              "line": 841
            },
            {
              "name": "....center",
              "line": 845
            },
            {
              "name": "isinstance",
              "line": 862
            },
            {
              "name": "logger.info",
              "line": 869
            },
            {
              "name": "self.optimizer.get_pattern_risks",
              "line": 876
            },
            {
              "name": "hasattr",
              "line": 903
            },
            {
              "name": "hasattr",
              "line": 903
            },
            {
              "name": "self.batch_optimizer.pattern_tracker.get_pattern_accuracies",
              "line": 904
            },
            {
              "name": "hasattr",
              "line": 921
            },
            {
              "name": "self._validate",
              "line": 927
            },
            {
              "name": "....append",
              "line": 928
            },
            {
              "name": "....append",
              "line": 929
            },
            {
              "name": "hasattr",
              "line": 936
            },
            {
              "name": "isinstance",
              "line": 943
            },
            {
              "name": "logger.info",
              "line": 1006
            },
            {
              "name": "logger.info",
              "line": 1007
            },
            {
              "name": "logger.info",
              "line": 1008
            },
            {
              "name": "max",
              "line": 1024
            },
            {
              "name": "logger.info",
              "line": 1027
            },
            {
              "name": "val_metrics_dict.items",
              "line": 1028
            },
            {
              "name": "logger.info",
              "line": 1037
            },
            {
              "name": "self.save_model",
              "line": 1096
            },
            {
              "name": "self.scheduler.step",
              "line": 1100
            },
            {
              "name": "self.batch_optimizer.hardware_analyzer.cleanup_memory",
              "line": 1104
            },
            {
              "name": "torch.cuda.is_available",
              "line": 1105
            },
            {
              "name": "history.get",
              "line": 646
            },
            {
              "name": "self.dataset_adaptations.append",
              "line": 665
            },
            {
              "name": "logger.info",
              "line": 670
            },
            {
              "name": "logger.info",
              "line": 671
            },
            {
              "name": "logger.info",
              "line": 675
            },
            {
              "name": "logger.info",
              "line": 678
            },
            {
              "name": "len",
              "line": 771
            },
            {
              "name": "torch.cuda.max_memory_allocated",
              "line": 786
            },
            {
              "name": "logger.info",
              "line": 799
            },
            {
              "name": "logger.info",
              "line": 800
            },
            {
              "name": "logger.info",
              "line": 801
            },
            {
              "name": "max",
              "line": 804
            },
            {
              "name": "logger.info",
              "line": 805
            },
            {
              "name": "logger.info",
              "line": 806
            },
            {
              "name": "pattern_accuracies.keys",
              "line": 809
            },
            {
              "name": "self.optimizer.calculate_risk_accuracy_ratios",
              "line": 816
            },
            {
              "name": "len",
              "line": 857
            },
            {
              "name": "str",
              "line": 868
            },
            {
              "name": "logger.info",
              "line": 878
            },
            {
              "name": "logger.info",
              "line": 879
            },
            {
              "name": "logger.info",
              "line": 880
            },
            {
              "name": "max",
              "line": 883
            },
            {
              "name": "logger.info",
              "line": 887
            },
            {
              "name": "logger.info",
              "line": 888
            },
            {
              "name": "pattern_risks.items",
              "line": 891
            },
            {
              "name": "logger.info",
              "line": 897
            },
            {
              "name": "....append",
              "line": 900
            },
            {
              "name": "logger.info",
              "line": 906
            },
            {
              "name": "pattern_accuracies.items",
              "line": 907
            },
            {
              "name": "....append",
              "line": 910
            },
            {
              "name": "self.batch_optimizer.hardware_analyzer.cleanup_memory",
              "line": 922
            },
            {
              "name": "torch.cuda.is_available",
              "line": 923
            },
            {
              "name": "self.batch_optimizer.update_with_epoch_metrics",
              "line": 937
            },
            {
              "name": "hasattr",
              "line": 945
            },
            {
              "name": "hasattr",
              "line": 968
            },
            {
              "name": "hasattr",
              "line": 971
            },
            {
              "name": "hasattr",
              "line": 975
            },
            {
              "name": "hasattr",
              "line": 993
            },
            {
              "name": "....center",
              "line": 1007
            },
            {
              "name": "len",
              "line": 1016
            },
            {
              "name": "isinstance",
              "line": 1029
            },
            {
              "name": "logger.info",
              "line": 1036
            },
            {
              "name": "isinstance",
              "line": 1040
            },
            {
              "name": "hasattr",
              "line": 1040
            },
            {
              "name": "self.optimizer.equilibrium_tracker.get_patterns_below_min",
              "line": 1041
            },
            {
              "name": "self.optimizer.equilibrium_tracker.get_patterns_above_max",
              "line": 1042
            },
            {
              "name": "hasattr",
              "line": 1045
            },
            {
              "name": "torch.cuda.empty_cache",
              "line": 1106
            },
            {
              "name": "torch.cuda.reset_peak_memory_stats",
              "line": 1107
            },
            {
              "name": "callback",
              "line": 1113
            },
            {
              "name": "logger.info",
              "line": 1118
            },
            {
              "name": "len",
              "line": 627
            },
            {
              "name": "logger.info",
              "line": 674
            },
            {
              "name": "isinstance",
              "line": 690
            },
            {
              "name": "logger.info",
              "line": 691
            },
            {
              "name": "high_risk_patterns.items",
              "line": 695
            },
            {
              "name": "str",
              "line": 771
            },
            {
              "name": "epoch_params.keys",
              "line": 771
            },
            {
              "name": "....center",
              "line": 800
            },
            {
              "name": "pattern_accuracies.get",
              "line": 810
            },
            {
              "name": "pattern_risks.get",
              "line": 811
            },
            {
              "name": "logger.info",
              "line": 812
            },
            {
              "name": "logger.info",
              "line": 820
            },
            {
              "name": "logger.info",
              "line": 823
            },
            {
              "name": "logger.info",
              "line": 824
            },
            {
              "name": "ratios.items",
              "line": 827
            },
            {
              "name": "str",
              "line": 857
            },
            {
              "name": "metrics.keys",
              "line": 857
            },
            {
              "name": "....center",
              "line": 879
            },
            {
              "name": "logger.info",
              "line": 895
            },
            {
              "name": "logger.info",
              "line": 908
            },
            {
              "name": "torch.cuda.empty_cache",
              "line": 924
            },
            {
              "name": "torch.cuda.reset_peak_memory_stats",
              "line": 925
            },
            {
              "name": "self.optimizer.update_accuracy_metrics_with_epoch",
              "line": 946
            },
            {
              "name": "hasattr",
              "line": 954
            },
            {
              "name": "self.optimizer.update_accuracy_metrics",
              "line": 965
            },
            {
              "name": "....append",
              "line": 969
            },
            {
              "name": "....append",
              "line": 972
            },
            {
              "name": "self.optimizer.get_pattern_weights",
              "line": 976
            },
            {
              "name": "self.optimizer.get_pattern_weight_decays",
              "line": 994
            },
            {
              "name": "len",
              "line": 1024
            },
            {
              "name": "str",
              "line": 1035
            },
            {
              "name": "self.optimizer.get_current_bounds",
              "line": 1046
            },
            {
              "name": "logger.info",
              "line": 1072
            },
            {
              "name": "logger.info",
              "line": 1073
            },
            {
              "name": "logger.info",
              "line": 1074
            },
            {
              "name": "self.save_model",
              "line": 1091
            },
            {
              "name": "logger.info",
              "line": 1092
            },
            {
              "name": "history.get",
              "line": 648
            },
            {
              "name": "pattern_risks.items",
              "line": 688
            },
            {
              "name": "min",
              "line": 697
            },
            {
              "name": "self.train_dataset.add_augmentations",
              "line": 700
            },
            {
              "name": "logger.info",
              "line": 703
            },
            {
              "name": "logger.info",
              "line": 706
            },
            {
              "name": "logger.info",
              "line": 712
            },
            {
              "name": "high_risk_patterns.items",
              "line": 727
            },
            {
              "name": "AugmentedDataset",
              "line": 731
            },
            {
              "name": "logger.info",
              "line": 741
            },
            {
              "name": "logger.info",
              "line": 742
            },
            {
              "name": "str",
              "line": 774
            },
            {
              "name": "len",
              "line": 804
            },
            {
              "name": "sum",
              "line": 819
            },
            {
              "name": "len",
              "line": 819
            },
            {
              "name": "logger.info",
              "line": 828
            },
            {
              "name": "str",
              "line": 869
            },
            {
              "name": "len",
              "line": 883
            },
            {
              "name": "self.optimizer.calculate_risk_accuracy_ratios",
              "line": 955
            },
            {
              "name": "self.optimizer.get_pattern_weights",
              "line": 969
            },
            {
              "name": "self.optimizer.get_pattern_weight_decays",
              "line": 972
            },
            {
              "name": "logger.info",
              "line": 978
            },
            {
              "name": "logger.info",
              "line": 979
            },
            {
              "name": "logger.info",
              "line": 980
            },
            {
              "name": "max",
              "line": 983
            },
            {
              "name": "logger.info",
              "line": 986
            },
            {
              "name": "logger.info",
              "line": 987
            },
            {
              "name": "weights.items",
              "line": 988
            },
            {
              "name": "logger.info",
              "line": 990
            },
            {
              "name": "logger.info",
              "line": 996
            },
            {
              "name": "max",
              "line": 997
            },
            {
              "name": "logger.info",
              "line": 1000
            },
            {
              "name": "decays.items",
              "line": 1001
            },
            {
              "name": "logger.info",
              "line": 1003
            },
            {
              "name": "str",
              "line": 1024
            },
            {
              "name": "val_metrics_dict.keys",
              "line": 1024
            },
            {
              "name": "logger.info",
              "line": 1048
            },
            {
              "name": "logger.info",
              "line": 1049
            },
            {
              "name": "logger.info",
              "line": 1050
            },
            {
              "name": "max",
              "line": 1053
            },
            {
              "name": "logger.info",
              "line": 1058
            },
            {
              "name": "logger.info",
              "line": 1059
            },
            {
              "name": "current_bounds.items",
              "line": 1062
            },
            {
              "name": "logger.info",
              "line": 1069
            },
            {
              "name": "....center",
              "line": 1073
            },
            {
              "name": "logger.info",
              "line": 1077
            },
            {
              "name": "logger.info",
              "line": 1082
            },
            {
              "name": "adaptation_metrics.get",
              "line": 678
            },
            {
              "name": "isinstance",
              "line": 711
            },
            {
              "name": "AugmentationMediator",
              "line": 717
            },
            {
              "name": "min",
              "line": 728
            },
            {
              "name": "pattern_accuracies.keys",
              "line": 804
            },
            {
              "name": "ratios.values",
              "line": 819
            },
            {
              "name": "pattern_risks.keys",
              "line": 883
            },
            {
              "name": "....join",
              "line": 960
            },
            {
              "name": "logger.info",
              "line": 962
            },
            {
              "name": "....center",
              "line": 979
            },
            {
              "name": "logger.info",
              "line": 989
            },
            {
              "name": "logger.info",
              "line": 1002
            },
            {
              "name": "str",
              "line": 1036
            },
            {
              "name": "....center",
              "line": 1049
            },
            {
              "name": "logger.info",
              "line": 1067
            },
            {
              "name": "logger.info",
              "line": 1079
            },
            {
              "name": "logger.info",
              "line": 1084
            },
            {
              "name": "....join",
              "line": 674
            },
            {
              "name": "sum",
              "line": 961
            },
            {
              "name": "len",
              "line": 961
            },
            {
              "name": "len",
              "line": 983
            },
            {
              "name": "len",
              "line": 997
            },
            {
              "name": "len",
              "line": 1053
            },
            {
              "name": "len",
              "line": 741
            },
            {
              "name": "len",
              "line": 742
            },
            {
              "name": "len",
              "line": 742
            },
            {
              "name": "curr_ratios.values",
              "line": 961
            },
            {
              "name": "weights.keys",
              "line": 983
            },
            {
              "name": "decays.keys",
              "line": 997
            },
            {
              "name": "current_bounds.keys",
              "line": 1053
            },
            {
              "name": "abs",
              "line": 1065
            },
            {
              "name": "curr_ratios.items",
              "line": 960
            },
            {
              "name": "pattern_risks.items",
              "line": 674
            }
          ],
          "docstring": "\n        Train the model with dynamic dataset adaptation based on unified risk/accuracy ratios.\n        \n        Args:\n            train_dataset: Training dataset\n            val_dataset: Validation dataset\n            epochs: Number of epochs\n            early_stopping: Whether to use early stopping\n            patience: Early stopping patience\n            test_interval: Interval for validation\n            checkpoint_interval: Interval for saving checkpoints\n            checkpoint_path: Path to save checkpoints\n            callbacks: List of callback functions\n            mini_val_interval: Interval for mini-validation during training\n        \n        Returns:\n            Training history\n        ",
          "code_snippet": "        return adjusted_batch\n    \n    def train(\n        self,\n        train_dataset,\n        val_dataset=None,\n        epochs=10,\n        early_stopping=None,\n        patience=None,\n        test_interval=1,\n        checkpoint_interval=None,\n        checkpoint_path=None,\n        callbacks=None,\n        mini_val_interval=50\n    ):\n        \"\"\"\n        Train the model with dynamic dataset adaptation based on unified risk/accuracy ratios.\n        \n        Args:\n            train_dataset: Training dataset\n            val_dataset: Validation dataset\n            epochs: Number of epochs\n            early_stopping: Whether to use early stopping\n            patience: Early stopping patience\n            test_interval: Interval for validation\n            checkpoint_interval: Interval for saving checkpoints\n            checkpoint_path: Path to save checkpoints\n            callbacks: List of callback functions\n            mini_val_interval: Interval for mini-validation during training\n        \n        Returns:\n            Training history\n        \"\"\"\n        # Initialize variables\n        history = {\n            'train_loss': [], 'train_acc': [],\n            'val_loss': [], 'val_acc': [],\n            'batch_sizes': [], 'epoch_times': [],\n            'dataset_sizes': [], 'memory_usage': [],\n            'pattern_recognition_rates': [],\n            'pattern_risks': [],\n            'risk_accuracy_ratios': {},\n            'weight_adjustments': [],\n            'dynamic_weight_decays': [],\n            'bound_adjustments': {},\n            'augmentation_counts': []\n        }\n        \n        best_val_loss = float('inf')\n        no_improve_count = 0\n        \n        # Initial dataset size\n        history['dataset_sizes'].append(len(train_dataset))\n        \n        # Store reference to train dataset for potential adaptation\n        self.train_dataset = train_dataset\n\n        # Convert to lazy augmented dataset if using augmentation\n        if self.use_augmentation and hasattr(self, 'lazy_augmentation_available') and self.lazy_augmentation_available:\n            logger.info(\"Creating lazy augmented dataset...\")\n            \n            # Define initial augmentation percentages\n            augmentation_percentages = {}\n            if hasattr(self, 'pattern_map') and self.pattern_map and 'pattern_distribution' in self.pattern_map:\n                for pattern_type in self.pattern_map['pattern_distribution'].keys():\n                    # Start with small percentage (5%)\n                    augmentation_percentages[pattern_type] = 0.05\n            \n            # Create lazy augmented dataset\n            self.train_dataset = self.create_lazy_augmented_dataset(\n                base_dataset=train_dataset,\n                augmentation_percentages=augmentation_percentages\n            )\n            \n            logger.info(f\"Using lazy augmented dataset with {len(self.train_dataset)} examples\")\n        \n        # Initialize augmentation mediator if enabled\n        if self.use_augmentation:\n            logger.info(\"Initializing augmentation mediator with dataset\")\n            # Create the mediator now that we have a dataset\n            if self.augmentation_mediator is None:\n                self.augmentation_mediator = AugmentationMediator(\n                    dataset=train_dataset,\n                    pattern_map=self.augmentation_params['pattern_map'],\n                    device=self.augmentation_params['device']\n                )\n            else:\n                # Update existing mediator with the dataset\n                self.augmentation_mediator.dataset = train_dataset\n        \n        for epoch in range(epochs):\n            # Update progress bar if function exists\n            if 'print_progress_bar' in globals():\n                test_acc = history['val_acc'][-1] if history.get('val_acc') else None\n                print_progress_bar(epoch+1, epochs, \n                                 train_acc=history['train_acc'][-1] if history.get('train_acc') else None,\n                                 test_acc=test_acc)\n                \n            logger.info(f\"Epoch {epoch+1}/{epochs}\")\n            \n            # Check if we should adapt the dataset based on risk/accuracy ratios\n            if hasattr(self.batch_optimizer, 'should_adapt_patterns') and self.batch_optimizer.should_adapt_patterns():\n                logger.info(\"Adapting dataset based on unified risk/accuracy assessment...\")\n                adapted_dataset, adaptation_metrics = self.batch_optimizer.adapt_dataset(self.train_dataset)\n                \n                if adaptation_metrics.get('adapted', False):\n                    # Use the adapted dataset for this epoch\n                    current_dataset = adapted_dataset\n                    # Update the reference\n                    self.train_dataset = adapted_dataset\n                    \n                    # Record adaptation\n                    self.dataset_adaptations.append({\n                        'epoch': epoch + 1,\n                        **adaptation_metrics\n                    })\n                    \n                    logger.info(f\"Dataset adapted successfully:\")\n                    logger.info(f\"  Added {adaptation_metrics['examples_added']} new examples\")\n                    if 'pattern_risks' in adaptation_metrics:\n                        pattern_risks = adaptation_metrics['pattern_risks']\n                        logger.info(f\"  Based on pattern risks: {', '.join([f'{k}:{v:.2f}' for k, v in pattern_risks.items()])}\")\n                    logger.info(f\"  New dataset size: {adaptation_metrics['total_size']} examples\")\n                else:\n                    current_dataset = self.train_dataset\n                    logger.info(f\"Dataset adaptation skipped: {adaptation_metrics.get('reason', 'unknown')}\")\n            else:\n                current_dataset = self.train_dataset\n            \n            # Try to apply augmentation for patterns with high risk\n            if self.use_augmentation and hasattr(self.optimizer, 'get_pattern_risks') and self.augmentation_mediator is not None:\n                pattern_risks = self.optimizer.get_pattern_risks()\n                \n                if pattern_risks:\n                    # Find patterns with high risk for augmentation\n                    high_risk_patterns = {k: v for k, v in pattern_risks.items() if v > 0.3}\n                    \n                    if high_risk_patterns and isinstance(self.train_dataset, AugmentedDataset):\n                        logger.info(\"Using augmentation mediator to add examples for high-risk patterns\")\n                        \n                        # Add augmentations for high-risk patterns\n                        total_added = 0\n                        for pattern_type, risk in high_risk_patterns.items():\n                            # Calculate percentage based on risk\n                            percentage = min(0.1, risk * 0.2)  # Scale percentage with risk\n                            \n                            # Add augmentations\n                            added = self.train_dataset.add_augmentations(pattern_type, percentage)\n                            total_added += added\n                            \n                            logger.info(f\"Added {added} augmentations for {pattern_type} (risk: {risk:.2f})\")\n                        \n                        if total_added > 0:\n                            logger.info(f\"Total augmentations added: {total_added}\")\n                            # Update current dataset reference\n                            current_dataset = self.train_dataset\n                    \n                    # If we're using a regular dataset, convert it to an AugmentedDataset\n                    elif high_risk_patterns and not isinstance(self.train_dataset, AugmentedDataset):\n                        logger.info(\"Converting to AugmentedDataset to support augmentations\")\n                        \n                        # Initialize the augmentation mediator with the dataset\n                        if self.augmentation_mediator is None:\n                            # Create the mediator now that we have a dataset\n                            self.augmentation_mediator = AugmentationMediator(\n                                dataset=self.train_dataset,\n                                pattern_map=self.augmentation_params['pattern_map'],\n                                device=self.augmentation_params['device']\n                            )\n                        else:\n                            self.augmentation_mediator.dataset = self.train_dataset\n                        \n                        # Create percentages dictionary\n                        percentages = {}\n                        for pattern_type, risk in high_risk_patterns.items():\n                            percentages[pattern_type] = min(0.05, risk * 0.1)\n                        \n                        # Create augmented dataset\n                        augmented_dataset = AugmentedDataset(\n                            base_dataset=self.train_dataset,\n                            augmentation_mediator=self.augmentation_mediator,\n                            augmentation_percentages=percentages\n                        )\n                        \n                        # Replace the dataset\n                        self.train_dataset = augmented_dataset\n                        current_dataset = self.train_dataset\n                        \n                        logger.info(f\"Created augmented dataset with {len(self.train_dataset)} total examples\")\n                        logger.info(f\"Original: {len(self.train_dataset.original_indices)}, Augmented: {len(self.train_dataset.augmented_examples)}\")\n            \n            # Get optimal batch size using the risk/accuracy adjusted approach\n            base_batch_size = self.batch_optimizer.get_optimal_batch_size()\n            adjusted_batch_size = self.adjust_batch_size_for_risk(base_batch_size, epoch)\n            batch_size = adjusted_batch_size\n            history['batch_sizes'].append(batch_size)\n            \n            # Track dataset size\n            history['dataset_sizes'].append(len(current_dataset))\n            \n            # Print more detailed epoch information in a structured format\n            logger.info(\"-\" * 80)\n            logger.info(f\"EPOCH {epoch+1}/{epochs}\".center(80))\n            logger.info(\"-\" * 80)\n            \n            # Create a dictionary of key parameters\n            # Get batch size range information from batch optimizer\n            min_batch = getattr(self.batch_optimizer, 'min_batch', 'N/A')\n            max_batch = getattr(self.batch_optimizer, 'max_batch', 'N/A')\n            batch_range = f\"{min_batch}-{max_batch}\" if min_batch != 'N/A' and max_batch != 'N/A' else 'N/A'\n            \n            epoch_params = {\n                \"Batch Size\": f\"{batch_size} (adjusted from {base_batch_size})\",\n                \"Batch Size Range\": batch_range,\n                \"Dataset Size\": len(current_dataset)\n            }\n            \n            # Format and display epoch parameters\n            key_width = max(len(str(k)) for k in epoch_params.keys())\n            val_width = 40\n            for key, value in epoch_params.items():\n                logger.info(f\"  {str(key):<{key_width}} : {value}\")\n            \n            # Update current epoch tracking\n            self.current_epoch = epoch\n            \n            # Train for one epoch\n            start_time = time.time()\n            train_metrics = self._train_epoch(current_dataset, batch_size, mini_val_interval)\n            epoch_time = time.time() - start_time\n            \n            # Get memory usage\n            if torch.cuda.is_available():\n                memory_usage = torch.cuda.max_memory_allocated(self.device) / (1024 ** 3)  # GB\n            else:\n                memory_usage = 0.0\n                \n            # Signal end of epoch to optimizer's internal mediator\n            if isinstance(self.optimizer, EVEUnifiedRatio) and hasattr(self.optimizer.pattern_mediator, 'end_epoch'):\n                logger.info(f\"Signaling end of epoch {epoch} to optimizer's pattern mediator\")\n                self.optimizer.pattern_mediator.end_epoch(epoch)\n                \n                # Get and log the pattern accuracies and risks from the mediator\n                pattern_accuracies = self.optimizer.pattern_mediator.get_pattern_accuracies()\n                pattern_risks = self.optimizer.pattern_mediator.get_pattern_risks()\n                if pattern_accuracies and pattern_risks:\n                    logger.info(\"\\n\" + \"-\" * 80)\n                    logger.info(\"END OF EPOCH PATTERN METRICS\".center(80))\n                    logger.info(\"-\" * 80)\n                    \n                    # Table header\n                    pattern_width = max(len(pattern_type) for pattern_type in pattern_accuracies.keys())\n                    logger.info(f\"{'Pattern':<{pattern_width}} | {'Accuracy':^12} | {'Risk':^12}\")\n                    logger.info(f\"{'-' * pattern_width}-+-{'-' * 14}-+-{'-' * 14}\")\n                    \n                    # Table rows for each pattern\n                    for pattern_type in pattern_accuracies.keys():\n                        acc = pattern_accuracies.get(pattern_type, 0.0)\n                        risk = pattern_risks.get(pattern_type, 0.0)\n                        logger.info(f\"{pattern_type:<{pattern_width}} | {acc:^12.4f} | {risk:^12.4f}\")\n                        \n                # Calculate and log unified risk/accuracy ratios\n                if hasattr(self.optimizer, 'calculate_risk_accuracy_ratios'):\n                    ratios = self.optimizer.calculate_risk_accuracy_ratios(force_refresh=True)\n                    if ratios:\n                        # Calculate average ratio\n                        avg_ratio = sum(ratios.values()) / len(ratios)\n                        logger.info(f\"\\nAverage risk/accuracy ratio: {avg_ratio:.4f}\")\n                        \n                        # Table header for ratios\n                        logger.info(f\"\\n{'Pattern':<{pattern_width}} | {'Ratio':^12}\")\n                        logger.info(f\"{'-' * pattern_width}-+-{'-' * 14}\")\n                        \n                        # Table rows for each ratio\n                        for pattern_type, ratio in ratios.items():\n                            logger.info(f\"{pattern_type:<{pattern_width}} | {ratio:^12.4f}\")\n                \n            # Add memory usage to metrics\n            train_metrics['memory'] = memory_usage\n            \n            # Add epoch time to metrics\n            train_metrics['time'] = epoch_time\n            \n            # Add batch size to metrics\n            train_metrics['batch_size'] = batch_size\n            \n            # Update optimizer with epoch metrics\n            if hasattr(self.batch_optimizer, 'update_with_epoch_metrics'):\n                self.batch_optimizer.update_with_epoch_metrics(train_metrics)\n            \n            # Print detailed completion information in a structured format\n            logger.info(\"\\n\" + \"-\" * 80)\n            logger.info(f\"EPOCH {epoch+1}/{epochs} COMPLETED\".center(80))\n            logger.info(\"-\" * 80)\n            \n            # Create metrics dictionary\n            metrics = {\n                \"Training Loss\": train_metrics['loss'],\n                \"Training Accuracy\": train_metrics['accuracy'],\n                \"Memory Usage (GB)\": memory_usage,\n                \"Time (seconds)\": epoch_time\n            }\n            \n            # Format and print metrics table\n            key_width = max(len(str(k)) for k in metrics.keys())\n            val_width = 15\n            \n            logger.info(\"-\" * (key_width + val_width + 3))\n            for key, value in metrics.items():\n                if isinstance(value, float):\n                    if key == \"Training Accuracy\":\n                        val_str = f\"{value:.2f}%\"\n                    else:\n                        val_str = f\"{value:.4f}\"\n                else:\n                    val_str = str(value)\n                logger.info(f\"  {str(key):<{key_width}} \u2502 {val_str:>{val_width}}\")\n            logger.info(\"-\" * (key_width + val_width + 3))\n            \n            # Pattern statistics if available\n            pattern_risks = {}\n            if hasattr(self.optimizer, 'get_pattern_risks'):\n                # Get pattern risks from the unified ratio tracker\n                pattern_risks = self.optimizer.get_pattern_risks()\n                if pattern_risks:\n                    logger.info(\"\\n\" + \"-\" * 80)\n                    logger.info(\"PATTERN RISK ASSESSMENT\".center(80))\n                    logger.info(\"-\" * 80)\n                    \n                    # Set up table format\n                    pattern_width = max(len(pattern_type) for pattern_type in pattern_risks.keys())\n                    risk_width = 10\n                    \n                    # Print table header\n                    logger.info(f\"{'Pattern':<{pattern_width}} \u2502 {'Risk':^{risk_width}}\")\n                    logger.info(\"-\" * (pattern_width + risk_width + 3))\n                    \n                    # Print each pattern risk\n                    for pattern_type, risk in pattern_risks.items():\n                        # Color coding with text indicators\n                        risk_indicator = \"LOW\" if risk < 0.3 else \"MEDIUM\" if risk < 0.6 else \"HIGH\"\n                        risk_str = f\"{risk:.2f} {risk_indicator}\"\n                        logger.info(f\"{pattern_type:<{pattern_width}} \u2502 {risk_str:^{risk_width+8}}\")\n                    \n                    logger.info(\"-\" * (pattern_width + risk_width + 3))\n                    \n                    # Save for history\n                    history['pattern_risks'].append(pattern_risks)\n                    \n            # Get pattern recognition rates if available\n            if hasattr(self.batch_optimizer, 'pattern_tracker') and hasattr(self.batch_optimizer.pattern_tracker, 'get_pattern_accuracies'):\n                pattern_accuracies = self.batch_optimizer.pattern_tracker.get_pattern_accuracies()\n                if pattern_accuracies:\n                    logger.info(\"  Pattern recognition rates:\")\n                    for pattern_type, rate in pattern_accuracies.items():\n                        logger.info(f\"    {pattern_type}: {rate:.2f}\")\n                    # Save for history\n                    history['pattern_recognition_rates'].append(pattern_accuracies)\n            \n            # Store metrics in history\n            history['train_loss'].append(train_metrics['loss'])\n            history['train_acc'].append(train_metrics['accuracy'])\n            history['epoch_times'].append(epoch_time)\n            history['memory_usage'].append(memory_usage)\n            \n            # Validate if a validation set is provided\n            if val_dataset is not None and (epoch + 1) % test_interval == 0:\n                # Clean up memory before validation\n                if hasattr(self.batch_optimizer, 'hardware_analyzer'):\n                    self.batch_optimizer.hardware_analyzer.cleanup_memory()\n                elif torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n                    torch.cuda.reset_peak_memory_stats()\n                    \n                val_metrics = self._validate(val_dataset)\n                history['val_loss'].append(val_metrics['loss'])\n                history['val_acc'].append(val_metrics['accuracy'])\n                \n                # Add validation metrics to train_metrics for batch optimizer\n                train_metrics['val_accuracy'] = val_metrics['accuracy']\n                train_metrics['val_loss'] = val_metrics['loss']\n                \n                # Update batch optimizer with complete metrics including validation\n                if hasattr(self.batch_optimizer, 'update_with_epoch_metrics'):\n                    self.batch_optimizer.update_with_epoch_metrics(train_metrics)\n                \n                # Store reference to validation dataset for mini-validation\n                self.val_dataset = val_dataset\n                \n                # Update the optimizer with accuracy metrics\n                if isinstance(self.optimizer, EVEUnifiedRatio):\n                    # Use the specialized update method for the unified ratio optimizer\n                    if hasattr(self.optimizer, 'update_accuracy_metrics_with_epoch'):\n                        self.optimizer.update_accuracy_metrics_with_epoch(\n                            train_metrics['accuracy'], \n                            val_metrics['accuracy'],\n                            epoch,\n                            epochs\n                        )\n                        \n                        # Capture risk/accuracy ratios for history after update\n                        if hasattr(self.optimizer, 'calculate_risk_accuracy_ratios'):\n                            curr_ratios = self.optimizer.calculate_risk_accuracy_ratios()\n                            if curr_ratios:\n                                history['risk_accuracy_ratios'][epoch] = curr_ratios\n                                \n                                # Log the ratios\n                                ratio_str = \", \".join([f\"{k}: {v:.2f}\" for k, v in curr_ratios.items()])\n                                avg_ratio = sum(curr_ratios.values()) / len(curr_ratios)\n                                logger.info(f\"  Risk/Accuracy ratios - Avg: {avg_ratio:.2f}, Pattern-specific: {ratio_str}\")\n                    else:\n                        # Fallback to basic metrics update\n                        self.optimizer.update_accuracy_metrics(train_metrics['accuracy'], val_metrics['accuracy'])\n                    \n                    # Track optimizer-specific metrics\n                    if hasattr(self.optimizer, 'get_pattern_weights'):\n                        history['weight_adjustments'].append(self.optimizer.get_pattern_weights())\n                        \n                    if hasattr(self.optimizer, 'get_pattern_weight_decays'):\n                        history['dynamic_weight_decays'].append(self.optimizer.get_pattern_weight_decays())\n                    \n                    # Log EVE-specific metrics in tabular format\n                    if hasattr(self.optimizer, 'get_pattern_weights'):\n                        weights = self.optimizer.get_pattern_weights()\n                        if weights:\n                            logger.info(\"\\n\" + \"-\" * 80)\n                            logger.info(\"EVE OPTIMIZER METRICS\".center(80))\n                            logger.info(\"-\" * 80)\n                            \n                            # Format and print pattern weights\n                            key_width = max(len(pattern_type) for pattern_type in weights.keys())\n                            val_width = 12\n                            \n                            logger.info(\"Pattern Weights:\")\n                            logger.info(\"-\" * (key_width + val_width + 3))\n                            for pattern_type, weight in weights.items():\n                                logger.info(f\"  {pattern_type:<{key_width}} \u2502 {weight:.4f}\")\n                            logger.info(\"-\" * (key_width + val_width + 3))\n                    \n                    # Weight decays table\n                    if hasattr(self.optimizer, 'get_pattern_weight_decays'):\n                        decays = self.optimizer.get_pattern_weight_decays()\n                        if decays:\n                            logger.info(\"\\nPattern Weight Decays:\")\n                            key_width = max(len(pattern_type) for pattern_type in decays.keys())\n                            val_width = 12\n                            \n                            logger.info(\"-\" * (key_width + val_width + 3))\n                            for pattern_type, decay in decays.items():\n                                logger.info(f\"  {pattern_type:<{key_width}} \u2502 {decay:.6f}\")\n                            logger.info(\"-\" * (key_width + val_width + 3))\n                \n                # Log validation results in a tabular format\n                logger.info(\"\\n\" + \"-\" * 80)\n                logger.info(\"VALIDATION RESULTS\".center(80))\n                logger.info(\"-\" * 80)\n                \n                val_metrics_dict = {\n                    \"Loss\": val_metrics['loss'],\n                    \"Accuracy\": val_metrics['accuracy']\n                }\n                \n                # Add comparison if we have previous validation results\n                if len(history['val_acc']) > 1:\n                    acc_diff = val_metrics['accuracy'] - history['val_acc'][-2]\n                    loss_diff = history['val_loss'][-2] - val_metrics['loss']  # Loss should decrease\n                    \n                    val_metrics_dict[\"Accuracy Change\"] = acc_diff\n                    val_metrics_dict[\"Loss Change\"] = loss_diff\n                \n                # Format and print the validation metrics\n                key_width = max(len(str(k)) for k in val_metrics_dict.keys())\n                val_width = 15\n                \n                logger.info(\"-\" * (key_width + val_width + 3))\n                for key, value in val_metrics_dict.items():\n                    if isinstance(value, float):\n                        if key == \"Accuracy\" or key == \"Accuracy Change\":\n                            val_str = f\"{value:+.2f}%\" if key == \"Accuracy Change\" else f\"{value:.2f}%\"\n                        else:\n                            val_str = f\"{value:+.4f}\" if key == \"Loss Change\" else f\"{value:.4f}\"\n                    else:\n                        val_str = str(value)\n                    logger.info(f\"  {str(key):<{key_width}} \u2502 {val_str:>{val_width}}\")\n                logger.info(\"-\" * (key_width + val_width + 3))\n                        \n                # Check for bound violations after validation\n                if isinstance(self.optimizer, EVEUnifiedRatio) and hasattr(self.optimizer, 'equilibrium_tracker'):\n                    below_min = self.optimizer.equilibrium_tracker.get_patterns_below_min()\n                    above_max = self.optimizer.equilibrium_tracker.get_patterns_above_max()\n                    \n                    # Display current bounds and their adjustment status\n                    if hasattr(self.optimizer, 'get_current_bounds'):\n                        current_bounds = self.optimizer.get_current_bounds()\n                        if current_bounds:\n                            logger.info(\"\\n\" + \"-\" * 80)\n                            logger.info(\"PATTERN EQUILIBRIUM BOUNDS\".center(80))\n                            logger.info(\"-\" * 80)\n                            \n                            # Determine column widths\n                            pattern_width = max(len(pattern) for pattern in current_bounds.keys())\n                            bound_width = 10\n                            adjust_width = 10\n                            \n                            # Table header\n                            logger.info(f\"{'Pattern':<{pattern_width}} \u2502 {'Min':^{bound_width}} \u2502 {'Max':^{bound_width}} \u2502 {'Adjustment':^{adjust_width}}\")\n                            logger.info(\"-\" * (pattern_width + bound_width*2 + adjust_width + 6))\n                            \n                            # Table rows\n                            for pattern, bounds in current_bounds.items():\n                                # Only show significant adjustments\n                                adj_pct = bounds['max_adjustment_pct']\n                                adj_str = f\"{adj_pct:.1f}%\" if abs(adj_pct) > 1.0 else \"-\"\n                                \n                                logger.info(f\"{pattern:<{pattern_width}} \u2502 {bounds['min']:^{bound_width}.2f} \u2502 {bounds['max']:^{bound_width}.2f} \u2502 {adj_str:^{adjust_width}}\")\n                            \n                            logger.info(\"-\" * (pattern_width + bound_width*2 + adjust_width + 6))\n                    \n                    if below_min or above_max:\n                        logger.info(\"\\n\" + \"-\" * 80)\n                        logger.info(\"PATTERN BOUND VIOLATIONS\".center(80))\n                        logger.info(\"-\" * 80)\n                        \n                        if below_min:\n                            logger.info(\"Patterns below minimum bound (underfitting):\")\n                            for pattern in below_min:\n                                logger.info(f\"  - {pattern}\")\n                        \n                        if above_max:\n                            logger.info(\"\\nPatterns above maximum bound (potential overfitting):\")\n                            for pattern in above_max:\n                                logger.info(f\"  - {pattern}\")\n                \n                # Save best model when validation improves\n                if val_metrics['loss'] < best_val_loss:\n                    best_val_loss = val_metrics['loss']\n                    # Save best model\n                    if checkpoint_path:\n                        self.save_model(f\"{checkpoint_path}_best.pth\")\n                        logger.info(f\"New best model saved at epoch {epoch+1}\")\n            \n            # Save checkpoint if interval is specified\n            if checkpoint_interval and (epoch + 1) % checkpoint_interval == 0 and checkpoint_path:\n                self.save_model(f\"{checkpoint_path}_epoch{epoch+1}.pth\")\n            \n            # Step the scheduler if it exists\n            if self.scheduler:\n                self.scheduler.step()\n            \n            # Clean up memory after epoch\n            if hasattr(self.batch_optimizer, 'hardware_analyzer'):\n                self.batch_optimizer.hardware_analyzer.cleanup_memory()\n            elif torch.cuda.is_available():\n                torch.cuda.empty_cache()\n                torch.cuda.reset_peak_memory_stats()\n            \n            # Execute callbacks if provided\n            if callbacks:\n                stop_training = False\n                for callback in callbacks:\n                    result = callback(epoch, history, self.model, self.optimizer)\n                    if result:  # If callback returns True, stop training\n                        stop_training = True\n                        \n                if stop_training:\n                    logger.info(f\"Training stopped by callback after epoch {epoch+1}\")\n                    break\n        \n        logger.info(\"Training complete\")\n        return history\n        \n    def _train_epoch(self, dataset, batch_size, mini_val_interval=50):\n        \"\"\"\n        Train for one epoch with mini-validation for continuous feedback."
        },
        "_train_epoch": {
          "start_line": 1124,
          "end_line": 1336,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "dataset"
            },
            {
              "name": "batch_size"
            },
            {
              "name": "mini_val_interval"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.model.train",
              "line": 1136
            },
            {
              "name": "enumerate",
              "line": 1197
            },
            {
              "name": "hasattr",
              "line": 1142
            },
            {
              "name": "self.create_optimized_dataloader",
              "line": 1144
            },
            {
              "name": "list",
              "line": 1203
            },
            {
              "name": "all_batch_indices.extend",
              "line": 1204
            },
            {
              "name": "self.optimizer.zero_grad",
              "line": 1207
            },
            {
              "name": "self.model",
              "line": 1210
            },
            {
              "name": "self.criterion",
              "line": 1211
            },
            {
              "name": "outputs.max",
              "line": 1214
            },
            {
              "name": "predicted.eq",
              "line": 1215
            },
            {
              "name": "list",
              "line": 1218
            },
            {
              "name": "isinstance",
              "line": 1221
            },
            {
              "name": "loss.item",
              "line": 1247
            },
            {
              "name": "....item",
              "line": 1248
            },
            {
              "name": "targets.size",
              "line": 1250
            },
            {
              "name": "loss.backward",
              "line": 1253
            },
            {
              "name": "hasattr",
              "line": 1256
            },
            {
              "name": "len",
              "line": 1331
            },
            {
              "name": "multiprocessing.cpu_count",
              "line": 1158
            },
            {
              "name": "max",
              "line": 1171
            },
            {
              "name": "torch.cuda.is_available",
              "line": 1173
            },
            {
              "name": "logger.info",
              "line": 1177
            },
            {
              "name": "torch.utils.data.DataLoader",
              "line": 1180
            },
            {
              "name": "inputs.to",
              "line": 1199
            },
            {
              "name": "targets.to",
              "line": 1199
            },
            {
              "name": "range",
              "line": 1203
            },
            {
              "name": "range",
              "line": 1218
            },
            {
              "name": "self.pattern_service.get_batch_pattern_states",
              "line": 1258
            },
            {
              "name": "self.optimizer.step",
              "line": 1259
            },
            {
              "name": "self.optimizer.step",
              "line": 1262
            },
            {
              "name": "hasattr",
              "line": 1265
            },
            {
              "name": "torch.utils.data.Subset",
              "line": 1267
            },
            {
              "name": "self.model.eval",
              "line": 1309
            },
            {
              "name": "self.model.train",
              "line": 1328
            },
            {
              "name": "min",
              "line": 1171
            },
            {
              "name": "min",
              "line": 1175
            },
            {
              "name": "logger.warning",
              "line": 1186
            },
            {
              "name": "torch.utils.data.DataLoader",
              "line": 1187
            },
            {
              "name": "min",
              "line": 1203
            },
            {
              "name": "min",
              "line": 1218
            },
            {
              "name": "hasattr",
              "line": 1226
            },
            {
              "name": "hasattr",
              "line": 1226
            },
            {
              "name": "self.optimizer.pattern_mediator.update_from_batch",
              "line": 1227
            },
            {
              "name": "hasattr",
              "line": 1234
            },
            {
              "name": "self.optimizer.get_pattern_risks",
              "line": 1235
            },
            {
              "name": "hasattr",
              "line": 1238
            },
            {
              "name": "self.optimizer.calculate_risk_accuracy_ratios",
              "line": 1239
            },
            {
              "name": "correct_mask.sum",
              "line": 1248
            },
            {
              "name": "list",
              "line": 1267
            },
            {
              "name": "hasattr",
              "line": 1270
            },
            {
              "name": "self.create_optimized_dataloader",
              "line": 1272
            },
            {
              "name": "torch.no_grad",
              "line": 1313
            },
            {
              "name": "isinstance",
              "line": 1325
            },
            {
              "name": "isinstance",
              "line": 1325
            },
            {
              "name": "self.optimizer.update_accuracy_metrics",
              "line": 1326
            },
            {
              "name": "psutil.cpu_percent",
              "line": 1163
            },
            {
              "name": "int",
              "line": 1171
            },
            {
              "name": "torch.cuda.is_available",
              "line": 1183
            },
            {
              "name": "len",
              "line": 1203
            },
            {
              "name": "len",
              "line": 1218
            },
            {
              "name": "logger.info",
              "line": 1237
            },
            {
              "name": "logger.info",
              "line": 1241
            },
            {
              "name": "range",
              "line": 1267
            },
            {
              "name": "multiprocessing.cpu_count",
              "line": 1287
            },
            {
              "name": "max",
              "line": 1288
            },
            {
              "name": "torch.cuda.is_available",
              "line": 1290
            },
            {
              "name": "torch.utils.data.DataLoader",
              "line": 1295
            },
            {
              "name": "self.model",
              "line": 1316
            },
            {
              "name": "val_outputs.max",
              "line": 1317
            },
            {
              "name": "....item",
              "line": 1318
            },
            {
              "name": "val_targets.size",
              "line": 1319
            },
            {
              "name": "min",
              "line": 1267
            },
            {
              "name": "min",
              "line": 1288
            },
            {
              "name": "min",
              "line": 1292
            },
            {
              "name": "logger.warning",
              "line": 1302
            },
            {
              "name": "torch.utils.data.DataLoader",
              "line": 1304
            },
            {
              "name": "val_inputs.to",
              "line": 1315
            },
            {
              "name": "val_targets.to",
              "line": 1315
            },
            {
              "name": "str",
              "line": 1186
            },
            {
              "name": "....join",
              "line": 1237
            },
            {
              "name": "....join",
              "line": 1241
            },
            {
              "name": "len",
              "line": 1267
            },
            {
              "name": "int",
              "line": 1288
            },
            {
              "name": "torch.cuda.is_available",
              "line": 1299
            },
            {
              "name": "....sum",
              "line": 1318
            },
            {
              "name": "str",
              "line": 1302
            },
            {
              "name": "val_predicted.eq",
              "line": 1318
            },
            {
              "name": "pattern_risks.items",
              "line": 1237
            },
            {
              "name": "ratios.items",
              "line": 1241
            }
          ],
          "docstring": "\n        Train for one epoch with mini-validation for continuous feedback.\n        \n        Args:\n            dataset: Dataset to train on\n            batch_size: Batch size\n            mini_val_interval: Interval for mini-validation during training\n            \n        Returns:\n            Dictionary with training metrics\n        ",
          "code_snippet": "        return history\n        \n    def _train_epoch(self, dataset, batch_size, mini_val_interval=50):\n        \"\"\"\n        Train for one epoch with mini-validation for continuous feedback.\n        \n        Args:\n            dataset: Dataset to train on\n            batch_size: Batch size\n            mini_val_interval: Interval for mini-validation during training\n            \n        Returns:\n            Dictionary with training metrics\n        \"\"\"\n        self.model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        # Create dataloader\n        if hasattr(self, 'using_lazy_augmentation') and self.using_lazy_augmentation:\n            # Use optimized dataloader from lazy augmentation module\n            dataloader = self.create_optimized_dataloader(\n                dataset=dataset,\n                batch_size=batch_size,\n                shuffle=True\n            )\n        else:\n            # Fallback to standard DataLoader with conservative settings\n            try:\n                # Calculate optimal number of workers based on system capabilities\n                import multiprocessing\n                import psutil\n                import math\n                \n                # Get number of CPU cores\n                cpu_count = multiprocessing.cpu_count()\n                \n                # Get system load\n                system_load = 0.5  # Default value if psutil fails\n                try:\n                    system_load = psutil.cpu_percent(interval=0.1) / 100.0\n                except:\n                    pass\n                    \n                # Calculate resource factor based on cognitive efficiency formula\n                resource_factor = (1 - system_load * 0.8)\n                \n                # Calculate optimal workers - apply parallel processing penalty\n                optimal_workers = max(0, min(int(cpu_count * resource_factor * 0.75), 4))\n                \n                if torch.cuda.is_available():\n                    # When using GPU, we want fewer workers\n                    optimal_workers = min(optimal_workers, 2)\n                    \n                logger.info(f\"Using {optimal_workers} DataLoader workers (cores: {cpu_count}, load: {system_load:.2f})\")\n                \n                # Use optimal workers for DataLoader\n                dataloader = torch.utils.data.DataLoader(\n                    dataset, batch_size=batch_size, shuffle=True, \n                    num_workers=optimal_workers,\n                    pin_memory=torch.cuda.is_available())\n                    \n            except Exception as e:\n                logger.warning(f\"Error creating DataLoader: {str(e)}\")\n                dataloader = torch.utils.data.DataLoader(\n                    dataset, batch_size=batch_size, shuffle=True, \n                    num_workers=0)\n        \n        # For batch-level risk assessment\n        all_batch_indices = []\n        \n        # Track batch numbers for mini-validation\n        batch_count = 0\n        \n        for i, (inputs, targets) in enumerate(dataloader):\n            batch_count += 1\n            inputs, targets = inputs.to(self.device), targets.to(self.device)\n            \n            # Get batch indices for pattern tracking\n            batch_start = i * batch_size\n            batch_indices = list(range(batch_start, min(batch_start + batch_size, len(dataset))))\n            all_batch_indices.extend(batch_indices)\n            \n            # Zero the parameter gradients\n            self.optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = self.model(inputs)\n            loss = self.criterion(outputs, targets)\n            \n            # Calculate per-example correctness for pattern recognition\n            _, predicted = outputs.max(1)\n            correct_mask = predicted.eq(targets)\n            \n            # Batch indices for pattern tracking\n            batch_indices_list = list(range(batch_start, min(batch_start + batch_size, len(dataset))))\n            \n            # Store batch data in the optimizer for pattern tracking\n            if isinstance(self.optimizer, EVEUnifiedRatio):\n                self.optimizer.last_batch_indices = batch_indices_list\n                self.optimizer.last_correct_mask = correct_mask\n                \n                # Explicitly update the pattern mediator with this batch data\n                if hasattr(self.optimizer, 'pattern_mediator') and hasattr(self.optimizer.pattern_mediator, 'update_from_batch'):\n                    self.optimizer.pattern_mediator.update_from_batch(\n                        batch_indices=batch_indices_list,\n                        correct_mask=correct_mask,\n                        epoch=self.current_epoch\n                    )\n                    \n                # Log pattern tracking stats periodically\n                if batch_count % 50 == 0 and hasattr(self.optimizer, 'get_pattern_risks'):\n                    pattern_risks = self.optimizer.get_pattern_risks()\n                    if pattern_risks:\n                        logger.info(f\"Batch {batch_count} pattern risks: \" + \", \".join([f\"{k}:{v:.2f}\" for k, v in pattern_risks.items()]))\n                if batch_count % 50 == 0 and hasattr(self.optimizer, 'calculate_risk_accuracy_ratios'):\n                    ratios = self.optimizer.calculate_risk_accuracy_ratios()\n                    if ratios:\n                        logger.info(f\"Batch {batch_count} risk/accuracy ratios: \" + \", \".join([f\"{k}:{v:.2f}\" for k, v in ratios.items()]))\n            \n            # We don't need to update the mediator, the optimizer will handle that\n            # The optimizer's internal mediator is used in EVEUnifiedRatio\n            \n            # Update metrics\n            running_loss += loss.item()\n            batch_correct = correct_mask.sum().item()\n            correct += batch_correct\n            total += targets.size(0)\n            \n            # Backward pass and optimize\n            loss.backward()\n            \n            # Special handling for EVE optimizers - pass pattern states\n            if hasattr(self, 'pattern_service'):\n                # Get pattern states for this batch\n                pattern_states = self.pattern_service.get_batch_pattern_states(batch_indices_list)\n                self.optimizer.step(pattern_states=pattern_states)\n            else:\n                # Normal optimization step\n                self.optimizer.step()\n            \n            # Perform mini-validation to get more frequent feedback\n            if batch_count % mini_val_interval == 0 and hasattr(self, 'val_dataset') and self.val_dataset is not None:\n                # Do quick validation on subset of validation data\n                validation_subset = torch.utils.data.Subset(self.val_dataset, list(range(min(1000, len(self.val_dataset)))))\n                \n                # Use optimized dataloader if available\n                if hasattr(self, 'using_lazy_augmentation') and self.using_lazy_augmentation:\n                    # Use the optimized dataloader from lazy augmentation module\n                    quick_val_loader = self.create_optimized_dataloader(\n                        dataset=validation_subset,\n                        batch_size=batch_size,\n                        shuffle=False,\n                        num_workers=1  # Use minimal workers for quick validation\n                    )\n                else:\n                    # Use the same optimal worker calculation approach\n                    try:\n                        # Calculate optimal number of workers based on system capabilities\n                        import multiprocessing\n                        import psutil\n                        import math\n                        \n                        # Get number of CPU cores and apply a lighter workload for mini-validation\n                        cpu_count = multiprocessing.cpu_count()\n                        optimal_workers = max(0, min(int(cpu_count * 0.5), 2))  # Lighter worker count for mini-val\n                        \n                        if torch.cuda.is_available():\n                            # When using GPU, prefer fewer workers for mini-val\n                            optimal_workers = min(optimal_workers, 1)\n                        \n                        # Create DataLoader with calculated workers\n                        quick_val_loader = torch.utils.data.DataLoader(\n                            validation_subset,\n                            batch_size=batch_size, shuffle=False, \n                            num_workers=optimal_workers,\n                            pin_memory=torch.cuda.is_available()\n                        )\n                    except Exception as e:\n                        logger.warning(f\"Error creating mini-validation DataLoader: {str(e)}\")\n                        # Fallback to basic loader\n                        quick_val_loader = torch.utils.data.DataLoader(\n                            validation_subset,\n                            batch_size=batch_size, shuffle=False, num_workers=0\n                        )\n                \n                self.model.eval()\n                val_correct = 0\n                val_total = 0\n                \n                with torch.no_grad():\n                    for val_inputs, val_targets in quick_val_loader:\n                        val_inputs, val_targets = val_inputs.to(self.device), val_targets.to(self.device)\n                        val_outputs = self.model(val_inputs)\n                        _, val_predicted = val_outputs.max(1)\n                        val_correct += val_predicted.eq(val_targets).sum().item()\n                        val_total += val_targets.size(0)\n                \n                quick_val_acc = 100. * val_correct / val_total\n                current_train_acc = 100. * correct / total\n                \n                # Update optimizer with more frequent feedback\n                if isinstance(self.optimizer, EVENaturalWeights) or isinstance(self.optimizer, EVEUnifiedRatio):\n                    self.optimizer.update_accuracy_metrics(current_train_acc, quick_val_acc)\n                \n                self.model.train()\n        \n        # Calculate epoch metrics\n        epoch_loss = running_loss / len(dataloader)\n        epoch_acc = 100. * correct / total\n        \n        return {'loss': epoch_loss, 'accuracy': epoch_acc}\n    \n    def _validate(self, dataset, batch_size=128):\n        \"\"\"\n        Validate the model on a dataset."
        },
        "_validate": {
          "start_line": 1336,
          "end_line": 1423,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "dataset"
            },
            {
              "name": "batch_size"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.model.eval",
              "line": 1347
            },
            {
              "name": "hasattr",
              "line": 1353
            },
            {
              "name": "self.create_optimized_dataloader",
              "line": 1355
            },
            {
              "name": "torch.no_grad",
              "line": 1403
            },
            {
              "name": "len",
              "line": 1418
            },
            {
              "name": "multiprocessing.cpu_count",
              "line": 1369
            },
            {
              "name": "max",
              "line": 1382
            },
            {
              "name": "torch.cuda.is_available",
              "line": 1384
            },
            {
              "name": "logger.info",
              "line": 1388
            },
            {
              "name": "torch.utils.data.DataLoader",
              "line": 1391
            },
            {
              "name": "self.model",
              "line": 1408
            },
            {
              "name": "self.criterion",
              "line": 1409
            },
            {
              "name": "loss.item",
              "line": 1412
            },
            {
              "name": "outputs.max",
              "line": 1413
            },
            {
              "name": "....item",
              "line": 1414
            },
            {
              "name": "targets.size",
              "line": 1415
            },
            {
              "name": "min",
              "line": 1382
            },
            {
              "name": "min",
              "line": 1386
            },
            {
              "name": "logger.warning",
              "line": 1397
            },
            {
              "name": "torch.utils.data.DataLoader",
              "line": 1399
            },
            {
              "name": "inputs.to",
              "line": 1405
            },
            {
              "name": "targets.to",
              "line": 1405
            },
            {
              "name": "psutil.cpu_percent",
              "line": 1374
            },
            {
              "name": "int",
              "line": 1382
            },
            {
              "name": "torch.cuda.is_available",
              "line": 1394
            },
            {
              "name": "....sum",
              "line": 1414
            },
            {
              "name": "str",
              "line": 1397
            },
            {
              "name": "predicted.eq",
              "line": 1414
            }
          ],
          "docstring": "\n        Validate the model on a dataset.\n        \n        Args:\n            dataset: Validation dataset\n            batch_size: Batch size\n            \n        Returns:\n            Dictionary with validation metrics\n        ",
          "code_snippet": "        return {'loss': epoch_loss, 'accuracy': epoch_acc}\n    \n    def _validate(self, dataset, batch_size=128):\n        \"\"\"\n        Validate the model on a dataset.\n        \n        Args:\n            dataset: Validation dataset\n            batch_size: Batch size\n            \n        Returns:\n            Dictionary with validation metrics\n        \"\"\"\n        self.model.eval()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        # Create dataloader\n        if hasattr(self, 'using_lazy_augmentation') and self.using_lazy_augmentation:\n            # Use optimized dataloader from lazy augmentation module\n            dataloader = self.create_optimized_dataloader(\n                dataset=dataset,\n                batch_size=batch_size,\n                shuffle=False\n            )\n        else:\n            # Fallback to standard DataLoader with conservative settings\n            try:\n                # Calculate optimal number of workers based on system capabilities\n                import multiprocessing\n                import psutil\n                import math\n                \n                # Get number of CPU cores\n                cpu_count = multiprocessing.cpu_count()\n                \n                # Get system load\n                system_load = 0.5  # Default value if psutil fails\n                try:\n                    system_load = psutil.cpu_percent(interval=0.1) / 100.0\n                except:\n                    pass\n                    \n                # Calculate resource factor based on cognitive efficiency formula\n                resource_factor = (1 - system_load * 0.8)\n                \n                # Calculate optimal workers - apply parallel processing penalty\n                optimal_workers = max(0, min(int(cpu_count * resource_factor * 0.75), 4))\n                \n                if torch.cuda.is_available():\n                    # When using GPU, we want fewer workers\n                    optimal_workers = min(optimal_workers, 2)\n                    \n                logger.info(f\"Using {optimal_workers} DataLoader workers for validation (cores: {cpu_count}, load: {system_load:.2f})\")\n                \n                # Use optimal workers for DataLoader\n                dataloader = torch.utils.data.DataLoader(\n                    dataset, batch_size=batch_size, shuffle=False, \n                    num_workers=optimal_workers,\n                    pin_memory=torch.cuda.is_available())\n                    \n            except Exception as e:\n                logger.warning(f\"Error creating optimized validation DataLoader: {str(e)}\")\n                # Fallback to single-process data loading\n                dataloader = torch.utils.data.DataLoader(\n                    dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n        \n        # Disable gradient calculation\n        with torch.no_grad():\n            for inputs, targets in dataloader:\n                inputs, targets = inputs.to(self.device), targets.to(self.device)\n                \n                # Forward pass\n                outputs = self.model(inputs)\n                loss = self.criterion(outputs, targets)\n                \n                # Update metrics\n                running_loss += loss.item()\n                _, predicted = outputs.max(1)\n                correct += predicted.eq(targets).sum().item()\n                total += targets.size(0)\n        \n        # Calculate metrics\n        val_loss = running_loss / len(dataloader)\n        val_acc = 100. * correct / total\n        \n        return {'loss': val_loss, 'accuracy': val_acc}\n\ndef calculate_optimal_workers():\n    \"\"\"\n    Calculate the optimal number of workers based on system capabilities."
        }
      },
      "class_variables": [],
      "bases": [
        "AdaptiveTrainer"
      ],
      "docstring": "\n    Enhanced adaptive trainer that uses the unified risk/accuracy ratio approach\n    for batch size adaptation and learning rate adjustments.\n    "
    }
  },
  "functions": {
    "load_cifar10_data": {
      "start_line": 18,
      "end_line": 47,
      "parameters": [],
      "return_type": null,
      "calls": [
        {
          "name": "transforms.Compose",
          "line": 26
        },
        {
          "name": "transforms.Compose",
          "line": 33
        },
        {
          "name": "torchvision.datasets.CIFAR10",
          "line": 39
        },
        {
          "name": "torchvision.datasets.CIFAR10",
          "line": 42
        },
        {
          "name": "transforms.RandomCrop",
          "line": 27
        },
        {
          "name": "transforms.RandomHorizontalFlip",
          "line": 28
        },
        {
          "name": "transforms.ToTensor",
          "line": 29
        },
        {
          "name": "transforms.Normalize",
          "line": 30
        },
        {
          "name": "transforms.ToTensor",
          "line": 34
        },
        {
          "name": "transforms.Normalize",
          "line": 35
        }
      ],
      "docstring": "\n    Load and prepare CIFAR-10 dataset.\n    \n    Returns:\n        Train dataset, test dataset\n    ",
      "code_snippet": "\"\"\"\n\ndef load_cifar10_data():\n    \"\"\"\n    Load and prepare CIFAR-10 dataset.\n    \n    Returns:\n        Train dataset, test dataset\n    \"\"\"\n    # Data transforms\n    transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n\n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n\n    # Load datasets\n    trainset = torchvision.datasets.CIFAR10(\n        root='./data', train=True, download=True, transform=transform_train)\n\n    testset = torchvision.datasets.CIFAR10(\n        root='./data', train=False, download=True, transform=transform_test)\n\n    return trainset, testset\n\nimport os\nimport sys\nimport time"
    },
    "register_unified_ratio_optimizer": {
      "start_line": 239,
      "end_line": 298,
      "parameters": [
        {
          "name": "config_params"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "logger.info",
          "line": 293
        },
        {
          "name": "logger.info",
          "line": 294
        },
        {
          "name": "logger.info",
          "line": 295
        },
        {
          "name": "logger.info",
          "line": 296
        },
        {
          "name": "config_params.get",
          "line": 257
        },
        {
          "name": "config_params.get",
          "line": 258
        },
        {
          "name": "config_params.get",
          "line": 259
        },
        {
          "name": "config_params.get",
          "line": 260
        },
        {
          "name": "config_params.get",
          "line": 261
        },
        {
          "name": "config_params.get",
          "line": 262
        },
        {
          "name": "config_params.get",
          "line": 263
        }
      ],
      "docstring": "\n    Register the EVEUnifiedRatio optimizer in the available optimizers.\n    \n    Args:\n        config_params: Optional dictionary with configuration parameters\n    ",
      "code_snippet": "\n# Register the unified ratio EVE optimizer\ndef register_unified_ratio_optimizer(config_params=None):\n    \"\"\"\n    Register the EVEUnifiedRatio optimizer in the available optimizers.\n    \n    Args:\n        config_params: Optional dictionary with configuration parameters\n    \"\"\"\n    # Set default configuration values\n    debug_ratios = False\n    debug_bounds = False\n    weight_adjustment_range = \"default\"  # Default value (0.85-1.15)\n    lr_check_interval = 10  # Default value\n    lr_change_threshold = 0.005  # Default value\n    lr_log_threshold = 0.05  # Default value\n    use_equilibrium_bounds = True  # Default to use equilibrium bounds\n    \n    # Override with provided parameters if any\n    if config_params:\n        debug_ratios = config_params.get('debug_ratios', debug_ratios)\n        debug_bounds = config_params.get('debug_bounds', debug_bounds)\n        weight_adjustment_range = config_params.get('weight_adjustment_range', weight_adjustment_range)\n        lr_check_interval = config_params.get('lr_check_interval', lr_check_interval)\n        lr_change_threshold = config_params.get('lr_change_threshold', lr_change_threshold)\n        lr_log_threshold = config_params.get('lr_log_threshold', lr_log_threshold)\n        use_equilibrium_bounds = config_params.get('use_equilibrium_bounds', use_equilibrium_bounds)\n    \n    # Create configuration for the unified ratio optimizer\n    unified_config = {\n        'default': {\n            'optimizer_class': EVEUnifiedRatio,\n            'optimizer_kwargs': {\n                'lr': 0.01,\n                'eps': 1e-8,\n                'base_confidence_threshold': 0.7,\n                'weight_decay': 0.0001,\n                'debug_ratios': debug_ratios,\n                'debug_bounds': debug_bounds,\n                'warmup_epochs': 1,  # Faster warmup\n                'weight_adjustment_range': weight_adjustment_range,\n                'lr_check_interval': lr_check_interval,\n                'lr_change_threshold': lr_change_threshold,\n                'lr_log_threshold': lr_log_threshold,\n                'use_equilibrium_bounds': use_equilibrium_bounds\n            },\n            'scheduler_class': optim.lr_scheduler.CosineAnnealingLR,\n            'scheduler_kwargs': {\n                'T_max': 200\n            }\n        }\n    }\n    \n    # Update ALL_CONFIGS with the new optimizer\n    ALL_CONFIGS['eve_unified'] = unified_config\n    \n    logger.info(\"EVEUnifiedRatio optimizer registered successfully\")\n    logger.info(f\"  Weight adjustment range: {weight_adjustment_range}\")\n    logger.info(f\"  LR sensitivity: check interval={lr_check_interval}, threshold={lr_change_threshold:.4f}\")\n    logger.info(f\"  Using equilibrium bounds: {use_equilibrium_bounds}\")\n\nclass UnifiedRatioTrainer(AdaptiveTrainer):\n    \"\"\"\n    Enhanced adaptive trainer that uses the unified risk/accuracy ratio approach"
    },
    "calculate_optimal_workers": {
      "start_line": 1423,
      "end_line": 1475,
      "parameters": [],
      "return_type": null,
      "calls": [
        {
          "name": "multiprocessing.cpu_count",
          "line": 1440
        },
        {
          "name": "math.ceil",
          "line": 1453
        },
        {
          "name": "torch.cuda.is_available",
          "line": 1456
        },
        {
          "name": "min",
          "line": 1465
        },
        {
          "name": "max",
          "line": 1466
        },
        {
          "name": "logger.info",
          "line": 1468
        },
        {
          "name": "psutil.cpu_percent",
          "line": 1443
        },
        {
          "name": "torch.cuda.device_count",
          "line": 1458
        },
        {
          "name": "min",
          "line": 1459
        },
        {
          "name": "max",
          "line": 1465
        },
        {
          "name": "min",
          "line": 1466
        },
        {
          "name": "logger.warning",
          "line": 1472
        },
        {
          "name": "psutil.virtual_memory",
          "line": 1446
        },
        {
          "name": "max",
          "line": 1459
        },
        {
          "name": "str",
          "line": 1472
        }
      ],
      "docstring": "\n    Calculate the optimal number of workers based on system capabilities.\n    \n    Following isekaiZen mathematical foundation, this function integrates\n    cognitive efficiency principles to determine the optimal number of\n    DataLoader workers that balances performance and stability.\n    \n    Returns:\n        Integer number of workers\n    ",
      "code_snippet": "        return {'loss': val_loss, 'accuracy': val_acc}\n\ndef calculate_optimal_workers():\n    \"\"\"\n    Calculate the optimal number of workers based on system capabilities.\n    \n    Following isekaiZen mathematical foundation, this function integrates\n    cognitive efficiency principles to determine the optimal number of\n    DataLoader workers that balances performance and stability.\n    \n    Returns:\n        Integer number of workers\n    \"\"\"\n    try:\n        import multiprocessing\n        import psutil\n        import math\n        \n        # Get number of CPU cores\n        cpu_count = multiprocessing.cpu_count()\n        \n        # Get current system load\n        system_load = psutil.cpu_percent(interval=0.1) / 100.0\n        \n        # Get memory usage as a factor\n        memory_usage = psutil.virtual_memory().percent / 100.0\n        \n        # Calculate resource factor using polynomial function\n        # This follows the cognitive efficiency function patterns from the isekaiZen mathematical foundation\n        resource_factor = (1 - system_load) * (1 - 0.8 * memory_usage)\n        \n        # Calculate base workers using the parallel processing penalty formula pattern\n        base_workers = math.ceil(cpu_count * resource_factor)\n        \n        # Add GPU factor if available\n        if torch.cuda.is_available():\n            # When using GPU, we need fewer workers to avoid bottlenecks\n            gpu_count = torch.cuda.device_count()\n            workers = min(base_workers, max(1, 2 * gpu_count))\n        else:\n            # For CPU-only, use calculated base workers\n            workers = base_workers\n        \n        # Apply bounds based on system capabilities\n        workers = min(workers, max(1, cpu_count - 1))  # Leave at least one core free\n        workers = max(1, min(workers, 4))  # Cap at 4 workers for stability\n        \n        logger.info(f\"Calculated optimal DataLoader workers: {workers} (from {cpu_count} cores, load: {system_load:.2f})\")\n        return workers\n        \n    except Exception as e:\n        logger.warning(f\"Error calculating optimal workers: {str(e)}. Using 0 workers for safety.\")\n        return 0  # Safe fallback\n\n# Callback for tracking unified ratio metrics\ndef track_unified_ratio_callback(epoch, history, model, optimizer):\n    \"\"\"Callback function to track unified ratio metrics and bounds status.\"\"\""
    },
    "track_unified_ratio_callback": {
      "start_line": 1476,
      "end_line": 1528,
      "parameters": [
        {
          "name": "epoch"
        },
        {
          "name": "history"
        },
        {
          "name": "model"
        },
        {
          "name": "optimizer"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "....append",
          "line": 1524
        },
        {
          "name": "isinstance",
          "line": 1479
        },
        {
          "name": "hasattr",
          "line": 1479
        },
        {
          "name": "optimizer.calculate_risk_accuracy_ratios",
          "line": 1480
        },
        {
          "name": "isinstance",
          "line": 1497
        },
        {
          "name": "hasattr",
          "line": 1497
        },
        {
          "name": "optimizer.get_bound_status_history",
          "line": 1498
        },
        {
          "name": "hasattr",
          "line": 1508
        },
        {
          "name": "logger.info",
          "line": 1490
        },
        {
          "name": "....join",
          "line": 1493
        },
        {
          "name": "logger.info",
          "line": 1494
        },
        {
          "name": "bound_history.items",
          "line": 1504
        },
        {
          "name": "optimizer.get_bound_adjustment_history",
          "line": 1509
        },
        {
          "name": "sum",
          "line": 1489
        },
        {
          "name": "len",
          "line": 1489
        },
        {
          "name": "adjustment_history.items",
          "line": 1515
        },
        {
          "name": "ratios.values",
          "line": 1489
        },
        {
          "name": "ratios.items",
          "line": 1493
        }
      ],
      "docstring": "Callback function to track unified ratio metrics and bounds status.",
      "code_snippet": "\n# Callback for tracking unified ratio metrics\ndef track_unified_ratio_callback(epoch, history, model, optimizer):\n    \"\"\"Callback function to track unified ratio metrics and bounds status.\"\"\"\n    # Track risk/accuracy ratios if available\n    if isinstance(optimizer, EVEUnifiedRatio) and hasattr(optimizer, 'calculate_risk_accuracy_ratios'):\n        ratios = optimizer.calculate_risk_accuracy_ratios()\n        \n        if ratios:\n            if 'risk_accuracy_ratios' not in history:\n                history['risk_accuracy_ratios'] = {}\n            \n            history['risk_accuracy_ratios'][epoch] = ratios\n            \n            # Calculate and log average ratio\n            avg_ratio = sum(ratios.values()) / len(ratios)\n            logger.info(f\"Epoch {epoch} - Average Risk/Accuracy ratio: {avg_ratio:.3f}\")\n            \n            # Store the detailed ratio information\n            ratio_str = \", \".join([f\"{k}: {v:.2f}\" for k, v in ratios.items()])\n            logger.info(f\"Risk/Accuracy ratios: {ratio_str}\")\n    \n    # Track equilibrium bounds status if available\n    if isinstance(optimizer, EVEUnifiedRatio) and hasattr(optimizer, 'get_bound_status_history'):\n        bound_history = optimizer.get_bound_status_history()\n        \n        if bound_history:\n            if 'equilibrium_bounds_history' not in history:\n                history['equilibrium_bounds_history'] = {}\n            \n            for e, statuses in bound_history.items():\n                history['equilibrium_bounds_history'][e] = statuses\n                \n        # Track bound adjustments if available\n        if hasattr(optimizer, 'get_bound_adjustment_history'):\n            adjustment_history = optimizer.get_bound_adjustment_history()\n            \n            if adjustment_history:\n                if 'bound_adjustments' not in history:\n                    history['bound_adjustments'] = {}\n                \n                for e, ratio in adjustment_history.items():\n                    history['bound_adjustments'][e] = ratio\n    \n    # Track learning rate\n    if 'learning_rates' not in history:\n        history['learning_rates'] = []\n    \n    # Get current learning rate\n    current_lr = optimizer.param_groups[0]['lr']\n    history['learning_rates'].append(current_lr)\n    \n    return False  # Continue training\n\ndef visualize_training_results(history, output_path=None):\n    \"\"\"\n    Visualize training results with unified risk/accuracy ratio information."
    },
    "visualize_training_results": {
      "start_line": 1528,
      "end_line": 1915,
      "parameters": [
        {
          "name": "history"
        },
        {
          "name": "output_path"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "plt.figure",
          "line": 1537
        },
        {
          "name": "range",
          "line": 1540
        },
        {
          "name": "plt.subplot",
          "line": 1548
        },
        {
          "name": "plt.plot",
          "line": 1550
        },
        {
          "name": "plt.title",
          "line": 1553
        },
        {
          "name": "plt.xlabel",
          "line": 1554
        },
        {
          "name": "plt.ylabel",
          "line": 1555
        },
        {
          "name": "plt.legend",
          "line": 1556
        },
        {
          "name": "plt.grid",
          "line": 1557
        },
        {
          "name": "plt.subplot",
          "line": 1560
        },
        {
          "name": "plt.plot",
          "line": 1562
        },
        {
          "name": "plt.title",
          "line": 1565
        },
        {
          "name": "plt.xlabel",
          "line": 1566
        },
        {
          "name": "plt.ylabel",
          "line": 1567
        },
        {
          "name": "plt.legend",
          "line": 1568
        },
        {
          "name": "plt.grid",
          "line": 1569
        },
        {
          "name": "plt.subplot",
          "line": 1572
        },
        {
          "name": "plt.subplot",
          "line": 1589
        },
        {
          "name": "plt.plot",
          "line": 1591
        },
        {
          "name": "plt.title",
          "line": 1592
        },
        {
          "name": "plt.xlabel",
          "line": 1593
        },
        {
          "name": "plt.ylabel",
          "line": 1594
        },
        {
          "name": "plt.grid",
          "line": 1595
        },
        {
          "name": "plt.subplot",
          "line": 1598
        },
        {
          "name": "plt.grid",
          "line": 1626
        },
        {
          "name": "plt.subplot",
          "line": 1629
        },
        {
          "name": "plt.grid",
          "line": 1657
        },
        {
          "name": "plt.subplot",
          "line": 1660
        },
        {
          "name": "plt.subplot",
          "line": 1695
        },
        {
          "name": "plt.subplot",
          "line": 1745
        },
        {
          "name": "plt.subplot",
          "line": 1779
        },
        {
          "name": "plt.subplot",
          "line": 1795
        },
        {
          "name": "plt.subplot",
          "line": 1837
        },
        {
          "name": "plt.subplot",
          "line": 1874
        },
        {
          "name": "plt.tight_layout",
          "line": 1906
        },
        {
          "name": "plt.close",
          "line": 1913
        },
        {
          "name": "plt.plot",
          "line": 1552
        },
        {
          "name": "plt.plot",
          "line": 1564
        },
        {
          "name": "plt.plot",
          "line": 1576
        },
        {
          "name": "plt.axhline",
          "line": 1577
        },
        {
          "name": "plt.title",
          "line": 1578
        },
        {
          "name": "plt.xlabel",
          "line": 1579
        },
        {
          "name": "plt.ylabel",
          "line": 1580
        },
        {
          "name": "plt.legend",
          "line": 1581
        },
        {
          "name": "plt.grid",
          "line": 1582
        },
        {
          "name": "plt.text",
          "line": 1584
        },
        {
          "name": "plt.title",
          "line": 1586
        },
        {
          "name": "set",
          "line": 1602
        },
        {
          "name": "plt.title",
          "line": 1616
        },
        {
          "name": "plt.xlabel",
          "line": 1617
        },
        {
          "name": "plt.ylabel",
          "line": 1618
        },
        {
          "name": "plt.legend",
          "line": 1619
        },
        {
          "name": "plt.text",
          "line": 1622
        },
        {
          "name": "plt.title",
          "line": 1624
        },
        {
          "name": "set",
          "line": 1633
        },
        {
          "name": "plt.title",
          "line": 1647
        },
        {
          "name": "plt.xlabel",
          "line": 1648
        },
        {
          "name": "plt.ylabel",
          "line": 1649
        },
        {
          "name": "plt.legend",
          "line": 1650
        },
        {
          "name": "plt.text",
          "line": 1653
        },
        {
          "name": "plt.title",
          "line": 1655
        },
        {
          "name": "sorted",
          "line": 1664
        },
        {
          "name": "set",
          "line": 1667
        },
        {
          "name": "....values",
          "line": 1668
        },
        {
          "name": "plt.title",
          "line": 1683
        },
        {
          "name": "plt.xlabel",
          "line": 1684
        },
        {
          "name": "plt.ylabel",
          "line": 1685
        },
        {
          "name": "plt.axhline",
          "line": 1686
        },
        {
          "name": "plt.legend",
          "line": 1687
        },
        {
          "name": "plt.grid",
          "line": 1688
        },
        {
          "name": "plt.text",
          "line": 1690
        },
        {
          "name": "plt.title",
          "line": 1692
        },
        {
          "name": "sorted",
          "line": 1699
        },
        {
          "name": "set",
          "line": 1702
        },
        {
          "name": "....values",
          "line": 1703
        },
        {
          "name": "plt.yticks",
          "line": 1732
        },
        {
          "name": "plt.title",
          "line": 1733
        },
        {
          "name": "plt.xlabel",
          "line": 1734
        },
        {
          "name": "plt.ylabel",
          "line": 1735
        },
        {
          "name": "plt.grid",
          "line": 1736
        },
        {
          "name": "plt.legend",
          "line": 1737
        },
        {
          "name": "plt.text",
          "line": 1740
        },
        {
          "name": "plt.title",
          "line": 1742
        },
        {
          "name": "set",
          "line": 1749
        },
        {
          "name": "plt.title",
          "line": 1767
        },
        {
          "name": "plt.xlabel",
          "line": 1768
        },
        {
          "name": "plt.ylabel",
          "line": 1769
        },
        {
          "name": "plt.axhline",
          "line": 1770
        },
        {
          "name": "plt.grid",
          "line": 1771
        },
        {
          "name": "plt.legend",
          "line": 1772
        },
        {
          "name": "plt.text",
          "line": 1774
        },
        {
          "name": "plt.title",
          "line": 1776
        },
        {
          "name": "plt.plot",
          "line": 1782
        },
        {
          "name": "plt.title",
          "line": 1783
        },
        {
          "name": "plt.xlabel",
          "line": 1784
        },
        {
          "name": "plt.ylabel",
          "line": 1785
        },
        {
          "name": "plt.yscale",
          "line": 1786
        },
        {
          "name": "plt.grid",
          "line": 1787
        },
        {
          "name": "plt.legend",
          "line": 1788
        },
        {
          "name": "plt.text",
          "line": 1790
        },
        {
          "name": "plt.title",
          "line": 1792
        },
        {
          "name": "set",
          "line": 1799
        },
        {
          "name": "plt.title",
          "line": 1826
        },
        {
          "name": "plt.xlabel",
          "line": 1827
        },
        {
          "name": "plt.ylabel",
          "line": 1828
        },
        {
          "name": "plt.grid",
          "line": 1829
        },
        {
          "name": "plt.legend",
          "line": 1830
        },
        {
          "name": "plt.text",
          "line": 1832
        },
        {
          "name": "plt.title",
          "line": 1834
        },
        {
          "name": "sorted",
          "line": 1841
        },
        {
          "name": "plt.text",
          "line": 1869
        },
        {
          "name": "plt.title",
          "line": 1871
        },
        {
          "name": "set",
          "line": 1878
        },
        {
          "name": "plt.title",
          "line": 1896
        },
        {
          "name": "plt.xlabel",
          "line": 1897
        },
        {
          "name": "plt.ylabel",
          "line": 1898
        },
        {
          "name": "plt.grid",
          "line": 1899
        },
        {
          "name": "plt.legend",
          "line": 1900
        },
        {
          "name": "plt.text",
          "line": 1902
        },
        {
          "name": "plt.title",
          "line": 1904
        },
        {
          "name": "plt.savefig",
          "line": 1910
        },
        {
          "name": "logger.info",
          "line": 1911
        },
        {
          "name": "len",
          "line": 1540
        },
        {
          "name": "pattern_types.update",
          "line": 1604
        },
        {
          "name": "pattern_types.update",
          "line": 1635
        },
        {
          "name": "....keys",
          "line": 1664
        },
        {
          "name": "pattern_types.update",
          "line": 1669
        },
        {
          "name": "....keys",
          "line": 1699
        },
        {
          "name": "pattern_types.update",
          "line": 1704
        },
        {
          "name": "pattern_types.update",
          "line": 1751
        },
        {
          "name": "enumerate",
          "line": 1759
        },
        {
          "name": "pattern_types.update",
          "line": 1801
        },
        {
          "name": "min",
          "line": 1809
        },
        {
          "name": "range",
          "line": 1811
        },
        {
          "name": "plt.plot",
          "line": 1853
        },
        {
          "name": "plt.axhline",
          "line": 1856
        },
        {
          "name": "plt.axhline",
          "line": 1857
        },
        {
          "name": "plt.title",
          "line": 1859
        },
        {
          "name": "plt.xlabel",
          "line": 1860
        },
        {
          "name": "plt.ylabel",
          "line": 1861
        },
        {
          "name": "plt.grid",
          "line": 1862
        },
        {
          "name": "plt.legend",
          "line": 1863
        },
        {
          "name": "plt.text",
          "line": 1865
        },
        {
          "name": "plt.title",
          "line": 1867
        },
        {
          "name": "pattern_types.update",
          "line": 1880
        },
        {
          "name": "enumerate",
          "line": 1888
        },
        {
          "name": "max",
          "line": 1575
        },
        {
          "name": "zip",
          "line": 1575
        },
        {
          "name": "rates.keys",
          "line": 1604
        },
        {
          "name": "epoch_rates.get",
          "line": 1607
        },
        {
          "name": "len",
          "line": 1609
        },
        {
          "name": "len",
          "line": 1609
        },
        {
          "name": "plt.plot",
          "line": 1610
        },
        {
          "name": "plt.plot",
          "line": 1614
        },
        {
          "name": "risks.keys",
          "line": 1635
        },
        {
          "name": "epoch_risks.get",
          "line": 1638
        },
        {
          "name": "len",
          "line": 1640
        },
        {
          "name": "len",
          "line": 1640
        },
        {
          "name": "plt.plot",
          "line": 1641
        },
        {
          "name": "plt.plot",
          "line": 1645
        },
        {
          "name": "ratios.keys",
          "line": 1669
        },
        {
          "name": "plt.plot",
          "line": 1681
        },
        {
          "name": "statuses.keys",
          "line": 1704
        },
        {
          "name": "weights.keys",
          "line": 1751
        },
        {
          "name": "plt.plot",
          "line": 1765
        },
        {
          "name": "risks.keys",
          "line": 1801
        },
        {
          "name": "len",
          "line": 1809
        },
        {
          "name": "len",
          "line": 1809
        },
        {
          "name": "plt.scatter",
          "line": 1817
        },
        {
          "name": "int",
          "line": 1841
        },
        {
          "name": "str",
          "line": 1846
        },
        {
          "name": "ratios.append",
          "line": 1847
        },
        {
          "name": "counts.keys",
          "line": 1880
        },
        {
          "name": "plt.plot",
          "line": 1894
        },
        {
          "name": "ratios.append",
          "line": 1677
        },
        {
          "name": "epochs_with_data.append",
          "line": 1678
        },
        {
          "name": "plt.scatter",
          "line": 1729
        },
        {
          "name": "weight_values.append",
          "line": 1761
        },
        {
          "name": "adjustment_epochs.append",
          "line": 1762
        },
        {
          "name": "risks.append",
          "line": 1813
        },
        {
          "name": "accuracies.append",
          "line": 1814
        },
        {
          "name": "len",
          "line": 1820
        },
        {
          "name": "np.polyfit",
          "line": 1821
        },
        {
          "name": "np.array",
          "line": 1822
        },
        {
          "name": "plt.plot",
          "line": 1824
        },
        {
          "name": "....keys",
          "line": 1841
        },
        {
          "name": "ratios.append",
          "line": 1849
        },
        {
          "name": "pattern_counts.append",
          "line": 1890
        },
        {
          "name": "pattern_epochs.append",
          "line": 1891
        },
        {
          "name": "str",
          "line": 1847
        },
        {
          "name": "len",
          "line": 1610
        },
        {
          "name": "len",
          "line": 1613
        },
        {
          "name": "len",
          "line": 1613
        },
        {
          "name": "len",
          "line": 1641
        },
        {
          "name": "len",
          "line": 1644
        },
        {
          "name": "len",
          "line": 1644
        },
        {
          "name": "x_pts.append",
          "line": 1723
        },
        {
          "name": "y_pts.append",
          "line": 1726
        },
        {
          "name": "min",
          "line": 1822
        },
        {
          "name": "max",
          "line": 1822
        },
        {
          "name": "....index",
          "line": 1725
        },
        {
          "name": "list",
          "line": 1725
        }
      ],
      "docstring": "\n    Visualize training results with unified risk/accuracy ratio information.\n    \n    Args:\n        history: Training history dictionary\n        output_path: Optional output file path\n    ",
      "code_snippet": "    return False  # Continue training\n\ndef visualize_training_results(history, output_path=None):\n    \"\"\"\n    Visualize training results with unified risk/accuracy ratio information.\n    \n    Args:\n        history: Training history dictionary\n        output_path: Optional output file path\n    \"\"\"\n    # Create figure with more subplots for EVE-specific metrics\n    plt.figure(figsize=(15, 28))  # Increased figure height for more plots\n    \n    # Create epochs range\n    epochs = range(1, len(history['train_loss']) + 1)\n    \n    # Plot arrangement\n    num_rows = 8  # Increased from 7 to 8 rows to accommodate the augmentation plot\n    num_cols = 2\n    plot_idx = 1\n    \n    # 1. Training metrics: Loss\n    plt.subplot(num_rows, num_cols, plot_idx)\n    plot_idx += 1\n    plt.plot(epochs, history['train_loss'], 'b-', label='Training Loss')\n    if 'val_loss' in history and history['val_loss']:\n        plt.plot(epochs, history['val_loss'], 'r-', label='Validation Loss')\n    plt.title('Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n    \n    # 2. Training metrics: Accuracy\n    plt.subplot(num_rows, num_cols, plot_idx)\n    plot_idx += 1\n    plt.plot(epochs, history['train_acc'], 'b-', label='Training Accuracy')\n    if 'val_acc' in history and history['val_acc']:\n        plt.plot(epochs, history['val_acc'], 'r-', label='Validation Accuracy')\n    plt.title('Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy (%)')\n    plt.legend()\n    plt.grid(True)\n    \n    # 3. Train/Test Accuracy Ratio\n    plt.subplot(num_rows, num_cols, plot_idx)\n    plot_idx += 1\n    if 'train_acc' in history and 'val_acc' in history:\n        ratios = [train/max(0.1, val) for train, val in zip(history['train_acc'], history['val_acc'])]\n        plt.plot(epochs, ratios, 'g-o', label='Train/Test Ratio')\n        plt.axhline(y=1.0, color='r', linestyle='--', label='Ideal Ratio (1.0)')\n        plt.title('Train/Test Accuracy Ratio')\n        plt.xlabel('Epoch')\n        plt.ylabel('Ratio')\n        plt.legend()\n        plt.grid(True)\n    else:\n        plt.text(0.5, 0.5, 'No accuracy data available',\n                horizontalalignment='center', verticalalignment='center')\n        plt.title('Train/Test Accuracy Ratio')\n    \n    # 4. Batch sizes\n    plt.subplot(num_rows, num_cols, plot_idx)\n    plot_idx += 1\n    plt.plot(epochs, history['batch_sizes'], 'g-o', label='Batch Size')\n    plt.title('Batch Size')\n    plt.xlabel('Epoch')\n    plt.ylabel('Batch Size')\n    plt.grid(True)\n    \n    # 5. Pattern recognition rates\n    plt.subplot(num_rows, num_cols, plot_idx)\n    plot_idx += 1\n    if 'pattern_recognition_rates' in history and history['pattern_recognition_rates']:\n        # Extract pattern types and create separate lines for each\n        pattern_types = set()\n        for rates in history['pattern_recognition_rates']:\n            pattern_types.update(rates.keys())\n        \n        for pattern_type in pattern_types:\n            rates = [epoch_rates.get(pattern_type, 0) for epoch_rates in history['pattern_recognition_rates']]\n            # Only plot if we have enough data points\n            if len(rates) >= len(epochs):\n                plt.plot(epochs[:len(rates)], rates, marker='o', alpha=0.7, label=pattern_type)\n            else:\n                # Pad with zeros if needed\n                padded_rates = [0] * (len(epochs) - len(rates)) + rates\n                plt.plot(epochs, padded_rates, marker='o', alpha=0.7, label=pattern_type)\n        \n        plt.title('Pattern Recognition Rates')\n        plt.xlabel('Epoch')\n        plt.ylabel('Recognition Rate')\n        plt.legend(loc='lower right')\n    else:\n        # Fallback\n        plt.text(0.5, 0.5, 'No pattern recognition data available', \n                horizontalalignment='center', verticalalignment='center')\n        plt.title('Pattern Recognition Rates')\n    \n    plt.grid(True)\n    \n    # 6. Pattern risks (derived from accuracy)\n    plt.subplot(num_rows, num_cols, plot_idx)\n    plot_idx += 1\n    if 'pattern_risks' in history and history['pattern_risks']:\n        # Extract pattern types and create separate lines for each\n        pattern_types = set()\n        for risks in history['pattern_risks']:\n            pattern_types.update(risks.keys())\n        \n        for pattern_type in pattern_types:\n            risks = [epoch_risks.get(pattern_type, 0.5) for epoch_risks in history['pattern_risks']]\n            # Only plot if we have enough data points\n            if len(risks) >= len(epochs):\n                plt.plot(epochs[:len(risks)], risks, marker='o', alpha=0.7, label=pattern_type)\n            else:\n                # Pad with default value (0.5) if needed\n                padded_risks = [0.5] * (len(epochs) - len(risks)) + risks\n                plt.plot(epochs, padded_risks, marker='o', alpha=0.7, label=pattern_type)\n        \n        plt.title('Pattern Risk Levels (Derived from Accuracy)')\n        plt.xlabel('Epoch')\n        plt.ylabel('Risk Level')\n        plt.legend(loc='upper right')\n    else:\n        # Fallback\n        plt.text(0.5, 0.5, 'No pattern risk data available', \n                horizontalalignment='center', verticalalignment='center')\n        plt.title('Pattern Risk Levels')\n    \n    plt.grid(True)\n    \n    # 7. Unified Risk/Accuracy Ratios\n    plt.subplot(num_rows, num_cols, plot_idx)\n    plot_idx += 1\n    if 'risk_accuracy_ratios' in history and history['risk_accuracy_ratios']:\n        # Sort epochs\n        epochs_list = sorted(history['risk_accuracy_ratios'].keys())\n        \n        # Extract pattern types\n        pattern_types = set()\n        for ratios in history['risk_accuracy_ratios'].values():\n            pattern_types.update(ratios.keys())\n        \n        for pattern_type in pattern_types:\n            ratios = []\n            epochs_with_data = []\n            \n            for epoch in epochs_list:\n                if pattern_type in history['risk_accuracy_ratios'][epoch]:\n                    ratios.append(history['risk_accuracy_ratios'][epoch][pattern_type])\n                    epochs_with_data.append(epoch + 1)  # 1-indexed epochs\n            \n            if ratios:\n                plt.plot(epochs_with_data, ratios, marker='o', alpha=0.7, label=pattern_type)\n        \n        plt.title('Unified Risk/Accuracy Ratios')\n        plt.xlabel('Epoch')\n        plt.ylabel('Risk/Accuracy Ratio')\n        plt.axhline(y=1.0, color='k', linestyle='--', alpha=0.5, label='Balanced (1.0)')\n        plt.legend(loc='upper right')\n        plt.grid(True)\n    else:\n        plt.text(0.5, 0.5, 'No unified ratio data available', \n                horizontalalignment='center', verticalalignment='center')\n        plt.title('Unified Risk/Accuracy Ratios')\n    \n    # 8. Pattern Equilibrium Bounds Visualization\n    plt.subplot(num_rows, num_cols, plot_idx)\n    plot_idx += 1\n    if 'equilibrium_bounds_history' in history and history['equilibrium_bounds_history']:\n        # Extract data for plotting\n        epochs_list = sorted(history['equilibrium_bounds_history'].keys())\n        \n        # Get all pattern types\n        pattern_types = set()\n        for statuses in history['equilibrium_bounds_history'].values():\n            pattern_types.update(statuses.keys())\n        \n        # Set up colors and markers\n        colors = {'min': 'red', 'max': 'blue'}\n        labels = {'min': 'Below Min', 'max': 'Above Max'}\n        \n        # Plot min and max bound violations separately\n        for bound_type in ['min', 'max']:\n            for pattern_type in pattern_types:\n                # Create lists to hold the data points\n                x_pts = []\n                y_pts = []\n                \n                for epoch in epochs_list:\n                    if pattern_type in history['equilibrium_bounds_history'][epoch]:\n                        status = history['equilibrium_bounds_history'][epoch][pattern_type]\n                        \n                        # If it violates the bound (value is False), add a point\n                        if bound_type in status and not status[bound_type]:\n                            x_pts.append(epoch + 1)  # 1-indexed for display\n                            # Offset points slightly based on pattern type for visibility\n                            y_offset = list(pattern_types).index(pattern_type) * 0.1\n                            y_pts.append(y_offset + (0 if bound_type == 'min' else 0.5))\n                \n                if x_pts:\n                    plt.scatter(x_pts, y_pts, label=f\"{pattern_type} {labels[bound_type]}\", \n                               color=colors[bound_type], alpha=0.7, marker='o')\n        \n        plt.yticks([0.1, 0.6], ['Min Bound', 'Max Bound'])\n        plt.title('Pattern Bound Violations')\n        plt.xlabel('Epoch')\n        plt.ylabel('Bound Type')\n        plt.grid(True, axis='x')\n        plt.legend(loc='best', fontsize='small')\n    else:\n        # Leave empty if not using bounds\n        plt.text(0.5, 0.5, 'No equilibrium bounds data available',\n                horizontalalignment='center', verticalalignment='center')\n        plt.title('Pattern Bound Violations')\n    \n    # 9. EVE Weight Adjustments\n    plt.subplot(num_rows, num_cols, plot_idx)\n    plot_idx += 1\n    if 'weight_adjustments' in history and history['weight_adjustments']:\n        # Extract pattern types\n        pattern_types = set()\n        for weights in history['weight_adjustments']:\n            pattern_types.update(weights.keys())\n        \n        # Create plot for each pattern type\n        for pattern_type in pattern_types:\n            weight_values = []\n            adjustment_epochs = []\n            \n            # Extract data points for this pattern\n            for i, weight_data in enumerate(history['weight_adjustments']):\n                if pattern_type in weight_data:\n                    weight_values.append(weight_data[pattern_type])\n                    adjustment_epochs.append(i + 1)  # 1-indexed epochs\n            \n            if weight_values:\n                plt.plot(adjustment_epochs, weight_values, marker='o', alpha=0.7, label=f\"{pattern_type}\")\n        \n        plt.title('EVE Weight Adjustments by Pattern')\n        plt.xlabel('Epoch')\n        plt.ylabel('Weight Adjustment Factor')\n        plt.axhline(y=1.0, color='k', linestyle='--', alpha=0.3, label='No Adjustment')\n        plt.grid(True)\n        plt.legend(loc='upper left')\n    else:\n        plt.text(0.5, 0.5, 'No weight adjustment data available', \n                horizontalalignment='center', verticalalignment='center')\n        plt.title('EVE Weight Adjustments by Pattern')\n    \n    # 9. Learning Rate\n    plt.subplot(num_rows, num_cols, plot_idx)\n    plot_idx += 1\n    if 'learning_rates' in history and history['learning_rates']:\n        plt.plot(epochs, history['learning_rates'], 'r-o', label='Learning Rate')\n        plt.title('Learning Rate Adaptation')\n        plt.xlabel('Epoch')\n        plt.ylabel('Learning Rate')\n        plt.yscale('log')  # Log scale often better for LR\n        plt.grid(True)\n        plt.legend()\n    else:\n        plt.text(0.5, 0.5, 'No learning rate data available', \n                horizontalalignment='center', verticalalignment='center')\n        plt.title('Learning Rate Adaptation')\n    \n    # 10. Risk/Accuracy Relationship\n    plt.subplot(num_rows, num_cols, plot_idx)\n    plot_idx += 1\n    if 'pattern_risks' in history and history['pattern_risks'] and 'pattern_recognition_rates' in history and history['pattern_recognition_rates']:\n        # Get all pattern types\n        pattern_types = set()\n        for risks in history['pattern_risks']:\n            pattern_types.update(risks.keys())\n        \n        for pattern_type in pattern_types:\n            # Extract data for this pattern\n            risks = []\n            accuracies = []\n            \n            # Use epochs where we have both risk and accuracy data\n            min_data_points = min(len(history['pattern_risks']), len(history['pattern_recognition_rates']))\n            \n            for i in range(min_data_points):\n                if pattern_type in history['pattern_risks'][i] and pattern_type in history['pattern_recognition_rates'][i]:\n                    risks.append(history['pattern_risks'][i][pattern_type])\n                    accuracies.append(history['pattern_recognition_rates'][i][pattern_type])\n            \n            if risks and accuracies:\n                plt.scatter(risks, accuracies, alpha=0.7, label=pattern_type)\n                \n                # Calculate trend line\n                if len(risks) > 1:\n                    a, b = np.polyfit(risks, accuracies, 1)\n                    x_line = np.array([min(risks), max(risks)])\n                    y_line = a * x_line + b\n                    plt.plot(x_line, y_line, '--', alpha=0.5)\n        \n        plt.title('Risk-Accuracy Relationship')\n        plt.xlabel('Risk Level')\n        plt.ylabel('Accuracy')\n        plt.grid(True)\n        plt.legend(loc='upper right')\n    else:\n        plt.text(0.5, 0.5, 'No risk-accuracy relationship data available', \n                horizontalalignment='center', verticalalignment='center')\n        plt.title('Risk-Accuracy Relationship')\n        \n    # 11. Equilibrium Bound Adjustments\n    plt.subplot(num_rows, num_cols, plot_idx)\n    plot_idx += 1\n    if 'bound_adjustments' in history and history['bound_adjustments']:\n        # Sort epochs\n        sorted_epochs = sorted([int(e) for e in history['bound_adjustments'].keys()])\n        # Extract train/test ratios\n        ratios = []\n        for e in sorted_epochs:\n            # Handle both string and integer keys\n            if str(e) in history['bound_adjustments']:\n                ratios.append(history['bound_adjustments'][str(e)])\n            elif e in history['bound_adjustments']:\n                ratios.append(history['bound_adjustments'][e])\n        \n        if sorted_epochs and ratios:  # Only plot if we have valid data\n            # Plot the train/test ratios that drive bound adjustments\n            plt.plot(sorted_epochs, ratios, 'r-o', label='Train/Test Ratio')\n            \n            # Show bounds\n            plt.axhline(y=1.0, color='k', linestyle='--', alpha=0.5, label='Balanced')\n            plt.axhline(y=1.0, color='r', linestyle='--', alpha=0.5, label='Adjustment threshold')\n                \n            plt.title('Equilibrium Bound Adjustment Factors')\n            plt.xlabel('Epoch')\n            plt.ylabel('Train/Test Ratio')\n            plt.grid(True)\n            plt.legend(loc='upper right')\n        else:\n            plt.text(0.5, 0.5, 'No valid bound adjustment data to plot', \n                    horizontalalignment='center', verticalalignment='center')\n            plt.title('Equilibrium Bound Adjustments')\n    else:\n        plt.text(0.5, 0.5, 'No bound adjustment data available', \n                horizontalalignment='center', verticalalignment='center')\n        plt.title('Equilibrium Bound Adjustments')\n    \n    # 12. Augmentation Counts\n    plt.subplot(num_rows, num_cols, plot_idx)\n    plot_idx += 1\n    if 'augmentation_counts' in history and history['augmentation_counts']:\n        # Extract pattern types\n        pattern_types = set()\n        for counts in history['augmentation_counts']:\n            pattern_types.update(counts.keys())\n        \n        # Plot counts for each pattern type\n        for pattern_type in pattern_types:\n            # Extract counts for this pattern type\n            pattern_counts = []\n            pattern_epochs = []\n            \n            for i, counts in enumerate(history['augmentation_counts']):\n                if pattern_type in counts:\n                    pattern_counts.append(counts[pattern_type])\n                    pattern_epochs.append(i + 1)  # 1-indexed epochs\n            \n            if pattern_counts:\n                plt.plot(pattern_epochs, pattern_counts, marker='o', alpha=0.7, label=pattern_type)\n        \n        plt.title('Augmentation Counts by Pattern Type')\n        plt.xlabel('Epoch')\n        plt.ylabel('Number of Augmented Examples')\n        plt.grid(True)\n        plt.legend(loc='upper left')\n    else:\n        plt.text(0.5, 0.5, 'No augmentation data available', \n                horizontalalignment='center', verticalalignment='center')\n        plt.title('Augmentation Counts')\n    \n    plt.tight_layout()\n    \n    # Save figure if output path provided\n    if output_path:\n        plt.savefig(output_path)\n        logger.info(f\"Training visualization saved to: {output_path}\")\n    \n    plt.close()\n\ndef format_metrics_table(metrics_dict, title=\"Metrics\", min_width=15):\n    \"\"\"Format metrics as a readable table with aligned columns.\"\"\"\n    # Calculate column widths"
    },
    "format_metrics_table": {
      "start_line": 1915,
      "end_line": 1940,
      "parameters": [
        {
          "name": "metrics_dict"
        },
        {
          "name": "title"
        },
        {
          "name": "min_width"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "max",
          "line": 1918
        },
        {
          "name": "max",
          "line": 1919
        },
        {
          "name": "max",
          "line": 1922
        },
        {
          "name": "max",
          "line": 1923
        },
        {
          "name": "metrics_dict.items",
          "line": 1931
        },
        {
          "name": "....join",
          "line": 1938
        },
        {
          "name": "isinstance",
          "line": 1932
        },
        {
          "name": "lines.append",
          "line": 1936
        },
        {
          "name": "len",
          "line": 1918
        },
        {
          "name": "len",
          "line": 1919
        },
        {
          "name": "str",
          "line": 1935
        },
        {
          "name": "str",
          "line": 1918
        },
        {
          "name": "metrics_dict.keys",
          "line": 1918
        },
        {
          "name": "metrics_dict.values",
          "line": 1919
        },
        {
          "name": "isinstance",
          "line": 1919
        },
        {
          "name": "str",
          "line": 1919
        },
        {
          "name": "str",
          "line": 1936
        }
      ],
      "docstring": "Format metrics as a readable table with aligned columns.",
      "code_snippet": "    plt.close()\n\ndef format_metrics_table(metrics_dict, title=\"Metrics\", min_width=15):\n    \"\"\"Format metrics as a readable table with aligned columns.\"\"\"\n    # Calculate column widths\n    key_width = max(len(str(k)) for k in metrics_dict.keys())\n    val_width = max(len(f\"{v:.4f}\" if isinstance(v, float) else str(v)) for v in metrics_dict.values())\n    \n    # Ensure minimum width\n    key_width = max(key_width, min_width)\n    val_width = max(val_width, min_width)\n    \n    # Create header\n    total_width = key_width + val_width + 3  # 3 for spacing and separator\n    header = f\"\\n{title}\\n\" + \"-\" * total_width\n    \n    # Format each line\n    lines = [header]\n    for key, value in metrics_dict.items():\n        if isinstance(value, float):\n            val_str = f\"{value:.4f}\"\n        else:\n            val_str = str(value)\n        lines.append(f\"{str(key):<{key_width}} | {val_str:>{val_width}}\")\n    \n    return \"\\n\".join(lines)\n\ndef log_section(title, width=80):\n    \"\"\"Print a clearly defined section header.\"\"\"\n    logger.info(\"\\n\" + \"=\" * width)"
    },
    "log_section": {
      "start_line": 1940,
      "end_line": 1946,
      "parameters": [
        {
          "name": "title"
        },
        {
          "name": "width"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "logger.info",
          "line": 1942
        },
        {
          "name": "logger.info",
          "line": 1943
        },
        {
          "name": "logger.info",
          "line": 1944
        },
        {
          "name": "title.center",
          "line": 1943
        }
      ],
      "docstring": "Print a clearly defined section header.",
      "code_snippet": "    return \"\\n\".join(lines)\n\ndef log_section(title, width=80):\n    \"\"\"Print a clearly defined section header.\"\"\"\n    logger.info(\"\\n\" + \"=\" * width)\n    logger.info(f\"{title.center(width)}\")\n    logger.info(\"=\" * width)\n\ndef create_model(model_type=\"resnet18\", use_pretrained=False, num_classes=10, input_channels=3, input_size=32):\n    \"\"\"\n    Create a model for the specified architecture, optionally with pre-trained weights."
    },
    "create_model": {
      "start_line": 1946,
      "end_line": 2121,
      "parameters": [
        {
          "name": "model_type"
        },
        {
          "name": "use_pretrained"
        },
        {
          "name": "num_classes"
        },
        {
          "name": "input_channels"
        },
        {
          "name": "input_size"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "logger.info",
          "line": 1960
        },
        {
          "name": "model_type.startswith",
          "line": 1963
        },
        {
          "name": "nn.Linear",
          "line": 2008
        },
        {
          "name": "model_type.startswith",
          "line": 2010
        },
        {
          "name": "models.resnet18",
          "line": 1966
        },
        {
          "name": "model.conv1.weight.clone",
          "line": 1980
        },
        {
          "name": "logger.info",
          "line": 1981
        },
        {
          "name": "model_type.startswith",
          "line": 2067
        },
        {
          "name": "models.resnet34",
          "line": 1968
        },
        {
          "name": "logger.info",
          "line": 1987
        },
        {
          "name": "nn.Conv2d",
          "line": 1988
        },
        {
          "name": "nn.Identity",
          "line": 1990
        },
        {
          "name": "nn.Conv2d",
          "line": 1993
        },
        {
          "name": "logger.info",
          "line": 2005
        },
        {
          "name": "models.vgg11",
          "line": 2013
        },
        {
          "name": "max",
          "line": 2027
        },
        {
          "name": "int",
          "line": 2031
        },
        {
          "name": "logger.info",
          "line": 2033
        },
        {
          "name": "nn.Sequential",
          "line": 2036
        },
        {
          "name": "nn.Linear",
          "line": 2048
        },
        {
          "name": "logger.warning",
          "line": 2052
        },
        {
          "name": "nn.Conv2d",
          "line": 2055
        },
        {
          "name": "hasattr",
          "line": 2075
        },
        {
          "name": "ValueError",
          "line": 2117
        },
        {
          "name": "models.resnet50",
          "line": 1970
        },
        {
          "name": "torch.no_grad",
          "line": 1997
        },
        {
          "name": "models.vgg13",
          "line": 2015
        },
        {
          "name": "nn.Linear",
          "line": 2037
        },
        {
          "name": "nn.ReLU",
          "line": 2038
        },
        {
          "name": "nn.Dropout",
          "line": 2039
        },
        {
          "name": "nn.Linear",
          "line": 2040
        },
        {
          "name": "nn.ReLU",
          "line": 2041
        },
        {
          "name": "nn.Dropout",
          "line": 2042
        },
        {
          "name": "nn.Linear",
          "line": 2043
        },
        {
          "name": "models.mobilenet_v2",
          "line": 2070
        },
        {
          "name": "ValueError",
          "line": 2072
        },
        {
          "name": "nn.Linear",
          "line": 2077
        },
        {
          "name": "nn.Linear",
          "line": 2081
        },
        {
          "name": "models.resnet101",
          "line": 1972
        },
        {
          "name": "model.conv1.weight.copy_",
          "line": 2000
        },
        {
          "name": "model.conv1.weight.copy_",
          "line": 2003
        },
        {
          "name": "models.vgg16",
          "line": 2017
        },
        {
          "name": "torch.no_grad",
          "line": 2062
        },
        {
          "name": "hasattr",
          "line": 2085
        },
        {
          "name": "hasattr",
          "line": 2085
        },
        {
          "name": "nn.Conv2d",
          "line": 2088
        },
        {
          "name": "nn.Conv2d",
          "line": 2103
        },
        {
          "name": "models.resnet152",
          "line": 1974
        },
        {
          "name": "ValueError",
          "line": 1976
        },
        {
          "name": "models.vgg19",
          "line": 2019
        },
        {
          "name": "ValueError",
          "line": 2021
        },
        {
          "name": "torch.no_grad",
          "line": 2096
        },
        {
          "name": "torch.no_grad",
          "line": 2111
        },
        {
          "name": "min",
          "line": 2063
        },
        {
          "name": "min",
          "line": 2063
        },
        {
          "name": "min",
          "line": 2097
        },
        {
          "name": "min",
          "line": 2097
        },
        {
          "name": "min",
          "line": 2112
        },
        {
          "name": "min",
          "line": 2112
        }
      ],
      "docstring": "\n    Create a model for the specified architecture, optionally with pre-trained weights.\n    \n    Args:\n        model_type: Type of model to create (resnet18, resnet34, vgg16, etc.)\n        use_pretrained: Whether to use pre-trained weights from ImageNet\n        num_classes: Number of output classes\n        input_channels: Number of input channels\n        input_size: Input image size\n        \n    Returns:\n        Model instance\n    ",
      "code_snippet": "    logger.info(\"=\" * width)\n\ndef create_model(model_type=\"resnet18\", use_pretrained=False, num_classes=10, input_channels=3, input_size=32):\n    \"\"\"\n    Create a model for the specified architecture, optionally with pre-trained weights.\n    \n    Args:\n        model_type: Type of model to create (resnet18, resnet34, vgg16, etc.)\n        use_pretrained: Whether to use pre-trained weights from ImageNet\n        num_classes: Number of output classes\n        input_channels: Number of input channels\n        input_size: Input image size\n        \n    Returns:\n        Model instance\n    \"\"\"\n    logger.info(f\"Creating {model_type} model for {num_classes} classes with input size {input_size}x{input_size}\")\n    \n    # Handle different model architectures\n    if model_type.startswith(\"resnet\"):\n        # Choose the appropriate ResNet model\n        if model_type == \"resnet18\":\n            model = models.resnet18(pretrained=use_pretrained)\n        elif model_type == \"resnet34\":\n            model = models.resnet34(pretrained=use_pretrained)\n        elif model_type == \"resnet50\":\n            model = models.resnet50(pretrained=use_pretrained)\n        elif model_type == \"resnet101\":\n            model = models.resnet101(pretrained=use_pretrained)\n        elif model_type == \"resnet152\":\n            model = models.resnet152(pretrained=use_pretrained)\n        else:\n            raise ValueError(f\"Unsupported ResNet model: {model_type}\")\n        \n        # Save the pre-trained conv1 weights if using pre-trained model\n        if use_pretrained:\n            pretrained_conv1_weight = model.conv1.weight.clone()\n            logger.info(f\"Using pre-trained {model_type} with weights from ImageNet\")\n        \n        # Check if we need to adapt the first conv layer\n        if input_size < 64 or input_channels != 3:\n            # For small image sizes like CIFAR-10 (32x32), we need to adapt the first conv layer\n            if input_size < 64:\n                logger.info(f\"Adapting first conv layer for small input size: {input_size}x{input_size}\")\n                model.conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, stride=1, padding=1, bias=False)\n                # Remove aggressive pooling for small images\n                model.maxpool = nn.Identity()\n            else:\n                # Just adapt channels if input size is large enough\n                model.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n            \n            # Adapt the pre-trained weights to the new conv layer if possible\n            if use_pretrained and input_channels == 3:\n                with torch.no_grad():\n                    if input_size < 64:\n                        # Take the center 3x3 section of the 7x7 kernels for small images\n                        model.conv1.weight.copy_(pretrained_conv1_weight[:, :, 2:5, 2:5])\n                    else:\n                        # For different number of channels but same kernel size\n                        model.conv1.weight.copy_(pretrained_conv1_weight)\n                        \n                logger.info(\"Adapted pre-trained conv1 weights for the new input size\")\n        \n        # Adapt the final fc layer for the specified number of classes\n        model.fc = nn.Linear(model.fc.in_features, num_classes)\n        \n    elif model_type.startswith(\"vgg\"):\n        # Choose the appropriate VGG model\n        if model_type == \"vgg11\":\n            model = models.vgg11(pretrained=use_pretrained)\n        elif model_type == \"vgg13\":\n            model = models.vgg13(pretrained=use_pretrained)\n        elif model_type == \"vgg16\":\n            model = models.vgg16(pretrained=use_pretrained)\n        elif model_type == \"vgg19\":\n            model = models.vgg19(pretrained=use_pretrained)\n        else:\n            raise ValueError(f\"Unsupported VGG model: {model_type}\")\n        \n        # Adapt the classifier for different input sizes and number of classes\n        if input_size != 224:\n            # Calculate the feature size after the convolutional layers\n            feature_size = input_size // 32  # VGG has 5 max pooling layers (2^5 = 32)\n            feature_size = max(1, feature_size)  # Ensure at least 1x1 feature size\n            \n            # Adjust the first FC layer to accommodate the new feature size\n            in_features = model.classifier[0].in_features // (49 / (feature_size * feature_size))\n            in_features = int(in_features)\n            \n            logger.info(f\"Adapting VGG classifier for input size {input_size}x{input_size}, features: {in_features}\")\n            \n            # Create a new classifier with adjusted input size\n            model.classifier = nn.Sequential(\n                nn.Linear(in_features, 4096),\n                nn.ReLU(True),\n                nn.Dropout(),\n                nn.Linear(4096, 4096),\n                nn.ReLU(True),\n                nn.Dropout(),\n                nn.Linear(4096, num_classes),\n            )\n        else:\n            # Just adapt the final layer for the number of classes\n            num_features = model.classifier[6].in_features\n            model.classifier[6] = nn.Linear(num_features, num_classes)\n        \n        # Handle different input channels if needed\n        if input_channels != 3 and use_pretrained:\n            logger.warning(f\"Pre-trained VGG models expect 3 input channels, but {input_channels} were specified.\")\n            # Adapt the first conv layer for different number of channels\n            first_conv = model.features[0]\n            new_conv = nn.Conv2d(input_channels, first_conv.out_channels, \n                                 kernel_size=first_conv.kernel_size, \n                                 stride=first_conv.stride,\n                                 padding=first_conv.padding)\n            \n            # Copy weights for shared channels if possible\n            if input_channels > 0:\n                with torch.no_grad():\n                    new_conv.weight[:, :min(3, input_channels)] = first_conv.weight[:, :min(3, input_channels)]\n            \n            model.features[0] = new_conv\n    \n    elif model_type.startswith(\"mobilenet\"):\n        # MobileNet models\n        if model_type == \"mobilenet_v2\":\n            model = models.mobilenet_v2(pretrained=use_pretrained)\n        else:\n            raise ValueError(f\"Unsupported MobileNet model: {model_type}\")\n        \n        # Adapt the classifier for the number of classes\n        if hasattr(model, 'classifier'):\n            in_features = model.classifier[-1].in_features\n            model.classifier[-1] = nn.Linear(in_features, num_classes)\n        else:\n            # For older versions of torchvision\n            in_features = model.last_channel\n            model.classifier = nn.Linear(in_features, num_classes)\n        \n        # Adapt the first conv layer for different input channels if needed\n        if input_channels != 3:\n            if hasattr(model, 'features') and hasattr(model.features[0], 'conv'):\n                # MobileNetV3\n                first_conv = model.features[0].conv\n                new_conv = nn.Conv2d(input_channels, first_conv.out_channels,\n                                     kernel_size=first_conv.kernel_size,\n                                     stride=first_conv.stride,\n                                     padding=first_conv.padding,\n                                     bias=False if first_conv.bias is None else True)\n                \n                # Copy weights for shared channels if possible and using pre-trained\n                if use_pretrained and input_channels > 0:\n                    with torch.no_grad():\n                        new_conv.weight[:, :min(3, input_channels)] = first_conv.weight[:, :min(3, input_channels)]\n                \n                model.features[0].conv = new_conv\n            else:\n                # MobileNetV2\n                first_conv = model.features[0][0]\n                new_conv = nn.Conv2d(input_channels, first_conv.out_channels,\n                                    kernel_size=first_conv.kernel_size,\n                                    stride=first_conv.stride,\n                                    padding=first_conv.padding,\n                                    bias=False if first_conv.bias is None else True)\n                \n                # Copy weights for shared channels if possible and using pre-trained\n                if use_pretrained and input_channels > 0:\n                    with torch.no_grad():\n                        new_conv.weight[:, :min(3, input_channels)] = first_conv.weight[:, :min(3, input_channels)]\n                \n                model.features[0][0] = new_conv\n    \n    else:\n        raise ValueError(f\"Unsupported model type: {model_type}\")\n    \n    return model\n\ndef modify_pattern_risk_accuracy_tracker():\n    \"\"\"\n    Modify the PatternRiskAccuracyTracker to initialize risk from complexity."
    },
    "modify_pattern_risk_accuracy_tracker": {
      "start_line": 2121,
      "end_line": 2177,
      "parameters": [],
      "return_type": null,
      "calls": [
        {
          "name": "logger.info",
          "line": 2175
        },
        {
          "name": "original_init",
          "line": 2134
        },
        {
          "name": "logger.info",
          "line": 2147
        },
        {
          "name": "complexities.items",
          "line": 2150
        },
        {
          "name": "self._initialize_risks_from_complexities",
          "line": 2138
        },
        {
          "name": "logger.warning",
          "line": 2144
        },
        {
          "name": "min",
          "line": 2165
        },
        {
          "name": "logger.info",
          "line": 2167
        },
        {
          "name": "isinstance",
          "line": 2153
        },
        {
          "name": "max",
          "line": 2165
        },
        {
          "name": "isinstance",
          "line": 2156
        }
      ],
      "docstring": "\n    Modify the PatternRiskAccuracyTracker to initialize risk from complexity.\n    This ensures compatibility with the streamlined pattern map format.\n    ",
      "code_snippet": "    return model\n\ndef modify_pattern_risk_accuracy_tracker():\n    \"\"\"\n    Modify the PatternRiskAccuracyTracker to initialize risk from complexity.\n    This ensures compatibility with the streamlined pattern map format.\n    \"\"\"\n    from isekaizen.core.optimizer.pattern_risk_accuracy_tracker import PatternRiskAccuracyTracker\n    \n    # Store the original __init__ method\n    original_init = PatternRiskAccuracyTracker.__init__\n    \n    # Define a new initialization method that handles complexity information\n    def new_init(self, pattern_map=None):\n        # Call the original initialization\n        original_init(self, pattern_map)\n        \n        # Initialize risks from complexity if available\n        if pattern_map and 'pattern_complexities' in pattern_map:\n            self._initialize_risks_from_complexities()\n    \n    # Define the new method to initialize risks from complexities\n    def initialize_risks_from_complexities(self):\n        \"\"\"Initialize risks from complexity scores if available.\"\"\"\n        if not self.pattern_map or 'pattern_complexities' not in self.pattern_map:\n            logger.warning(\"No pattern complexities found for risk initialization\")\n            return\n            \n        logger.info(\"Initializing pattern risks from complexity scores\")\n        complexities = self.pattern_map['pattern_complexities']\n        \n        for pattern_type, complexity_info in complexities.items():\n            if pattern_type in self.pattern_stats:\n                # Use complexity score to initialize risk\n                if isinstance(complexity_info, dict) and 'avg_complexity' in complexity_info:\n                    avg_complexity = complexity_info['avg_complexity']\n                else:\n                    avg_complexity = complexity_info if isinstance(complexity_info, (int, float)) else 2.5\n                    \n                # Since the complexity in the pattern map is very low (~0.1), scale it up\n                if avg_complexity < 0.2:\n                    # Scale up low complexities to get meaningful risk values\n                    scaled_complexity = avg_complexity * 2.5\n                else:\n                    scaled_complexity = avg_complexity / 5.0  # Keep original scale for higher values\n                    \n                initial_risk = min(1.0, max(0.1, scaled_complexity))  # Minimum risk of 0.1\n                self.pattern_stats[pattern_type]['risk'] = initial_risk\n                logger.info(f\"Initialized {pattern_type} risk to {initial_risk:.2f} based on complexity {avg_complexity}\")\n    \n    # Add the new method to the class\n    PatternRiskAccuracyTracker._initialize_risks_from_complexities = initialize_risks_from_complexities\n    \n    # Replace the __init__ method\n    PatternRiskAccuracyTracker.__init__ = new_init\n    \n    logger.info(\"Enhanced PatternRiskAccuracyTracker to initialize risk from complexity scores\")\n\ndef main():\n    \"\"\"\n    Run streamlined pattern-responsive training with unified risk/accuracy ratio approach."
    },
    "main": {
      "start_line": 2177,
      "end_line": 2576,
      "parameters": [],
      "return_type": null,
      "calls": [
        {
          "name": "register_unified_ratio_optimizer",
          "line": 2182
        },
        {
          "name": "modify_pattern_risk_accuracy_tracker",
          "line": 2185
        },
        {
          "name": "argparse.ArgumentParser",
          "line": 2188
        },
        {
          "name": "parser.add_argument",
          "line": 2192
        },
        {
          "name": "parser.add_argument",
          "line": 2193
        },
        {
          "name": "parser.add_argument",
          "line": 2195
        },
        {
          "name": "parser.add_argument",
          "line": 2198
        },
        {
          "name": "parser.add_argument",
          "line": 2202
        },
        {
          "name": "parser.add_argument",
          "line": 2206
        },
        {
          "name": "parser.add_argument",
          "line": 2208
        },
        {
          "name": "parser.add_argument",
          "line": 2210
        },
        {
          "name": "parser.add_argument",
          "line": 2212
        },
        {
          "name": "parser.add_argument",
          "line": 2214
        },
        {
          "name": "parser.add_argument",
          "line": 2216
        },
        {
          "name": "parser.add_argument",
          "line": 2218
        },
        {
          "name": "parser.add_argument",
          "line": 2220
        },
        {
          "name": "parser.add_argument",
          "line": 2222
        },
        {
          "name": "parser.parse_args",
          "line": 2225
        },
        {
          "name": "torch.device",
          "line": 2228
        },
        {
          "name": "logger.info",
          "line": 2229
        },
        {
          "name": "logger.info",
          "line": 2232
        },
        {
          "name": "logger.info",
          "line": 2266
        },
        {
          "name": "transforms.Compose",
          "line": 2271
        },
        {
          "name": "transforms.Compose",
          "line": 2278
        },
        {
          "name": "torchvision.datasets.CIFAR10",
          "line": 2285
        },
        {
          "name": "torchvision.datasets.CIFAR10",
          "line": 2288
        },
        {
          "name": "logger.info",
          "line": 2291
        },
        {
          "name": "logger.info",
          "line": 2294
        },
        {
          "name": "create_model",
          "line": 2295
        },
        {
          "name": "model.to",
          "line": 2302
        },
        {
          "name": "register_unified_ratio_optimizer",
          "line": 2336
        },
        {
          "name": "configure_optimizer",
          "line": 2339
        },
        {
          "name": "logger.info",
          "line": 2355
        },
        {
          "name": "UnifiedRatioTrainer",
          "line": 2357
        },
        {
          "name": "os.path.join",
          "line": 2372
        },
        {
          "name": "os.makedirs",
          "line": 2373
        },
        {
          "name": "os.path.join",
          "line": 2376
        },
        {
          "name": "logger.info",
          "line": 2382
        },
        {
          "name": "logger.info",
          "line": 2383
        },
        {
          "name": "logger.info",
          "line": 2384
        },
        {
          "name": "logger.info",
          "line": 2385
        },
        {
          "name": "logger.info",
          "line": 2386
        },
        {
          "name": "logger.info",
          "line": 2387
        },
        {
          "name": "logger.info",
          "line": 2388
        },
        {
          "name": "logger.info",
          "line": 2389
        },
        {
          "name": "logger.info",
          "line": 2390
        },
        {
          "name": "logger.info",
          "line": 2391
        },
        {
          "name": "logger.info",
          "line": 2398
        },
        {
          "name": "logger.info",
          "line": 2399
        },
        {
          "name": "logger.info",
          "line": 2400
        },
        {
          "name": "torch.cuda.is_available",
          "line": 2403
        },
        {
          "name": "trainer.train",
          "line": 2407
        },
        {
          "name": "....strftime",
          "line": 2416
        },
        {
          "name": "os.path.join",
          "line": 2419
        },
        {
          "name": "visualize_training_results",
          "line": 2420
        },
        {
          "name": "os.path.join",
          "line": 2423
        },
        {
          "name": "trainer.save_model",
          "line": 2424
        },
        {
          "name": "logger.info",
          "line": 2425
        },
        {
          "name": "os.path.join",
          "line": 2428
        },
        {
          "name": "logger.info",
          "line": 2436
        },
        {
          "name": "logger.info",
          "line": 2439
        },
        {
          "name": "logger.info",
          "line": 2440
        },
        {
          "name": "logger.info",
          "line": 2441
        },
        {
          "name": "len",
          "line": 2457
        },
        {
          "name": "max",
          "line": 2461
        },
        {
          "name": "table_lines.append",
          "line": 2466
        },
        {
          "name": "metrics.items",
          "line": 2469
        },
        {
          "name": "table_lines.append",
          "line": 2478
        },
        {
          "name": "logger.info",
          "line": 2571
        },
        {
          "name": "logger.info",
          "line": 2572
        },
        {
          "name": "logger.info",
          "line": 2573
        },
        {
          "name": "logger.info",
          "line": 2574
        },
        {
          "name": "logger.info",
          "line": 2234
        },
        {
          "name": "logger.info",
          "line": 2244
        },
        {
          "name": "load_latest_pattern_map",
          "line": 2245
        },
        {
          "name": "logger.info",
          "line": 2248
        },
        {
          "name": "logger.warning",
          "line": 2262
        },
        {
          "name": "logger.info",
          "line": 2306
        },
        {
          "name": "logger.info",
          "line": 2307
        },
        {
          "name": "logger.info",
          "line": 2308
        },
        {
          "name": "logger.info",
          "line": 2309
        },
        {
          "name": "logger.info",
          "line": 2311
        },
        {
          "name": "logger.info",
          "line": 2393
        },
        {
          "name": "logger.info",
          "line": 2395
        },
        {
          "name": "logger.info",
          "line": 2397
        },
        {
          "name": "torch.cuda.empty_cache",
          "line": 2404
        },
        {
          "name": "str",
          "line": 2432
        },
        {
          "name": "open",
          "line": 2434
        },
        {
          "name": "json.dump",
          "line": 2435
        },
        {
          "name": "....center",
          "line": 2440
        },
        {
          "name": "isinstance",
          "line": 2470
        },
        {
          "name": "table_lines.append",
          "line": 2476
        },
        {
          "name": "logger.info",
          "line": 2482
        },
        {
          "name": "max",
          "line": 2487
        },
        {
          "name": "hasattr",
          "line": 2499
        },
        {
          "name": "sum",
          "line": 2500
        },
        {
          "name": "logger.info",
          "line": 2502
        },
        {
          "name": "logger.info",
          "line": 2503
        },
        {
          "name": "logger.info",
          "line": 2504
        },
        {
          "name": "max",
          "line": 2513
        },
        {
          "name": "logger.info",
          "line": 2516
        },
        {
          "name": "adaptation_metrics.items",
          "line": 2517
        },
        {
          "name": "logger.info",
          "line": 2520
        },
        {
          "name": "sum",
          "line": 2525
        },
        {
          "name": "logger.info",
          "line": 2527
        },
        {
          "name": "logger.info",
          "line": 2528
        },
        {
          "name": "logger.info",
          "line": 2529
        },
        {
          "name": "logger.info",
          "line": 2532
        },
        {
          "name": "torch.cuda.is_available",
          "line": 2228
        },
        {
          "name": "logger.info",
          "line": 2238
        },
        {
          "name": "logger.info",
          "line": 2253
        },
        {
          "name": "translate_pattern_map_to_standard_format",
          "line": 2254
        },
        {
          "name": "logger.info",
          "line": 2255
        },
        {
          "name": "transforms.RandomHorizontalFlip",
          "line": 2273
        },
        {
          "name": "transforms.ToTensor",
          "line": 2274
        },
        {
          "name": "transforms.Normalize",
          "line": 2275
        },
        {
          "name": "transforms.Normalize",
          "line": 2281
        },
        {
          "name": "nn.CrossEntropyLoss",
          "line": 2359
        },
        {
          "name": "datetime.now",
          "line": 2416
        },
        {
          "name": "history.items",
          "line": 2431
        },
        {
          "name": "....items",
          "line": 2432
        },
        {
          "name": "sum",
          "line": 2454
        },
        {
          "name": "len",
          "line": 2454
        },
        {
          "name": "len",
          "line": 2461
        },
        {
          "name": "key.endswith",
          "line": 2472
        },
        {
          "name": "str",
          "line": 2475
        },
        {
          "name": "....keys",
          "line": 2487
        },
        {
          "name": "logger.info",
          "line": 2491
        },
        {
          "name": "max",
          "line": 2492
        },
        {
          "name": "logger.info",
          "line": 2493
        },
        {
          "name": "last_ratios.items",
          "line": 2494
        },
        {
          "name": "logger.info",
          "line": 2496
        },
        {
          "name": "....center",
          "line": 2503
        },
        {
          "name": "len",
          "line": 2509
        },
        {
          "name": "str",
          "line": 2518
        },
        {
          "name": "logger.info",
          "line": 2519
        },
        {
          "name": "aug_counts.values",
          "line": 2525
        },
        {
          "name": "....center",
          "line": 2528
        },
        {
          "name": "logger.info",
          "line": 2536
        },
        {
          "name": "max",
          "line": 2537
        },
        {
          "name": "logger.info",
          "line": 2540
        },
        {
          "name": "aug_counts.items",
          "line": 2541
        },
        {
          "name": "logger.info",
          "line": 2543
        },
        {
          "name": "hasattr",
          "line": 2546
        },
        {
          "name": "trainer.augmentation_mediator.get_metrics",
          "line": 2547
        },
        {
          "name": "logger.info",
          "line": 2549
        },
        {
          "name": "max",
          "line": 2557
        },
        {
          "name": "logger.info",
          "line": 2560
        },
        {
          "name": "mediator_metric_names.items",
          "line": 2561
        },
        {
          "name": "logger.info",
          "line": 2569
        },
        {
          "name": "open",
          "line": 2236
        },
        {
          "name": "json.load",
          "line": 2237
        },
        {
          "name": "logger.error",
          "line": 2240
        },
        {
          "name": "logger.info",
          "line": 2241
        },
        {
          "name": "load_latest_pattern_map",
          "line": 2242
        },
        {
          "name": "logger.error",
          "line": 2259
        },
        {
          "name": "logger.warning",
          "line": 2260
        },
        {
          "name": "transforms.Resize",
          "line": 2272
        },
        {
          "name": "transforms.RandomCrop",
          "line": 2272
        },
        {
          "name": "transforms.Resize",
          "line": 2279
        },
        {
          "name": "transforms.ToTensor",
          "line": 2279
        },
        {
          "name": "transforms.ToTensor",
          "line": 2280
        },
        {
          "name": "len",
          "line": 2291
        },
        {
          "name": "len",
          "line": 2291
        },
        {
          "name": "len",
          "line": 2446
        },
        {
          "name": "str",
          "line": 2461
        },
        {
          "name": "metrics.keys",
          "line": 2461
        },
        {
          "name": "logger.info",
          "line": 2495
        },
        {
          "name": "len",
          "line": 2513
        },
        {
          "name": "logger.info",
          "line": 2542
        },
        {
          "name": "history.get",
          "line": 2432
        },
        {
          "name": "str",
          "line": 2435
        },
        {
          "name": "str",
          "line": 2476
        },
        {
          "name": "len",
          "line": 2492
        },
        {
          "name": "str",
          "line": 2513
        },
        {
          "name": "adaptation_metrics.keys",
          "line": 2513
        },
        {
          "name": "len",
          "line": 2537
        },
        {
          "name": "len",
          "line": 2557
        },
        {
          "name": "logger.info",
          "line": 2568
        },
        {
          "name": "last_ratios.keys",
          "line": 2492
        },
        {
          "name": "str",
          "line": 2519
        },
        {
          "name": "aug_counts.keys",
          "line": 2537
        },
        {
          "name": "mediator_metric_names.values",
          "line": 2557
        },
        {
          "name": "str",
          "line": 2567
        },
        {
          "name": "str",
          "line": 2240
        },
        {
          "name": "str",
          "line": 2259
        }
      ],
      "docstring": "\n    Run streamlined pattern-responsive training with unified risk/accuracy ratio approach.\n    ",
      "code_snippet": "    logger.info(\"Enhanced PatternRiskAccuracyTracker to initialize risk from complexity scores\")\n\ndef main():\n    \"\"\"\n    Run streamlined pattern-responsive training with unified risk/accuracy ratio approach.\n    \"\"\"\n    # Register the unified ratio optimizer\n    register_unified_ratio_optimizer()\n    \n    # Modify the PatternRiskAccuracyTracker to handle complexity information\n    modify_pattern_risk_accuracy_tracker()\n    \n    # Parse command line arguments\n    parser = argparse.ArgumentParser(\n        description=\"Run streamlined pattern-responsive training with unified risk/accuracy ratio\",\n        formatter_class=argparse.RawTextHelpFormatter\n    )\n    parser.add_argument(\"--epochs\", type=int, default=15, help=\"Number of epochs for training (default: 15)\")\n    parser.add_argument(\"--target-accuracy\", type=float, default=None, \n                      help=\"Target accuracy to dynamically adjust training behavior\")\n    parser.add_argument(\"--optimizer\", type=str, default=\"eve_unified\", \n                      choices=[\"eve\", \"eve_unified\"],\n                      help=\"Optimizer to use (default: eve_unified)\")\n    parser.add_argument(\"--pretrained\", action=\"store_true\",\n                      help=\"Use pre-trained model with ImageNet weights (default: False)\")\n    \n    # New model-related arguments\n    parser.add_argument(\"--model\", type=str, default=\"resnet18\", \n                      choices=[\"resnet18\", \"resnet34\", \"resnet50\", \"resnet101\", \"resnet152\", \n                              \"vgg11\", \"vgg13\", \"vgg16\", \"vgg19\", \"mobilenet_v2\"],\n                      help=\"Model architecture to use (default: resnet18)\")\n    parser.add_argument(\"--input-size\", type=int, default=32,\n                      help=\"Input image size (default: 32 for CIFAR-10)\")\n    parser.add_argument(\"--num-classes\", type=int, default=10,\n                      help=\"Number of output classes (default: 10 for CIFAR-10)\")\n    parser.add_argument(\"--input-channels\", type=int, default=3,\n                      help=\"Number of input channels (default: 3 for RGB images)\")\n    parser.add_argument(\"--pattern-map-path\", type=str, default=None, \n                      help=\"Path to a specific pattern map file (default: use latest)\")\n    parser.add_argument(\"--debug-ratios\", action=\"store_true\",\n                      help=\"Enable detailed logging of ratio calculations\")\n    parser.add_argument(\"--weight-range\", type=str, default=\"wide\", choices=[\"narrow\", \"wide\", \"default\"],\n                      help=\"Weight adjustment range: narrow (0.9-1.1), default (0.85-1.15), or wide (0.8-1.2)\")\n    parser.add_argument(\"--lr-sensitivity\", type=str, default=\"high\", choices=[\"low\", \"medium\", \"high\"],\n                      help=\"Learning rate adaptation sensitivity: low, medium, or high\")\n    parser.add_argument(\"--debug-bounds\", action=\"store_true\",\n                      help=\"Enable detailed logging of equilibrium bounds calculations\")\n    parser.add_argument(\"--use-augmentation\", action=\"store_true\",\n                      help=\"Enable the augmentation mediator for data augmentation\")\n    \n    args = parser.parse_args()\n    \n    # Set device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    logger.info(f\"Using device: {device}\")\n    \n    # Load pattern map\n    logger.info(\"Loading pattern map...\")\n    if args.pattern_map_path:\n        logger.info(f\"Loading pattern map from specified path: {args.pattern_map_path}\")\n        try:\n            with open(args.pattern_map_path, 'r') as f:\n                pattern_map = json.load(f)\n            logger.info(\"Pattern map loaded successfully from specified path\")\n        except Exception as e:\n            logger.error(f\"Error loading pattern map from {args.pattern_map_path}: {str(e)}\")\n            logger.info(\"Falling back to latest pattern map\")\n            pattern_map = load_latest_pattern_map()\n    else:\n        logger.info(\"Loading latest pattern map from default location\")\n        pattern_map = load_latest_pattern_map()\n    \n    if pattern_map:\n        logger.info(\"Pattern map loaded successfully\")\n        \n        # Convert to standardized format\n        from isekaizen.utils.pattern_map_utils import translate_pattern_map_to_standard_format\n        try:\n            logger.info(\"Converting pattern map to standardized format...\")\n            standardized_pattern_map = translate_pattern_map_to_standard_format(pattern_map)\n            logger.info(\"Pattern map successfully converted to standardized format\")\n            \n            pattern_map = standardized_pattern_map\n        except Exception as e:\n            logger.error(f\"Error converting pattern map to standardized format: {str(e)}\")\n            logger.warning(\"Proceeding with original pattern map\")\n    else:\n        logger.warning(\"No pattern map found. Creating a new map will be required.\")\n        pattern_map = None\n    \n    # Load dataset with appropriate transformations for the model\n    logger.info(f\"Loading dataset with input size {args.input_size}x{args.input_size}...\")\n    \n    # Adjust transforms based on model requirements\n    need_resize = args.input_size != 32\n    \n    transform_train = transforms.Compose([\n        transforms.Resize(args.input_size) if need_resize else transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n\n    transform_test = transforms.Compose([\n        transforms.Resize(args.input_size) if need_resize else transforms.ToTensor(),\n        transforms.ToTensor() if need_resize else lambda x: x,  # Add ToTensor only if we're resizing\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n\n    # Load datasets\n    trainset = torchvision.datasets.CIFAR10(\n        root='./data', train=True, download=True, transform=transform_train)\n\n    testset = torchvision.datasets.CIFAR10(\n        root='./data', train=False, download=True, transform=transform_test)\n    \n    logger.info(f\"Dataset loaded: {len(trainset)} training samples, {len(testset)} test samples\")\n    \n    # Create model with specified architecture\n    logger.info(f\"Creating {args.model} model...\")\n    model = create_model(\n        model_type=args.model,\n        use_pretrained=args.pretrained,\n        num_classes=args.num_classes,\n        input_channels=args.input_channels,\n        input_size=args.input_size\n    )\n    model = model.to(device)\n    \n    # Log model configuration\n    if args.pretrained:\n        logger.info(\"======= USING PRE-TRAINED MODEL =======\")\n        logger.info(\"Using pre-trained ResNet-18 with ImageNet weights\")\n        logger.info(\"Conv1 weights adapted from 7x7 to 3x3 for CIFAR-10\")\n        logger.info(\"===========================================\")\n    else:\n        logger.info(\"Using ResNet-18 initialized with random weights (training from scratch)\")\n    \n    # Make sure the unified ratio optimizer is registered with appropriate parameters\n    config_params = {}\n    \n    # Set learning rate sensitivity parameters\n    if args.lr_sensitivity == \"high\":\n        config_params['lr_check_interval'] = 5\n        config_params['lr_change_threshold'] = 0.003\n        config_params['lr_log_threshold'] = 0.03\n    elif args.lr_sensitivity == \"medium\":\n        config_params['lr_check_interval'] = 7\n        config_params['lr_change_threshold'] = 0.004\n        config_params['lr_log_threshold'] = 0.04\n    # Low sensitivity uses defaults\n    \n    # Set weight adjustment range\n    config_params['weight_adjustment_range'] = args.weight_range\n    \n    # Set debug flags\n    config_params['debug_ratios'] = args.debug_ratios\n    config_params['debug_bounds'] = args.debug_bounds\n    \n    # The optimizer now has its own internal mediator, so we don't need the extra steps\n    # but we still need to register it first\n    register_unified_ratio_optimizer(config_params)\n    \n    # Create optimizer\n    optimizer, scheduler = configure_optimizer(\n        model, \n        optimizer_type=args.optimizer,\n        custom_params={\n            \"pattern_map\": pattern_map\n        }\n    )\n    \n    # Create trainer - the optimizer has its own mediator\n    batch_optimizer_kwargs = {\n        \"pattern_map\": pattern_map,\n        \"target_accuracy\": args.target_accuracy,\n        \"run_diagnostics\": True,\n        \"total_epochs\": args.epochs\n    }\n    \n    logger.info(f\"Initializing batch optimizer with kwargs: {batch_optimizer_kwargs}\")\n    \n    trainer = UnifiedRatioTrainer(\n        model=model,\n        criterion=nn.CrossEntropyLoss(),\n        optimizer_class=optimizer.__class__,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        device=device,\n        pattern_map=pattern_map,\n        val_dataset=testset,\n        batch_optimizer_class=EnhancedPatternResponsiveOptimizer,\n        batch_optimizer_kwargs=batch_optimizer_kwargs,\n        use_augmentation=args.use_augmentation\n    )\n    \n    # Create output directory\n    output_dir = os.path.join(\"examples\", \"output\")\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Define checkpoint path\n    checkpoint_path = os.path.join(output_dir, \"unified_ratio_model\")\n    \n    # Define callbacks\n    callbacks = [track_unified_ratio_callback]\n    \n    # Print the training configuration information\n    logger.info(\"=====================================================\")\n    logger.info(f\"STARTING TRAINING WITH UNIFIED RISK/ACCURACY RATIO\")\n    logger.info(\"=====================================================\")\n    logger.info(f\"Epochs: {args.epochs}\")\n    logger.info(f\"Optimizer: {args.optimizer} with unified risk/accuracy ratio\")\n    logger.info(f\"Learning rate sensitivity: {args.lr_sensitivity}\")\n    logger.info(f\"Weight adjustment range: {args.weight_range}\")\n    logger.info(\"Using pattern equilibrium bounds for adaptation\")\n    logger.info(\"Pattern bounds calculated from complexity and prevalence\")\n    logger.info(\"Using pattern data mediator for efficient pattern tracking\")\n    if args.use_augmentation:\n        logger.info(\"Using augmentation mediator for template-based data augmentation\")\n    if args.debug_ratios:\n        logger.info(\"Detailed ratio logging: ENABLED\")\n    if args.debug_bounds:\n        logger.info(\"Detailed bounds logging: ENABLED\")\n    logger.info(f\"Model: ResNet-18 for CIFAR-10 ({('pre-trained with ImageNet weights' if args.pretrained else 'from scratch')})\")\n    logger.info(f\"Device: {device}\")\n    logger.info(\"====================================================\")\n    \n    # Clean up any residual memory before starting\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    # Start training\n    history = trainer.train(\n        train_dataset=trainset,\n        val_dataset=testset,\n        epochs=args.epochs,\n        checkpoint_path=checkpoint_path,\n        callbacks=callbacks\n    )\n    \n    # Create timestamp for output filenames\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    \n    # Visualize results\n    output_path = os.path.join(output_dir, f\"unified_ratio_training_{timestamp}.png\")\n    visualize_training_results(history, output_path)\n    \n    # Save model\n    model_path = os.path.join(output_dir, f\"unified_ratio_model_{timestamp}.pth\")\n    trainer.save_model(model_path)\n    logger.info(f\"Model saved to {model_path}\")\n    \n    # Save metrics\n    metrics_path = os.path.join(output_dir, f\"unified_ratio_metrics_{timestamp}.json\")\n    \n    # Convert ratios to serializable format\n    serializable_history = {k: v for k, v in history.items() if k != 'risk_accuracy_ratios'}\n    serializable_history['risk_accuracy_ratios'] = {str(k): v for k, v in history.get('risk_accuracy_ratios', {}).items()}\n    \n    with open(metrics_path, 'w') as f:\n        json.dump(serializable_history, f, indent=2, default=lambda x: str(x))\n    logger.info(f\"Training metrics saved to {metrics_path}\")\n    \n    # Print comprehensive training summary with improved formatting\n    logger.info(\"\\n\" + \"=\" * 80)\n    logger.info(\"TRAINING SUMMARY\".center(80))\n    logger.info(\"=\" * 80)\n    \n    # Create a metrics dictionary for table formatting\n    metrics = {\n        \"Model\": \"ResNet-18\",\n        \"Dataset\": f\"CIFAR-10 ({len(trainset)} samples)\",\n        \"Optimizer\": args.optimizer,\n        \"Final Training Accuracy\": history['train_acc'][-1],\n        \"Final Test Accuracy\": history['val_acc'][-1]\n    }\n    \n    # Calculate average epoch time\n    if history['epoch_times']:\n        avg_epoch_time = sum(history['epoch_times']) / len(history['epoch_times'])\n        metrics[\"Average Epoch Time (s)\"] = avg_epoch_time\n    \n    metrics[\"Total Epochs Completed\"] = len(history['train_acc'])\n    \n    # Format and print the metrics table\n    table_lines = []\n    key_width = max(len(str(k)) for k in metrics.keys())\n    val_width = 25\n    \n    # Create header\n    total_width = key_width + val_width + 3  # 3 for spacing and separator\n    table_lines.append(\"-\" * total_width)\n    \n    # Format each line\n    for key, value in metrics.items():\n        if isinstance(value, float):\n            val_str = f\"{value:.2f}\"\n            if key.endswith(\"Accuracy\"):\n                val_str += \"%\"  # Add percent sign for accuracies\n        else:\n            val_str = str(value)\n        table_lines.append(f\"{str(key):<{key_width}} \u2502 {val_str:>{val_width}}\")\n    \n    table_lines.append(\"-\" * total_width)\n    \n    # Print the formatted table\n    for line in table_lines:\n        logger.info(line)\n    \n    # Add ratio info if available\n    if 'risk_accuracy_ratios' in history and history['risk_accuracy_ratios']:\n        # Get the last epoch's ratios\n        last_epoch = max(history['risk_accuracy_ratios'].keys())\n        last_ratios = history['risk_accuracy_ratios'][last_epoch]\n        \n        if last_ratios:\n            logger.info(\"\\nFinal Risk/Accuracy Ratios:\")\n            ratio_width = max(len(k) for k in last_ratios.keys())\n            logger.info(\"-\" * (ratio_width + 15))\n            for pattern_type, ratio in last_ratios.items():\n                logger.info(f\"{pattern_type:<{ratio_width}} \u2502 {ratio:.3f}\")\n            logger.info(\"-\" * (ratio_width + 15))\n    \n    # Show dataset adaptation info with improved formatting\n    if hasattr(trainer, 'dataset_adaptations') and trainer.dataset_adaptations:\n        total_added = sum(adaptation['examples_added'] for adaptation in trainer.dataset_adaptations)\n        \n        logger.info(\"\\n\" + \"=\" * 80)\n        logger.info(\"DATASET ADAPTATION SUMMARY\".center(80))\n        logger.info(\"=\" * 80)\n        \n        adaptation_metrics = {\n            \"Total Examples Added\": total_added,\n            \"Final Dataset Size\": history['dataset_sizes'][-1],\n            \"Adaptation Events\": len(trainer.dataset_adaptations)\n        }\n        \n        # Format and print the adaptation metrics\n        key_width = max(len(str(k)) for k in adaptation_metrics.keys())\n        val_width = 25\n        \n        logger.info(\"-\" * (key_width + val_width + 3))\n        for key, value in adaptation_metrics.items():\n            val_str = str(value)\n            logger.info(f\"{str(key):<{key_width}} \u2502 {val_str:>{val_width}}\")\n        logger.info(\"-\" * (key_width + val_width + 3))\n    \n    # Show augmentation info if available with improved formatting\n    if 'augmentation_counts' in history and history['augmentation_counts'] and history['augmentation_counts'][-1]:\n        aug_counts = history['augmentation_counts'][-1]\n        total_augmented = sum(aug_counts.values())\n        \n        logger.info(\"\\n\" + \"=\" * 80)\n        logger.info(\"AUGMENTATION SUMMARY\".center(80))\n        logger.info(\"=\" * 80)\n        \n        # Main augmentation metrics\n        logger.info(f\"Total Augmented Examples: {total_augmented}\")\n        \n        # Pattern-specific counts\n        if aug_counts:\n            logger.info(\"\\nAugmentation Counts by Pattern Type:\")\n            pattern_width = max(len(k) for k in aug_counts.keys())\n            count_width = 10\n            \n            logger.info(\"-\" * (pattern_width + count_width + 3))\n            for pattern_type, count in aug_counts.items():\n                logger.info(f\"{pattern_type:<{pattern_width}} \u2502 {count:>{count_width}}\")\n            logger.info(\"-\" * (pattern_width + count_width + 3))\n        \n        # Add augmentation mediator metrics if available\n        if hasattr(trainer, 'augmentation_mediator') and trainer.augmentation_mediator is not None:\n            mediator_metrics = trainer.augmentation_mediator.get_metrics()\n            \n            logger.info(\"\\nAugmentation Mediator Metrics:\")\n            mediator_metric_names = {\n                \"cache_hit_rate\": \"Cache Hit Rate\",\n                \"total_templates\": \"Templates Generated\",\n                \"total_augmentations_generated\": \"Total Augmentations\",\n                \"cache_size\": \"Cache Size\"\n            }\n            \n            key_width = max(len(v) for v in mediator_metric_names.values())\n            val_width = 15\n            \n            logger.info(\"-\" * (key_width + val_width + 3))\n            for metric_key, display_name in mediator_metric_names.items():\n                if metric_key in mediator_metrics:\n                    value = mediator_metrics[metric_key]\n                    if metric_key == \"cache_hit_rate\":\n                        val_str = f\"{value:.2f}\"\n                    else:\n                        val_str = str(value)\n                    logger.info(f\"{display_name:<{key_width}} \u2502 {val_str:>{val_width}}\")\n            logger.info(\"-\" * (key_width + val_width + 3))\n    \n    logger.info(\"\\n\" + \"=\" * 80)\n    logger.info(f\"Results visualization saved to: {output_path}\")\n    logger.info(f\"Training metrics saved to: {metrics_path}\")\n    logger.info(\"Unified ratio training completed successfully\")\n\nif __name__ == \"__main__\":\n    try:\n        main()"
    }
  },
  "constants": {}
}