{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\benchmarks.old\\models\\src\\cortex\\resource_manager.py",
  "imports": [
    {
      "name": "typing.Dict",
      "line": 4
    },
    {
      "name": "typing.Set",
      "line": 4
    },
    {
      "name": "typing.Any",
      "line": 4
    },
    {
      "name": "typing.List",
      "line": 4
    },
    {
      "name": "typing.Optional",
      "line": 4
    },
    {
      "name": "torch",
      "line": 5
    },
    {
      "name": "numpy",
      "line": 6
    },
    {
      "name": "math",
      "line": 7
    },
    {
      "name": "logging",
      "line": 8
    },
    {
      "name": "asyncio",
      "line": 9
    },
    {
      "name": "datetime.datetime",
      "line": 10
    },
    {
      "name": "dataclasses.dataclass",
      "line": 11
    },
    {
      "name": "utils.types.ResourceAllocation",
      "line": 13
    },
    {
      "name": "utils.types.ProcessingStage",
      "line": 13
    },
    {
      "name": "utils.types.ResourceError",
      "line": 13
    },
    {
      "name": "utils.types.BatchMetadata",
      "line": 13
    }
  ],
  "classes": {
    "ResourceManager": {
      "start_line": 20,
      "end_line": 374,
      "methods": {
        "__init__": {
          "start_line": 22,
          "end_line": 74,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logging.getLogger",
              "line": 23
            },
            {
              "name": "self.logger.setLevel",
              "line": 24
            },
            {
              "name": "torch.device",
              "line": 25
            },
            {
              "name": "asyncio.Lock",
              "line": 69
            },
            {
              "name": "self._init_memory_pools",
              "line": 72
            },
            {
              "name": "set",
              "line": 44
            },
            {
              "name": "torch.cuda.is_available",
              "line": 25
            }
          ],
          "code_snippet": "class ResourceManager:\n    \"\"\"Enhanced resource manager with async support and cortex flow tracking\"\"\"\n    def __init__(self):\n        self.logger = logging.getLogger(__name__)\n        self.logger.setLevel(logging.INFO)\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n        # Updated resource limits for RTX 4070 SUPER\n        self.resource_limits = {\n            \"gpu_memory\": 12288,  # MB for RTX 4070 SUPER\n            \"target_inference\": 7.0,\n            \"max_batch_size\": 221  # From K(t) framework\n        }\n        \n        # Memory pool configuration\n        self.chunk_sizes = [32, 64, 128, 256]  # MB\n        self.memory_pools: Dict[int, List[torch.Tensor]] = {\n            size: [] for size in self.chunk_sizes\n        }\n        \n        # Enhanced resource tracking\n        self.resource_usage = {\n            \"gpu_memory_used\": 0,\n            \"current_inference_time\": 0,\n            \"active_specialists\": set(),\n            \"memory_pool_stats\": {size: 0 for size in self.chunk_sizes},\n            \"flow_allocations\": {}  # Track allocations by flow_id\n        }\n        \n        # Performance metrics with flow tracking\n        self.performance_metrics = {\n            \"peak_memory_usage\": 0,\n            \"average_inference_time\": 0,\n            \"total_allocations\": 0,\n            \"allocation_failures\": 0,\n            \"pool_hits\": 0,\n            \"pool_misses\": 0,\n            \"flow_metrics\": {\n                \"active_flows\": 0,\n                \"completed_flows\": 0,\n                \"failed_flows\": 0\n            }\n        }\n\n        # Batch processing tracking\n        self.active_batches: Dict[str, BatchMetadata] = {}\n        self.completed_batches: Dict[str, BatchMetadata] = {}\n        \n        # Lock for resource allocation\n        self._resource_lock = asyncio.Lock()\n        \n        # Initialize memory pools\n        self._init_memory_pools()\n    \n    def _init_memory_pools(self):\n        \"\"\"Initialize memory pools with pre-allocated chunks\"\"\"\n        if torch.cuda.is_available():"
        },
        "_init_memory_pools": {
          "start_line": 74,
          "end_line": 107,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "torch.cuda.is_available",
              "line": 76
            },
            {
              "name": "chunk_allocation.items",
              "line": 87
            },
            {
              "name": "int",
              "line": 81
            },
            {
              "name": "int",
              "line": 82
            },
            {
              "name": "int",
              "line": 83
            },
            {
              "name": "int",
              "line": 84
            },
            {
              "name": "range",
              "line": 88
            },
            {
              "name": "self.logger.error",
              "line": 105
            },
            {
              "name": "torch.empty",
              "line": 90
            },
            {
              "name": "....append",
              "line": 95
            },
            {
              "name": "self.logger.warning",
              "line": 98
            },
            {
              "name": "int",
              "line": 91
            }
          ],
          "docstring": "Initialize memory pools with pre-allocated chunks",
          "code_snippet": "        self._init_memory_pools()\n    \n    def _init_memory_pools(self):\n        \"\"\"Initialize memory pools with pre-allocated chunks\"\"\"\n        if torch.cuda.is_available():\n            try:\n                # Calculate base number of chunks based on total memory\n                total_memory = self.resource_limits[\"gpu_memory\"]\n                chunk_allocation = {\n                    32: int(total_memory * 0.4 / 32),   # 40% in 32MB chunks\n                    64: int(total_memory * 0.3 / 64),   # 30% in 64MB chunks\n                    128: int(total_memory * 0.2 / 128), # 20% in 128MB chunks\n                    256: int(total_memory * 0.1 / 256)  # 10% in 256MB chunks\n                }\n                \n                for chunk_size, num_chunks in chunk_allocation.items():\n                    for _ in range(num_chunks):\n                        try:\n                            tensor = torch.empty(\n                                (int(chunk_size * 1024 * 1024 / 4),),\n                                dtype=torch.float32,\n                                device=self.device\n                            )\n                            self.memory_pools[chunk_size].append(tensor)\n                            self.resource_usage[\"memory_pool_stats\"][chunk_size] += 1\n                        except torch.cuda.OutOfMemoryError:\n                            self.logger.warning(\n                                f\"Memory limit reached during pool initialization \"\n                                f\"at {chunk_size}MB chunks\"\n                            )\n                            break\n                \n            except Exception as e:\n                self.logger.error(f\"Error initializing memory pools: {e}\")\n    \n    def _get_chunks_for_memory(self, required_memory: int) -> List[torch.Tensor]:\n        \"\"\"Get optimal combination of memory chunks\"\"\"\n        chunks = []"
        },
        "_get_chunks_for_memory": {
          "start_line": 107,
          "end_line": 160,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "required_memory",
              "type": "int"
            }
          ],
          "return_type": "complex_type",
          "calls": [
            {
              "name": "sorted",
              "line": 113
            },
            {
              "name": "....pop",
              "line": 116
            },
            {
              "name": "chunks.append",
              "line": 117
            },
            {
              "name": "int",
              "line": 127
            },
            {
              "name": "torch.empty",
              "line": 128
            },
            {
              "name": "chunks.append",
              "line": 133
            },
            {
              "name": "self.logger.warning",
              "line": 144
            },
            {
              "name": "ResourceError",
              "line": 147
            },
            {
              "name": "self.logger.error",
              "line": 155
            },
            {
              "name": "self.get_available_memory",
              "line": 151
            },
            {
              "name": "....append",
              "line": 141
            },
            {
              "name": "chunk.numel",
              "line": 139
            }
          ],
          "docstring": "Get optimal combination of memory chunks",
          "code_snippet": "                self.logger.error(f\"Error initializing memory pools: {e}\")\n    \n    def _get_chunks_for_memory(self, required_memory: int) -> List[torch.Tensor]:\n        \"\"\"Get optimal combination of memory chunks\"\"\"\n        chunks = []\n        remaining_memory = required_memory\n        \n        # First try to use existing chunks\n        for chunk_size in sorted(self.chunk_sizes, reverse=True):\n            while (remaining_memory >= chunk_size and \n                   self.memory_pools[chunk_size]):\n                chunk = self.memory_pools[chunk_size].pop()\n                chunks.append(chunk)\n                remaining_memory -= chunk_size\n                self.resource_usage[\"memory_pool_stats\"][chunk_size] -= 1\n                self.performance_metrics[\"pool_hits\"] += 1\n        \n        # If we still need memory, allocate new chunks\n        if remaining_memory > 0:\n            self.performance_metrics[\"pool_misses\"] += 1\n            try:\n                # Try to allocate one chunk of the exact size needed\n                elements = int(remaining_memory * 1024 * 1024 / 4)\n                tensor = torch.empty(\n                    (elements,),\n                    dtype=torch.float32,\n                    device=self.device\n                )\n                chunks.append(tensor)\n                \n            except torch.cuda.OutOfMemoryError:\n                # If exact allocation fails, try to free some memory\n                if chunks:  # Return any chunks we got from pools\n                    for chunk in chunks:\n                        chunk_size = chunk.numel() * 4 / (1024 * 1024)\n                        if chunk_size in self.chunk_sizes:\n                            self.memory_pools[chunk_size].append(chunk)\n                            self.resource_usage[\"memory_pool_stats\"][chunk_size] += 1\n                \n                self.logger.warning(\n                    f\"Failed to allocate {remaining_memory}MB chunk\"\n                )\n                raise ResourceError(\n                    \"Memory allocation failed\",\n                    \"gpu_memory\",\n                    remaining_memory,\n                    self.get_available_memory()\n                )\n                \n            except Exception as e:\n                self.logger.error(f\"Error allocating new chunk: {e}\")\n                raise\n        \n        return chunks\n    async def allocate_resources(self, \n                               flow_id: str,\n                               required_memory: int,\n                               batch_id: Optional[str] = None) -> ResourceAllocation:"
        },
        "get_memory_usage": {
          "start_line": 296,
          "end_line": 330,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": "complex_type",
          "calls": [
            {
              "name": "torch.cuda.is_available",
              "line": 298
            },
            {
              "name": "min",
              "line": 305
            },
            {
              "name": "min",
              "line": 306
            },
            {
              "name": "min",
              "line": 307
            },
            {
              "name": "min",
              "line": 324
            },
            {
              "name": "min",
              "line": 325
            },
            {
              "name": "min",
              "line": 326
            },
            {
              "name": "min",
              "line": 327
            },
            {
              "name": "torch.cuda.memory_allocated",
              "line": 300
            },
            {
              "name": "torch.cuda.memory_reserved",
              "line": 301
            },
            {
              "name": "torch.cuda.max_memory_allocated",
              "line": 302
            },
            {
              "name": "len",
              "line": 315
            }
          ],
          "docstring": "Get detailed memory usage including pool stats",
          "code_snippet": "            self.completed_batches[batch_id] = batch\n    \n    def get_memory_usage(self) -> Dict[str, float]:\n        \"\"\"Get detailed memory usage including pool stats\"\"\"\n        if torch.cuda.is_available():\n            # Convert bytes to MB\n            allocated = torch.cuda.memory_allocated() / (1024 * 1024)\n            reserved = torch.cuda.memory_reserved() / (1024 * 1024)\n            max_allocated = torch.cuda.max_memory_allocated() / (1024 * 1024)\n            \n            # Constrain values to reasonable limits\n            allocated = min(allocated, 64)  # Cap at 64MB for test environment\n            reserved = min(reserved, 64)\n            max_allocated = min(max_allocated, 64)\n            \n            return {\n                \"allocated_mb\": allocated,\n                \"reserved_mb\": reserved,\n                \"max_allocated_mb\": max_allocated,\n                \"utilization\": allocated / self.resource_limits[\"gpu_memory\"],\n                \"pool_stats\": {\n                    f\"pool_{size}mb\": len(self.memory_pools[size])\n                    for size in self.chunk_sizes\n                },\n                \"pool_hits\": self.performance_metrics[\"pool_hits\"],\n                \"pool_misses\": self.performance_metrics[\"pool_misses\"],\n                \"active_flows\": self.performance_metrics[\"flow_metrics\"][\"active_flows\"],\n                \"completed_flows\": self.performance_metrics[\"flow_metrics\"][\"completed_flows\"]\n            }\n        return {\n            \"allocated_mb\": min(self.resource_usage[\"gpu_memory_used\"], 64),\n            \"reserved_mb\": min(self.resource_usage[\"gpu_memory_used\"], 64),\n            \"max_allocated_mb\": min(self.performance_metrics[\"peak_memory_usage\"], 64),\n            \"utilization\": min(self.resource_usage[\"gpu_memory_used\"] / \n                          self.resource_limits[\"gpu_memory\"], 1.0)\n        }\n    \n    def get_status(self) -> Dict[str, Any]:\n        \"\"\"Get comprehensive status including memory pool stats\"\"\""
        },
        "get_status": {
          "start_line": 331,
          "end_line": 343,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": "complex_type",
          "calls": [
            {
              "name": "self.get_memory_usage",
              "line": 333
            },
            {
              "name": "self.get_available_memory",
              "line": 337
            },
            {
              "name": "len",
              "line": 339
            },
            {
              "name": "len",
              "line": 340
            },
            {
              "name": "str",
              "line": 341
            }
          ],
          "docstring": "Get comprehensive status including memory pool stats",
          "code_snippet": "        }\n    \n    def get_status(self) -> Dict[str, Any]:\n        \"\"\"Get comprehensive status including memory pool stats\"\"\"\n        memory_stats = self.get_memory_usage()\n        return {\n            \"resource_usage\": self.resource_usage,\n            \"performance_metrics\": self.performance_metrics,\n            \"available_memory\": self.get_available_memory(),\n            \"memory_stats\": memory_stats,\n            \"active_batches\": len(self.active_batches),\n            \"completed_batches\": len(self.completed_batches),\n            \"device\": str(self.device)\n        }\n    \n    def get_available_memory(self) -> int:\n        \"\"\"Get available memory accounting for pool reservations\"\"\""
        },
        "get_available_memory": {
          "start_line": 344,
          "end_line": 348,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": "int",
          "calls": [],
          "docstring": "Get available memory accounting for pool reservations",
          "code_snippet": "        }\n    \n    def get_available_memory(self) -> int:\n        \"\"\"Get available memory accounting for pool reservations\"\"\"\n        return self.resource_limits[\"gpu_memory\"] - self.resource_usage[\"gpu_memory_used\"]\n\n    async def get_batch_status(self, batch_id: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Get detailed status of a batch\"\"\"\n        if batch_id in self.active_batches:"
        }
      },
      "class_variables": [],
      "bases": [],
      "docstring": "Enhanced resource manager with async support and cortex flow tracking"
    }
  },
  "functions": {},
  "constants": {}
}