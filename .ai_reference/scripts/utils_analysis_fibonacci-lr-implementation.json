{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\utils\\analysis\\fibonacci-lr-implementation.py",
  "imports": [],
  "classes": {
    "EVEUnifiedRatioWithFibonacci": {
      "start_line": 93,
      "end_line": 126,
      "methods": {
        "__init__": {
          "start_line": 96,
          "end_line": 101,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "params"
            },
            {
              "name": "fibonacci_intervals"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "....__init__",
              "line": 97
            },
            {
              "name": "super",
              "line": 97
            }
          ],
          "code_snippet": "    \"\"\"Extended EVE optimizer with Fibonacci-based learning rate sensitivity.\"\"\"\n    \n    def __init__(self, params, fibonacci_intervals=None, **kwargs):\n        super().__init__(params, **kwargs)\n        self.fibonacci_intervals = fibonacci_intervals or [5] * 1000  # Default fallback\n        self.current_epoch = 0\n    \n    def step(self, closure=None):\n        \"\"\"Modified step to update check interval based on current epoch.\"\"\"\n        # Update check interval based on current epoch"
        },
        "step": {
          "start_line": 101,
          "end_line": 115,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "closure"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "....step",
              "line": 108
            },
            {
              "name": "len",
              "line": 104
            },
            {
              "name": "super",
              "line": 108
            }
          ],
          "docstring": "Modified step to update check interval based on current epoch.",
          "code_snippet": "        self.current_epoch = 0\n    \n    def step(self, closure=None):\n        \"\"\"Modified step to update check interval based on current epoch.\"\"\"\n        # Update check interval based on current epoch\n        if self.current_epoch < len(self.fibonacci_intervals):\n            self.lr_check_interval = self.fibonacci_intervals[self.current_epoch]\n        \n        # Call parent step\n        loss = super().step(closure)\n        \n        # Update epoch counter\n        self.current_epoch += 1\n        \n        return loss\n    \n    def get_golden_ratio_factor(self, epoch):\n        \"\"\"Calculate learning rate adjustment factor based on golden ratio.\"\"\"\n        # As epochs progress, the ratio of consecutive Fibonacci numbers approaches \u03c6"
        },
        "get_golden_ratio_factor": {
          "start_line": 115,
          "end_line": 126,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "epoch"
            }
          ],
          "return_type": null,
          "calls": [],
          "docstring": "Calculate learning rate adjustment factor based on golden ratio.",
          "code_snippet": "        return loss\n    \n    def get_golden_ratio_factor(self, epoch):\n        \"\"\"Calculate learning rate adjustment factor based on golden ratio.\"\"\"\n        # As epochs progress, the ratio of consecutive Fibonacci numbers approaches \u03c6\n        if epoch < 2:\n            return 1.0\n        else:\n            # Use the ratio of consecutive Fibonacci numbers as a scaling factor\n            fib_ratio = self.fibonacci_intervals[epoch] / self.fibonacci_intervals[epoch-1]\n            # Normalize around 1.0 (\u03c6 \u2248 1.618)\n            return 1.0 + (fib_ratio - 1.0) * 0.1  # Dampen the effect\n\n# Mathematical analysis of why Fibonacci/Golden Ratio works\ndef analyze_fibonacci_learning():\n    \"\"\""
        }
      },
      "class_variables": [],
      "bases": [
        "EVEUnifiedRatio"
      ],
      "docstring": "Extended EVE optimizer with Fibonacci-based learning rate sensitivity."
    }
  },
  "functions": {
    "get_fibonacci_check_intervals": {
      "start_line": 3,
      "end_line": 34,
      "parameters": [
        {
          "name": "total_epochs"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "range",
          "line": 23
        },
        {
          "name": "fib_sequence.append",
          "line": 16
        },
        {
          "name": "intervals.append",
          "line": 29
        },
        {
          "name": "len",
          "line": 25
        }
      ],
      "docstring": "\n    Pre-calculate Fibonacci-based check intervals for the entire training.\n    \n    Args:\n        total_epochs: Total number of training epochs\n        \n    Returns:\n        List of check intervals for each epoch\n    ",
      "code_snippet": "# Implementation of Fibonacci-based learning rate sensitivity\n\ndef get_fibonacci_check_intervals(total_epochs):\n    \"\"\"\n    Pre-calculate Fibonacci-based check intervals for the entire training.\n    \n    Args:\n        total_epochs: Total number of training epochs\n        \n    Returns:\n        List of check intervals for each epoch\n    \"\"\"\n    # Generate Fibonacci sequence\n    fib_sequence = [1, 1]\n    while fib_sequence[-1] < total_epochs:\n        fib_sequence.append(fib_sequence[-1] + fib_sequence[-2])\n    \n    # Create interval mapping for each epoch\n    intervals = []\n    current_fib_index = 0\n    epoch_counter = 0\n    \n    for epoch in range(total_epochs):\n        # Move to next Fibonacci number if we've used current one enough times\n        if epoch_counter >= fib_sequence[current_fib_index] and current_fib_index < len(fib_sequence) - 1:\n            current_fib_index += 1\n            epoch_counter = 0\n        \n        intervals.append(fib_sequence[current_fib_index])\n        epoch_counter += 1\n    \n    return intervals\n\ndef register_unified_ratio_optimizer(config_params=None, total_epochs=100):\n    \"\"\"\n    Register the EVEUnifiedRatio optimizer with Fibonacci-based sensitivity."
    },
    "register_unified_ratio_optimizer": {
      "start_line": 34,
      "end_line": 92,
      "parameters": [
        {
          "name": "config_params"
        },
        {
          "name": "total_epochs"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "get_fibonacci_check_intervals",
          "line": 56
        },
        {
          "name": "logger.info",
          "line": 87
        },
        {
          "name": "logger.info",
          "line": 88
        },
        {
          "name": "logger.info",
          "line": 89
        },
        {
          "name": "logger.info",
          "line": 90
        },
        {
          "name": "config_params.get",
          "line": 50
        },
        {
          "name": "config_params.get",
          "line": 51
        },
        {
          "name": "config_params.get",
          "line": 52
        },
        {
          "name": "config_params.get",
          "line": 53
        },
        {
          "name": "....format",
          "line": 88
        },
        {
          "name": "....format",
          "line": 90
        },
        {
          "name": "....format",
          "line": 89
        }
      ],
      "docstring": "\n    Register the EVEUnifiedRatio optimizer with Fibonacci-based sensitivity.\n    \n    Args:\n        config_params: Optional dictionary with configuration parameters\n        total_epochs: Total number of epochs for training\n    ",
      "code_snippet": "    return intervals\n\ndef register_unified_ratio_optimizer(config_params=None, total_epochs=100):\n    \"\"\"\n    Register the EVEUnifiedRatio optimizer with Fibonacci-based sensitivity.\n    \n    Args:\n        config_params: Optional dictionary with configuration parameters\n        total_epochs: Total number of epochs for training\n    \"\"\"\n    # Set default configuration values\n    debug_ratios = False\n    debug_bounds = False\n    weight_adjustment_range = \"default\"\n    use_equilibrium_bounds = True\n    \n    # Override with provided parameters if any\n    if config_params:\n        debug_ratios = config_params.get('debug_ratios', debug_ratios)\n        debug_bounds = config_params.get('debug_bounds', debug_bounds)\n        weight_adjustment_range = config_params.get('weight_adjustment_range', weight_adjustment_range)\n        use_equilibrium_bounds = config_params.get('use_equilibrium_bounds', use_equilibrium_bounds)\n    \n    # Pre-calculate Fibonacci intervals for the entire training\n    fibonacci_intervals = get_fibonacci_check_intervals(total_epochs)\n    \n    # Create configuration for the unified ratio optimizer\n    unified_config = {\n        'default': {\n            'optimizer_class': EVEUnifiedRatio,\n            'optimizer_kwargs': {\n                'lr': 0.01,\n                'eps': 1e-8,\n                'base_confidence_threshold': 0.7,\n                'weight_decay': 0.0001,\n                'debug_ratios': debug_ratios,\n                'debug_bounds': debug_bounds,\n                'warmup_epochs': 1,\n                'weight_adjustment_range': weight_adjustment_range,\n                'lr_check_interval': fibonacci_intervals[0],  # Start with first interval\n                'lr_change_threshold': 0.003,\n                'lr_log_threshold': 0.03,\n                'use_equilibrium_bounds': use_equilibrium_bounds,\n                'fibonacci_intervals': fibonacci_intervals  # Pass the pre-calculated intervals\n            },\n            'scheduler_class': optim.lr_scheduler.CosineAnnealingLR,\n            'scheduler_kwargs': {\n                'T_max': 200\n            }\n        }\n    }\n    \n    # Update ALL_CONFIGS with the new optimizer\n    ALL_CONFIGS['eve_unified'] = unified_config\n    \n    logger.info(\"EVEUnifiedRatio optimizer registered with Fibonacci-based sensitivity\")\n    logger.info(\"  Weight adjustment range: {}\".format(weight_adjustment_range))\n    logger.info(\"  Fibonacci intervals: {}\".format(fibonacci_intervals[:10]) + \"...\")\n    logger.info(\"  Using equilibrium bounds: {}\".format(use_equilibrium_bounds))\n\n# Modification to EVEUnifiedRatio class to use Fibonacci intervals\nclass EVEUnifiedRatioWithFibonacci(EVEUnifiedRatio):\n    \"\"\"Extended EVE optimizer with Fibonacci-based learning rate sensitivity.\"\"\""
    },
    "analyze_fibonacci_learning": {
      "start_line": 127,
      "end_line": 157,
      "parameters": [],
      "return_type": null,
      "calls": [
        {
          "name": "math.sqrt",
          "line": 142
        }
      ],
      "docstring": "\n    Analyze why Fibonacci sequence aligns with neural network learning dynamics.\n    ",
      "code_snippet": "\n# Mathematical analysis of why Fibonacci/Golden Ratio works\ndef analyze_fibonacci_learning():\n    \"\"\"\n    Analyze why Fibonacci sequence aligns with neural network learning dynamics.\n    \"\"\"\n    # The golden ratio appears in:\n    # 1. Natural growth patterns\n    # 2. Optimal search algorithms\n    # 3. Information theory\n    # 4. Dynamic systems stability\n    \n    # Neural network learning exhibits similar patterns:\n    # - Rapid initial progress\n    # - Gradually diminishing returns\n    # - Natural equilibrium points\n    \n    phi = (1 + math.sqrt(5)) / 2  # Golden ratio\n    \n    # Learning rate decay often follows: lr_t = lr_0 / (1 + decay * t)\n    # Fibonacci intervals provide a natural decay pattern that approximates:\n    # interval_t \u2248 interval_0 * \u03c6^t\n    \n    # This creates a self-similar pattern across different time scales,\n    # which is optimal for multi-scale learning dynamics in neural networks.\n    \n    return {\n        'golden_ratio': phi,\n        'fibonacci_convergence': \"lim(F_n+1/F_n) \u2192 \u03c6 as n \u2192 \u221e\",\n        'learning_dynamics': \"Natural alignment with diminishing returns\",\n        'optimization_theory': \"Optimal search patterns in high-dimensional spaces\"\n    }"
    }
  },
  "constants": {}
}