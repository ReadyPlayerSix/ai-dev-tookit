{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\isekaizen\\optimizers\\registration.py",
  "imports": [
    {
      "name": "logging",
      "line": 8
    },
    {
      "name": "typing.Dict",
      "line": 9
    },
    {
      "name": "typing.Any",
      "line": 9
    },
    {
      "name": "typing.Type",
      "line": 9
    },
    {
      "name": "typing.Optional",
      "line": 9
    },
    {
      "name": "torch.optim",
      "line": 10
    },
    {
      "name": "isekaizen.optimizers.configs.optimizer_configs.ALL_CONFIGS",
      "line": 12
    },
    {
      "name": "isekaizen.utils.training_utils.get_fibonacci_check_intervals",
      "line": 13
    },
    {
      "name": "isekaizen.optimizers.eve_unified_ratio.EVEUnifiedRatio",
      "line": 85
    }
  ],
  "classes": {},
  "functions": {
    "register_unified_ratio_optimizer": {
      "start_line": 17,
      "end_line": 102,
      "parameters": [
        {
          "name": "configs_dict"
        },
        {
          "name": "optimizer_class"
        },
        {
          "name": "config_params"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "logger.info",
          "line": 92
        },
        {
          "name": "logger.info",
          "line": 93
        },
        {
          "name": "logger.info",
          "line": 94
        },
        {
          "name": "logger.info",
          "line": 95
        },
        {
          "name": "logger.info",
          "line": 98
        },
        {
          "name": "config_params.get",
          "line": 39
        },
        {
          "name": "config_params.get",
          "line": 40
        },
        {
          "name": "config_params.get",
          "line": 41
        },
        {
          "name": "config_params.get",
          "line": 42
        },
        {
          "name": "config_params.get",
          "line": 43
        },
        {
          "name": "config_params.get",
          "line": 44
        },
        {
          "name": "config_params.get",
          "line": 45
        },
        {
          "name": "config_params.get",
          "line": 46
        },
        {
          "name": "config_params.get",
          "line": 47
        },
        {
          "name": "logger.info",
          "line": 97
        }
      ],
      "docstring": "\n    Register the EVEUnifiedRatio optimizer in the available optimizers.\n    \n    Args:\n        configs_dict: Dictionary of optimizer configurations to update\n        optimizer_class: The EVEUnifiedRatio optimizer class\n        config_params: Optional dictionary with configuration parameters\n    ",
      "code_snippet": "logger = logging.getLogger(__name__)\n\ndef register_unified_ratio_optimizer(configs_dict=None, optimizer_class=None, config_params=None):\n    \"\"\"\n    Register the EVEUnifiedRatio optimizer in the available optimizers.\n    \n    Args:\n        configs_dict: Dictionary of optimizer configurations to update\n        optimizer_class: The EVEUnifiedRatio optimizer class\n        config_params: Optional dictionary with configuration parameters\n    \"\"\"\n    # Set default configuration values\n    debug_ratios = False\n    debug_bounds = False\n    weight_adjustment_range = \"default\"  # Default value (0.85-1.15)\n    weight_range_iris = False  # Default to disabled iris feature\n    lr_check_interval = 10  # Default value\n    lr_change_threshold = 0.005  # Default value\n    lr_log_threshold = 0.05  # Default value\n    use_equilibrium_bounds = True  # Default to use equilibrium bounds\n    fibonacci_intervals = None  # For Fibonacci-based sensitivity\n    \n    # Override with provided parameters if any\n    if config_params:\n        debug_ratios = config_params.get('debug_ratios', debug_ratios)\n        debug_bounds = config_params.get('debug_bounds', debug_bounds)\n        weight_adjustment_range = config_params.get('weight_adjustment_range', weight_adjustment_range)\n        weight_range_iris = config_params.get('weight_range_iris', weight_range_iris)\n        lr_check_interval = config_params.get('lr_check_interval', lr_check_interval)\n        lr_change_threshold = config_params.get('lr_change_threshold', lr_change_threshold)\n        lr_log_threshold = config_params.get('lr_log_threshold', lr_log_threshold)\n        use_equilibrium_bounds = config_params.get('use_equilibrium_bounds', use_equilibrium_bounds)\n        fibonacci_intervals = config_params.get('fibonacci_intervals', None)\n    \n    # Create configuration for the unified ratio optimizer\n    unified_config = {\n        'default': {\n            'optimizer_class': optimizer_class,\n            'optimizer_kwargs': {\n                'lr': 0.01,\n                'eps': 1e-8,\n                'base_confidence_threshold': 0.7,\n                'weight_decay': 0.0001,\n                'debug_ratios': debug_ratios,\n                'debug_bounds': debug_bounds,\n                'warmup_epochs': 1,  # Faster warmup\n                'weight_adjustment_range': weight_adjustment_range,\n                'weight_range_iris': weight_range_iris,\n                'lr_check_interval': lr_check_interval,\n                'lr_change_threshold': lr_change_threshold,\n                'lr_log_threshold': lr_log_threshold,\n                'use_equilibrium_bounds': use_equilibrium_bounds\n            },\n            'scheduler_class': optim.lr_scheduler.CosineAnnealingLR,\n            'scheduler_kwargs': {\n                'T_max': 200\n            }\n        }\n    }\n    \n    # If using Fibonacci intervals, we'll set them as an attribute on the optimizer after creation\n    if fibonacci_intervals:\n        unified_config['default']['optimizer_kwargs']['fibonacci_intervals'] = fibonacci_intervals\n    \n    # Update configs_dict with the new optimizer\n    if configs_dict is not None:\n        configs_dict['eve_unified'] = unified_config\n    \n    # If optimizer_class wasn't provided, import it directly\n    if optimizer_class is None:\n        from isekaizen.optimizers.eve_unified_ratio import EVEUnifiedRatio\n        optimizer_class = EVEUnifiedRatio\n        unified_config['default']['optimizer_class'] = optimizer_class\n    \n    # Also update the global ALL_CONFIGS\n    ALL_CONFIGS['eve_unified'] = unified_config\n    \n    logger.info(\"EVEUnifiedRatio optimizer registered successfully\")\n    logger.info(f\"  Weight adjustment range: {weight_adjustment_range}\")\n    logger.info(f\"  Weight range iris: {'Enabled' if weight_range_iris else 'Disabled'}\")\n    logger.info(f\"  LR sensitivity: check interval={lr_check_interval}, threshold={lr_change_threshold:.4f}\")\n    if fibonacci_intervals:\n        logger.info(\"  Using Fibonacci intervals for dynamic sensitivity\")\n    logger.info(f\"  Using equilibrium bounds: {use_equilibrium_bounds}\")\n    \n    return unified_config"
    }
  },
  "constants": {}
}