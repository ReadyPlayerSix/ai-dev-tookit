{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\isekaizen\\optimizers\\lr_boundary\\lr_boundary_calculator.py",
  "imports": [
    {
      "name": "math",
      "line": 13
    },
    {
      "name": "logging",
      "line": 14
    },
    {
      "name": "typing.Dict",
      "line": 15
    },
    {
      "name": "typing.Any",
      "line": 15
    },
    {
      "name": "typing.Optional",
      "line": 15
    },
    {
      "name": "typing.List",
      "line": 15
    },
    {
      "name": "typing.Tuple",
      "line": 15
    }
  ],
  "classes": {
    "LRBoundaryCalculator": {
      "start_line": 19,
      "end_line": 250,
      "methods": {
        "__init__": {
          "start_line": 27,
          "end_line": 50,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "initial_lr"
            },
            {
              "name": "pattern_map"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self._initialize_pattern_factors",
              "line": 37
            },
            {
              "name": "logger.info",
              "line": 45
            },
            {
              "name": "logger.info",
              "line": 46
            },
            {
              "name": "logger.warning",
              "line": 48
            }
          ],
          "code_snippet": "    \"\"\"\n    \n    def __init__(self, initial_lr=0.01, pattern_map=None):\n        # Core parameters\n        self.initial_lr = initial_lr\n        self.pattern_map = pattern_map\n        \n        # Learning rate bounds\n        self.global_min_lr = 0.0001\n        self.global_max_lr = 0.1\n        \n        # Pattern-specific boundaries - initialize even without pattern map\n        self.pattern_lr_factors = self._initialize_pattern_factors()\n        \n        # Training history\n        self.train_acc_history = []\n        self.val_acc_history = []\n        self.lr_history = []\n        self.gap_history = []  # Kept for tracking, though no longer used for calculations\n        \n        logger.info(f\"LRBoundaryCalculator initialized with initial LR: {initial_lr}\")\n        logger.info(f\"Global LR bounds: [{self.global_min_lr}, {self.global_max_lr}]\")\n        if not pattern_map:\n            logger.warning(\"No pattern map provided - using default pattern LR factors\")\n        \n    def _initialize_pattern_factors(self):\n        \"\"\"Initialize pattern-specific LR adjustment factors for simplified taxonomy.\"\"\"\n        # Only use simplified taxonomy patterns"
        },
        "_initialize_pattern_factors": {
          "start_line": 50,
          "end_line": 118,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logger.info",
              "line": 62
            },
            {
              "name": "logger.info",
              "line": 67
            },
            {
              "name": "....items",
              "line": 78
            },
            {
              "name": "default_factors.copy",
              "line": 108
            },
            {
              "name": "merged_factors.update",
              "line": 109
            },
            {
              "name": "logger.info",
              "line": 111
            },
            {
              "name": "logger.info",
              "line": 114
            },
            {
              "name": "logger.info",
              "line": 69
            },
            {
              "name": "logger.warning",
              "line": 71
            },
            {
              "name": "logger.warning",
              "line": 73
            },
            {
              "name": "logger.debug",
              "line": 101
            },
            {
              "name": "len",
              "line": 67
            },
            {
              "name": "logger.warning",
              "line": 82
            },
            {
              "name": "isinstance",
              "line": 85
            },
            {
              "name": "logger.debug",
              "line": 87
            },
            {
              "name": "logger.debug",
              "line": 90
            },
            {
              "name": "logger.warning",
              "line": 103
            },
            {
              "name": "default_factors.get",
              "line": 105
            },
            {
              "name": "self.pattern_map.keys",
              "line": 67
            },
            {
              "name": "len",
              "line": 69
            },
            {
              "name": "str",
              "line": 73
            },
            {
              "name": "isinstance",
              "line": 89
            },
            {
              "name": "str",
              "line": 103
            }
          ],
          "docstring": "Initialize pattern-specific LR adjustment factors for simplified taxonomy.",
          "code_snippet": "            logger.warning(\"No pattern map provided - using default pattern LR factors\")\n        \n    def _initialize_pattern_factors(self):\n        \"\"\"Initialize pattern-specific LR adjustment factors for simplified taxonomy.\"\"\"\n        # Only use simplified taxonomy patterns\n        default_factors = {\n            'structural': 1.0,   # Spatial organization and relationships\n            'statistical': 1.05, # Distribution and variance patterns\n            'temporal': 0.8,     # Time-related patterns\n            'default': 1.0\n        }\n        \n        # Check if pattern map is available\n        if self.pattern_map is None:\n            logger.info(f\"Using default pattern LR factors: {default_factors}\")\n            return default_factors\n            \n        # Log pattern map contents for debugging\n        try:\n            logger.info(f\"Pattern map has {len(self.pattern_map.keys())} top-level keys\")\n            if 'pattern_complexities' in self.pattern_map:\n                logger.info(f\"Found pattern_complexities with {len(self.pattern_map['pattern_complexities'])} pattern types\")\n            else:\n                logger.warning(\"No pattern_complexities found in pattern map\")\n        except Exception as e:\n            logger.warning(f\"Error examining pattern map: {str(e)}\")\n        \n        # If pattern map available, initialize from complexity information\n        if self.pattern_map and 'pattern_complexities' in self.pattern_map:\n            factors = {}\n            for pattern_type, complexity_data in self.pattern_map['pattern_complexities'].items():\n                try:\n                    # Only process patterns from the simplified taxonomy\n                    if pattern_type not in default_factors:\n                        logger.warning(f\"Skipping pattern type '{pattern_type}' that is not in simplified taxonomy\")\n                        continue\n                        \n                    if isinstance(complexity_data, dict) and 'avg_complexity' in complexity_data:\n                        complexity = complexity_data['avg_complexity']\n                        logger.debug(f\"Found complexity data for {pattern_type}: {complexity:.2f} (from avg_complexity)\")\n                    else:\n                        complexity = complexity_data if isinstance(complexity_data, (int, float)) else 0.5\n                        logger.debug(f\"Found complexity data for {pattern_type}: {complexity:.2f} (direct value)\")\n                    \n                    # Map complexity to factor - higher complexity = lower LR\n                    if complexity > 0.8:\n                        factors[pattern_type] = 0.8  # Complex patterns need lower LR\n                    elif complexity < 0.3:\n                        factors[pattern_type] = 1.2  # Simple patterns can use higher LR\n                    else:\n                        # Linear mapping from complexity to factor\n                        factors[pattern_type] = 1.2 - (0.5 * complexity)\n                        \n                    logger.debug(f\"Mapped complexity {complexity:.2f} to factor {factors[pattern_type]:.2f} for {pattern_type}\")\n                except Exception as e:\n                    logger.warning(f\"Error processing complexity data for {pattern_type}: {str(e)}\")\n                    # Use default factor\n                    factors[pattern_type] = default_factors.get(pattern_type, 1.0)\n            \n            # Merge with defaults for any missing patterns\n            merged_factors = default_factors.copy()\n            merged_factors.update(factors)\n            \n            logger.info(f\"Pattern LR factors initialized from pattern map: {factors}\")\n            return merged_factors\n        else:\n            logger.info(f\"Using default pattern LR factors: {default_factors}\")\n        \n        return default_factors\n    \n    def calculate_optimal_lr(self, current_lr, train_acc, val_acc, epoch, total_epochs, pattern_states):\n        \"\"\"Calculate optimal learning rate based on train/test ratio.\"\"\"\n        # Calculate training progress (0-1)"
        },
        "calculate_optimal_lr": {
          "start_line": 118,
          "end_line": 159,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "current_lr"
            },
            {
              "name": "train_acc"
            },
            {
              "name": "val_acc"
            },
            {
              "name": "epoch"
            },
            {
              "name": "total_epochs"
            },
            {
              "name": "pattern_states"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.gap_history.append",
              "line": 127
            },
            {
              "name": "self.train_acc_history.append",
              "line": 130
            },
            {
              "name": "self.val_acc_history.append",
              "line": 131
            },
            {
              "name": "self.lr_history.append",
              "line": 132
            },
            {
              "name": "self._calculate_lr_adjustment_from_ratio",
              "line": 142
            },
            {
              "name": "max",
              "line": 148
            },
            {
              "name": "logger.debug",
              "line": 151
            },
            {
              "name": "logger.debug",
              "line": 152
            },
            {
              "name": "logger.debug",
              "line": 153
            },
            {
              "name": "logger.debug",
              "line": 154
            },
            {
              "name": "logger.debug",
              "line": 155
            },
            {
              "name": "max",
              "line": 124
            },
            {
              "name": "min",
              "line": 148
            }
          ],
          "docstring": "Calculate optimal learning rate based on train/test ratio.",
          "code_snippet": "        return default_factors\n    \n    def calculate_optimal_lr(self, current_lr, train_acc, val_acc, epoch, total_epochs, pattern_states):\n        \"\"\"Calculate optimal learning rate based on train/test ratio.\"\"\"\n        # Calculate training progress (0-1)\n        progress = epoch / total_epochs\n        \n        # Calculate train-test ratio and gap\n        ratio = train_acc / max(0.1, val_acc)  # Safe division\n        gap = train_acc - val_acc  # Still used for history tracking\n        \n        self.gap_history.append(gap)\n        \n        # Store history\n        self.train_acc_history.append(train_acc)\n        self.val_acc_history.append(val_acc)\n        self.lr_history.append(current_lr)\n        \n        # For logging purposes, categorize the situation\n        situation_desc = \"normal\"\n        if ratio > 1.1:  # Train accuracy is significantly higher\n            situation_desc = \"potential overfitting\"\n        elif ratio < 0.95:  # Val accuracy higher than train\n            situation_desc = \"underfitting\"\n        \n        # Calculate learning rate adjustment factor based on ratio\n        adjustment_factor = self._calculate_lr_adjustment_from_ratio(ratio, progress, pattern_states)\n        \n        # Apply adjustment to current learning rate\n        new_lr = current_lr * adjustment_factor\n        \n        # Apply global bounds\n        bounded_lr = max(self.global_min_lr, min(self.global_max_lr, new_lr))\n        \n        # Log the calculation at debug level to reduce verbosity\n        logger.debug(f\"LR adjustment calculation:\")\n        logger.debug(f\"  Current LR: {current_lr:.6f}\")\n        logger.debug(f\"  Train/test ratio: {ratio:.2f} ({situation_desc})\")\n        logger.debug(f\"  Adjustment factor: {adjustment_factor:.2f}\")\n        logger.debug(f\"  New LR: {bounded_lr:.6f}\")\n        \n        return bounded_lr\n    \n    def _calculate_lr_adjustment_from_ratio(self, ratio, progress, pattern_states):\n        \"\"\"Calculate learning rate adjustment factor based on train/test ratio with increased sensitivity.\"\"\"\n        # Only reduce learning rate when ratio indicates overfitting"
        },
        "_calculate_lr_adjustment_from_ratio": {
          "start_line": 159,
          "end_line": 183,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "ratio"
            },
            {
              "name": "progress"
            },
            {
              "name": "pattern_states"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "max",
              "line": 166
            },
            {
              "name": "logger.debug",
              "line": 167
            },
            {
              "name": "logger.debug",
              "line": 171
            },
            {
              "name": "self._calculate_pattern_adjustment",
              "line": 175
            },
            {
              "name": "logger.debug",
              "line": 179
            }
          ],
          "docstring": "Calculate learning rate adjustment factor based on train/test ratio with increased sensitivity.",
          "code_snippet": "        return bounded_lr\n    \n    def _calculate_lr_adjustment_from_ratio(self, ratio, progress, pattern_states):\n        \"\"\"Calculate learning rate adjustment factor based on train/test ratio with increased sensitivity.\"\"\"\n        # Only reduce learning rate when ratio indicates overfitting\n        if ratio > 1.00:  # Reduced threshold for overfitting detection (was 1.05)\n            # More aggressive reduction formula\n            # ratio 1.05 -> 0.98 adjustment\n            # ratio 1.2 -> 0.94 adjustment\n            adjustment = max(0.8, 1.0 - (ratio - 1.0) * 0.4)  # Increased factor (was 0.3)\n            logger.debug(f\"High train/test ratio detected ({ratio:.2f}), reducing LR by factor {adjustment:.2f}\")\n        else:\n            # Good ratio = maintain learning rate\n            adjustment = 1.0\n            logger.debug(f\"Good train/test ratio ({ratio:.2f}), maintaining LR\")\n        \n        # Apply pattern-specific adjustments when ratio indicates complex patterns\n        if adjustment < 1.0 and pattern_states:\n            pattern_adjustment = self._calculate_pattern_adjustment(pattern_states)\n            # Only allow pattern adjustments to further reduce LR, not increase it\n            if pattern_adjustment < 1.0:\n                adjustment *= pattern_adjustment\n                logger.debug(f\"Applied pattern adjustment: {pattern_adjustment:.2f}\")\n        \n        return adjustment\n    \n    def _calculate_pattern_adjustment(self, pattern_states):\n        \"\"\"Calculate pattern-specific adjustment factor.\"\"\"\n        # Weight adjustment by pattern importance"
        },
        "_calculate_pattern_adjustment": {
          "start_line": 183,
          "end_line": 199,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "pattern_states"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "sum",
              "line": 186
            },
            {
              "name": "pattern_states.items",
              "line": 191
            },
            {
              "name": "info.get",
              "line": 192
            },
            {
              "name": "self.pattern_lr_factors.get",
              "line": 193
            },
            {
              "name": "info.get",
              "line": 186
            },
            {
              "name": "pattern_states.values",
              "line": 186
            }
          ],
          "docstring": "Calculate pattern-specific adjustment factor.",
          "code_snippet": "        return adjustment\n    \n    def _calculate_pattern_adjustment(self, pattern_states):\n        \"\"\"Calculate pattern-specific adjustment factor.\"\"\"\n        # Weight adjustment by pattern importance\n        total_importance = sum(info.get('importance', 0) for info in pattern_states.values())\n        if total_importance < 1e-5:\n            return 1.0  # No adjustment if no important patterns\n            \n        weighted_adjustment = 0.0\n        for pattern_type, info in pattern_states.items():\n            importance = info.get('importance', 0)\n            pattern_factor = self.pattern_lr_factors.get(pattern_type, \n                                                        self.pattern_lr_factors['default'])\n            weighted_adjustment += (importance / total_importance) * pattern_factor\n        \n        return weighted_adjustment\n    \n    def get_current_bounds(self, pattern_states=None):\n        \"\"\"Get current learning rate bounds based on train/test ratio history.\"\"\"\n        # Base bounds"
        },
        "get_current_bounds": {
          "start_line": 199,
          "end_line": 231,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "pattern_states"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self._calculate_pattern_bounds",
              "line": 224
            },
            {
              "name": "len",
              "line": 206
            },
            {
              "name": "len",
              "line": 206
            },
            {
              "name": "sum",
              "line": 212
            },
            {
              "name": "len",
              "line": 212
            },
            {
              "name": "max",
              "line": 226
            },
            {
              "name": "min",
              "line": 227
            },
            {
              "name": "max",
              "line": 211
            },
            {
              "name": "zip",
              "line": 211
            },
            {
              "name": "len",
              "line": 219
            }
          ],
          "docstring": "Get current learning rate bounds based on train/test ratio history.",
          "code_snippet": "        return weighted_adjustment\n    \n    def get_current_bounds(self, pattern_states=None):\n        \"\"\"Get current learning rate bounds based on train/test ratio history.\"\"\"\n        # Base bounds\n        min_bound = self.global_min_lr\n        max_bound = self.global_max_lr\n        \n        # Adjust based on train/test ratio history if available\n        if len(self.train_acc_history) >= 2 and len(self.val_acc_history) >= 2:\n            recent_train = self.train_acc_history[-3:]\n            recent_val = self.val_acc_history[-3:]\n            \n            # Calculate recent ratios\n            recent_ratios = [t / max(0.1, v) for t, v in zip(recent_train, recent_val)]\n            avg_ratio = sum(recent_ratios) / len(recent_ratios)\n            \n            # If ratio is consistently high, lower the upper bound\n            if avg_ratio > 1.1:  # Above threshold\n                max_bound = max_bound * (2.0 - avg_ratio)  # Reduce max bound proportionally\n            \n            # If ratio is trending upward, lower the upper bound further\n            if len(recent_ratios) >= 2 and recent_ratios[-1] > recent_ratios[0]:\n                max_bound *= 0.9\n        \n        # Adjust based on pattern states if available\n        if pattern_states:\n            pattern_bounds = self._calculate_pattern_bounds(pattern_states)\n            if pattern_bounds:\n                min_bound = max(min_bound, pattern_bounds['min'])\n                max_bound = min(max_bound, pattern_bounds['max'])\n        \n        return {'min': min_bound, 'max': max_bound}\n    \n    def _calculate_pattern_bounds(self, pattern_states):\n        \"\"\"Calculate bounds based on pattern states.\"\"\"\n        # Extract pattern types from states"
        },
        "_calculate_pattern_bounds": {
          "start_line": 231,
          "end_line": 250,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "pattern_states"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "list",
              "line": 234
            },
            {
              "name": "min",
              "line": 241
            },
            {
              "name": "max",
              "line": 242
            },
            {
              "name": "pattern_states.keys",
              "line": 234
            },
            {
              "name": "self.pattern_lr_factors.get",
              "line": 237
            }
          ],
          "docstring": "Calculate bounds based on pattern states.",
          "code_snippet": "        return {'min': min_bound, 'max': max_bound}\n    \n    def _calculate_pattern_bounds(self, pattern_states):\n        \"\"\"Calculate bounds based on pattern states.\"\"\"\n        # Extract pattern types from states\n        pattern_types = list(pattern_states.keys())\n        \n        # Get adjustment factors for these patterns\n        factors = [self.pattern_lr_factors.get(pt, self.pattern_lr_factors['default']) \n                  for pt in pattern_types]\n        \n        # Find min and max adjustment factors\n        min_factor = min(factors)\n        max_factor = max(factors)\n        \n        # Calculate bounds from factors\n        min_bound = self.global_min_lr * max_factor  # Higher factor raises min bound\n        max_bound = self.global_max_lr * min_factor  # Lower factor reduces max bound\n        \n        return {'min': min_bound, 'max': max_bound}"
        }
      },
      "class_variables": [],
      "bases": [],
      "docstring": "Calculate optimal learning rate boundaries based on train/test accuracy ratios.\n    \n    This class focuses on adjusting the learning rate when there are signs of overfitting\n    (train/test ratio > 1.1) while maintaining the current rate during normal training.\n    Pattern-specific adjustments are applied only when the ratio indicates a need for adjustment.\n    "
    }
  },
  "functions": {},
  "constants": {}
}