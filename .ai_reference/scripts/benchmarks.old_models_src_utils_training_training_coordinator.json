{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\benchmarks.old\\models\\src\\utils\\training\\training_coordinator.py",
  "imports": [
    {
      "name": "pathlib.Path",
      "line": 1
    },
    {
      "name": "asyncio",
      "line": 2
    },
    {
      "name": "json",
      "line": 3
    },
    {
      "name": "logging",
      "line": 4
    },
    {
      "name": "datetime.datetime",
      "line": 5
    },
    {
      "name": "typing.Dict",
      "line": 6
    },
    {
      "name": "typing.List",
      "line": 6
    },
    {
      "name": "typing.Any",
      "line": 6
    },
    {
      "name": "typing.Optional",
      "line": 6
    },
    {
      "name": "dataclasses.dataclass",
      "line": 7
    },
    {
      "name": "utils.config.SystemConfiguration",
      "line": 9
    },
    {
      "name": "data_fetcher.EnhancedDataFetcher",
      "line": 10
    },
    {
      "name": "data_fetcher.TrainingBatch",
      "line": 10
    }
  ],
  "classes": {
    "TrainingState": {
      "start_line": 13,
      "end_line": 22,
      "methods": {},
      "class_variables": [],
      "bases": [],
      "docstring": "Represents current training state"
    },
    "TrainingCoordinator": {
      "start_line": 22,
      "end_line": 265,
      "methods": {
        "__init__": {
          "start_line": 25,
          "end_line": 56,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "config",
              "type": "SystemConfiguration"
            },
            {
              "name": "data_fetcher",
              "type": "EnhancedDataFetcher"
            },
            {
              "name": "checkpoint_dir",
              "type": "str"
            },
            {
              "name": "logs_dir",
              "type": "str"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "Path",
              "line": 32
            },
            {
              "name": "Path",
              "line": 33
            },
            {
              "name": "self.checkpoint_dir.mkdir",
              "line": 36
            },
            {
              "name": "self.logs_dir.mkdir",
              "line": 37
            },
            {
              "name": "logging.getLogger",
              "line": 40
            },
            {
              "name": "logging.FileHandler",
              "line": 41
            },
            {
              "name": "handler.setFormatter",
              "line": 42
            },
            {
              "name": "self.logger.addHandler",
              "line": 43
            },
            {
              "name": "self.logger.setLevel",
              "line": 44
            },
            {
              "name": "logging.Formatter",
              "line": 42
            }
          ],
          "code_snippet": "    \"\"\"Coordinates the training process including pattern recognition, RPG progression, and resource optimization\"\"\"\n    \n    def __init__(self, \n                 config: SystemConfiguration,\n                 data_fetcher: EnhancedDataFetcher,\n                 checkpoint_dir: str = \"checkpoints\",\n                 logs_dir: str = \"logs/training\"):\n        self.config = config\n        self.data_fetcher = data_fetcher\n        self.checkpoint_dir = Path(checkpoint_dir)\n        self.logs_dir = Path(logs_dir)\n        \n        # Ensure directories exist\n        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n        self.logs_dir.mkdir(parents=True, exist_ok=True)\n        \n        # Configure logging\n        self.logger = logging.getLogger(\"TrainingCoordinator\")\n        handler = logging.FileHandler(self.logs_dir / \"training.log\")\n        handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n        self.logger.addHandler(handler)\n        self.logger.setLevel(logging.INFO)\n        \n        # Initialize training state\n        self.training_state = None\n        self.performance_history = []\n        \n        # Resource tracking\n        self.resource_metrics = {\n            \"memory_usage\": [],\n            \"batch_times\": [],\n            \"cognitive_loads\": []\n        }\n    \n    async def initialize_training(self, \n                                epochs: int,"
        },
        "_should_checkpoint": {
          "start_line": 195,
          "end_line": 203,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": "bool",
          "calls": [
            {
              "name": "datetime.now",
              "line": 200
            },
            {
              "name": "time_since_checkpoint.total_seconds",
              "line": 201
            }
          ],
          "docstring": "Determine if checkpoint should be saved",
          "code_snippet": "        pass\n    \n    def _should_checkpoint(self) -> bool:\n        \"\"\"Determine if checkpoint should be saved\"\"\"\n        if not self.training_state.last_checkpoint:\n            return True\n            \n        time_since_checkpoint = datetime.now() - self.training_state.last_checkpoint\n        return time_since_checkpoint.total_seconds() > 3600  # Checkpoint every hour\n    \n    async def save_checkpoint(self):\n        \"\"\"Save training checkpoint\"\"\"\n        checkpoint_path = self.checkpoint_dir / f\"checkpoint_epoch{self.training_state.epoch}_batch{self.training_state.batch_index}.json\""
        },
        "get_training_metrics": {
          "start_line": 252,
          "end_line": 265,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": "complex_type",
          "calls": [],
          "docstring": "Get current training metrics",
          "code_snippet": "        self.logger.info(f\"Loaded checkpoint from {checkpoint_path}\")\n        \n    def get_training_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get current training metrics\"\"\"\n        if not self.training_state:\n            return {}\n            \n        return {\n            \"epoch\": self.training_state.epoch,\n            \"batch\": self.training_state.batch_index,\n            \"metrics\": self.training_state.current_metrics,\n            \"resource_usage\": {\n                \"memory\": self.resource_metrics[\"memory_usage\"][-1] if self.resource_metrics[\"memory_usage\"] else 0,\n                \"cognitive_load\": self.resource_metrics[\"cognitive_loads\"][-1] if self.resource_metrics[\"cognitive_loads\"] else 0\n            }\n        }"
        }
      },
      "class_variables": [],
      "bases": [],
      "docstring": "Coordinates the training process including pattern recognition, RPG progression, and resource optimization"
    }
  },
  "functions": {},
  "constants": {}
}