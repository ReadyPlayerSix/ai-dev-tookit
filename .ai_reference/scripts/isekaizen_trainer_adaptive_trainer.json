{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\isekaizen\\trainer\\adaptive_trainer.py",
  "imports": [
    {
      "name": "torch",
      "line": 8
    },
    {
      "name": "logging",
      "line": 9
    },
    {
      "name": "time",
      "line": 10
    },
    {
      "name": "typing.Dict",
      "line": 11
    },
    {
      "name": "typing.List",
      "line": 11
    },
    {
      "name": "typing.Any",
      "line": 11
    },
    {
      "name": "typing.Optional",
      "line": 11
    },
    {
      "name": "typing.Tuple",
      "line": 11
    },
    {
      "name": "isekaizen.trainer.model_trainer.ModelTrainer",
      "line": 13
    },
    {
      "name": "isekaizen.core.optimizer.pattern_responsive_optimizer.PatternResponsiveOptimizer",
      "line": 14
    },
    {
      "name": "isekaizen.core.optimizer.pre_augment_optimizer.PreAugmentOptimizer",
      "line": 86
    },
    {
      "name": "isekaizen.core.optimizer.pattern_responsive_optimizer.PatternResponsiveOptimizer",
      "line": 102
    }
  ],
  "classes": {
    "AdaptiveTrainer": {
      "start_line": 18,
      "end_line": 332,
      "methods": {
        "__init__": {
          "start_line": 26,
          "end_line": 126,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "model"
            },
            {
              "name": "criterion"
            },
            {
              "name": "optimizer_class"
            },
            {
              "name": "optimizer_kwargs"
            },
            {
              "name": "scheduler_class"
            },
            {
              "name": "scheduler_kwargs"
            },
            {
              "name": "device"
            },
            {
              "name": "pattern_map"
            },
            {
              "name": "use_pre_augmented"
            },
            {
              "name": "pattern_adaptation_start"
            },
            {
              "name": "pattern_adaptation_frequency"
            },
            {
              "name": "augmentation_strength"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "....__init__",
              "line": 61
            },
            {
              "name": "kwargs.copy",
              "line": 75
            },
            {
              "name": "kwargs_copy.pop",
              "line": 76
            },
            {
              "name": "kwargs_copy.pop",
              "line": 77
            },
            {
              "name": "kwargs_copy.pop",
              "line": 78
            },
            {
              "name": "kwargs_copy.pop",
              "line": 79
            },
            {
              "name": "kwargs_copy.pop",
              "line": 80
            },
            {
              "name": "kwargs_copy.pop",
              "line": 81
            },
            {
              "name": "logger.info",
              "line": 124
            },
            {
              "name": "PreAugmentOptimizer",
              "line": 87
            },
            {
              "name": "logger.info",
              "line": 98
            },
            {
              "name": "logger.info",
              "line": 99
            },
            {
              "name": "PatternResponsiveOptimizer",
              "line": 103
            },
            {
              "name": "logger.info",
              "line": 118
            },
            {
              "name": "logger.info",
              "line": 119
            },
            {
              "name": "super",
              "line": 61
            }
          ],
          "docstring": "\n        Initialize the adaptive trainer.\n        \n        Args:\n            model: PyTorch model to train\n            criterion: Loss function\n            optimizer_class: PyTorch optimizer class\n            optimizer_kwargs: Optimizer parameters\n            scheduler_class: Learning rate scheduler class\n            scheduler_kwargs: Scheduler parameters\n            device: Computation device\n            pattern_map: Pattern map containing pattern information\n            use_pre_augmented: Whether to use pre-augmented mode (disables pattern-based batch adjustments)\n            pattern_adaptation_start: Epoch to start pattern adaptation (only used if not pre-augmented)\n            pattern_adaptation_frequency: How often to adapt patterns (only used if not pre-augmented)\n            augmentation_strength: Strength of augmentation (0.0-1.0) (only used if not pre-augmented)\n            **kwargs: Additional parameters for the base trainer\n        ",
          "code_snippet": "    \"\"\"\n    \n    def __init__(\n        self,\n        model,\n        criterion,\n        optimizer_class,\n        optimizer_kwargs=None,\n        scheduler_class=None,\n        scheduler_kwargs=None,\n        device=None,\n        pattern_map=None,\n        use_pre_augmented=True,  # New flag for pre-augmented mode\n        pattern_adaptation_start=5,\n        pattern_adaptation_frequency=3,\n        augmentation_strength=0.5,\n        **kwargs\n    ):\n        \"\"\"\n        Initialize the adaptive trainer.\n        \n        Args:\n            model: PyTorch model to train\n            criterion: Loss function\n            optimizer_class: PyTorch optimizer class\n            optimizer_kwargs: Optimizer parameters\n            scheduler_class: Learning rate scheduler class\n            scheduler_kwargs: Scheduler parameters\n            device: Computation device\n            pattern_map: Pattern map containing pattern information\n            use_pre_augmented: Whether to use pre-augmented mode (disables pattern-based batch adjustments)\n            pattern_adaptation_start: Epoch to start pattern adaptation (only used if not pre-augmented)\n            pattern_adaptation_frequency: How often to adapt patterns (only used if not pre-augmented)\n            augmentation_strength: Strength of augmentation (0.0-1.0) (only used if not pre-augmented)\n            **kwargs: Additional parameters for the base trainer\n        \"\"\"\n        # Initialize with pattern-responsive optimizer\n        super().__init__(\n            model=model,\n            criterion=criterion,\n            optimizer_class=optimizer_class,\n            optimizer_kwargs=optimizer_kwargs,\n            scheduler_class=scheduler_class,\n            scheduler_kwargs=scheduler_kwargs,\n            device=device,\n            use_risk_aware=True,  # Always use risk-aware optimization\n            pattern_map=pattern_map,\n            **kwargs\n        )\n        \n        # Extract necessary parameters from kwargs to avoid duplicates\n        kwargs_copy = kwargs.copy()\n        total_epochs = kwargs_copy.pop('total_epochs', 50)\n        run_diagnostics = kwargs_copy.pop('run_diagnostics', True)\n        override_batch_min = kwargs_copy.pop('override_batch_min', None)\n        override_batch_max = kwargs_copy.pop('override_batch_max', None)\n        risk_aversion = kwargs_copy.pop('risk_aversion', 0.5)\n        exploration_rate = kwargs_copy.pop('exploration_rate', 0.0 if use_pre_augmented else 0.1)\n        \n        # Override the risk-aware optimizer with appropriate version based on mode\n        if use_pre_augmented:\n            # Use the pre-augment optimizer for pre-augmented training\n            from isekaizen.core.optimizer.pre_augment_optimizer import PreAugmentOptimizer\n            self.batch_optimizer = PreAugmentOptimizer(\n                model=model,\n                device=device,\n                pattern_map=pattern_map,\n                total_epochs=total_epochs,\n                run_diagnostics=run_diagnostics,\n                min_batch_size=override_batch_min,\n                max_batch_size=override_batch_max,\n                risk_aversion=risk_aversion,\n                **kwargs_copy\n            )\n            logger.info(\"Using PreAugmentOptimizer for pre-augmented training\")\n            logger.info(\"Pattern-based batch size adjustments disabled\")\n        else:\n            # Use pattern-responsive optimizer for dynamic adaptation\n            from isekaizen.core.optimizer.pattern_responsive_optimizer import PatternResponsiveOptimizer\n            self.batch_optimizer = PatternResponsiveOptimizer(\n                model=model,\n                device=device,\n                pattern_map=pattern_map,\n                pattern_adaptation_start=pattern_adaptation_start,\n                pattern_adaptation_frequency=pattern_adaptation_frequency,\n                augmentation_strength=augmentation_strength,\n                total_epochs=total_epochs,\n                run_diagnostics=run_diagnostics,\n                override_batch_min=override_batch_min,\n                override_batch_max=override_batch_max,\n                risk_aversion=risk_aversion,\n                exploration_rate=exploration_rate,\n                **kwargs_copy\n            )\n            logger.info(\"Using PatternResponsiveOptimizer for dynamic adaptation\")\n            logger.info(f\"Pattern-based batch size adjustments enabled with exploration rate {exploration_rate}\")\n        \n        # Track dataset adaptations\n        self.dataset_adaptations = []\n        \n        logger.info(\"Adaptive trainer initialized\")\n    \n    def train(\n        self,\n        train_dataset,"
        },
        "train": {
          "start_line": 126,
          "end_line": 286,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "train_dataset"
            },
            {
              "name": "val_dataset"
            },
            {
              "name": "epochs"
            },
            {
              "name": "early_stopping"
            },
            {
              "name": "patience"
            },
            {
              "name": "test_interval"
            },
            {
              "name": "checkpoint_interval"
            },
            {
              "name": "checkpoint_path"
            },
            {
              "name": "callbacks"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "float",
              "line": 164
            },
            {
              "name": "....append",
              "line": 168
            },
            {
              "name": "range",
              "line": 170
            },
            {
              "name": "logger.info",
              "line": 283
            },
            {
              "name": "len",
              "line": 168
            },
            {
              "name": "logger.info",
              "line": 171
            },
            {
              "name": "self.batch_optimizer.get_optimal_batch_size",
              "line": 199
            },
            {
              "name": "....append",
              "line": 200
            },
            {
              "name": "....append",
              "line": 203
            },
            {
              "name": "logger.info",
              "line": 206
            },
            {
              "name": "logger.info",
              "line": 209
            },
            {
              "name": "time.time",
              "line": 212
            },
            {
              "name": "self._train_epoch",
              "line": 213
            },
            {
              "name": "logger.info",
              "line": 217
            },
            {
              "name": "logger.info",
              "line": 218
            },
            {
              "name": "hasattr",
              "line": 221
            },
            {
              "name": "....append",
              "line": 228
            },
            {
              "name": "....append",
              "line": 229
            },
            {
              "name": "....append",
              "line": 230
            },
            {
              "name": "hasattr",
              "line": 174
            },
            {
              "name": "self.batch_optimizer.should_adapt_patterns",
              "line": 174
            },
            {
              "name": "logger.info",
              "line": 175
            },
            {
              "name": "self.batch_optimizer.adapt_dataset",
              "line": 176
            },
            {
              "name": "adaptation_metrics.get",
              "line": 178
            },
            {
              "name": "len",
              "line": 203
            },
            {
              "name": "hasattr",
              "line": 207
            },
            {
              "name": "hasattr",
              "line": 207
            },
            {
              "name": "logger.info",
              "line": 208
            },
            {
              "name": "time.time",
              "line": 214
            },
            {
              "name": "....format",
              "line": 217
            },
            {
              "name": "....format",
              "line": 218
            },
            {
              "name": "self.batch_optimizer.pattern_recognition_tracker.get_current_recognition_rates",
              "line": 222
            },
            {
              "name": "self._validate",
              "line": 234
            },
            {
              "name": "....append",
              "line": 235
            },
            {
              "name": "....append",
              "line": 236
            },
            {
              "name": "logger.info",
              "line": 238
            },
            {
              "name": "self.save_model",
              "line": 272
            },
            {
              "name": "self.scheduler.step",
              "line": 276
            },
            {
              "name": "self.dataset_adaptations.append",
              "line": 183
            },
            {
              "name": "logger.info",
              "line": 188
            },
            {
              "name": "logger.info",
              "line": 189
            },
            {
              "name": "logger.info",
              "line": 190
            },
            {
              "name": "logger.info",
              "line": 191
            },
            {
              "name": "logger.info",
              "line": 194
            },
            {
              "name": "logger.info",
              "line": 224
            },
            {
              "name": "recognition_rates.items",
              "line": 225
            },
            {
              "name": "....format",
              "line": 238
            },
            {
              "name": "len",
              "line": 241
            },
            {
              "name": "callback",
              "line": 281
            },
            {
              "name": "len",
              "line": 209
            },
            {
              "name": "logger.info",
              "line": 226
            },
            {
              "name": "logger.info",
              "line": 246
            },
            {
              "name": "logger.info",
              "line": 251
            },
            {
              "name": "logger.info",
              "line": 267
            },
            {
              "name": "....format",
              "line": 226
            },
            {
              "name": "....format",
              "line": 246
            },
            {
              "name": "logger.info",
              "line": 248
            },
            {
              "name": "....format",
              "line": 251
            },
            {
              "name": "logger.info",
              "line": 253
            },
            {
              "name": "self.save_model",
              "line": 262
            },
            {
              "name": "....format",
              "line": 267
            },
            {
              "name": "....join",
              "line": 190
            },
            {
              "name": "adaptation_metrics.get",
              "line": 194
            },
            {
              "name": "....format",
              "line": 248
            },
            {
              "name": "....format",
              "line": 253
            },
            {
              "name": "abs",
              "line": 248
            },
            {
              "name": "abs",
              "line": 253
            }
          ],
          "docstring": "\n        Train the model with dynamic dataset adaptation.\n        \n        Args:\n            train_dataset: Training dataset\n            val_dataset: Validation dataset\n            epochs: Number of epochs\n            early_stopping: Whether to use early stopping\n            patience: Early stopping patience\n            test_interval: Interval for validation\n            checkpoint_interval: Interval for saving checkpoints\n            checkpoint_path: Path to save checkpoints\n            callbacks: List of callback functions\n            \n        Returns:\n            Training history\n        ",
          "code_snippet": "        logger.info(\"Adaptive trainer initialized\")\n    \n    def train(\n        self,\n        train_dataset,\n        val_dataset=None,\n        epochs=10,\n        early_stopping=False,\n        patience=3,\n        test_interval=1,\n        checkpoint_interval=None,\n        checkpoint_path=None,\n        callbacks=None,\n        **kwargs  # Add kwargs to capture additional parameters\n    ):\n        \"\"\"\n        Train the model with dynamic dataset adaptation.\n        \n        Args:\n            train_dataset: Training dataset\n            val_dataset: Validation dataset\n            epochs: Number of epochs\n            early_stopping: Whether to use early stopping\n            patience: Early stopping patience\n            test_interval: Interval for validation\n            checkpoint_interval: Interval for saving checkpoints\n            checkpoint_path: Path to save checkpoints\n            callbacks: List of callback functions\n            \n        Returns:\n            Training history\n        \"\"\"\n        # Initialize variables\n        history = {\n            'train_loss': [], 'train_acc': [],\n            'val_loss': [], 'val_acc': [],\n            'batch_sizes': [], 'epoch_times': [],\n            'dataset_sizes': []\n        }\n        \n        best_val_loss = float('inf')\n        no_improve_count = 0\n        \n        # Initial dataset size\n        history['dataset_sizes'].append(len(train_dataset))\n        \n        for epoch in range(epochs):\n            logger.info(f\"Epoch {epoch+1}/{epochs}\")\n            \n            # Check if we should adapt the dataset\n            if hasattr(self.batch_optimizer, 'should_adapt_patterns') and self.batch_optimizer.should_adapt_patterns():\n                logger.info(\"Adapting dataset based on pattern responsiveness...\")\n                adapted_dataset, adaptation_metrics = self.batch_optimizer.adapt_dataset(train_dataset)\n                \n                if adaptation_metrics.get('adapted', False):\n                    # Use the adapted dataset for this epoch\n                    current_dataset = adapted_dataset\n                    \n                    # Record adaptation\n                    self.dataset_adaptations.append({\n                        'epoch': epoch + 1,\n                        **adaptation_metrics\n                    })\n                    \n                    logger.info(f\"Dataset adapted successfully:\")\n                    logger.info(f\"  Added {adaptation_metrics['examples_added']} new examples\")\n                    logger.info(f\"  Based on responsive patterns: {', '.join(adaptation_metrics['responsive_patterns'])}\")\n                    logger.info(f\"  New dataset size: {adaptation_metrics['total_size']} examples\")\n                else:\n                    current_dataset = train_dataset\n                    logger.info(f\"Dataset adaptation skipped: {adaptation_metrics.get('reason', 'unknown')}\")\n            else:\n                current_dataset = train_dataset\n            \n            # Get optimal batch size\n            batch_size = self.batch_optimizer.get_optimal_batch_size()\n            history['batch_sizes'].append(batch_size)\n            \n            # Track dataset size\n            history['dataset_sizes'].append(len(current_dataset))\n            \n            # Print more detailed epoch information\n            logger.info(f\"Starting epoch {epoch+1}/{epochs} with batch size {batch_size}\")\n            if hasattr(self.batch_optimizer, 'min_batch') and hasattr(self.batch_optimizer, 'max_batch'):\n                logger.info(f\"  Batch size range: [{self.batch_optimizer.min_batch}, {self.batch_optimizer.max_batch}]\")\n            logger.info(f\"  Dataset size: {len(current_dataset)} examples\")\n            \n            # Train for one epoch\n            start_time = time.time()\n            train_loss, train_acc = self._train_epoch(current_dataset, batch_size)\n            epoch_time = time.time() - start_time\n            \n            # Print detailed completion information\n            logger.info(\"Epoch {}/{} completed in {:.2f} seconds\".format(epoch+1, epochs, epoch_time))\n            logger.info(\"  Training - Loss: {:.4f}, Accuracy: {:.2f}%\".format(train_loss, train_acc))\n            \n            # Pattern statistics if available\n            if hasattr(self.batch_optimizer, 'pattern_recognition_tracker'):\n                recognition_rates = self.batch_optimizer.pattern_recognition_tracker.get_current_recognition_rates()\n                if recognition_rates:\n                    logger.info(\"  Pattern recognition rates:\")\n                    for pattern_type, rate in recognition_rates.items():\n                        logger.info(\"    {}: {:.2f}\".format(pattern_type, rate))\n            \n            history['train_loss'].append(train_loss)\n            history['train_acc'].append(train_acc)\n            history['epoch_times'].append(epoch_time)\n            \n            # Validate if a validation set is provided\n            if val_dataset is not None and (epoch + 1) % test_interval == 0:\n                val_loss, val_acc = self._validate(val_dataset)\n                history['val_loss'].append(val_loss)\n                history['val_acc'].append(val_acc)\n                \n                logger.info(\"  Validation - Loss: {:.4f}, Accuracy: {:.2f}%\".format(val_loss, val_acc))\n                \n                # Display improvement/decline compared to previous validation\n                if len(history['val_acc']) > 1:\n                    acc_diff = val_acc - history['val_acc'][-2]\n                    loss_diff = history['val_loss'][-2] - val_loss  # Loss should decrease\n                    \n                    if acc_diff > 0:\n                        logger.info(\"  Validation accuracy improved by {:.2f}%\".format(acc_diff))\n                    elif acc_diff < 0:\n                        logger.info(\"  Validation accuracy decreased by {:.2f}%\".format(abs(acc_diff)))\n                    \n                    if loss_diff > 0:\n                        logger.info(\"  Validation loss improved by {:.4f}\".format(loss_diff))\n                    elif loss_diff < 0:\n                        logger.info(\"  Validation loss worsened by {:.4f}\".format(abs(loss_diff)))\n                \n                # Check for early stopping\n                if early_stopping:\n                    if val_loss < best_val_loss:\n                        best_val_loss = val_loss\n                        no_improve_count = 0\n                        # Save best model\n                        if checkpoint_path:\n                            self.save_model(f\"{checkpoint_path}_best.pth\")\n                    else:\n                        no_improve_count += 1\n                        \n                    if no_improve_count >= patience:\n                        logger.info(\"Early stopping triggered after {} epochs\".format(epoch+1))\n                        break\n            \n            # Save checkpoint if interval is specified\n            if checkpoint_interval and (epoch + 1) % checkpoint_interval == 0 and checkpoint_path:\n                self.save_model(f\"{checkpoint_path}_epoch{epoch+1}.pth\")\n            \n            # Step the scheduler if it exists\n            if self.scheduler:\n                self.scheduler.step()\n            \n            # Execute callbacks if provided\n            if callbacks:\n                for callback in callbacks:\n                    callback(epoch, history, self.model, self.optimizer)\n        \n        logger.info(\"Training complete\")\n        return history\n    \n    def _train_epoch(self, dataset, batch_size):\n        \"\"\"\n        Train for one epoch."
        },
        "_train_epoch": {
          "start_line": 286,
          "end_line": 332,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "dataset"
            },
            {
              "name": "batch_size"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.model.train",
              "line": 297
            },
            {
              "name": "torch.utils.data.DataLoader",
              "line": 303
            },
            {
              "name": "self.optimizer.zero_grad",
              "line": 310
            },
            {
              "name": "self.model",
              "line": 313
            },
            {
              "name": "self.criterion",
              "line": 314
            },
            {
              "name": "loss.backward",
              "line": 317
            },
            {
              "name": "self.optimizer.step",
              "line": 318
            },
            {
              "name": "loss.item",
              "line": 321
            },
            {
              "name": "outputs.max",
              "line": 322
            },
            {
              "name": "targets.size",
              "line": 323
            },
            {
              "name": "....item",
              "line": 324
            },
            {
              "name": "len",
              "line": 327
            },
            {
              "name": "inputs.to",
              "line": 307
            },
            {
              "name": "targets.to",
              "line": 307
            },
            {
              "name": "....sum",
              "line": 324
            },
            {
              "name": "predicted.eq",
              "line": 324
            }
          ],
          "docstring": "\n        Train for one epoch.\n        \n        Args:\n            dataset: Dataset to train on\n            batch_size: Batch size\n            \n        Returns:\n            Tuple of (loss, accuracy)\n        ",
          "code_snippet": "        return history\n    \n    def _train_epoch(self, dataset, batch_size):\n        \"\"\"\n        Train for one epoch.\n        \n        Args:\n            dataset: Dataset to train on\n            batch_size: Batch size\n            \n        Returns:\n            Tuple of (loss, accuracy)\n        \"\"\"\n        self.model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        # Create data loader\n        dataloader = torch.utils.data.DataLoader(\n            dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        \n        for inputs, targets in dataloader:\n            inputs, targets = inputs.to(self.device), targets.to(self.device)\n            \n            # Zero the parameter gradients\n            self.optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = self.model(inputs)\n            loss = self.criterion(outputs, targets)\n            \n            # Backward pass and optimize\n            loss.backward()\n            self.optimizer.step()\n            \n            # Update metrics\n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n        \n        # Calculate epoch metrics\n        epoch_loss = running_loss / len(dataloader)\n        epoch_acc = 100. * correct / total\n        \n        return epoch_loss, epoch_acc"
        }
      },
      "class_variables": [],
      "bases": [
        "ModelTrainer"
      ],
      "docstring": "\n    Adaptive trainer with pattern-responsive capabilities.\n    \n    This trainer extends the base model trainer with dynamic dataset adaptation\n    based on pattern responsiveness.\n    "
    }
  },
  "functions": {},
  "constants": {}
}