{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\isekaizen\\utils\\memory.py",
  "imports": [
    {
      "name": "torch",
      "line": 18
    },
    {
      "name": "math",
      "line": 19
    },
    {
      "name": "pandas",
      "line": 20
    },
    {
      "name": "typing.Dict",
      "line": 21
    },
    {
      "name": "typing.List",
      "line": 21
    },
    {
      "name": "typing.Tuple",
      "line": 21
    },
    {
      "name": "typing.Optional",
      "line": 21
    },
    {
      "name": "typing.Callable",
      "line": 21
    },
    {
      "name": "logging",
      "line": 22
    },
    {
      "name": "time",
      "line": 23
    },
    {
      "name": "matplotlib.pyplot",
      "line": 151
    }
  ],
  "classes": {
    "MemoryTracker": {
      "start_line": 27,
      "end_line": 198,
      "methods": {
        "__init__": {
          "start_line": 32,
          "end_line": 55,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "device"
            },
            {
              "name": "log_level",
              "type": "int"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logging.getLogger",
              "line": 48
            },
            {
              "name": "self.logger.setLevel",
              "line": 49
            },
            {
              "name": "self.logger.warning",
              "line": 53
            },
            {
              "name": "torch.cuda.is_available",
              "line": 44
            },
            {
              "name": "torch.device",
              "line": 44
            },
            {
              "name": "torch.device",
              "line": 44
            }
          ],
          "docstring": "\n        Initialize memory tracker.\n        \n        Args:\n            device: Device to track memory for\n            log_level: Logging level\n        ",
          "code_snippet": "    \"\"\"\n    \n    def __init__(\n        self, \n        device: Optional[torch.device] = None,\n        log_level: int = logging.INFO,\n    ):\n        \"\"\"\n        Initialize memory tracker.\n        \n        Args:\n            device: Device to track memory for\n            log_level: Logging level\n        \"\"\"\n        self.device = device or (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n        self.memory_log = []\n        \n        # Set up logging\n        self.logger = logging.getLogger(f\"{__name__}.MemoryTracker\")\n        self.logger.setLevel(log_level)\n        \n        # Verify device support\n        if self.device.type != \"cuda\":\n            self.logger.warning(\"Memory tracking is only available for CUDA devices\")\n    \n    def reset(self):\n        \"\"\"Reset memory tracking\"\"\"\n        self.memory_log = []"
        },
        "reset": {
          "start_line": 55,
          "end_line": 61,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "torch.cuda.reset_peak_memory_stats",
              "line": 59
            }
          ],
          "docstring": "Reset memory tracking",
          "code_snippet": "            self.logger.warning(\"Memory tracking is only available for CUDA devices\")\n    \n    def reset(self):\n        \"\"\"Reset memory tracking\"\"\"\n        self.memory_log = []\n        if self.device.type == \"cuda\":\n            torch.cuda.reset_peak_memory_stats(self.device)\n    \n    def snapshot(self, label: str = \"\") -> Dict[str, float]:\n        \"\"\"\n        Take a snapshot of current memory usage."
        },
        "snapshot": {
          "start_line": 61,
          "end_line": 100,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "label",
              "type": "str"
            }
          ],
          "return_type": "complex_type",
          "calls": [
            {
              "name": "self.memory_log.append",
              "line": 95
            },
            {
              "name": "self.logger.debug",
              "line": 96
            },
            {
              "name": "self.memory_log.append",
              "line": 79
            },
            {
              "name": "torch.cuda.memory_allocated",
              "line": 83
            },
            {
              "name": "torch.cuda.memory_reserved",
              "line": 84
            },
            {
              "name": "torch.cuda.max_memory_allocated",
              "line": 85
            },
            {
              "name": "time.time",
              "line": 89
            },
            {
              "name": "time.time",
              "line": 74
            }
          ],
          "docstring": "\n        Take a snapshot of current memory usage.\n        \n        Args:\n            label: Label for this snapshot\n            \n        Returns:\n            Dictionary with memory statistics\n        ",
          "code_snippet": "            torch.cuda.reset_peak_memory_stats(self.device)\n    \n    def snapshot(self, label: str = \"\") -> Dict[str, float]:\n        \"\"\"\n        Take a snapshot of current memory usage.\n        \n        Args:\n            label: Label for this snapshot\n            \n        Returns:\n            Dictionary with memory statistics\n        \"\"\"\n        if self.device.type != \"cuda\":\n            snapshot = {\n                \"label\": label,\n                \"timestamp\": time.time(),\n                \"allocated_mb\": 0,\n                \"reserved_mb\": 0,\n                \"peak_allocated_mb\": 0,\n            }\n            self.memory_log.append(snapshot)\n            return snapshot\n        \n        # Get memory stats\n        allocated = torch.cuda.memory_allocated(self.device) / (1024 ** 2)  # MB\n        reserved = torch.cuda.memory_reserved(self.device) / (1024 ** 2)  # MB\n        peak = torch.cuda.max_memory_allocated(self.device) / (1024 ** 2)  # MB\n        \n        snapshot = {\n            \"label\": label,\n            \"timestamp\": time.time(),\n            \"allocated_mb\": allocated,\n            \"reserved_mb\": reserved,\n            \"peak_allocated_mb\": peak,\n        }\n        \n        self.memory_log.append(snapshot)\n        self.logger.debug(f\"Memory snapshot '{label}': {allocated:.2f} MB allocated, {peak:.2f} MB peak\")\n        \n        return snapshot\n    \n    def track_function(\n        self, \n        func: Callable, "
        },
        "track_function": {
          "start_line": 100,
          "end_line": 130,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "func",
              "type": "Callable"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.snapshot",
              "line": 120
            },
            {
              "name": "func",
              "line": 123
            },
            {
              "name": "self.snapshot",
              "line": 126
            }
          ],
          "docstring": "\n        Track memory usage before, during, and after a function call.\n        \n        Args:\n            func: Function to track\n            *args: Arguments to pass to function\n            label: Base label for tracking points\n            **kwargs: Keyword arguments to pass to function\n            \n        Returns:\n            Result of the function\n        ",
          "code_snippet": "        return snapshot\n    \n    def track_function(\n        self, \n        func: Callable, \n        *args, \n        label: str = \"\",\n        **kwargs\n    ):\n        \"\"\"\n        Track memory usage before, during, and after a function call.\n        \n        Args:\n            func: Function to track\n            *args: Arguments to pass to function\n            label: Base label for tracking points\n            **kwargs: Keyword arguments to pass to function\n            \n        Returns:\n            Result of the function\n        \"\"\"\n        # Take pre-execution snapshot\n        self.snapshot(f\"{label}_before\")\n        \n        # Execute function\n        result = func(*args, **kwargs)\n        \n        # Take post-execution snapshot\n        self.snapshot(f\"{label}_after\")\n        \n        return result\n    \n    def get_log(self) -> pd.DataFrame:\n        \"\"\"\n        Get memory log as DataFrame."
        },
        "get_log": {
          "start_line": 130,
          "end_line": 139,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "pd.DataFrame",
              "line": 137
            }
          ],
          "docstring": "\n        Get memory log as DataFrame.\n        \n        Returns:\n            DataFrame with memory usage history\n        ",
          "code_snippet": "        return result\n    \n    def get_log(self) -> pd.DataFrame:\n        \"\"\"\n        Get memory log as DataFrame.\n        \n        Returns:\n            DataFrame with memory usage history\n        \"\"\"\n        return pd.DataFrame(self.memory_log)\n    \n    def plot_memory_usage(\n        self, \n        figsize: Tuple[int, int] = (12, 6),"
        },
        "plot_memory_usage": {
          "start_line": 139,
          "end_line": 198,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "figsize"
            },
            {
              "name": "save_path"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.get_log",
              "line": 157
            },
            {
              "name": "....min",
              "line": 160
            },
            {
              "name": "plt.figure",
              "line": 164
            },
            {
              "name": "plt.plot",
              "line": 167
            },
            {
              "name": "plt.plot",
              "line": 170
            },
            {
              "name": "plt.plot",
              "line": 173
            },
            {
              "name": "df.iterrows",
              "line": 176
            },
            {
              "name": "plt.xlabel",
              "line": 187
            },
            {
              "name": "plt.ylabel",
              "line": 188
            },
            {
              "name": "plt.title",
              "line": 189
            },
            {
              "name": "plt.grid",
              "line": 190
            },
            {
              "name": "plt.legend",
              "line": 191
            },
            {
              "name": "self.logger.warning",
              "line": 154
            },
            {
              "name": "plt.savefig",
              "line": 194
            },
            {
              "name": "plt.show",
              "line": 196
            },
            {
              "name": "plt.annotate",
              "line": 178
            }
          ],
          "docstring": "\n        Plot memory usage over time.\n        \n        Args:\n            figsize: Figure size\n            save_path: Path to save plot\n        ",
          "code_snippet": "        return pd.DataFrame(self.memory_log)\n    \n    def plot_memory_usage(\n        self, \n        figsize: Tuple[int, int] = (12, 6),\n        save_path: Optional[str] = None,\n    ):\n        \"\"\"\n        Plot memory usage over time.\n        \n        Args:\n            figsize: Figure size\n            save_path: Path to save plot\n        \"\"\"\n        import matplotlib.pyplot as plt\n        \n        if not self.memory_log:\n            self.logger.warning(\"No memory data to plot\")\n            return\n        \n        df = self.get_log()\n        \n        # Convert timestamps to relative time\n        start_time = df[\"timestamp\"].min()\n        df[\"relative_time\"] = df[\"timestamp\"] - start_time\n        \n        # Create plot\n        plt.figure(figsize=figsize)\n        \n        # Plot allocated memory\n        plt.plot(df[\"relative_time\"], df[\"allocated_mb\"], marker=\"o\", label=\"Allocated\")\n        \n        # Plot reserved memory\n        plt.plot(df[\"relative_time\"], df[\"reserved_mb\"], marker=\"s\", label=\"Reserved\")\n        \n        # Plot peak allocated memory\n        plt.plot(df[\"relative_time\"], df[\"peak_allocated_mb\"], marker=\"^\", label=\"Peak\")\n        \n        # Add labels\n        for i, row in df.iterrows():\n            if row[\"label\"]:\n                plt.annotate(\n                    row[\"label\"],\n                    (row[\"relative_time\"], row[\"allocated_mb\"]),\n                    textcoords=\"offset points\",\n                    xytext=(0, 10),\n                    ha=\"center\",\n                )\n        \n        # Add labels and title\n        plt.xlabel(\"Time (seconds)\")\n        plt.ylabel(\"Memory (MB)\")\n        plt.title(\"GPU Memory Usage Over Time\")\n        plt.grid(True)\n        plt.legend()\n        \n        if save_path:\n            plt.savefig(save_path)\n        else:\n            plt.show()"
        }
      },
      "class_variables": [],
      "bases": [],
      "docstring": "\n    Track and analyze memory usage for PyTorch models.\n    "
    }
  },
  "functions": {},
  "constants": {}
}