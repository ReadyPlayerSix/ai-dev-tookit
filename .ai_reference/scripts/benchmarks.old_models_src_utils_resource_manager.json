{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\benchmarks.old\\models\\src\\utils\\resource_manager.py",
  "imports": [
    {
      "name": "typing.Dict",
      "line": 5
    },
    {
      "name": "typing.Set",
      "line": 5
    },
    {
      "name": "typing.Any",
      "line": 5
    },
    {
      "name": "torch",
      "line": 6
    }
  ],
  "classes": {
    "ResourceManager": {
      "start_line": 8,
      "end_line": 79,
      "methods": {
        "__init__": {
          "start_line": 10,
          "end_line": 29,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "torch.device",
              "line": 11
            },
            {
              "name": "set",
              "line": 20
            },
            {
              "name": "torch.cuda.is_available",
              "line": 11
            }
          ],
          "code_snippet": "class ResourceManager:\n    \"\"\"Basic resource manager for specialists\"\"\"\n    def __init__(self):\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.resource_limits = {\n            \"gpu_memory\": 4089,  # MB for RTX 4070 SUPER\n            \"target_inference\": 7.0,  # ms\n        }\n        \n        self.resource_usage = {\n            \"gpu_memory_used\": 0,\n            \"current_inference_time\": 0,\n            \"active_specialists\": set()\n        }\n        \n        self.performance_metrics = {\n            \"peak_memory_usage\": 0,\n            \"average_inference_time\": 0,\n            \"total_allocations\": 0,\n            \"allocation_failures\": 0\n        }\n\n    def allocate_resources(self, specialist_id: str, required_memory: int) -> bool:\n        \"\"\"Basic resource allocation for specialists\"\"\""
        },
        "allocate_resources": {
          "start_line": 30,
          "end_line": 50,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "specialist_id",
              "type": "str"
            },
            {
              "name": "required_memory",
              "type": "int"
            }
          ],
          "return_type": "bool",
          "calls": [
            {
              "name": "....add",
              "line": 38
            },
            {
              "name": "max",
              "line": 40
            }
          ],
          "docstring": "Basic resource allocation for specialists",
          "code_snippet": "        }\n\n    def allocate_resources(self, specialist_id: str, required_memory: int) -> bool:\n        \"\"\"Basic resource allocation for specialists\"\"\"\n        self.performance_metrics[\"total_allocations\"] += 1\n        \n        if (self.resource_usage[\"gpu_memory_used\"] + required_memory <= \n            self.resource_limits[\"gpu_memory\"]):\n            \n            self.resource_usage[\"gpu_memory_used\"] += required_memory\n            self.resource_usage[\"active_specialists\"].add(specialist_id)\n            \n            self.performance_metrics[\"peak_memory_usage\"] = max(\n                self.performance_metrics[\"peak_memory_usage\"],\n                self.resource_usage[\"gpu_memory_used\"]\n            )\n            \n            return True\n        \n        self.performance_metrics[\"allocation_failures\"] += 1\n        return False\n\n    def release_resources(self, specialist_id: str, released_memory: int):\n        \"\"\"Release resources for specialists\"\"\"\n        if specialist_id in self.resource_usage[\"active_specialists\"]:"
        },
        "release_resources": {
          "start_line": 50,
          "end_line": 59,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "specialist_id",
              "type": "str"
            },
            {
              "name": "released_memory",
              "type": "int"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "max",
              "line": 53
            },
            {
              "name": "....remove",
              "line": 57
            }
          ],
          "docstring": "Release resources for specialists",
          "code_snippet": "        return False\n\n    def release_resources(self, specialist_id: str, released_memory: int):\n        \"\"\"Release resources for specialists\"\"\"\n        if specialist_id in self.resource_usage[\"active_specialists\"]:\n            self.resource_usage[\"gpu_memory_used\"] = max(\n                0,\n                self.resource_usage[\"gpu_memory_used\"] - released_memory\n            )\n            self.resource_usage[\"active_specialists\"].remove(specialist_id)\n\n    def get_available_memory(self) -> int:\n        \"\"\"Get available GPU memory\"\"\"\n        return self.resource_limits[\"gpu_memory\"] - self.resource_usage[\"gpu_memory_used\"]"
        },
        "get_available_memory": {
          "start_line": 59,
          "end_line": 63,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": "int",
          "calls": [],
          "docstring": "Get available GPU memory",
          "code_snippet": "            self.resource_usage[\"active_specialists\"].remove(specialist_id)\n\n    def get_available_memory(self) -> int:\n        \"\"\"Get available GPU memory\"\"\"\n        return self.resource_limits[\"gpu_memory\"] - self.resource_usage[\"gpu_memory_used\"]\n\n    def get_status(self) -> Dict[str, Any]:\n        \"\"\"Get basic resource status\"\"\"\n        memory_stats = {"
        },
        "get_status": {
          "start_line": 63,
          "end_line": 79,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": "complex_type",
          "calls": [
            {
              "name": "self.get_available_memory",
              "line": 75
            },
            {
              "name": "str",
              "line": 77
            }
          ],
          "docstring": "Get basic resource status",
          "code_snippet": "        return self.resource_limits[\"gpu_memory\"] - self.resource_usage[\"gpu_memory_used\"]\n\n    def get_status(self) -> Dict[str, Any]:\n        \"\"\"Get basic resource status\"\"\"\n        memory_stats = {\n            \"allocated_mb\": self.resource_usage[\"gpu_memory_used\"],\n            \"reserved_mb\": self.resource_usage[\"gpu_memory_used\"],\n            \"max_allocated_mb\": self.performance_metrics[\"peak_memory_usage\"],\n            \"utilization\": self.resource_usage[\"gpu_memory_used\"] / self.resource_limits[\"gpu_memory\"]\n        }\n        \n        return {\n            \"resource_usage\": self.resource_usage,\n            \"performance_metrics\": self.performance_metrics,\n            \"available_memory\": self.get_available_memory(),\n            \"memory_stats\": memory_stats,\n            \"device\": str(self.device)\n        }"
        }
      },
      "class_variables": [],
      "bases": [],
      "docstring": "Basic resource manager for specialists"
    }
  },
  "functions": {},
  "constants": {}
}