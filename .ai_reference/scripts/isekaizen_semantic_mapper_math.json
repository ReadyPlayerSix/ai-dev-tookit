{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\isekaizen\\semantic\\mapper_math.py",
  "imports": [
    {
      "name": "os",
      "line": 11
    },
    {
      "name": "time",
      "line": 12
    },
    {
      "name": "json",
      "line": 13
    },
    {
      "name": "random",
      "line": 14
    },
    {
      "name": "logging",
      "line": 15
    },
    {
      "name": "numpy",
      "line": 16
    },
    {
      "name": "multiprocessing",
      "line": 17
    },
    {
      "name": "psutil",
      "line": 18
    },
    {
      "name": "joblib.Parallel",
      "line": 19
    },
    {
      "name": "joblib.delayed",
      "line": 19
    },
    {
      "name": "matrix_ops.calculate_euclidean_distance",
      "line": 26
    },
    {
      "name": "matrix_ops.find_k_nearest_neighbors",
      "line": 26
    },
    {
      "name": "matrix_ops.batch_find_neighbors",
      "line": 26
    },
    {
      "name": "matrix_ops.softmax",
      "line": 26
    },
    {
      "name": "matrix_ops.argmax",
      "line": 26
    },
    {
      "name": "matrix_ops.bincount",
      "line": 26
    },
    {
      "name": "matrix_ops.mean",
      "line": 26
    },
    {
      "name": "matrix_ops.std_dev",
      "line": 26
    },
    {
      "name": "torch",
      "line": 36
    },
    {
      "name": "torch.nn",
      "line": 37
    },
    {
      "name": "torch.utils.data.DataLoader",
      "line": 38
    },
    {
      "name": "torch.utils.data.Subset",
      "line": 38
    },
    {
      "name": "sklearn.cluster.KMeans",
      "line": 39
    },
    {
      "name": "sklearn.ensemble.RandomForestClassifier",
      "line": 40
    },
    {
      "name": "sklearn.linear_model.LogisticRegression",
      "line": 41
    },
    {
      "name": "sklearn.svm.LinearSVC",
      "line": 42
    },
    {
      "name": "collections.defaultdict",
      "line": 43
    },
    {
      "name": "topology.TopographicalPredictor",
      "line": 22
    },
    {
      "name": "joblib.Parallel",
      "line": 425
    },
    {
      "name": "joblib.delayed",
      "line": 425
    },
    {
      "name": "sklearn.neighbors.KDTree",
      "line": 596
    },
    {
      "name": "numpy",
      "line": 597
    },
    {
      "name": "matplotlib.pyplot",
      "line": 279
    }
  ],
  "classes": {
    "SemanticTopographicalMapper": {
      "start_line": 47,
      "end_line": 857,
      "methods": {
        "__init__": {
          "start_line": 54,
          "end_line": 97,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "proxy_model_types"
            },
            {
              "name": "ensemble_size"
            },
            {
              "name": "n_neighbors"
            },
            {
              "name": "difficulty_levels"
            },
            {
              "name": "batch_size"
            },
            {
              "name": "num_workers"
            },
            {
              "name": "sample_limit"
            },
            {
              "name": "device"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self._detect_system_resources",
              "line": 77
            },
            {
              "name": "logger.info",
              "line": 90
            },
            {
              "name": "logger.info",
              "line": 91
            },
            {
              "name": "logger.info",
              "line": 92
            },
            {
              "name": "logger.info",
              "line": 93
            },
            {
              "name": "logger.info",
              "line": 94
            },
            {
              "name": "logger.info",
              "line": 95
            },
            {
              "name": "torch.device",
              "line": 87
            },
            {
              "name": "torch.cuda.is_available",
              "line": 87
            }
          ],
          "docstring": "\n        Initialize the mapper with configuration for difficulty assessment.\n        \n        Args:\n            proxy_model_types: List of simple model types to use for ensemble disagreement\n            ensemble_size: Number of models in ensemble (None = auto-detect)\n            n_neighbors: Number of neighbors for local topology analysis (None = auto-detect)\n            difficulty_levels: Number of difficulty categories to create\n            batch_size: Batch size for data processing (None = auto-detect)\n            num_workers: Number of worker processes for data loading (None = auto-detect)\n            sample_limit: Maximum number of samples to analyze (None = no limit)\n            device: Device to use for PyTorch operations (defaults to CUDA if available)\n        ",
          "code_snippet": "    \"\"\"\n    \n    def __init__(self, \n                 proxy_model_types=None,\n                 ensemble_size=None,\n                 n_neighbors=None,\n                 difficulty_levels=5,\n                 batch_size=None,\n                 num_workers=None,\n                 sample_limit=None,\n                 device=None):\n        \"\"\"\n        Initialize the mapper with configuration for difficulty assessment.\n        \n        Args:\n            proxy_model_types: List of simple model types to use for ensemble disagreement\n            ensemble_size: Number of models in ensemble (None = auto-detect)\n            n_neighbors: Number of neighbors for local topology analysis (None = auto-detect)\n            difficulty_levels: Number of difficulty categories to create\n            batch_size: Batch size for data processing (None = auto-detect)\n            num_workers: Number of worker processes for data loading (None = auto-detect)\n            sample_limit: Maximum number of samples to analyze (None = no limit)\n            device: Device to use for PyTorch operations (defaults to CUDA if available)\n        \"\"\"\n        # Auto-detect optimal parameters based on system resources\n        self._detect_system_resources()\n        \n        # Set parameters, using auto-detected values for None\n        self.proxy_model_types = proxy_model_types or ['forest', 'logistic', 'svm']\n        self.ensemble_size = ensemble_size or self._optimal_ensemble_size\n        self.n_neighbors = n_neighbors or self._optimal_neighbors\n        self.difficulty_levels = difficulty_levels\n        self.batch_size = batch_size or self._optimal_batch_size\n        self.num_workers = num_workers or self._optimal_workers\n        self.sample_limit = sample_limit  # Can be None for no limit\n        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.map = None\n        \n        logger.info(f\"SemanticTopographicalMapper initialized with:\")\n        logger.info(f\"  - CPU cores used: {self.num_workers} of {self._cpu_count} available\")\n        logger.info(f\"  - RAM available: {self._ram_gb:.1f} GB\")\n        logger.info(f\"  - Batch size: {self.batch_size}\")\n        logger.info(f\"  - Ensemble size: {self.ensemble_size}\")\n        logger.info(f\"  - Neighbors for topology: {self.n_neighbors}\")\n        \n    def _detect_system_resources(self):\n        \"\"\"\n        Detect system resources and determine optimal parameters."
        },
        "_detect_system_resources": {
          "start_line": 97,
          "end_line": 121,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "multiprocessing.cpu_count",
              "line": 102
            },
            {
              "name": "max",
              "line": 104
            },
            {
              "name": "psutil.virtual_memory",
              "line": 107
            },
            {
              "name": "min",
              "line": 113
            },
            {
              "name": "min",
              "line": 116
            },
            {
              "name": "min",
              "line": 119
            },
            {
              "name": "max",
              "line": 113
            },
            {
              "name": "max",
              "line": 116
            },
            {
              "name": "max",
              "line": 119
            },
            {
              "name": "int",
              "line": 113
            },
            {
              "name": "int",
              "line": 119
            }
          ],
          "docstring": "\n        Detect system resources and determine optimal parameters.\n        ",
          "code_snippet": "        logger.info(f\"  - Neighbors for topology: {self.n_neighbors}\")\n        \n    def _detect_system_resources(self):\n        \"\"\"\n        Detect system resources and determine optimal parameters.\n        \"\"\"\n        # Detect available CPU cores\n        self._cpu_count = multiprocessing.cpu_count()\n        # Leave some cores free for system processes\n        self._optimal_workers = max(1, self._cpu_count - 2)\n        \n        # Detect available RAM\n        mem = psutil.virtual_memory()\n        self._ram_gb = mem.total / (1024**3)  # Convert to GB\n        self._ram_available_gb = mem.available / (1024**3)  # Available RAM in GB\n        \n        # Optimal batch size based on available RAM\n        # Baseline: 128 batch size for 8GB RAM, scale up for more RAM\n        self._optimal_batch_size = min(2048, max(128, int(128 * (self._ram_available_gb / 8))))\n        \n        # Optimal ensemble size based on CPU count\n        self._optimal_ensemble_size = min(16, max(5, self._cpu_count // 2))\n        \n        # Optimal neighbors based on RAM (more neighbors = more memory usage)\n        self._optimal_neighbors = min(50, max(10, int(10 * (self._ram_available_gb / 8))))\n        \n    def create_map(self, dataset, testset=None):\n        \"\"\"\n        Analyze dataset and create the semantic difficulty map."
        },
        "create_map": {
          "start_line": 121,
          "end_line": 202,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "dataset"
            },
            {
              "name": "testset"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logger.info",
              "line": 132
            },
            {
              "name": "time.time",
              "line": 133
            },
            {
              "name": "list",
              "line": 137
            },
            {
              "name": "self._extract_features_labels",
              "line": 140
            },
            {
              "name": "logger.info",
              "line": 142
            },
            {
              "name": "self._train_proxy_ensemble",
              "line": 145
            },
            {
              "name": "logger.info",
              "line": 148
            },
            {
              "name": "self._get_ensemble_predictions",
              "line": 149
            },
            {
              "name": "logger.info",
              "line": 152
            },
            {
              "name": "self._calculate_disagreement",
              "line": 153
            },
            {
              "name": "logger.info",
              "line": 156
            },
            {
              "name": "self._analyze_local_topology",
              "line": 157
            },
            {
              "name": "logger.info",
              "line": 160
            },
            {
              "name": "self._calculate_information_measures",
              "line": 161
            },
            {
              "name": "logger.info",
              "line": 164
            },
            {
              "name": "self._combine_difficulty_scores",
              "line": 165
            },
            {
              "name": "logger.info",
              "line": 169
            },
            {
              "name": "self._cluster_examples",
              "line": 170
            },
            {
              "name": "time.time",
              "line": 194
            },
            {
              "name": "logger.info",
              "line": 197
            },
            {
              "name": "logger.info",
              "line": 198
            },
            {
              "name": "range",
              "line": 137
            },
            {
              "name": "str",
              "line": 173
            },
            {
              "name": "int",
              "line": 173
            },
            {
              "name": "self._validate_map",
              "line": 191
            },
            {
              "name": "len",
              "line": 137
            },
            {
              "name": "enumerate",
              "line": 174
            },
            {
              "name": "len",
              "line": 180
            },
            {
              "name": "len",
              "line": 181
            },
            {
              "name": "int",
              "line": 182
            },
            {
              "name": "len",
              "line": 142
            },
            {
              "name": "time.time",
              "line": 182
            },
            {
              "name": "self._summarize_difficulty_distribution",
              "line": 198
            }
          ],
          "docstring": "\n        Analyze dataset and create the semantic difficulty map.\n        \n        Args:\n            dataset: Training dataset to analyze\n            testset: Optional test dataset for validation\n            \n        Returns:\n            Semantic difficulty map\n        ",
          "code_snippet": "        self._optimal_neighbors = min(50, max(10, int(10 * (self._ram_available_gb / 8))))\n        \n    def create_map(self, dataset, testset=None):\n        \"\"\"\n        Analyze dataset and create the semantic difficulty map.\n        \n        Args:\n            dataset: Training dataset to analyze\n            testset: Optional test dataset for validation\n            \n        Returns:\n            Semantic difficulty map\n        \"\"\"\n        logger.info(\"Starting semantic topographical mapping...\")\n        start_time = time.time()\n        \n        # Use the full dataset (sample_limit is removed)\n        dataset_subset = dataset\n        indices = list(range(len(dataset)))\n        \n        # Extract features and labels\n        features, labels = self._extract_features_labels(dataset_subset)\n        \n        logger.info(f\"Extracted features from {len(features)} samples\")\n        \n        # 1. Train ensemble of proxy models\n        ensemble = self._train_proxy_ensemble(features, labels)\n        \n        # 2. Get predictions from all models\n        logger.info(\"Getting ensemble predictions...\")\n        predictions = self._get_ensemble_predictions(ensemble, features)\n        \n        # 3. Calculate disagreement scores\n        logger.info(\"Calculating disagreement scores...\")\n        disagreement_scores = self._calculate_disagreement(predictions, labels)\n        \n        # 4. Analyze local topology\n        logger.info(\"Analyzing local topology...\")\n        topology_scores = self._analyze_local_topology(features, labels)\n        \n        # 5. Calculate information-theoretic measures\n        logger.info(\"Calculating information-theoretic measures...\")\n        information_scores = self._calculate_information_measures(predictions, labels)\n        \n        # 6. Combine scores into final difficulty rating\n        logger.info(\"Combining difficulty scores...\")\n        difficulty_ratings = self._combine_difficulty_scores(\n            disagreement_scores, topology_scores, information_scores)\n        \n        # 7. Cluster examples by feature similarity and difficulty\n        logger.info(\"Clustering examples...\")\n        cluster_info = self._cluster_examples(features, difficulty_ratings)\n        \n        # 8. Create the map\n        difficulty_ratings_dict = {str(indices[i]): int(rating) \n                                  for i, rating in enumerate(difficulty_ratings)}\n        \n        self.map = {\n            'difficulty_ratings': difficulty_ratings_dict,\n            'cluster_info': cluster_info,\n            'metadata': {\n                'dataset_size': len(dataset),\n                'mapped_samples': len(features),\n                'creation_time': int(time.time()),\n                'difficulty_levels': self.difficulty_levels,\n                'ensemble_size': self.ensemble_size,\n                'proxy_model_types': self.proxy_model_types\n            }\n        }\n        \n        # Add validation metrics if testset provided\n        if testset is not None:\n            validation_metrics = self._validate_map(testset, difficulty_ratings_dict)\n            self.map['validation'] = validation_metrics\n        \n        end_time = time.time()\n        self.map['metadata']['creation_time_seconds'] = end_time - start_time\n        \n        logger.info(f\"Semantic map created in {end_time - start_time:.2f} seconds\")\n        logger.info(f\"Difficulty distribution: {self._summarize_difficulty_distribution(difficulty_ratings)}\")\n        \n        return self.map\n    \n    def save_map(self, filepath):\n        \"\"\"\n        Save the semantic map to disk."
        },
        "save_map": {
          "start_line": 202,
          "end_line": 221,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "filepath"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "os.makedirs",
              "line": 213
            },
            {
              "name": "logger.info",
              "line": 219
            },
            {
              "name": "ValueError",
              "line": 210
            },
            {
              "name": "os.path.dirname",
              "line": 213
            },
            {
              "name": "open",
              "line": 216
            },
            {
              "name": "json.dump",
              "line": 217
            },
            {
              "name": "os.path.abspath",
              "line": 213
            }
          ],
          "docstring": "\n        Save the semantic map to disk.\n        \n        Args:\n            filepath: Path to save the map\n        ",
          "code_snippet": "        return self.map\n    \n    def save_map(self, filepath):\n        \"\"\"\n        Save the semantic map to disk.\n        \n        Args:\n            filepath: Path to save the map\n        \"\"\"\n        if self.map is None:\n            raise ValueError(\"No map to save. Call create_map() first.\")\n            \n        # Ensure directory exists\n        os.makedirs(os.path.dirname(os.path.abspath(filepath)), exist_ok=True)\n        \n        # Save map to file\n        with open(filepath, 'w') as f:\n            json.dump(self.map, f, indent=2)\n            \n        logger.info(f\"Semantic map saved to {filepath}\")\n        \n    def load_map(self, filepath):\n        \"\"\"\n        Load a semantic map from disk."
        },
        "load_map": {
          "start_line": 221,
          "end_line": 239,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "filepath"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logger.info",
              "line": 234
            },
            {
              "name": "logger.info",
              "line": 235
            },
            {
              "name": "open",
              "line": 231
            },
            {
              "name": "json.load",
              "line": 232
            },
            {
              "name": "len",
              "line": 235
            }
          ],
          "docstring": "\n        Load a semantic map from disk.\n        \n        Args:\n            filepath: Path to load the map from\n            \n        Returns:\n            Loaded semantic map\n        ",
          "code_snippet": "        logger.info(f\"Semantic map saved to {filepath}\")\n        \n    def load_map(self, filepath):\n        \"\"\"\n        Load a semantic map from disk.\n        \n        Args:\n            filepath: Path to load the map from\n            \n        Returns:\n            Loaded semantic map\n        \"\"\"\n        with open(filepath, 'r') as f:\n            self.map = json.load(f)\n            \n        logger.info(f\"Semantic map loaded from {filepath}\")\n        logger.info(f\"Map contains difficulty ratings for {len(self.map['difficulty_ratings'])} samples\")\n        \n        return self.map\n        \n    def visualize_map(self, output_file=None):\n        \"\"\"\n        Create visualization of the difficulty map."
        },
        "visualize_map": {
          "start_line": 239,
          "end_line": 321,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "output_file"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "defaultdict",
              "line": 253
            },
            {
              "name": "....items",
              "line": 254
            },
            {
              "name": "sum",
              "line": 258
            },
            {
              "name": "ValueError",
              "line": 250
            },
            {
              "name": "difficulty_counts.values",
              "line": 258
            },
            {
              "name": "plt.subplots",
              "line": 282
            },
            {
              "name": "sorted",
              "line": 285
            },
            {
              "name": "ax1.bar",
              "line": 287
            },
            {
              "name": "ax1.set_xlabel",
              "line": 288
            },
            {
              "name": "ax1.set_ylabel",
              "line": 289
            },
            {
              "name": "ax1.set_title",
              "line": 290
            },
            {
              "name": "ax1.grid",
              "line": 291
            },
            {
              "name": "plt.tight_layout",
              "line": 313
            },
            {
              "name": "plt.savefig",
              "line": 314
            },
            {
              "name": "plt.close",
              "line": 315
            },
            {
              "name": "logger.info",
              "line": 317
            },
            {
              "name": "difficulty_counts.items",
              "line": 259
            },
            {
              "name": "cluster_data.append",
              "line": 265
            },
            {
              "name": "difficulty_counts.keys",
              "line": 285
            },
            {
              "name": "ax2.bar",
              "line": 300
            },
            {
              "name": "ax2.set_xlabel",
              "line": 301
            },
            {
              "name": "ax2.set_ylabel",
              "line": 302
            },
            {
              "name": "ax2.tick_params",
              "line": 303
            },
            {
              "name": "ax2.twinx",
              "line": 305
            },
            {
              "name": "ax3.plot",
              "line": 306
            },
            {
              "name": "ax3.set_ylabel",
              "line": 307
            },
            {
              "name": "ax3.tick_params",
              "line": 308
            },
            {
              "name": "ax2.set_title",
              "line": 310
            },
            {
              "name": "ax2.grid",
              "line": 311
            }
          ],
          "docstring": "\n        Create visualization of the difficulty map.\n        \n        Args:\n            output_file: Optional path to save visualization\n            \n        Returns:\n            Visualization data that can be used for plotting\n        ",
          "code_snippet": "        return self.map\n        \n    def visualize_map(self, output_file=None):\n        \"\"\"\n        Create visualization of the difficulty map.\n        \n        Args:\n            output_file: Optional path to save visualization\n            \n        Returns:\n            Visualization data that can be used for plotting\n        \"\"\"\n        if self.map is None:\n            raise ValueError(\"No map to visualize. Call create_map() or load_map() first.\")\n            \n        # Create difficulty distribution visualization\n        difficulty_counts = defaultdict(int)\n        for _, rating in self.map['difficulty_ratings'].items():\n            difficulty_counts[rating] += 1\n            \n        # Normalize to percentages\n        total = sum(difficulty_counts.values())\n        difficulty_percentages = {k: (v * 100.0 / total) for k, v in difficulty_counts.items()}\n        \n        # Cluster visualization\n        cluster_data = []\n        if 'cluster_info' in self.map and 'clusters' in self.map['cluster_info']:\n            for cluster in self.map['cluster_info']['clusters']:\n                cluster_data.append({\n                    'id': cluster['id'],\n                    'size': cluster['size'],\n                    'avg_difficulty': cluster['avg_difficulty']\n                })\n        \n        visualization_data = {\n            'difficulty_distribution': difficulty_counts,\n            'difficulty_percentages': difficulty_percentages,\n            'clusters': cluster_data\n        }\n        \n        # Save to file if output_file provided\n        if output_file:\n            import matplotlib.pyplot as plt\n            \n            # Create figure with two subplots\n            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n            \n            # Plot difficulty distribution\n            difficulties = sorted(difficulty_counts.keys())\n            counts = [difficulty_counts[d] for d in difficulties]\n            ax1.bar(difficulties, counts)\n            ax1.set_xlabel('Difficulty Level')\n            ax1.set_ylabel('Number of Examples')\n            ax1.set_title('Difficulty Distribution')\n            ax1.grid(True)\n            \n            # Plot cluster information\n            if cluster_data:\n                cluster_ids = [c['id'] for c in cluster_data]\n                cluster_sizes = [c['size'] for c in cluster_data]\n                cluster_difficulties = [c['avg_difficulty'] for c in cluster_data]\n                \n                # Use twin axes for cluster size and difficulty\n                ax2.bar(cluster_ids, cluster_sizes, alpha=0.7)\n                ax2.set_xlabel('Cluster ID')\n                ax2.set_ylabel('Cluster Size', color='b')\n                ax2.tick_params(axis='y', labelcolor='b')\n                \n                ax3 = ax2.twinx()\n                ax3.plot(cluster_ids, cluster_difficulties, 'r-', marker='o')\n                ax3.set_ylabel('Avg Difficulty', color='r')\n                ax3.tick_params(axis='y', labelcolor='r')\n                \n                ax2.set_title('Cluster Analysis')\n                ax2.grid(True)\n            \n            plt.tight_layout()\n            plt.savefig(output_file)\n            plt.close()\n            \n            logger.info(f\"Visualization saved to {output_file}\")\n            \n        return visualization_data\n    \n    # Private helper methods\n    def _extract_features_labels(self, dataset):\n        \"\"\""
        },
        "_extract_features_labels": {
          "start_line": 322,
          "end_line": 387,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "dataset"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "len",
              "line": 333
            },
            {
              "name": "min",
              "line": 334
            },
            {
              "name": "DataLoader",
              "line": 337
            },
            {
              "name": "logger.info",
              "line": 343
            },
            {
              "name": "....to",
              "line": 349
            },
            {
              "name": "feature_extractor.eval",
              "line": 350
            },
            {
              "name": "len",
              "line": 353
            },
            {
              "name": "time.time",
              "line": 354
            },
            {
              "name": "logger.info",
              "line": 383
            },
            {
              "name": "torch.no_grad",
              "line": 357
            },
            {
              "name": "enumerate",
              "line": 358
            },
            {
              "name": "time.time",
              "line": 382
            },
            {
              "name": "max",
              "line": 334
            },
            {
              "name": "self._create_feature_extractor",
              "line": 349
            },
            {
              "name": "time.time",
              "line": 360
            },
            {
              "name": "data.to",
              "line": 371
            },
            {
              "name": "feature_extractor",
              "line": 374
            },
            {
              "name": "....tolist",
              "line": 377
            },
            {
              "name": "all_features.extend",
              "line": 378
            },
            {
              "name": "all_labels.extend",
              "line": 379
            },
            {
              "name": "logger.info",
              "line": 365
            },
            {
              "name": "target.tolist",
              "line": 379
            },
            {
              "name": "len",
              "line": 383
            },
            {
              "name": "....detach",
              "line": 377
            },
            {
              "name": "max",
              "line": 361
            },
            {
              "name": "features.cpu",
              "line": 377
            }
          ],
          "docstring": "\n        Extract features and labels from the dataset with improved parallelism.\n        \n        Args:\n            dataset: PyTorch dataset to extract from\n            \n        Returns:\n            Tuple of (features, labels)\n        ",
          "code_snippet": "    \n    # Private helper methods\n    def _extract_features_labels(self, dataset):\n        \"\"\"\n        Extract features and labels from the dataset with improved parallelism.\n        \n        Args:\n            dataset: PyTorch dataset to extract from\n            \n        Returns:\n            Tuple of (features, labels)\n        \"\"\"\n        # Calculate optimal batch size based on dataset size and available RAM\n        dataset_size = len(dataset)\n        optimal_batch = min(self.batch_size, dataset_size // max(1, (self.num_workers * 2)))\n        \n        # Use more workers for data loading when extracting features\n        loader = DataLoader(dataset, \n                            batch_size=optimal_batch, \n                            num_workers=self.num_workers,\n                            pin_memory=True,  # Speeds up CPU->GPU transfers\n                            shuffle=False)\n        \n        logger.info(f\"Extracting features using batch size {optimal_batch} with {self.num_workers} workers\")\n        \n        all_features = []\n        all_labels = []\n        \n        # Create a simple feature extractor network if needed\n        feature_extractor = self._create_feature_extractor().to(self.device)\n        feature_extractor.eval()\n        \n        # Track progress\n        total_batches = len(loader)\n        start_time = time.time()\n        last_log_time = start_time\n        \n        with torch.no_grad():\n            for batch_idx, (data, target) in enumerate(loader):\n                # Log progress more frequently\n                current_time = time.time()\n                if batch_idx % max(1, (total_batches // 20)) == 0 or current_time - last_log_time > 10:\n                    elapsed = current_time - start_time\n                    progress = (batch_idx + 1) / total_batches\n                    eta = (elapsed / progress) * (1 - progress) if progress > 0 else 0\n                    logger.info(f\"Processing batch {batch_idx+1}/{total_batches} - \"\n                              f\"{progress*100:.1f}% complete - \"\n                              f\"ETA: {eta:.1f}s\")\n                    last_log_time = current_time\n                \n                # Move to device efficiently\n                data = data.to(self.device, non_blocking=True)  # Non-blocking transfer is faster\n                \n                # Extract features\n                features = feature_extractor(data)\n                \n                # Move to CPU and convert to list\n                features_cpu = features.cpu().detach().tolist()\n                all_features.extend(features_cpu)\n                all_labels.extend(target.tolist())\n        \n        # Report completion time\n        total_time = time.time() - start_time\n        logger.info(f\"Feature extraction completed in {total_time:.2f} seconds for {len(all_features)} samples\")\n        \n        return all_features, all_labels\n    \n    def _create_feature_extractor(self):\n        \"\"\"\n        Create a simple feature extractor network."
        },
        "_create_feature_extractor": {
          "start_line": 387,
          "end_line": 414,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "FeatureExtractor",
              "line": 412
            },
            {
              "name": "....__init__",
              "line": 397
            },
            {
              "name": "nn.Sequential",
              "line": 398
            },
            {
              "name": "self.features",
              "line": 410
            },
            {
              "name": "nn.Conv2d",
              "line": 399
            },
            {
              "name": "nn.ReLU",
              "line": 400
            },
            {
              "name": "nn.MaxPool2d",
              "line": 401
            },
            {
              "name": "nn.Conv2d",
              "line": 402
            },
            {
              "name": "nn.ReLU",
              "line": 403
            },
            {
              "name": "nn.MaxPool2d",
              "line": 404
            },
            {
              "name": "nn.Flatten",
              "line": 405
            },
            {
              "name": "nn.Linear",
              "line": 406
            },
            {
              "name": "super",
              "line": 397
            }
          ],
          "docstring": "\n        Create a simple feature extractor network.\n        \n        Returns:\n            Feature extractor model\n        ",
          "code_snippet": "        return all_features, all_labels\n    \n    def _create_feature_extractor(self):\n        \"\"\"\n        Create a simple feature extractor network.\n        \n        Returns:\n            Feature extractor model\n        \"\"\"\n        # Simple feature extractor based on a small CNN\n        class FeatureExtractor(nn.Module):\n            def __init__(self, feature_dim=64):\n                super().__init__()\n                self.features = nn.Sequential(\n                    nn.Conv2d(3, 16, kernel_size=3, padding=1),\n                    nn.ReLU(inplace=True),\n                    nn.MaxPool2d(kernel_size=2, stride=2),\n                    nn.Conv2d(16, 32, kernel_size=3, padding=1),\n                    nn.ReLU(inplace=True),\n                    nn.MaxPool2d(kernel_size=2, stride=2),\n                    nn.Flatten(),\n                    nn.Linear(32 * 8 * 8, feature_dim)\n                )\n                \n            def forward(self, x):\n                return self.features(x)\n        \n        return FeatureExtractor()\n    \n    def _train_proxy_ensemble(self, features, labels):\n        \"\"\"\n        Train an ensemble of simple, diverse models with multi-core acceleration."
        },
        "_train_proxy_ensemble": {
          "start_line": 414,
          "end_line": 490,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "features"
            },
            {
              "name": "labels"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "min",
              "line": 482
            },
            {
              "name": "logger.info",
              "line": 483
            },
            {
              "name": "len",
              "line": 469
            },
            {
              "name": "random.sample",
              "line": 470
            },
            {
              "name": "logger.info",
              "line": 475
            },
            {
              "name": "model.fit",
              "line": 476
            },
            {
              "name": "Parallel",
              "line": 486
            },
            {
              "name": "TopographicalPredictor",
              "line": 435
            },
            {
              "name": "random.choice",
              "line": 442
            },
            {
              "name": "range",
              "line": 470
            },
            {
              "name": "int",
              "line": 470
            },
            {
              "name": "random.random",
              "line": 430
            },
            {
              "name": "RandomForestClassifier",
              "line": 447
            },
            {
              "name": "delayed",
              "line": 486
            },
            {
              "name": "range",
              "line": 486
            },
            {
              "name": "LogisticRegression",
              "line": 454
            },
            {
              "name": "LinearSVC",
              "line": 460
            },
            {
              "name": "ValueError",
              "line": 466
            }
          ],
          "docstring": "\n        Train an ensemble of simple, diverse models with multi-core acceleration.\n        \n        Args:\n            features: Feature matrix\n            labels: Labels vector\n            \n        Returns:\n            List of trained models\n        ",
          "code_snippet": "        return FeatureExtractor()\n    \n    def _train_proxy_ensemble(self, features, labels):\n        \"\"\"\n        Train an ensemble of simple, diverse models with multi-core acceleration.\n        \n        Args:\n            features: Feature matrix\n            labels: Labels vector\n            \n        Returns:\n            List of trained models\n        \"\"\"\n        from joblib import Parallel, delayed\n        \n        # Function to train a single model to be used with parallel processing\n        def train_model(model_idx):\n            # Determine whether to use topographical predictor\n            use_topo = topographical_predictor_available and random.random() < 0.3\n            \n            if use_topo:\n                # Use topographical predictor\n                model_type = 'topographical'\n                model = TopographicalPredictor(\n                    randomize_factor=model_idx,\n                    cognitive_threshold=6.5 + (model_idx * 0.2 - 0.4),  # Add diversity\n                    efficiency_sigma=0.8 + (model_idx * 0.05 - 0.1)     # Add diversity\n                )\n            else:\n                # Randomly select traditional model type for diversity\n                model_type = random.choice(self.proxy_model_types)\n                \n                # Create and train model with n_jobs for multi-core\n                if model_type == 'forest':\n                    # RandomForest can use multiple cores\n                    model = RandomForestClassifier(\n                        n_estimators=20,  # Increase for better accuracy\n                        max_depth=5, \n                        random_state=model_idx,\n                        n_jobs=2  # Use some cores but not all to allow parallel model training\n                    )\n                elif model_type == 'logistic':\n                    model = LogisticRegression(\n                        max_iter=1000, \n                        random_state=model_idx,\n                        n_jobs=2  # Use multi-core processing\n                    )\n                elif model_type == 'svm':\n                    model = LinearSVC(\n                        max_iter=1000, \n                        dual=False, \n                        random_state=model_idx\n                    )  # SVC doesn't support n_jobs\n                else:\n                    raise ValueError(f\"Unknown model type: {model_type}\")\n            \n            # Sample subset of data for bootstrapping\n            n_samples = len(features)\n            sample_indices = random.sample(range(n_samples), int(0.8 * n_samples))\n            sample_features = [features[i] for i in sample_indices]\n            sample_labels = [labels[i] for i in sample_indices]\n            \n            # Train model\n            logger.info(f\"Training ensemble model {model_idx+1}/{self.ensemble_size} ({model_type})\")\n            model.fit(sample_features, sample_labels)\n            \n            return (model_type, model)\n        \n        # Use joblib for parallel execution - train models in parallel\n        # Fewer jobs than CPU cores to avoid overloading\n        n_jobs = min(self.num_workers, self.ensemble_size)\n        logger.info(f\"Training ensemble of {self.ensemble_size} models using {n_jobs} parallel jobs\")\n        \n        # Use parallel processing to train the models\n        ensemble = Parallel(n_jobs=n_jobs)(delayed(train_model)(i) for i in range(self.ensemble_size))\n        \n        return ensemble\n    \n    def _get_ensemble_predictions(self, ensemble, features):\n        \"\"\"\n        Get predictions from all models in the ensemble."
        },
        "_get_ensemble_predictions": {
          "start_line": 490,
          "end_line": 536,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "ensemble"
            },
            {
              "name": "features"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "enumerate",
              "line": 503
            },
            {
              "name": "all_predictions.append",
              "line": 532
            },
            {
              "name": "model.predict_proba",
              "line": 507
            },
            {
              "name": "predictions.tolist",
              "line": 508
            },
            {
              "name": "hasattr",
              "line": 509
            },
            {
              "name": "model.predict_proba",
              "line": 511
            },
            {
              "name": "predictions.tolist",
              "line": 512
            },
            {
              "name": "model.decision_function",
              "line": 515
            },
            {
              "name": "len",
              "line": 518
            },
            {
              "name": "np.exp",
              "line": 527
            },
            {
              "name": "softmax_values.tolist",
              "line": 530
            },
            {
              "name": "np.sum",
              "line": 528
            },
            {
              "name": "np.exp",
              "line": 520
            },
            {
              "name": "zip",
              "line": 522
            },
            {
              "name": "np.max",
              "line": 526
            }
          ],
          "docstring": "\n        Get predictions from all models in the ensemble.\n        \n        Args:\n            ensemble: List of trained models\n            features: Feature matrix\n            \n        Returns:\n            Array of predictions from each model\n        ",
          "code_snippet": "        return ensemble\n    \n    def _get_ensemble_predictions(self, ensemble, features):\n        \"\"\"\n        Get predictions from all models in the ensemble.\n        \n        Args:\n            ensemble: List of trained models\n            features: Feature matrix\n            \n        Returns:\n            Array of predictions from each model\n        \"\"\"\n        all_predictions = []\n        \n        for i, (model_type, model) in enumerate(ensemble):\n            # Get predictions based on model type\n            if model_type == 'topographical':\n                # TopographicalPredictor always has predict_proba\n                predictions = model.predict_proba(features)\n                predictions_list = predictions.tolist()\n            elif hasattr(model, 'predict_proba'):\n                # Get probability predictions if available\n                predictions = model.predict_proba(features)\n                predictions_list = predictions.tolist()\n            else:\n                # Get decision function and convert to pseudo-probabilities\n                decision = model.decision_function(features)\n                \n                # Handle multi-class case\n                if len(decision.shape) == 1:  # Binary classification\n                    # Convert to probabilities using sigmoid (vectorized with numpy)\n                    pos_probs = 1 / (1 + np.exp(-decision))\n                    neg_probs = 1 - pos_probs\n                    predictions_list = [[n, p] for n, p in zip(neg_probs, pos_probs)]\n                else:  # Multi-class\n                    # Convert to probabilities using softmax (vectorized with numpy)\n                    # For numerical stability, subtract the max of each row\n                    shifted_decision = decision - np.max(decision, axis=1)[:, np.newaxis]\n                    exp_values = np.exp(shifted_decision)\n                    sum_exp = np.sum(exp_values, axis=1)[:, np.newaxis]\n                    softmax_values = exp_values / sum_exp\n                    predictions_list = softmax_values.tolist()\n            \n            all_predictions.append(predictions_list)\n        \n        return all_predictions\n    \n    def _calculate_disagreement(self, predictions, labels):\n        \"\"\"\n        Calculate disagreement scores based on ensemble predictions."
        },
        "_calculate_disagreement": {
          "start_line": 536,
          "end_line": 584,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "predictions"
            },
            {
              "name": "labels"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "len",
              "line": 547
            },
            {
              "name": "len",
              "line": 548
            },
            {
              "name": "range",
              "line": 564
            },
            {
              "name": "class_predictions.append",
              "line": 561
            },
            {
              "name": "defaultdict",
              "line": 569
            },
            {
              "name": "sum",
              "line": 577
            },
            {
              "name": "sample_preds.index",
              "line": 559
            },
            {
              "name": "model_class_preds.append",
              "line": 560
            },
            {
              "name": "max",
              "line": 574
            },
            {
              "name": "max",
              "line": 559
            },
            {
              "name": "class_counts.items",
              "line": 574
            }
          ],
          "docstring": "\n        Calculate disagreement scores based on ensemble predictions.\n        \n        Args:\n            predictions: List of prediction arrays from ensemble models\n            labels: True labels\n            \n        Returns:\n            Array of disagreement scores for each sample\n        ",
          "code_snippet": "        return all_predictions\n    \n    def _calculate_disagreement(self, predictions, labels):\n        \"\"\"\n        Calculate disagreement scores based on ensemble predictions.\n        \n        Args:\n            predictions: List of prediction arrays from ensemble models\n            labels: True labels\n            \n        Returns:\n            Array of disagreement scores for each sample\n        \"\"\"\n        n_models = len(predictions)\n        n_samples = len(labels)\n        \n        # Initialize disagreement scores\n        disagreement_scores = [0.0] * n_samples\n        \n        # Get class predictions from each model (argmax)\n        class_predictions = []\n        for model_preds in predictions:\n            model_class_preds = []\n            for sample_preds in model_preds:\n                # Find index of maximum value (argmax)\n                max_idx = sample_preds.index(max(sample_preds))\n                model_class_preds.append(max_idx)\n            class_predictions.append(model_class_preds)\n        \n        # For each sample, compute disagreement among models\n        for i in range(n_samples):\n            # Count predictions for each class\n            sample_predictions = [pred[i] for pred in class_predictions]\n            \n            # Count occurrences of each class\n            class_counts = defaultdict(int)\n            for pred in sample_predictions:\n                class_counts[pred] += 1\n            \n            # Find majority class\n            majority_class = max(class_counts.items(), key=lambda x: x[1])[0]\n            \n            # Count disagreements with majority class\n            disagreements = sum(1 for pred in sample_predictions if pred != majority_class)\n            \n            # Normalize disagreement score (0 to 1)\n            disagreement_scores[i] = disagreements / n_models\n        \n        return disagreement_scores\n    \n    def _analyze_local_topology(self, features, labels):\n        \"\"\"\n        Analyze the local topology/geometry of the data using KDTree for efficiency."
        },
        "_analyze_local_topology": {
          "start_line": 584,
          "end_line": 663,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "features"
            },
            {
              "name": "labels"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "len",
              "line": 599
            },
            {
              "name": "np.zeros",
              "line": 600
            },
            {
              "name": "np.array",
              "line": 603
            },
            {
              "name": "np.array",
              "line": 604
            },
            {
              "name": "logger.info",
              "line": 606
            },
            {
              "name": "KDTree",
              "line": 610
            },
            {
              "name": "min",
              "line": 613
            },
            {
              "name": "range",
              "line": 619
            },
            {
              "name": "topology_scores.tolist",
              "line": 661
            },
            {
              "name": "logger.info",
              "line": 617
            },
            {
              "name": "min",
              "line": 621
            },
            {
              "name": "tree.query",
              "line": 631
            },
            {
              "name": "....reshape",
              "line": 652
            },
            {
              "name": "logger.info",
              "line": 624
            },
            {
              "name": "isinstance",
              "line": 640
            },
            {
              "name": "np.sum",
              "line": 656
            },
            {
              "name": "len",
              "line": 640
            }
          ],
          "docstring": "\n        Analyze the local topology/geometry of the data using KDTree for efficiency.\n        With optimizations for larger datasets and multi-core systems.\n        \n        Args:\n            features: Feature matrix\n            labels: Labels vector\n            \n        Returns:\n            Array of topology scores for each sample\n        ",
          "code_snippet": "        return disagreement_scores\n    \n    def _analyze_local_topology(self, features, labels):\n        \"\"\"\n        Analyze the local topology/geometry of the data using KDTree for efficiency.\n        With optimizations for larger datasets and multi-core systems.\n        \n        Args:\n            features: Feature matrix\n            labels: Labels vector\n            \n        Returns:\n            Array of topology scores for each sample\n        \"\"\"\n        from sklearn.neighbors import KDTree\n        import numpy as np\n        \n        n_samples = len(labels)\n        topology_scores = np.zeros(n_samples, dtype=np.float32)\n        \n        # Convert features to numpy array if not already\n        features_array = np.array(features)\n        labels_array = np.array(labels)\n        \n        logger.info(f\"Building KDTree for {n_samples} samples using {self.num_workers} worker threads...\")\n        \n        # Build a KDTree for efficient nearest neighbor search\n        # Use n_jobs parameter to parallelize the tree construction\n        tree = KDTree(features_array, leaf_size=40, metric='euclidean')\n        \n        # For very large datasets, process in chunks to avoid memory issues\n        chunk_size = min(10000, n_samples)\n        n_chunks = (n_samples + chunk_size - 1) // chunk_size\n        \n        if n_chunks > 1:\n            logger.info(f\"Processing topology in {n_chunks} chunks of size {chunk_size}\")\n        \n        for chunk_idx in range(n_chunks):\n            start_idx = chunk_idx * chunk_size\n            end_idx = min(start_idx + chunk_size, n_samples)\n            \n            if n_chunks > 1:\n                logger.info(f\"Processing chunk {chunk_idx+1}/{n_chunks} (samples {start_idx} to {end_idx})\")\n            \n            # Query for k+1 neighbors for this chunk\n            # n_jobs parameter uses multiple threads for the query\n            # When return_distance=True (default), it returns both distances and indices\n            # We need to explicitly set return_distance to False to get only indices\n            # Make sure we only get indices, not distances\n            query_result = tree.query(\n                features_array[start_idx:end_idx],\n                k=self.n_neighbors + 1,\n                return_distance=False,  # This should return only indices\n                dualtree=True,\n                sort_results=False  # Faster if we don't need sorted results\n            )\n            \n            # Make sure we handle whatever is returned correctly\n            if isinstance(query_result, tuple) and len(query_result) == 2:\n                # If it's still returning (distances, indices) despite return_distance=False\n                _, neighbors_indices = query_result\n            else:\n                # If it's correctly returning only indices\n                neighbors_indices = query_result\n            \n            # Skip the first neighbor (self) for each point\n            neighbors_indices = neighbors_indices[:, 1:]\n            \n            # Get the labels for these neighbors\n            neighbor_labels = labels_array[neighbors_indices]\n            own_labels = labels_array[start_idx:end_idx].reshape(-1, 1)\n            \n            # Calculate proportion of neighbors with different labels\n            different_labels = (neighbor_labels != own_labels)\n            chunk_scores = np.sum(different_labels, axis=1) / self.n_neighbors\n            \n            # Store results for this chunk\n            topology_scores[start_idx:end_idx] = chunk_scores\n        \n        return topology_scores.tolist()\n    \n    def _calculate_information_measures(self, predictions, labels):\n        \"\"\"\n        Calculate information-theoretic measures of sample difficulty."
        },
        "_calculate_information_measures": {
          "start_line": 663,
          "end_line": 713,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "predictions"
            },
            {
              "name": "labels"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "len",
              "line": 674
            },
            {
              "name": "range",
              "line": 679
            },
            {
              "name": "range",
              "line": 696
            },
            {
              "name": "len",
              "line": 684
            },
            {
              "name": "avg_probs.append",
              "line": 693
            },
            {
              "name": "np.array",
              "line": 699
            },
            {
              "name": "len",
              "line": 704
            },
            {
              "name": "np.log2",
              "line": 705
            },
            {
              "name": "range",
              "line": 688
            },
            {
              "name": "len",
              "line": 692
            },
            {
              "name": "len",
              "line": 701
            },
            {
              "name": "np.sum",
              "line": 701
            },
            {
              "name": "np.log2",
              "line": 701
            }
          ],
          "docstring": "\n        Calculate information-theoretic measures of sample difficulty.\n        \n        Args:\n            predictions: List of prediction arrays from ensemble models\n            labels: True labels\n            \n        Returns:\n            Array of information scores for each sample\n        ",
          "code_snippet": "        return topology_scores.tolist()\n    \n    def _calculate_information_measures(self, predictions, labels):\n        \"\"\"\n        Calculate information-theoretic measures of sample difficulty.\n        \n        Args:\n            predictions: List of prediction arrays from ensemble models\n            labels: True labels\n            \n        Returns:\n            Array of information scores for each sample\n        \"\"\"\n        n_samples = len(labels)\n        information_scores = [0.0] * n_samples\n        \n        # First, average prediction probabilities across models\n        avg_probs = []\n        for i in range(n_samples):\n            # Get probabilities for this sample from all models\n            sample_probs = [model_preds[i] for model_preds in predictions]\n            \n            # Calculate average probability for each class\n            n_classes = len(sample_probs[0])\n            avg_sample_probs = [0.0] * n_classes\n            \n            for probs in sample_probs:\n                for j in range(n_classes):\n                    avg_sample_probs[j] += probs[j]\n            \n            # Divide by number of models\n            avg_sample_probs = [p / len(predictions) for p in avg_sample_probs]\n            avg_probs.append(avg_sample_probs)\n        \n        # For each sample, calculate entropy\n        for i in range(n_samples):\n            probs = avg_probs[i]\n            # Calculate entropy using numpy (with handling for log(0))\n            probs_array = np.array(probs)\n            non_zero_probs = probs_array[probs_array > 0]  # Filter out zeros\n            entropy = -np.sum(non_zero_probs * np.log2(non_zero_probs)) if len(non_zero_probs) > 0 else 0.0\n            \n            # Normalize entropy to [0, 1] based on number of classes\n            n_classes = len(probs)\n            max_entropy = np.log2(n_classes)\n            normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0\n            \n            # High entropy = high uncertainty = high difficulty\n            information_scores[i] = normalized_entropy\n        \n        return information_scores\n    \n    def _combine_difficulty_scores(self, *scores):\n        \"\"\"\n        Combine multiple difficulty scores into final ratings."
        },
        "_combine_difficulty_scores": {
          "start_line": 713,
          "end_line": 742,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "len",
              "line": 723
            },
            {
              "name": "len",
              "line": 724
            },
            {
              "name": "range",
              "line": 728
            },
            {
              "name": "difficulty_ratings.append",
              "line": 738
            },
            {
              "name": "round",
              "line": 738
            }
          ],
          "docstring": "\n        Combine multiple difficulty scores into final ratings.\n        \n        Args:\n            *scores: Variable number of score arrays\n            \n        Returns:\n            Array of difficulty ratings (integers from 0 to difficulty_levels-1)\n        ",
          "code_snippet": "        return information_scores\n    \n    def _combine_difficulty_scores(self, *scores):\n        \"\"\"\n        Combine multiple difficulty scores into final ratings.\n        \n        Args:\n            *scores: Variable number of score arrays\n            \n        Returns:\n            Array of difficulty ratings (integers from 0 to difficulty_levels-1)\n        \"\"\"\n        n_samples = len(scores[0])\n        n_scores = len(scores)\n        \n        # Simple approach: average all scores\n        combined_scores = [0.0] * n_samples\n        for i in range(n_samples):\n            for score_list in scores:\n                combined_scores[i] += score_list[i]\n            combined_scores[i] /= n_scores\n        \n        # Discretize into difficulty levels\n        # Scale from [0, 1] to [0, difficulty_levels-1]\n        difficulty_ratings = []\n        for score in combined_scores:\n            scaled_score = score * (self.difficulty_levels - 1)\n            difficulty_ratings.append(round(scaled_score))\n        \n        return difficulty_ratings\n    \n    def _cluster_examples(self, features, difficulty_ratings):\n        \"\"\"\n        Cluster examples by feature similarity and difficulty."
        },
        "_cluster_examples": {
          "start_line": 742,
          "end_line": 794,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "features"
            },
            {
              "name": "difficulty_ratings"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "min",
              "line": 754
            },
            {
              "name": "KMeans",
              "line": 758
            },
            {
              "name": "kmeans.fit_predict",
              "line": 759
            },
            {
              "name": "range",
              "line": 763
            },
            {
              "name": "int",
              "line": 754
            },
            {
              "name": "np.array",
              "line": 778
            },
            {
              "name": "np.std",
              "line": 779
            },
            {
              "name": "clusters.append",
              "line": 782
            },
            {
              "name": "len",
              "line": 791
            },
            {
              "name": "np.sqrt",
              "line": 754
            },
            {
              "name": "sum",
              "line": 775
            },
            {
              "name": "len",
              "line": 775
            },
            {
              "name": "len",
              "line": 754
            },
            {
              "name": "enumerate",
              "line": 765
            },
            {
              "name": "int",
              "line": 783
            },
            {
              "name": "....tolist",
              "line": 784
            },
            {
              "name": "len",
              "line": 785
            },
            {
              "name": "float",
              "line": 786
            },
            {
              "name": "float",
              "line": 787
            }
          ],
          "docstring": "\n        Cluster examples by feature similarity and difficulty.\n        \n        Args:\n            features: Feature matrix\n            difficulty_ratings: Difficulty ratings for each sample\n            \n        Returns:\n            Dictionary with cluster information\n        ",
          "code_snippet": "        return difficulty_ratings\n    \n    def _cluster_examples(self, features, difficulty_ratings):\n        \"\"\"\n        Cluster examples by feature similarity and difficulty.\n        \n        Args:\n            features: Feature matrix\n            difficulty_ratings: Difficulty ratings for each sample\n            \n        Returns:\n            Dictionary with cluster information\n        \"\"\"\n        # Determine number of clusters (heuristic: square root of samples)\n        n_clusters = min(int(np.sqrt(len(features))), 50)\n        \n        # Use scikit-learn's KMeans which is more efficient than a custom implementation\n        # This still uses numpy internally but is much more optimized and encapsulated\n        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n        cluster_labels = kmeans.fit_predict(features)\n        \n        # Calculate cluster statistics\n        clusters = []\n        for cluster_id in range(n_clusters):\n            # Get indices of samples in this cluster\n            cluster_indices = [i for i, label in enumerate(cluster_labels) if label == cluster_id]\n            \n            # Skip empty clusters\n            if not cluster_indices:\n                continue\n                \n            # Calculate cluster statistics\n            cluster_difficulties = [difficulty_ratings[i] for i in cluster_indices]\n            \n            # Calculate mean\n            avg_difficulty = sum(cluster_difficulties) / len(cluster_difficulties)\n            \n            # Calculate standard deviation using numpy\n            cluster_difficulties_array = np.array(cluster_difficulties)\n            difficulty_std = np.std(cluster_difficulties_array)\n            \n            # Store cluster info\n            clusters.append({\n                'id': int(cluster_id),\n                'center': kmeans.cluster_centers_[cluster_id].tolist(),\n                'size': len(cluster_indices),\n                'avg_difficulty': float(avg_difficulty),\n                'difficulty_std': float(difficulty_std)\n            })\n        \n        return {\n            'n_clusters': len(clusters),\n            'clusters': clusters\n        }\n    \n    def _validate_map(self, testset, difficulty_ratings):\n        \"\"\""
        },
        "_validate_map": {
          "start_line": 795,
          "end_line": 827,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "testset"
            },
            {
              "name": "difficulty_ratings"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self._extract_features_labels",
              "line": 807
            },
            {
              "name": "defaultdict",
              "line": 810
            },
            {
              "name": "difficulty_ratings.items",
              "line": 811
            },
            {
              "name": "difficulty_by_class.items",
              "line": 819
            },
            {
              "name": "int",
              "line": 812
            },
            {
              "name": "float",
              "line": 820
            },
            {
              "name": "len",
              "line": 825
            },
            {
              "name": "len",
              "line": 813
            },
            {
              "name": "int",
              "line": 814
            },
            {
              "name": "....append",
              "line": 815
            },
            {
              "name": "str",
              "line": 820
            },
            {
              "name": "sum",
              "line": 820
            },
            {
              "name": "len",
              "line": 820
            }
          ],
          "docstring": "\n        Validate the semantic map using a test dataset.\n        \n        Args:\n            testset: Test dataset\n            difficulty_ratings: Difficulty ratings for training samples\n            \n        Returns:\n            Dictionary with validation metrics\n        ",
          "code_snippet": "        }\n    \n    def _validate_map(self, testset, difficulty_ratings):\n        \"\"\"\n        Validate the semantic map using a test dataset.\n        \n        Args:\n            testset: Test dataset\n            difficulty_ratings: Difficulty ratings for training samples\n            \n        Returns:\n            Dictionary with validation metrics\n        \"\"\"\n        # Extract features from test set\n        test_features, test_labels = self._extract_features_labels(testset)\n        \n        # Calculate mean difficulty by class\n        difficulty_by_class = defaultdict(list)\n        for idx, rating in difficulty_ratings.items():\n            sample_idx = int(idx)\n            if sample_idx < len(test_labels):\n                label = int(test_labels[sample_idx])\n                difficulty_by_class[label].append(rating)\n        \n        # Calculate average difficulty per class\n        class_difficulty = {}\n        for label, difficulties in difficulty_by_class.items():\n            class_difficulty[str(label)] = float(sum(difficulties) / len(difficulties))\n        \n        # Return validation metrics\n        return {\n            'class_difficulty': class_difficulty,\n            'test_samples': len(test_labels)\n        }\n    \n    def _summarize_difficulty_distribution(self, difficulty_ratings):\n        \"\"\""
        },
        "_summarize_difficulty_distribution": {
          "start_line": 828,
          "end_line": 857,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "difficulty_ratings"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "defaultdict",
              "line": 839
            },
            {
              "name": "len",
              "line": 844
            },
            {
              "name": "sorted",
              "line": 848
            },
            {
              "name": "difficulty_counts.keys",
              "line": 848
            },
            {
              "name": "int",
              "line": 841
            },
            {
              "name": "difficulty_counts.items",
              "line": 845
            }
          ],
          "docstring": "\n        Summarize the distribution of difficulty ratings.\n        \n        Args:\n            difficulty_ratings: Array of difficulty ratings\n            \n        Returns:\n            Dictionary with difficulty distribution summary\n        ",
          "code_snippet": "        }\n    \n    def _summarize_difficulty_distribution(self, difficulty_ratings):\n        \"\"\"\n        Summarize the distribution of difficulty ratings.\n        \n        Args:\n            difficulty_ratings: Array of difficulty ratings\n            \n        Returns:\n            Dictionary with difficulty distribution summary\n        \"\"\"\n        # Count occurrences of each difficulty level\n        difficulty_counts = defaultdict(int)\n        for rating in difficulty_ratings:\n            difficulty_counts[int(rating)] += 1\n            \n        # Convert to percentages\n        total = len(difficulty_ratings)\n        difficulty_percentages = {k: (v * 100.0 / total) for k, v in difficulty_counts.items()}\n        \n        # Return as sorted lists for easier reading\n        levels = sorted(difficulty_counts.keys())\n        counts = [difficulty_counts[level] for level in levels]\n        percentages = [difficulty_percentages[level] for level in levels]\n        \n        return {\n            'levels': levels,\n            'counts': counts,\n            'percentages': percentages\n        }"
        }
      },
      "class_variables": [],
      "bases": [],
      "docstring": "\n    Creates a semantic difficulty map of a dataset before training begins.\n    This map can then be used to inform training without forcing the model\n    to address difficult regions directly.\n    "
    },
    "FeatureExtractor": {
      "start_line": 395,
      "end_line": 412,
      "methods": {
        "__init__": {
          "start_line": 396,
          "end_line": 408,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "feature_dim"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "....__init__",
              "line": 397
            },
            {
              "name": "nn.Sequential",
              "line": 398
            },
            {
              "name": "nn.Conv2d",
              "line": 399
            },
            {
              "name": "nn.ReLU",
              "line": 400
            },
            {
              "name": "nn.MaxPool2d",
              "line": 401
            },
            {
              "name": "nn.Conv2d",
              "line": 402
            },
            {
              "name": "nn.ReLU",
              "line": 403
            },
            {
              "name": "nn.MaxPool2d",
              "line": 404
            },
            {
              "name": "nn.Flatten",
              "line": 405
            },
            {
              "name": "nn.Linear",
              "line": 406
            },
            {
              "name": "super",
              "line": 397
            }
          ],
          "code_snippet": "        # Simple feature extractor based on a small CNN\n        class FeatureExtractor(nn.Module):\n            def __init__(self, feature_dim=64):\n                super().__init__()\n                self.features = nn.Sequential(\n                    nn.Conv2d(3, 16, kernel_size=3, padding=1),\n                    nn.ReLU(inplace=True),\n                    nn.MaxPool2d(kernel_size=2, stride=2),\n                    nn.Conv2d(16, 32, kernel_size=3, padding=1),\n                    nn.ReLU(inplace=True),\n                    nn.MaxPool2d(kernel_size=2, stride=2),\n                    nn.Flatten(),\n                    nn.Linear(32 * 8 * 8, feature_dim)\n                )\n                \n            def forward(self, x):\n                return self.features(x)"
        },
        "forward": {
          "start_line": 409,
          "end_line": 412,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "x"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.features",
              "line": 410
            }
          ],
          "code_snippet": "                )\n                \n            def forward(self, x):\n                return self.features(x)\n        \n        return FeatureExtractor()\n    \n    def _train_proxy_ensemble(self, features, labels):"
        }
      },
      "class_variables": [],
      "bases": [
        "..."
      ]
    }
  },
  "functions": {},
  "constants": {}
}