{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\isekaizen\\core\\training\\callbacks\\model_checkpoint.py",
  "imports": [
    {
      "name": "os",
      "line": 11
    },
    {
      "name": "logging",
      "line": 12
    },
    {
      "name": "torch",
      "line": 13
    },
    {
      "name": "numpy",
      "line": 14
    },
    {
      "name": "typing.Dict",
      "line": 15
    },
    {
      "name": "typing.Any",
      "line": 15
    },
    {
      "name": "typing.List",
      "line": 15
    },
    {
      "name": "typing.Optional",
      "line": 15
    },
    {
      "name": "typing.Union",
      "line": 15
    },
    {
      "name": "typing.Callable",
      "line": 15
    }
  ],
  "classes": {
    "ModelCheckpointCallback": {
      "start_line": 19,
      "end_line": 141,
      "methods": {
        "__init__": {
          "start_line": 33,
          "end_line": 69,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "filepath",
              "type": "str"
            },
            {
              "name": "monitor",
              "type": "str"
            },
            {
              "name": "save_best_only",
              "type": "bool"
            },
            {
              "name": "save_weights_only",
              "type": "bool"
            },
            {
              "name": "mode",
              "type": "str"
            },
            {
              "name": "verbose",
              "type": "bool"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "os.makedirs",
              "line": 65
            },
            {
              "name": "logger.debug",
              "line": 67
            },
            {
              "name": "os.path.dirname",
              "line": 65
            },
            {
              "name": "os.path.abspath",
              "line": 65
            }
          ],
          "docstring": "\n        Initialize the model checkpoint callback.\n        \n        Args:\n            filepath: Path template for saving checkpoints\n            monitor: Metric to monitor ('val_loss' or 'val_acc')\n            save_best_only: Whether to save only the best model\n            save_weights_only: Whether to save only model weights\n            mode: 'min' for metrics like loss, 'max' for metrics like accuracy\n            verbose: Whether to log messages when saving\n        ",
          "code_snippet": "    \"\"\"\n    \n    def __init__(\n        self,\n        filepath: str,\n        monitor: str = 'val_loss',\n        save_best_only: bool = True,\n        save_weights_only: bool = False,\n        mode: str = 'min',\n        verbose: bool = True\n    ):\n        \"\"\"\n        Initialize the model checkpoint callback.\n        \n        Args:\n            filepath: Path template for saving checkpoints\n            monitor: Metric to monitor ('val_loss' or 'val_acc')\n            save_best_only: Whether to save only the best model\n            save_weights_only: Whether to save only model weights\n            mode: 'min' for metrics like loss, 'max' for metrics like accuracy\n            verbose: Whether to log messages when saving\n        \"\"\"\n        self.filepath = filepath\n        self.monitor = monitor\n        self.save_best_only = save_best_only\n        self.save_weights_only = save_weights_only\n        self.mode = mode\n        self.verbose = verbose\n        \n        # Set initial best value based on mode\n        self.best_value = np.inf if mode == 'min' else -np.inf\n        self.best_epoch = -1\n        \n        # Ensure directory exists\n        os.makedirs(os.path.dirname(os.path.abspath(filepath)), exist_ok=True)\n        \n        logger.debug(f\"ModelCheckpointCallback initialized: monitoring {monitor}, filepath={filepath}\")\n    \n    def __call__(\n        self, \n        epoch: int, "
        },
        "__call__": {
          "start_line": 69,
          "end_line": 141,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "epoch",
              "type": "int"
            },
            {
              "name": "history"
            },
            {
              "name": "model"
            },
            {
              "name": "optimizer"
            }
          ],
          "return_type": "bool",
          "calls": [
            {
              "name": "formatted_filepath.replace",
              "line": 98
            },
            {
              "name": "formatted_filepath.replace",
              "line": 100
            },
            {
              "name": "logger.info",
              "line": 115
            },
            {
              "name": "logger.info",
              "line": 119
            },
            {
              "name": "torch.save",
              "line": 125
            },
            {
              "name": "torch.save",
              "line": 128
            },
            {
              "name": "logger.error",
              "line": 136
            },
            {
              "name": "model.state_dict",
              "line": 125
            },
            {
              "name": "model.state_dict",
              "line": 130
            },
            {
              "name": "optimizer.state_dict",
              "line": 131
            },
            {
              "name": "str",
              "line": 136
            }
          ],
          "docstring": "\n        Check if model should be saved based on monitoring criteria.\n        \n        Args:\n            epoch: Current epoch number (0-indexed)\n            history: Training history with metrics\n            model: Current model\n            optimizer: Current optimizer\n            \n        Returns:\n            bool: Always False (never stops training)\n        ",
          "code_snippet": "        logger.debug(f\"ModelCheckpointCallback initialized: monitoring {monitor}, filepath={filepath}\")\n    \n    def __call__(\n        self, \n        epoch: int, \n        history: Dict[str, List[Any]], \n        model: torch.nn.Module, \n        optimizer: torch.optim.Optimizer\n    ) -> bool:\n        \"\"\"\n        Check if model should be saved based on monitoring criteria.\n        \n        Args:\n            epoch: Current epoch number (0-indexed)\n            history: Training history with metrics\n            model: Current model\n            optimizer: Current optimizer\n            \n        Returns:\n            bool: Always False (never stops training)\n        \"\"\"\n        # Check if we have the monitored metric\n        if self.monitor not in history or not history[self.monitor]:\n            return False\n        \n        # Get current value\n        current = history[self.monitor][-1]\n        \n        # Format the filepath with epoch and metric value\n        formatted_filepath = self.filepath\n        if '{epoch}' in formatted_filepath:\n            formatted_filepath = formatted_filepath.replace('{epoch}', f'{epoch+1:03d}')\n        if '{' + self.monitor + '}' in formatted_filepath:\n            formatted_filepath = formatted_filepath.replace('{' + self.monitor + '}', f'{current:.4f}')\n        \n        # Check if improved\n        if self.mode == 'min':\n            improved = current < self.best_value\n        else:\n            improved = current > self.best_value\n            \n        # Save if not using save_best_only or if improved\n        if not self.save_best_only or improved:\n            if improved:\n                self.best_value = current\n                self.best_epoch = epoch\n                \n                if self.verbose:\n                    logger.info(f\"Epoch {epoch+1}: {self.monitor} improved from \"\n                              f\"{self.best_value if self.best_epoch == epoch else 'previous best'} \"\n                              f\"to {current}, saving model to {formatted_filepath}\")\n            elif self.verbose:\n                logger.info(f\"Epoch {epoch+1}: saving model to {formatted_filepath}\")\n                \n            # Save the model\n            try:\n                if self.save_weights_only:\n                    # Save only weights\n                    torch.save(model.state_dict(), formatted_filepath)\n                else:\n                    # Save the whole checkpoint\n                    torch.save({\n                        'epoch': epoch,\n                        'model_state_dict': model.state_dict(),\n                        'optimizer_state_dict': optimizer.state_dict(),\n                        'history': history,\n                        self.monitor: current\n                    }, formatted_filepath)\n            except Exception as e:\n                logger.error(f\"Error saving model checkpoint: {str(e)}\")\n        \n        # Always return False - this callback never stops training\n        return False"
        }
      },
      "class_variables": [],
      "bases": [],
      "docstring": "\n    Saves model checkpoints during training when performance improves.\n    \n    Attributes:\n        filepath: Path template for saving checkpoints\n        monitor: Metric to monitor ('val_loss' or 'val_acc')\n        save_best_only: Whether to save only the best model\n        save_weights_only: Whether to save only model weights\n        mode: 'min' for metrics like loss, 'max' for metrics like accuracy\n        best_value: Best value of the monitored metric\n        best_epoch: Epoch with the best performance\n    "
    }
  },
  "functions": {},
  "constants": {}
}