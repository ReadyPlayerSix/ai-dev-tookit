{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\isekaizen\\optimizers\\eve\\risk_aware.py",
  "imports": [
    {
      "name": "math",
      "line": 11
    },
    {
      "name": "torch",
      "line": 12
    },
    {
      "name": "typing.Dict",
      "line": 13
    },
    {
      "name": "typing.Optional",
      "line": 13
    },
    {
      "name": "typing.Any",
      "line": 13
    },
    {
      "name": "typing.List",
      "line": 13
    },
    {
      "name": "logging",
      "line": 15
    },
    {
      "name": "isekaizen.optimizers.eve.unified_ratio.EVEUnifiedRatio",
      "line": 19
    },
    {
      "name": "isekaizen.utils.training_utils.get_fibonacci_check_intervals",
      "line": 20
    }
  ],
  "classes": {
    "RiskAwareEVEOptimizer": {
      "start_line": 22,
      "end_line": 601,
      "methods": {
        "__init__": {
          "start_line": 35,
          "end_line": 141,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "params"
            },
            {
              "name": "lr"
            },
            {
              "name": "betas"
            },
            {
              "name": "eps"
            },
            {
              "name": "weight_decay"
            },
            {
              "name": "pattern_map"
            },
            {
              "name": "weight_adjustment_range"
            },
            {
              "name": "weight_range_iris"
            },
            {
              "name": "fibonacci_intervals"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "....__init__",
              "line": 81
            },
            {
              "name": "logger.info",
              "line": 138
            },
            {
              "name": "logger.info",
              "line": 139
            },
            {
              "name": "logger.info",
              "line": 69
            },
            {
              "name": "kwargs.get",
              "line": 101
            },
            {
              "name": "get_fibonacci_check_intervals",
              "line": 102
            },
            {
              "name": "logger.info",
              "line": 65
            },
            {
              "name": "super",
              "line": 81
            },
            {
              "name": "logger.info",
              "line": 76
            },
            {
              "name": "logger.info",
              "line": 78
            },
            {
              "name": "len",
              "line": 69
            },
            {
              "name": "pattern_map.keys",
              "line": 69
            },
            {
              "name": "len",
              "line": 76
            }
          ],
          "docstring": "\n        Initialize the risk-aware EVE optimizer.\n        \n        Args:\n            params: Iterable of parameters to optimize\n            lr: Learning rate\n            betas: Coefficients for computing running averages\n            eps: Term for numerical stability\n            weight_decay: Weight decay (L2 penalty)\n            pattern_map: Pattern map for pattern-aware optimization\n            weight_adjustment_range: Range for weight adjustments\n            weight_range_iris: Whether to use weight range iris\n            fibonacci_intervals: List of epoch indices for Fibonacci-based checks\n            **kwargs: Additional arguments\n        ",
          "code_snippet": "    \"\"\"\n    \n    def __init__(self, \n                params, \n                lr=1e-3, \n                betas=(0.9, 0.999), \n                eps=1e-8, \n                weight_decay=0, \n                pattern_map=None,\n                weight_adjustment_range=None,  # Parameter kept for backward compatibility but not used \n                weight_range_iris=None,  # Parameter kept for backward compatibility but not used\n                fibonacci_intervals=None, \n                **kwargs):\n        \"\"\"\n        Initialize the risk-aware EVE optimizer.\n        \n        Args:\n            params: Iterable of parameters to optimize\n            lr: Learning rate\n            betas: Coefficients for computing running averages\n            eps: Term for numerical stability\n            weight_decay: Weight decay (L2 penalty)\n            pattern_map: Pattern map for pattern-aware optimization\n            weight_adjustment_range: Range for weight adjustments\n            weight_range_iris: Whether to use weight range iris\n            fibonacci_intervals: List of epoch indices for Fibonacci-based checks\n            **kwargs: Additional arguments\n        \"\"\"\n        # Preprocessing to ensure pattern_map has all required data\n        if pattern_map:\n            # Check and process pattern map for required EVE optimizer data\n            if 'weighted_distribution' not in pattern_map and 'weighted_scores' in pattern_map:\n                logger.info(\"Adding weighted_distribution to pattern map from weighted_scores\")\n                pattern_map['weighted_distribution'] = pattern_map['weighted_scores']\n            \n            # Log critical pattern map components for debugging\n            logger.info(f\"Pattern map contains {len(pattern_map.keys())} top-level keys\")\n            critical_components = [\n                'weighted_distribution', 'pattern_distribution', 'pattern_complexities',\n                'sample_to_pattern', 'pattern_types'\n            ]\n            for component in critical_components:\n                if component in pattern_map:\n                    logger.info(f\"  Pattern map contains '{component}' with {len(pattern_map[component])} entries\")\n                else:\n                    logger.info(f\"  Pattern map missing '{component}'\")\n        \n        # Initialize parent class first\n        super().__init__(\n            params, \n            lr=lr, \n            betas=betas, \n            eps=eps, \n            weight_decay=weight_decay, \n            pattern_map=pattern_map,\n            # Always use iris mode internally\n            weight_adjustment_range=\"default\",  # Default base range \n            weight_range_iris=True,  # Always use iris mode\n            fibonacci_intervals=fibonacci_intervals, \n            **kwargs\n        )\n        \n        # Store complete original pattern map to ensure we have all data\n        self._complete_pattern_map = pattern_map\n        \n        # Initialize Fibonacci check intervals if not provided\n        if fibonacci_intervals is None:\n            # Default to 100 epochs if total_epochs not specified\n            total_epochs = kwargs.get('total_epochs', 100)\n            self.fibonacci_intervals = get_fibonacci_check_intervals(total_epochs)\n        else:\n            self.fibonacci_intervals = fibonacci_intervals\n        \n        # Initialize epoch tracking\n        self.current_epoch = 0\n        self.last_lr_update_epoch = -1  # Track when we last updated learning rates\n        self.epoch_changed = False  # Flag to track epoch transitions\n        \n        # Initialize accuracy tracking for warm-up criteria\n        self.latest_train_acc = 0.0\n        self.latest_val_acc = 0.0\n        \n        # Determine sensitivity based on weight adjustment range\n        if self.weight_range_iris:\n            # More sensitive adjustment for iris mode\n            self.lr_risk_sensitivity = 0.15  # Fixed value, no user configuration\n        else:\n            # Standard sensitivity\n            self.lr_risk_sensitivity = 0.1   # Fixed value, no user configuration\n        \n        # Initialize pattern-specific learning rate multipliers\n        self.pattern_lr_multipliers = {}\n        \n        # Initialize history tracking for pattern-specific learning rates\n        self.pattern_lr_history = {}\n        \n        # Initialize learning rate history\n        self.lr_history = []\n        \n        # Always use iris mode as directed by recent changes\n        self.weight_range_iris = True\n        \n        # Use a fixed sensitivity for iris mode\n        self.lr_risk_sensitivity = 0.15\n        \n        logger.info(\"RiskAwareEVEOptimizer initialized with risk/accuracy ratio-based LR adjustments\")\n        logger.info(\"  Using iris mode for weight adjustments (fixed setting)\")\n    \n    def _is_fibonacci_check_point(self, epoch: int) -> bool:\n        \"\"\"\n        Determine if the current epoch is a Fibonacci check point."
        },
        "_is_fibonacci_check_point": {
          "start_line": 141,
          "end_line": 177,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "epoch",
              "type": "int"
            }
          ],
          "return_type": "bool",
          "calls": [
            {
              "name": "logger.info",
              "line": 170
            },
            {
              "name": "len",
              "line": 154
            }
          ],
          "docstring": "\n        Determine if the current epoch is a Fibonacci check point.\n        \n        This function checks if the current epoch corresponds to a change in Fibonacci interval.\n        \n        Args:\n            epoch: Current epoch index\n            \n        Returns:\n            bool: Whether this epoch should perform a Fibonacci-scheduled check\n        ",
          "code_snippet": "        logger.info(\"  Using iris mode for weight adjustments (fixed setting)\")\n    \n    def _is_fibonacci_check_point(self, epoch: int) -> bool:\n        \"\"\"\n        Determine if the current epoch is a Fibonacci check point.\n        \n        This function checks if the current epoch corresponds to a change in Fibonacci interval.\n        \n        Args:\n            epoch: Current epoch index\n            \n        Returns:\n            bool: Whether this epoch should perform a Fibonacci-scheduled check\n        \"\"\"\n        # First, make sure we have a valid epoch index\n        if epoch < 0 or epoch >= len(self.fibonacci_intervals):\n            return False\n            \n        # CRITICAL: Check if we've already processed this epoch\n        if epoch == self.last_lr_update_epoch:\n            return False\n            \n        # Get the current interval and previous interval (if available)\n        current_interval = self.fibonacci_intervals[epoch]\n        prev_interval = self.fibonacci_intervals[epoch-1] if epoch > 0 else 0\n        \n        # It's a checkpoint if the interval changes to the next Fibonacci number\n        interval_change = current_interval != prev_interval\n        \n        if interval_change:\n            # Log the interval change (only when it actually happens)\n            logger.info(f\"CHECKPOINT DETECTED: Epoch {epoch+1} (interval change: {prev_interval} -> {current_interval})\")\n            # Immediately set last_lr_update_epoch to prevent multiple detections\n            self.last_lr_update_epoch = epoch\n            return True\n        else:\n            return False\n    \n    def update_learning_rates_by_pattern(self):\n        \"\"\"\n        Update learning rates based on risk/accuracy ratios at Fibonacci check points."
        },
        "update_learning_rates_by_pattern": {
          "start_line": 177,
          "end_line": 344,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self._is_fibonacci_check_point",
              "line": 193
            },
            {
              "name": "hasattr",
              "line": 209
            },
            {
              "name": "logger.debug",
              "line": 270
            },
            {
              "name": "pattern_risks.items",
              "line": 296
            },
            {
              "name": "getattr",
              "line": 189
            },
            {
              "name": "logger.info",
              "line": 190
            },
            {
              "name": "logger.info",
              "line": 197
            },
            {
              "name": "logger.info",
              "line": 199
            },
            {
              "name": "logger.info",
              "line": 200
            },
            {
              "name": "self.pattern_mediator.get_pattern_risks",
              "line": 211
            },
            {
              "name": "self.pattern_mediator.get_pattern_accuracies",
              "line": 212
            },
            {
              "name": "logger.info",
              "line": 232
            },
            {
              "name": "logger.info",
              "line": 246
            },
            {
              "name": "logger.info",
              "line": 247
            },
            {
              "name": "logger.info",
              "line": 248
            },
            {
              "name": "self.ratio_tracker.get_pattern_risks",
              "line": 261
            },
            {
              "name": "self.pattern_tracker.get_pattern_accuracies",
              "line": 262
            },
            {
              "name": "logger.info",
              "line": 265
            },
            {
              "name": "logger.info",
              "line": 266
            },
            {
              "name": "logger.info",
              "line": 267
            },
            {
              "name": "logger.warning",
              "line": 281
            },
            {
              "name": "hasattr",
              "line": 283
            },
            {
              "name": "hasattr",
              "line": 289
            },
            {
              "name": "self.equilibrium_tracker.get_current_bounds",
              "line": 290
            },
            {
              "name": "current_bounds.items",
              "line": 291
            },
            {
              "name": "hasattr",
              "line": 217
            },
            {
              "name": "logger.info",
              "line": 234
            },
            {
              "name": "logger.info",
              "line": 236
            },
            {
              "name": "hasattr",
              "line": 239
            },
            {
              "name": "logger.info",
              "line": 252
            },
            {
              "name": "self.pattern_mediator.end_epoch",
              "line": 253
            },
            {
              "name": "self.pattern_mediator.get_pattern_risks",
              "line": 254
            },
            {
              "name": "self.pattern_mediator.get_pattern_accuracies",
              "line": 255
            },
            {
              "name": "logger.info",
              "line": 257
            },
            {
              "name": "logger.info",
              "line": 258
            },
            {
              "name": "logger.info",
              "line": 284
            },
            {
              "name": "max",
              "line": 303
            },
            {
              "name": "....append",
              "line": 338
            },
            {
              "name": "len",
              "line": 215
            },
            {
              "name": "logger.info",
              "line": 219
            },
            {
              "name": "logger.info",
              "line": 241
            },
            {
              "name": "min",
              "line": 323
            },
            {
              "name": "abs",
              "line": 341
            },
            {
              "name": "logger.info",
              "line": 342
            },
            {
              "name": "logger.info",
              "line": 224
            },
            {
              "name": "logger.info",
              "line": 228
            },
            {
              "name": "logger.info",
              "line": 243
            },
            {
              "name": "bool",
              "line": 247
            },
            {
              "name": "bool",
              "line": 248
            },
            {
              "name": "bool",
              "line": 266
            },
            {
              "name": "bool",
              "line": 267
            },
            {
              "name": "max",
              "line": 327
            },
            {
              "name": "bool",
              "line": 257
            },
            {
              "name": "bool",
              "line": 258
            },
            {
              "name": "len",
              "line": 284
            },
            {
              "name": "list",
              "line": 277
            },
            {
              "name": "len",
              "line": 224
            },
            {
              "name": "len",
              "line": 228
            },
            {
              "name": "list",
              "line": 276
            },
            {
              "name": "pattern_accuracies.keys",
              "line": 277
            },
            {
              "name": "len",
              "line": 275
            },
            {
              "name": "pattern_risks.keys",
              "line": 276
            },
            {
              "name": "bool",
              "line": 273
            },
            {
              "name": "len",
              "line": 274
            },
            {
              "name": "bool",
              "line": 272
            }
          ],
          "docstring": "\n        Update learning rates based on risk/accuracy ratios at Fibonacci check points.\n        \n        This method is called during the step() function at Fibonacci-scheduled \n        intervals to adjust learning rates based on pattern-specific risk/accuracy ratios.\n        ",
          "code_snippet": "            return False\n    \n    def update_learning_rates_by_pattern(self):\n        \"\"\"\n        Update learning rates based on risk/accuracy ratios at Fibonacci check points.\n        \n        This method is called during the step() function at Fibonacci-scheduled \n        intervals to adjust learning rates based on pattern-specific risk/accuracy ratios.\n        \"\"\"\n        # Get the current epoch number\n        epoch = self.current_epoch\n        \n        # Only log detailed progress information when epoch changes to avoid spam\n        if self.epoch_changed:\n            step_count = getattr(self, 'step_counter', 0) \n            logger.info(f\"LR Update Check - Epoch {epoch+1}, Step {step_count}, Current LR: {self.param_groups[0]['lr']:.6f}\")\n        \n        # Only perform check at Fibonacci check points\n        is_checkpoint = self._is_fibonacci_check_point(epoch)\n        \n        # Log every epoch for diagnostics, but only when epoch changes\n        if self.epoch_changed and is_checkpoint:\n            logger.info(f\"Epoch {epoch+1} - Fibonacci checkpoint detected\")\n            self.epoch_changed = False  # Reset flag after logging\n            logger.info(f\"*** FIBONACCI CHECKPOINT ACTIVE ***\")\n            logger.info(f\"Performing learning rate update at epoch {epoch+1}\")\n        elif self.epoch_changed:\n            # Just reset the flag without logging anything to reduce spam\n            self.epoch_changed = False\n            \n        if not is_checkpoint:\n            return\n        \n        # Get pattern metrics\n        if hasattr(self, 'pattern_mediator'):\n            # Try to get pattern metrics from mediator\n            pattern_risks = self.pattern_mediator.get_pattern_risks(epoch)\n            pattern_accuracies = self.pattern_mediator.get_pattern_accuracies(epoch)\n            \n            # If data is not available, try getting from our cache if available\n            current_interval = self.fibonacci_intervals[epoch] if epoch < len(self.fibonacci_intervals) else -1\n            \n            if (not pattern_risks or not pattern_accuracies) and hasattr(self, '_pattern_data_cache'):\n                if current_interval in self._pattern_data_cache:\n                    logger.info(f\"[LR DEBUG] Using cached pattern data for interval {current_interval}\")\n                    cached_data = self._pattern_data_cache[current_interval]\n                    \n                    if not pattern_accuracies and cached_data['accuracies']:\n                        pattern_accuracies = cached_data['accuracies']\n                        logger.info(f\"[LR DEBUG] Retrieved accuracies from cache for {len(pattern_accuracies)} patterns\")\n                        \n                    if not pattern_risks and cached_data['risks']:\n                        pattern_risks = cached_data['risks']\n                        logger.info(f\"[LR DEBUG] Retrieved risks from cache for {len(pattern_risks)} patterns\")\n            \n            # Enhanced debugging for pattern mediator data\n            # Log actual pattern data for debugging\n            logger.info(f\"[LR CRITICAL DEBUG] Pattern data from mediator: \")\n            if pattern_risks:\n                logger.info(f\"  Pattern risks: {pattern_risks}\")\n            if pattern_accuracies:\n                logger.info(f\"  Pattern accuracies: {pattern_accuracies}\")\n            \n            # Check pattern map for iris mode\n            if hasattr(self, '_complete_pattern_map') and self._complete_pattern_map:\n                if 'weighted_distribution' in self._complete_pattern_map:\n                    logger.info(f\"[LR CRITICAL DEBUG] Weighted distribution found in pattern map: {self._complete_pattern_map['weighted_distribution']}\")\n                elif 'weighted_scores' in self._complete_pattern_map:\n                    logger.info(f\"[LR CRITICAL DEBUG] Weighted scores found in pattern map: {self._complete_pattern_map['weighted_scores']}\")\n\n            # Add debugging for pattern data availability\n            logger.info(f\"[LR DEBUG] Using mediator data for checkpoint {self.current_epoch+1}\")\n            logger.info(f\"[LR DEBUG] Checkpoint {self.current_epoch+1}: Pattern risks available: {bool(pattern_risks)}\")\n            logger.info(f\"[LR DEBUG] Checkpoint {self.current_epoch+1}: Pattern accuracies available: {bool(pattern_accuracies)}\")\n            \n            # If still missing data, try forcing recalculation\n            if not pattern_risks or not pattern_accuracies:\n                logger.info(f\"[LR DEBUG] Missing pattern data, trying to force mediator recalculation\")\n                self.pattern_mediator.end_epoch(epoch-1)  # Process previous epoch data\n                pattern_risks = self.pattern_mediator.get_pattern_risks(epoch-1, force_recalculate=True)\n                pattern_accuracies = self.pattern_mediator.get_pattern_accuracies(epoch-1, force_recalculate=True)\n                \n                logger.info(f\"[LR DEBUG] After forced recalculation: Pattern risks available: {bool(pattern_risks)}\")\n                logger.info(f\"[LR DEBUG] After forced recalculation: Pattern accuracies available: {bool(pattern_accuracies)}\")\n        else:\n            # Fallback to trackers\n            pattern_risks = self.ratio_tracker.get_pattern_risks()\n            pattern_accuracies = self.pattern_tracker.get_pattern_accuracies()\n            \n            # Add debugging for pattern data availability\n            logger.info(f\"[LR DEBUG] Using tracker data for checkpoint {self.current_epoch+1}\")\n            logger.info(f\"[LR DEBUG] Checkpoint {self.current_epoch+1}: Pattern risks available: {bool(pattern_risks)}\")\n            logger.info(f\"[LR DEBUG] Checkpoint {self.current_epoch+1}: Pattern accuracies available: {bool(pattern_accuracies)}\")\n        \n        # Highly structured diagnostic output for easy parsing\n        logger.debug(f\"<CLAUDE:DIAGNOSTIC:LR_CHECKPOINT>\\n\" +\n                    f\"epoch: {self.current_epoch+1}\\n\" +\n                    f\"has_pattern_risks: {bool(pattern_risks)}\\n\" +\n                    f\"has_pattern_accuracies: {bool(pattern_accuracies)}\\n\" +\n                    f\"pattern_risk_count: {len(pattern_risks) if pattern_risks else 0}\\n\" +\n                    f\"pattern_acc_count: {len(pattern_accuracies) if pattern_accuracies else 0}\\n\" +\n                    f\"pattern_risk_keys: {list(pattern_risks.keys()) if pattern_risks else []}\\n\" +\n                    f\"pattern_acc_keys: {list(pattern_accuracies.keys()) if pattern_accuracies else []}\\n\" +\n                    f\"</CLAUDE:DIAGNOSTIC:LR_CHECKPOINT>\")\n        \n        if not pattern_risks or not pattern_accuracies:\n            logger.warning(f\"[LR DEBUG] Missing pattern data at Fibonacci checkpoint {self.current_epoch+1}\")\n            # Add additional debugging info\n            if hasattr(self.pattern_tracker, 'data'):\n                logger.info(f\"[LR DEBUG] Pattern tracker data size: {len(self.pattern_tracker.data)}\")\n        \n        # Get pattern equilibrium bounds if available\n        min_bounds = {}\n        max_bounds = {}\n        if hasattr(self, 'equilibrium_tracker') and self.use_equilibrium_bounds:\n            current_bounds = self.equilibrium_tracker.get_current_bounds()\n            for pattern_type, bounds in current_bounds.items():\n                min_bounds[pattern_type] = bounds['min']\n                max_bounds[pattern_type] = bounds['max']\n        \n        # Only adjust learning rates for patterns with both risk and accuracy data\n        for pattern_type, risk in pattern_risks.items():\n            if pattern_type not in pattern_accuracies:\n                continue\n                \n            accuracy = pattern_accuracies[pattern_type]\n            \n            # Calculate risk/accuracy ratio\n            ratio = risk / max(accuracy, 0.1)  # Prevent division by zero\n            \n            # Check if pattern is within equilibrium bounds (if available)\n            within_bounds = True\n            if pattern_type in min_bounds and pattern_type in max_bounds:\n                min_bound = min_bounds[pattern_type]\n                max_bound = max_bounds[pattern_type]\n                within_bounds = (min_bound <= accuracy <= max_bound)\n            \n            # Only apply adjustments if pattern is within equilibrium bounds\n            if within_bounds:\n                # Always use iris mode adjustment ranges\n                # Start with base adjustment range\n                adjustment_min = 0.92 * 0.95  # Base 0.92 with iris 5% reduction = 0.874\n                adjustment_max = 1.08 * 1.05  # Base 1.08 with iris 5% increase = 1.134\n                \n                # Calculate learning rate adjustment factor\n                if ratio > 1.2:  # High risk relative to accuracy - struggling pattern\n                    # Reduce LR reduction (or slightly increase LR) to encourage exploration\n                    # Scale based on ratio deviation from 1.0 and clamp to max adjustment\n                    adjustment = min(adjustment_max, 1.0 + (ratio - 1.0) * self.lr_risk_sensitivity)\n                elif ratio < 0.8:  # Low risk relative to accuracy - well-learned pattern\n                    # Increase LR reduction to fine-tune well-learned patterns\n                    # Scale based on ratio deviation from 1.0 and clamp to min adjustment\n                    adjustment = max(adjustment_min, 1.0 - (1.0 - ratio) * self.lr_risk_sensitivity)\n                else:\n                    # Balanced ratio - no special adjustment\n                    adjustment = 1.0\n                \n                # Store the adjustment factor for this pattern\n                self.pattern_lr_multipliers[pattern_type] = adjustment\n                \n                # Store in history\n                if pattern_type not in self.pattern_lr_history:\n                    self.pattern_lr_history[pattern_type] = []\n                self.pattern_lr_history[pattern_type].append((self.current_epoch, adjustment))\n                \n                # Log significant adjustments\n                if abs(adjustment - 1.0) > 0.05:\n                    logger.info(f\"Pattern {pattern_type} learning rate adjustment: {adjustment:.3f} (ratio: {ratio:.3f})\")\n    \n    def apply_pattern_weighted_lr(self):\n        \"\"\"\n        Apply pattern-weighted learning rate adjustments to parameters."
        },
        "apply_pattern_weighted_lr": {
          "start_line": 344,
          "end_line": 431,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.pattern_lr_multipliers.items",
              "line": 380
            },
            {
              "name": "hasattr",
              "line": 423
            },
            {
              "name": "logger.info",
              "line": 429
            },
            {
              "name": "hasattr",
              "line": 365
            },
            {
              "name": "logger.debug",
              "line": 367
            },
            {
              "name": "logger.debug",
              "line": 374
            },
            {
              "name": "self.lr_history.append",
              "line": 424
            },
            {
              "name": "hasattr",
              "line": 368
            },
            {
              "name": "logger.debug",
              "line": 370
            },
            {
              "name": "logger.debug",
              "line": 376
            },
            {
              "name": "logger.debug",
              "line": 378
            },
            {
              "name": "....get",
              "line": 388
            },
            {
              "name": "logger.debug",
              "line": 389
            },
            {
              "name": "len",
              "line": 367
            },
            {
              "name": "list",
              "line": 374
            },
            {
              "name": "....get",
              "line": 392
            },
            {
              "name": "logger.debug",
              "line": 393
            },
            {
              "name": "pattern_map_to_use.keys",
              "line": 367
            },
            {
              "name": "len",
              "line": 370
            },
            {
              "name": "pattern_map_to_use.keys",
              "line": 374
            },
            {
              "name": "len",
              "line": 376
            },
            {
              "name": "len",
              "line": 378
            },
            {
              "name": "sum",
              "line": 397
            },
            {
              "name": "logger.debug",
              "line": 403
            },
            {
              "name": "pattern_map_to_use.keys",
              "line": 370
            },
            {
              "name": "....values",
              "line": 397
            },
            {
              "name": "....get",
              "line": 399
            },
            {
              "name": "logger.debug",
              "line": 401
            }
          ],
          "docstring": "\n        Apply pattern-weighted learning rate adjustments to parameters.\n        \n        This method calculates a weighted average learning rate adjustment based\n        on the pattern-specific multipliers and applies it to the global learning rate.\n        ",
          "code_snippet": "                    logger.info(f\"Pattern {pattern_type} learning rate adjustment: {adjustment:.3f} (ratio: {ratio:.3f})\")\n    \n    def apply_pattern_weighted_lr(self):\n        \"\"\"\n        Apply pattern-weighted learning rate adjustments to parameters.\n        \n        This method calculates a weighted average learning rate adjustment based\n        on the pattern-specific multipliers and applies it to the global learning rate.\n        \"\"\"\n        # Skip if no pattern multipliers are available or if we've already applied for this epoch\n        if not self.pattern_lr_multipliers:\n            return\n            \n        # Skip if we haven't hit a Fibonacci checkpoint yet\n        if self.current_epoch != self.last_lr_update_epoch:\n            return\n        \n        # Calculate weighted average adjustment\n        total_weight = 0.0\n        weighted_adjustment = 0.0\n        \n        # First try the complete pattern map if available\n        pattern_map_to_use = None\n        if hasattr(self, '_complete_pattern_map') and self._complete_pattern_map:\n            pattern_map_to_use = self._complete_pattern_map\n            logger.debug(f\"[LR DEBUG] Using complete pattern map with {len(pattern_map_to_use.keys())} keys\")\n        elif hasattr(self, '_original_pattern_map') and self._original_pattern_map:\n            pattern_map_to_use = self._original_pattern_map\n            logger.debug(f\"[LR DEBUG] Using original pattern map with {len(pattern_map_to_use.keys())} keys\")\n        \n        # Debug log critical pattern map info\n        if pattern_map_to_use:\n            logger.debug(f\"[LR DEBUG] Pattern map keys: {list(pattern_map_to_use.keys())}\")\n            if 'weighted_distribution' in pattern_map_to_use:\n                logger.debug(f\"[LR DEBUG] Pattern map has weighted_distribution with {len(pattern_map_to_use['weighted_distribution'])} patterns\")\n            if 'pattern_distribution' in pattern_map_to_use:\n                logger.debug(f\"[LR DEBUG] Pattern map has pattern_distribution with {len(pattern_map_to_use['pattern_distribution'])} patterns\")\n        \n        for pattern_type, multiplier in self.pattern_lr_multipliers.items():\n            # Use pattern prevalence from pattern map as weight if available\n            weight = 1.0\n            \n            # Try multiple sources for pattern weight data\n            if pattern_map_to_use:\n                # First try to use weighted_distribution (floating point values)\n                if 'weighted_distribution' in pattern_map_to_use:\n                    weight = pattern_map_to_use['weighted_distribution'].get(pattern_type, 1.0)\n                    logger.debug(f\"[LR DEBUG] Using weighted_distribution for {pattern_type}: {weight}\")\n                # Next try weighted_scores which is an alternative name for weighted_distribution\n                elif 'weighted_scores' in pattern_map_to_use:\n                    weight = pattern_map_to_use['weighted_scores'].get(pattern_type, 1.0)\n                    logger.debug(f\"[LR DEBUG] Using weighted_scores for {pattern_type}: {weight}\")\n                # Fall back to pattern_distribution if weighted_distribution is not available\n                elif 'pattern_distribution' in pattern_map_to_use:\n                    # Convert integer count to proportion\n                    total_patterns = sum(pattern_map_to_use['pattern_distribution'].values())\n                    if total_patterns > 0:\n                        count = pattern_map_to_use['pattern_distribution'].get(pattern_type, 0)\n                        weight = count / total_patterns\n                        logger.debug(f\"[LR DEBUG] Using pattern_distribution for {pattern_type}: {count}/{total_patterns} = {weight}\")\n                else:\n                    logger.debug(f\"[LR DEBUG] No weight source found for {pattern_type}, using default weight=1.0\")\n            \n            weighted_adjustment += multiplier * weight\n            total_weight += weight\n        \n        # Calculate final adjustment factor\n        if total_weight > 0:\n            final_adjustment = weighted_adjustment / total_weight\n        else:\n            final_adjustment = 1.0\n        \n        # Apply the adjustment to the global learning rate\n        current_lr = self.param_groups[0]['lr']\n        new_lr = current_lr * final_adjustment\n        \n        # Apply the new learning rate\n        for param_group in self.param_groups:\n            param_group['lr'] = new_lr\n        \n        # Store in history\n        if hasattr(self, 'lr_history'):\n            self.lr_history.append(new_lr)\n        else:\n            self.lr_history = [new_lr]\n        \n        # Always log learning rate changes for better diagnostics\n        logger.info(f\"[LR APPLIED] Learning rate adjusted: {current_lr:.6f} -> {new_lr:.6f} (factor: {final_adjustment:.4f})\")\n    \n    def step(self, closure=None, pattern_states=None):\n        \"\"\"\n        Perform a single optimization step with risk/accuracy-based LR adjustments."
        },
        "step": {
          "start_line": 431,
          "end_line": 491,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "closure"
            },
            {
              "name": "pattern_states"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.update_learning_rates_by_pattern",
              "line": 452
            },
            {
              "name": "self.apply_pattern_weighted_lr",
              "line": 455
            },
            {
              "name": "....step",
              "line": 458
            },
            {
              "name": "hasattr",
              "line": 443
            },
            {
              "name": "logger.info",
              "line": 449
            },
            {
              "name": "hasattr",
              "line": 461
            },
            {
              "name": "hasattr",
              "line": 461
            },
            {
              "name": "self.pattern_mediator.get_pattern_risks",
              "line": 463
            },
            {
              "name": "self.pattern_mediator.get_pattern_accuracies",
              "line": 464
            },
            {
              "name": "super",
              "line": 458
            },
            {
              "name": "hasattr",
              "line": 467
            },
            {
              "name": "hasattr",
              "line": 467
            },
            {
              "name": "self.ratio_tracker.update_pattern_accuracies",
              "line": 468
            },
            {
              "name": "logger.debug",
              "line": 469
            },
            {
              "name": "hasattr",
              "line": 471
            },
            {
              "name": "hasattr",
              "line": 471
            },
            {
              "name": "self.ratio_tracker.update_pattern_risks",
              "line": 472
            },
            {
              "name": "logger.debug",
              "line": 473
            },
            {
              "name": "hasattr",
              "line": 476
            },
            {
              "name": "logger.debug",
              "line": 487
            },
            {
              "name": "len",
              "line": 480
            },
            {
              "name": "len",
              "line": 469
            },
            {
              "name": "len",
              "line": 473
            },
            {
              "name": "len",
              "line": 487
            }
          ],
          "docstring": "\n        Perform a single optimization step with risk/accuracy-based LR adjustments.\n        \n        Args:\n            closure: A closure that reevaluates the model and returns the loss\n            pattern_states: Additional pattern states for pattern-aware optimization\n            \n        Returns:\n            float: Loss value\n        ",
          "code_snippet": "        logger.info(f\"[LR APPLIED] Learning rate adjusted: {current_lr:.6f} -> {new_lr:.6f} (factor: {final_adjustment:.4f})\")\n    \n    def step(self, closure=None, pattern_states=None):\n        \"\"\"\n        Perform a single optimization step with risk/accuracy-based LR adjustments.\n        \n        Args:\n            closure: A closure that reevaluates the model and returns the loss\n            pattern_states: Additional pattern states for pattern-aware optimization\n            \n        Returns:\n            float: Loss value\n        \"\"\"\n        # Track step count for more detailed logging but don't log every step\n        if not hasattr(self, 'step_counter'):\n            self.step_counter = 0\n        self.step_counter += 1\n        \n        # Only log every 1000 steps to reduce output spam\n        if self.step_counter % 1000 == 0:\n            logger.info(f\"Step {self.step_counter} - Epoch {self.current_epoch+1} - LR: {self.param_groups[0]['lr']:.6f}\")\n        \n        # Update learning rates based on risk/accuracy ratios at Fibonacci check points\n        self.update_learning_rates_by_pattern()\n        \n        # Apply the pattern-weighted learning rate adjustments\n        self.apply_pattern_weighted_lr()\n        \n        # Call the parent step method\n        loss = super().step(closure=closure, pattern_states=pattern_states)\n        \n        # Update pattern data from mediator if available\n        if hasattr(self, 'pattern_mediator') and hasattr(self.pattern_mediator, 'get_pattern_risks'):\n            # Get latest pattern data (force recalculation at this stage)\n            latest_risks = self.pattern_mediator.get_pattern_risks(self.current_epoch, force_recalculate=True)\n            latest_accuracies = self.pattern_mediator.get_pattern_accuracies(self.current_epoch, force_recalculate=True)\n            \n            # Transfer to ratio tracker if needed\n            if latest_accuracies and hasattr(self, 'ratio_tracker') and hasattr(self.ratio_tracker, 'update_pattern_accuracies'):\n                self.ratio_tracker.update_pattern_accuracies(latest_accuracies)\n                logger.debug(f\"Updated ratio tracker with {len(latest_accuracies)} pattern accuracies\")\n            \n            if latest_risks and hasattr(self, 'ratio_tracker') and hasattr(self.ratio_tracker, 'update_pattern_risks'):\n                self.ratio_tracker.update_pattern_risks(latest_risks)\n                logger.debug(f\"Updated ratio tracker with {len(latest_risks)} pattern risks\")\n                \n            # Cache the data for future use\n            if not hasattr(self, '_pattern_data_cache'):\n                self._pattern_data_cache = {}\n                \n            # Cache by current fibonacci interval\n            current_interval = self.fibonacci_intervals[self.current_epoch] if self.current_epoch < len(self.fibonacci_intervals) else -1\n            self._pattern_data_cache[current_interval] = {\n                'accuracies': latest_accuracies,\n                'risks': latest_risks\n            }\n            \n            if latest_accuracies and latest_risks:\n                logger.debug(f\"Cached pattern data for interval {current_interval} with {len(latest_accuracies)} patterns\")\n        \n        return loss\n    \n    def update_accuracy_metrics_with_epoch(self, train_acc, test_acc, epoch, total_epochs):\n        \"\"\"\n        Update accuracy metrics with epoch information."
        },
        "update_accuracy_metrics_with_epoch": {
          "start_line": 491,
          "end_line": 570,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "train_acc"
            },
            {
              "name": "test_acc"
            },
            {
              "name": "epoch"
            },
            {
              "name": "total_epochs"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logger.info",
              "line": 508
            },
            {
              "name": "logger.info",
              "line": 516
            },
            {
              "name": "....update_accuracy_metrics_with_epoch",
              "line": 568
            },
            {
              "name": "hasattr",
              "line": 520
            },
            {
              "name": "hasattr",
              "line": 520
            },
            {
              "name": "self.pattern_mediator.end_epoch",
              "line": 522
            },
            {
              "name": "self.pattern_mediator.get_pattern_accuracies",
              "line": 525
            },
            {
              "name": "self.pattern_mediator.get_pattern_risks",
              "line": 526
            },
            {
              "name": "logger.debug",
              "line": 528
            },
            {
              "name": "hasattr",
              "line": 531
            },
            {
              "name": "logger.debug",
              "line": 559
            },
            {
              "name": "self._pattern_data_cache.items",
              "line": 562
            },
            {
              "name": "hasattr",
              "line": 541
            },
            {
              "name": "pattern_accs.items",
              "line": 543
            },
            {
              "name": "logger.debug",
              "line": 546
            },
            {
              "name": "hasattr",
              "line": 549
            },
            {
              "name": "logger.debug",
              "line": 563
            },
            {
              "name": "logger.debug",
              "line": 565
            },
            {
              "name": "super",
              "line": 568
            },
            {
              "name": "hasattr",
              "line": 532
            },
            {
              "name": "self.ratio_tracker.update_pattern_accuracies",
              "line": 533
            },
            {
              "name": "logger.debug",
              "line": 534
            },
            {
              "name": "hasattr",
              "line": 536
            },
            {
              "name": "self.ratio_tracker.update_pattern_risks",
              "line": 537
            },
            {
              "name": "logger.debug",
              "line": 538
            },
            {
              "name": "hasattr",
              "line": 544
            },
            {
              "name": "len",
              "line": 553
            },
            {
              "name": "list",
              "line": 564
            },
            {
              "name": "bool",
              "line": 528
            },
            {
              "name": "bool",
              "line": 528
            },
            {
              "name": "self.pattern_tracker.update_accuracy",
              "line": 545
            },
            {
              "name": "....keys",
              "line": 564
            },
            {
              "name": "bool",
              "line": 563
            },
            {
              "name": "bool",
              "line": 563
            },
            {
              "name": "list",
              "line": 534
            },
            {
              "name": "list",
              "line": 538
            },
            {
              "name": "pattern_accs.keys",
              "line": 534
            },
            {
              "name": "pattern_risks.keys",
              "line": 538
            }
          ],
          "docstring": "\n        Update accuracy metrics with epoch information.\n        \n        This method is called at the end of each epoch with complete metrics.\n        \n        Args:\n            train_acc: Training accuracy (0-100 scale)\n            test_acc: Test accuracy (0-100 scale)\n            epoch: Current epoch number\n            total_epochs: Total number of epochs\n        ",
          "code_snippet": "        return loss\n    \n    def update_accuracy_metrics_with_epoch(self, train_acc, test_acc, epoch, total_epochs):\n        \"\"\"\n        Update accuracy metrics with epoch information.\n        \n        This method is called at the end of each epoch with complete metrics.\n        \n        Args:\n            train_acc: Training accuracy (0-100 scale)\n            test_acc: Test accuracy (0-100 scale)\n            epoch: Current epoch number\n            total_epochs: Total number of epochs\n        \"\"\"\n        # Store the latest accuracy values\n        self.latest_train_acc = train_acc\n        self.latest_val_acc = test_acc\n        \n        # Make sure to properly update the epoch counter to match trainer's counter\n        logger.info(f\"[EPOCH UPDATE] Received epoch update: {self.current_epoch} \u2192 {epoch}\")\n        \n        # CRITICAL: Update the current epoch to be in sync with the trainer\n        self.current_epoch = epoch\n        self.epoch_changed = True\n        \n        # Log accuracy metrics for diagnostic purposes\n        train_val_ratio = train_acc / test_acc if test_acc > 0 else 0\n        logger.info(f\"[EPOCH UPDATE] Training metrics - Train: {train_acc:.2f}%, Val: {test_acc:.2f}%, Ratio: {train_val_ratio:.4f}\")\n        \n        # Update pattern metrics from mediator if available\n        # This ensures pattern data flows to both pattern_tracker and ratio_tracker\n        if hasattr(self, 'pattern_mediator') and hasattr(self.pattern_mediator, 'get_pattern_accuracies'):\n            # Force mediator to process epoch data if not already done\n            self.pattern_mediator.end_epoch(epoch-1)  # Process previous epoch data\n            \n            # Get pattern metrics from mediator with forced recalculation\n            pattern_accs = self.pattern_mediator.get_pattern_accuracies(epoch-1, force_recalculate=True)\n            pattern_risks = self.pattern_mediator.get_pattern_risks(epoch-1, force_recalculate=True)\n            \n            logger.debug(f\"[METRICS FLOW] Got pattern data from mediator: accs={bool(pattern_accs)}, risks={bool(pattern_risks)}\")\n            \n            # Update ratio tracker\n            if hasattr(self, 'ratio_tracker'):\n                if pattern_accs and hasattr(self.ratio_tracker, 'update_pattern_accuracies'):\n                    self.ratio_tracker.update_pattern_accuracies(pattern_accs)\n                    logger.debug(f\"[METRICS FLOW] Updated ratio tracker accuracies, patterns: {list(pattern_accs.keys())}\")\n                \n                if pattern_risks and hasattr(self.ratio_tracker, 'update_pattern_risks'):\n                    self.ratio_tracker.update_pattern_risks(pattern_risks)\n                    logger.debug(f\"[METRICS FLOW] Updated ratio tracker risks, patterns: {list(pattern_risks.keys())}\")\n                \n            # Update pattern tracker if available\n            if pattern_accs and hasattr(self, 'pattern_tracker'):\n                # For each pattern, update the pattern tracker\n                for pattern_type, accuracy in pattern_accs.items():\n                    if hasattr(self.pattern_tracker, 'update_accuracy'):\n                        self.pattern_tracker.update_accuracy(pattern_type, accuracy)\n                logger.debug(f\"[METRICS FLOW] Updated pattern tracker with pattern data from mediator\")\n                \n            # Store the latest data in our own cache for checkpoint access\n            if not hasattr(self, '_pattern_data_cache'):\n                self._pattern_data_cache = {}\n            \n            # Cache the data for the current epoch, indexed by fibonacci checkpoint\n            current_interval = self.fibonacci_intervals[epoch] if epoch < len(self.fibonacci_intervals) else -1\n            self._pattern_data_cache[current_interval] = {\n                'accuracies': pattern_accs,\n                'risks': pattern_risks\n            }\n            \n            logger.debug(f\"[METRICS FLOW] Cached pattern data for interval {current_interval}\")\n                \n            # Debug the cached data at debug level only\n            for interval, data in self._pattern_data_cache.items():\n                logger.debug(f\"[METRICS FLOW] Cache has data for interval {interval}: accs={bool(data['accuracies'])}, risks={bool(data['risks'])}\")\n                pattern_types = list(data['accuracies'].keys()) if data['accuracies'] else []\n                logger.debug(f\"[METRICS FLOW] Pattern types in cache for interval {interval}: {pattern_types}\")\n        \n        # Call parent method\n        super().update_accuracy_metrics_with_epoch(train_acc, test_acc, epoch, total_epochs)\n    \n    def get_pattern_lr_multipliers(self):\n        \"\"\"\n        Get the current pattern-specific learning rate multipliers."
        },
        "get_pattern_lr_multipliers": {
          "start_line": 570,
          "end_line": 579,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [],
          "docstring": "\n        Get the current pattern-specific learning rate multipliers.\n        \n        Returns:\n            dict: Pattern-specific learning rate multipliers\n        ",
          "code_snippet": "        super().update_accuracy_metrics_with_epoch(train_acc, test_acc, epoch, total_epochs)\n    \n    def get_pattern_lr_multipliers(self):\n        \"\"\"\n        Get the current pattern-specific learning rate multipliers.\n        \n        Returns:\n            dict: Pattern-specific learning rate multipliers\n        \"\"\"\n        return self.pattern_lr_multipliers\n    \n    def get_pattern_lr_history(self):\n        \"\"\"\n        Get the history of pattern-specific learning rate adjustments."
        },
        "get_pattern_lr_history": {
          "start_line": 579,
          "end_line": 588,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [],
          "docstring": "\n        Get the history of pattern-specific learning rate adjustments.\n        \n        Returns:\n            dict: Pattern-specific learning rate adjustment history\n        ",
          "code_snippet": "        return self.pattern_lr_multipliers\n    \n    def get_pattern_lr_history(self):\n        \"\"\"\n        Get the history of pattern-specific learning rate adjustments.\n        \n        Returns:\n            dict: Pattern-specific learning rate adjustment history\n        \"\"\"\n        return self.pattern_lr_history\n        \n    def set_pattern_mediator(self, mediator):\n        \"\"\"\n        Set the pattern mediator for this optimizer."
        },
        "set_pattern_mediator": {
          "start_line": 588,
          "end_line": 601,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "mediator"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logger.info",
              "line": 599
            }
          ],
          "docstring": "\n        Set the pattern mediator for this optimizer.\n        \n        The pattern mediator provides reliable pattern recognition data\n        that is collected regardless of the trainer's pattern tracking settings.\n        \n        Args:\n            mediator: Pattern mediator instance\n        ",
          "code_snippet": "        return self.pattern_lr_history\n        \n    def set_pattern_mediator(self, mediator):\n        \"\"\"\n        Set the pattern mediator for this optimizer.\n        \n        The pattern mediator provides reliable pattern recognition data\n        that is collected regardless of the trainer's pattern tracking settings.\n        \n        Args:\n            mediator: Pattern mediator instance\n        \"\"\"\n        self.pattern_mediator = mediator\n        logger.info(\"Pattern mediator set for RiskAwareEVEOptimizer\")"
        }
      },
      "class_variables": [],
      "bases": [
        "EVEUnifiedRatio"
      ],
      "docstring": "\n    EVE optimizer with pattern-specific learning rate adjustments based on risk/accuracy ratios.\n    \n    This optimizer extends EVEUnifiedRatio to incorporate risk/accuracy ratio analysis\n    for learning rate adjustments at Fibonacci-scheduled intervals.\n    \n    Attributes:\n        fibonacci_intervals: List of check intervals based on Fibonacci sequence\n        current_epoch: Current training epoch\n        pattern_lr_multipliers: Dictionary of pattern-specific learning rate multipliers\n    "
    }
  },
  "functions": {},
  "constants": {}
}