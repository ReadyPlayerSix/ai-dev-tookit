{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\isekaizen\\pattern\\pre_augmentation.py",
  "imports": [
    {
      "name": "torch",
      "line": 10
    },
    {
      "name": "logging",
      "line": 11
    },
    {
      "name": "numpy",
      "line": 12
    },
    {
      "name": "random",
      "line": 13
    },
    {
      "name": "typing.Dict",
      "line": 14
    },
    {
      "name": "typing.List",
      "line": 14
    },
    {
      "name": "typing.Tuple",
      "line": 14
    },
    {
      "name": "typing.Optional",
      "line": 14
    },
    {
      "name": "typing.Any",
      "line": 14
    },
    {
      "name": "typing.Set",
      "line": 14
    },
    {
      "name": "torch.utils.data.Dataset",
      "line": 15
    },
    {
      "name": "torch.utils.data.ConcatDataset",
      "line": 15
    },
    {
      "name": "json",
      "line": 568
    },
    {
      "name": "isekaizen.pattern.detection.PatternRecognitionService",
      "line": 500
    },
    {
      "name": "isekaizen.pattern.detection.PatternRecognitionService",
      "line": 405
    }
  ],
  "classes": {
    "AugmentedDataset": {
      "start_line": 19,
      "end_line": 99,
      "methods": {
        "__init__": {
          "start_line": 27,
          "end_line": 56,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "original_dataset"
            },
            {
              "name": "augmented_samples"
            },
            {
              "name": "augmentation_metadata"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "len",
              "line": 40
            },
            {
              "name": "len",
              "line": 41
            },
            {
              "name": "logger.info",
              "line": 49
            },
            {
              "name": "logger.info",
              "line": 53
            },
            {
              "name": "len",
              "line": 54
            }
          ],
          "docstring": "\n        Initialize with original dataset and augmented samples.\n        \n        Args:\n            original_dataset: The original dataset\n            augmented_samples: List of augmented (input, target) pairs\n            augmentation_metadata: Optional metadata about augmentations\n        ",
          "code_snippet": "    \"\"\"\n    \n    def __init__(self, original_dataset, augmented_samples, augmentation_metadata=None):\n        \"\"\"\n        Initialize with original dataset and augmented samples.\n        \n        Args:\n            original_dataset: The original dataset\n            augmented_samples: List of augmented (input, target) pairs\n            augmentation_metadata: Optional metadata about augmentations\n        \"\"\"\n        self.original_dataset = original_dataset\n        self.augmented_samples = augmented_samples or []\n        self.metadata = augmentation_metadata or {}\n        \n        self.original_len = len(original_dataset)\n        self.total_len = self.original_len + len(self.augmented_samples)\n        \n        # Flag to identify pre-augmented datasets\n        self.is_pre_augmented = True\n        \n        # If we have metadata but no augmented samples, check if metadata has augmented_count\n        # This handles the case where we're reusing existing augmentation metadata\n        if not self.augmented_samples and 'augmented_count' in self.metadata:\n            logger.info(f\"Using existing augmentation metadata with {self.metadata['augmented_count']} virtual augmented samples\")\n            self.virtual_augmentation = True\n        else:\n            self.virtual_augmentation = False\n            logger.info(f\"Created pre-augmented dataset: {self.original_len} original + \"\n                       f\"{len(self.augmented_samples)} augmented = {self.total_len} total\")\n    \n    def __len__(self):\n        if hasattr(self, 'virtual_augmentation') and self.virtual_augmentation:\n            # If using virtual augmentation, use the augmented_count from metadata"
        },
        "__len__": {
          "start_line": 56,
          "end_line": 62,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "hasattr",
              "line": 57
            },
            {
              "name": "self.metadata.get",
              "line": 59
            }
          ],
          "code_snippet": "                       f\"{len(self.augmented_samples)} augmented = {self.total_len} total\")\n    \n    def __len__(self):\n        if hasattr(self, 'virtual_augmentation') and self.virtual_augmentation:\n            # If using virtual augmentation, use the augmented_count from metadata\n            return self.original_len + self.metadata.get('augmented_count', 0)\n        return self.total_len\n    \n    def __getitem__(self, idx):\n        if idx < self.original_len:\n            # Return original sample"
        },
        "__getitem__": {
          "start_line": 62,
          "end_line": 79,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "idx"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "hasattr",
              "line": 68
            }
          ],
          "code_snippet": "        return self.total_len\n    \n    def __getitem__(self, idx):\n        if idx < self.original_len:\n            # Return original sample\n            return self.original_dataset[idx]\n        else:\n            # Check if we're using virtual augmentation\n            if hasattr(self, 'virtual_augmentation') and self.virtual_augmentation:\n                # For virtual augmentation, just return an original sample\n                # This effectively duplicates some samples from the original dataset\n                # which is fine since we're just using this for metadata\n                virtual_idx = idx % self.original_len\n                return self.original_dataset[virtual_idx]\n            else:\n                # Return actual augmented sample\n                aug_idx = idx - self.original_len\n                return self.augmented_samples[aug_idx]\n    \n    def get_augmentation_info(self):\n        \"\"\"Get metadata about the augmentations.\"\"\"\n        if hasattr(self, 'virtual_augmentation') and self.virtual_augmentation:"
        },
        "get_augmentation_info": {
          "start_line": 79,
          "end_line": 99,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "hasattr",
              "line": 81
            },
            {
              "name": "self.metadata.get",
              "line": 83
            },
            {
              "name": "len",
              "line": 95
            }
          ],
          "docstring": "Get metadata about the augmentations.",
          "code_snippet": "                return self.augmented_samples[aug_idx]\n    \n    def get_augmentation_info(self):\n        \"\"\"Get metadata about the augmentations.\"\"\"\n        if hasattr(self, 'virtual_augmentation') and self.virtual_augmentation:\n            # For virtual augmentation, use the original metadata\n            augmented_count = self.metadata.get('augmented_count', 0)\n            return {\n                'original_count': self.original_len,\n                'augmented_count': augmented_count,\n                'total_count': self.original_len + augmented_count,\n                'metadata': self.metadata,\n                'virtual_augmentation': True\n            }\n        else:\n            # Normal case\n            return {\n                'original_count': self.original_len,\n                'augmented_count': len(self.augmented_samples),\n                'total_count': self.total_len,\n                'metadata': self.metadata\n            }\n\ndef apply_pattern_augmentation(image, pattern_type, aug_level=1):\n    \"\"\""
        }
      },
      "class_variables": [],
      "bases": [
        "Dataset"
      ],
      "docstring": "\n    Dataset that includes pre-computed augmentations.\n    \n    This dataset wraps the original dataset and adds pre-computed\n    augmentations with metadata tracking.\n    "
    }
  },
  "functions": {
    "apply_pattern_augmentation": {
      "start_line": 100,
      "end_line": 168,
      "parameters": [
        {
          "name": "image"
        },
        {
          "name": "pattern_type"
        },
        {
          "name": "aug_level"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "image.clone",
          "line": 117
        },
        {
          "name": "isinstance",
          "line": 113
        },
        {
          "name": "torch.tensor",
          "line": 114
        },
        {
          "name": "int",
          "line": 125
        },
        {
          "name": "....squeeze",
          "line": 131
        },
        {
          "name": "random.randint",
          "line": 126
        },
        {
          "name": "random.randint",
          "line": 127
        },
        {
          "name": "random.uniform",
          "line": 142
        },
        {
          "name": "augmented.mean",
          "line": 143
        },
        {
          "name": "torch.clamp",
          "line": 145
        },
        {
          "name": "torch.clamp",
          "line": 151
        },
        {
          "name": "min",
          "line": 125
        },
        {
          "name": "torch.nn.functional.interpolate",
          "line": 131
        },
        {
          "name": "torch.randn_like",
          "line": 150
        },
        {
          "name": "random.choice",
          "line": 159
        },
        {
          "name": "torch.roll",
          "line": 163
        },
        {
          "name": "augmented.unsqueeze",
          "line": 132
        }
      ],
      "docstring": "\n    Apply pattern-specific augmentation to an image.\n    \n    Args:\n        image: Input image tensor\n        pattern_type: Type of pattern ('structural', 'statistical', 'temporal')\n        aug_level: Intensity of augmentation (1-3)\n        \n    Returns:\n        Augmented image tensor\n    ",
      "code_snippet": "            }\n\ndef apply_pattern_augmentation(image, pattern_type, aug_level=1):\n    \"\"\"\n    Apply pattern-specific augmentation to an image.\n    \n    Args:\n        image: Input image tensor\n        pattern_type: Type of pattern ('structural', 'statistical', 'temporal')\n        aug_level: Intensity of augmentation (1-3)\n        \n    Returns:\n        Augmented image tensor\n    \"\"\"\n    # Ensure input is a tensor\n    if not isinstance(image, torch.Tensor):\n        image = torch.tensor(image)\n        \n    # Clone image to avoid modifying original\n    augmented = image.clone()\n    \n    # Apply pattern-specific augmentation\n    if pattern_type == 'structural':\n        # Structural augmentations focus on spatial transformations\n        if aug_level >= 1:\n            # Random crop and resize\n            h, w = augmented.shape[-2:]\n            crop_size = int(min(h, w) * 0.8)\n            top = random.randint(0, h - crop_size) if h > crop_size else 0\n            left = random.randint(0, w - crop_size) if w > crop_size else 0\n            augmented = augmented[:, top:top+crop_size, left:left+crop_size]\n            \n            # This would resize back to original size - in practice use torchvision.transforms\n            augmented = torch.nn.functional.interpolate(\n                augmented.unsqueeze(0), size=(h, w), mode='bilinear', align_corners=False).squeeze(0)\n        \n        if aug_level >= 2:\n            # Random rotation (add rotation transform)\n            pass\n            \n    elif pattern_type == 'statistical':\n        # Statistical augmentations focus on intensity/color distributions\n        if aug_level >= 1:\n            # Adjust contrast\n            contrast_factor = random.uniform(0.7, 1.3)\n            mean = augmented.mean(dim=[-1, -2], keepdim=True)\n            augmented = (augmented - mean) * contrast_factor + mean\n            augmented = torch.clamp(augmented, 0, 1)\n        \n        if aug_level >= 2:\n            # Add Gaussian noise\n            noise_level = 0.05\n            noise = torch.randn_like(augmented) * noise_level\n            augmented = torch.clamp(augmented + noise, 0, 1)\n            \n    elif pattern_type == 'temporal':\n        # Temporal augmentations - for images, simulate motion or change\n        if aug_level >= 1:\n            # Directional blur (simulating motion)\n            # Basic implementation - in practice use more sophisticated convolution\n            kernel_size = 3\n            direction = random.choice([(1, 0), (0, 1), (1, 1), (-1, 1)])\n            dx, dy = direction\n            \n            # Simple shifted averaging - in practice use proper convolution\n            shifted = torch.roll(augmented, shifts=(dx, dy), dims=(-2, -1))\n            augmented = (augmented + shifted) / 2\n    \n    return augmented\n\n\ndef calculate_three_pattern_weighting(bias_scores: Dict[str, float], comfort_factor: float = 0.2) -> Dict[str, float]:\n    \"\"\""
    },
    "calculate_three_pattern_weighting": {
      "start_line": 169,
      "end_line": 222,
      "parameters": [
        {
          "name": "bias_scores"
        },
        {
          "name": "comfort_factor",
          "type": "float"
        }
      ],
      "return_type": "complex_type",
      "calls": [
        {
          "name": "sorted",
          "line": 197
        },
        {
          "name": "enumerate",
          "line": 201
        },
        {
          "name": "sum",
          "line": 214
        },
        {
          "name": "logger.warning",
          "line": 189
        },
        {
          "name": "max",
          "line": 190
        },
        {
          "name": "bias_scores.get",
          "line": 194
        },
        {
          "name": "complete_scores.items",
          "line": 197
        },
        {
          "name": "weighted_scores.values",
          "line": 214
        },
        {
          "name": "min",
          "line": 190
        },
        {
          "name": "weighted_scores.items",
          "line": 216
        },
        {
          "name": "len",
          "line": 218
        }
      ],
      "docstring": "\n    Apply comfort-based weighting specifically designed for isekaiZen's 3-pattern system.\n    \n    This function implements a rank-based weighting strategy that gently emphasizes\n    patterns the model naturally handles well, creating a positive feedback loop\n    in the learning process.\n    \n    Args:\n        bias_scores: Dictionary of bias scores for structural, statistical, and temporal patterns\n        comfort_factor: Factor to emphasize the model's natural bias (0.0-0.5 recommended)\n        \n    Returns:\n        Dictionary of weighted bias scores normalized to sum to 1.0\n        \n    Raises:\n        ValueError: If comfort_factor is outside the recommended range\n    ",
      "code_snippet": "\n\ndef calculate_three_pattern_weighting(bias_scores: Dict[str, float], comfort_factor: float = 0.2) -> Dict[str, float]:\n    \"\"\"\n    Apply comfort-based weighting specifically designed for isekaiZen's 3-pattern system.\n    \n    This function implements a rank-based weighting strategy that gently emphasizes\n    patterns the model naturally handles well, creating a positive feedback loop\n    in the learning process.\n    \n    Args:\n        bias_scores: Dictionary of bias scores for structural, statistical, and temporal patterns\n        comfort_factor: Factor to emphasize the model's natural bias (0.0-0.5 recommended)\n        \n    Returns:\n        Dictionary of weighted bias scores normalized to sum to 1.0\n        \n    Raises:\n        ValueError: If comfort_factor is outside the recommended range\n    \"\"\"\n    # Validate comfort factor range\n    if comfort_factor < 0.0 or comfort_factor > 0.5:\n        logger.warning(f\"Comfort factor {comfort_factor} outside recommended range (0.0-0.5)\")\n        comfort_factor = max(0.0, min(0.5, comfort_factor))  # Clamp to valid range\n    \n    # Ensure all three patterns have scores (even if zero)\n    pattern_types = [\"structural\", \"statistical\", \"temporal\"]\n    complete_scores = {pattern: bias_scores.get(pattern, 0.0) for pattern in pattern_types}\n    \n    # Sort patterns by score (high to low)\n    sorted_patterns = sorted(complete_scores.items(), key=lambda x: x[1], reverse=True)\n    \n    # Apply rank-based weighting\n    weighted_scores = {}\n    for i, (pattern_type, score) in enumerate(sorted_patterns):\n        if score > 0:  # Only apply weighting to patterns that exist in the dataset\n            # Weight is based on rank (highest gets most emphasis)\n            if i == 0:  # Top pattern\n                weighted_scores[pattern_type] = score * (1 + comfort_factor)\n            elif i == 1:  # Middle pattern\n                weighted_scores[pattern_type] = score\n            else:  # Bottom pattern\n                weighted_scores[pattern_type] = score * (1 - comfort_factor)\n        else:\n            weighted_scores[pattern_type] = 0  # Keep zero scores as zero\n    \n    # Normalize to ensure sum = 1.0\n    total = sum(weighted_scores.values())\n    if total > 0:\n        normalized_scores = {k: v / total for k, v in weighted_scores.items()}\n    else:\n        normalized_scores = {k: 1.0 / len(pattern_types) for k in pattern_types}\n    \n    return normalized_scores\n\n\ndef calculate_adaptive_comfort_factor(train_acc, val_acc, base_comfort=0.2, min_comfort=0.15, max_comfort=0.25, threshold=70.0):\n    \"\"\""
    },
    "calculate_adaptive_comfort_factor": {
      "start_line": 223,
      "end_line": 261,
      "parameters": [
        {
          "name": "train_acc"
        },
        {
          "name": "val_acc"
        },
        {
          "name": "base_comfort"
        },
        {
          "name": "min_comfort"
        },
        {
          "name": "max_comfort"
        },
        {
          "name": "threshold"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "min",
          "line": 252
        },
        {
          "name": "min",
          "line": 256
        },
        {
          "name": "abs",
          "line": 252
        }
      ],
      "docstring": "\n    Calculate comfort factor based on model's training state.\n    \n    This function implements an adaptive comfort factor that activates only after\n    the model reaches a threshold validation accuracy. The comfort factor influences\n    how much to emphasize patterns the model naturally handles well.\n    \n    Args:\n        train_acc: Training accuracy (0-100 scale)\n        val_acc: Validation accuracy (0-100 scale)\n        base_comfort: Default comfort factor\n        min_comfort: Minimum comfort factor\n        max_comfort: Maximum comfort factor\n        threshold: Validation accuracy threshold to activate adaptation\n        \n    Returns:\n        float: Calculated comfort factor\n    ",
      "code_snippet": "\n\ndef calculate_adaptive_comfort_factor(train_acc, val_acc, base_comfort=0.2, min_comfort=0.15, max_comfort=0.25, threshold=70.0):\n    \"\"\"\n    Calculate comfort factor based on model's training state.\n    \n    This function implements an adaptive comfort factor that activates only after\n    the model reaches a threshold validation accuracy. The comfort factor influences\n    how much to emphasize patterns the model naturally handles well.\n    \n    Args:\n        train_acc: Training accuracy (0-100 scale)\n        val_acc: Validation accuracy (0-100 scale)\n        base_comfort: Default comfort factor\n        min_comfort: Minimum comfort factor\n        max_comfort: Maximum comfort factor\n        threshold: Validation accuracy threshold to activate adaptation\n        \n    Returns:\n        float: Calculated comfort factor\n    \"\"\"\n    # Only apply adaptive logic after threshold is reached\n    if val_acc < threshold:\n        return base_comfort\n        \n    # Calculate train/val gap\n    gap = val_acc - train_acc\n    \n    # Apply adaptive logic based on overfitting/underfitting\n    if train_acc > val_acc:  # Overfitting tendency\n        # Linearly scale between base and min based on gap magnitude (up to 5%)\n        gap_scale = min(1.0, abs(gap) / 5.0)\n        return base_comfort - gap_scale * (base_comfort - min_comfort)\n    elif gap > 5.0:  # Significant underfitting\n        # Linearly scale between base and max based on gap magnitude (5-15%)\n        gap_scale = min(1.0, (gap - 5.0) / 10.0)\n        return base_comfort + gap_scale * (max_comfort - base_comfort)\n    else:  # Balanced learning\n        return base_comfort\n\n\ndef create_pattern_biased_augmentations(dataset, pattern_map, bias_scores=None, \n                                      augmentation_ratio=0.3, device=None,"
    },
    "create_pattern_biased_augmentations": {
      "start_line": 262,
      "end_line": 557,
      "parameters": [
        {
          "name": "dataset"
        },
        {
          "name": "pattern_map"
        },
        {
          "name": "bias_scores"
        },
        {
          "name": "augmentation_ratio"
        },
        {
          "name": "device"
        },
        {
          "name": "comfort_factor"
        },
        {
          "name": "train_acc"
        },
        {
          "name": "val_acc"
        },
        {
          "name": "augmentation_data"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "int",
          "line": 465
        },
        {
          "name": "logger.info",
          "line": 466
        },
        {
          "name": "calculate_three_pattern_weighting",
          "line": 477
        },
        {
          "name": "logger.info",
          "line": 478
        },
        {
          "name": "weighted_scores.items",
          "line": 485
        },
        {
          "name": "augmentations_by_pattern.items",
          "line": 517
        },
        {
          "name": "AugmentedDataset",
          "line": 555
        },
        {
          "name": "torch.device",
          "line": 286
        },
        {
          "name": "logger.info",
          "line": 290
        },
        {
          "name": "logger.info",
          "line": 316
        },
        {
          "name": "calculate_three_pattern_weighting",
          "line": 369
        },
        {
          "name": "logger.info",
          "line": 370
        },
        {
          "name": "int",
          "line": 379
        },
        {
          "name": "logger.info",
          "line": 380
        },
        {
          "name": "weighted_scores.items",
          "line": 384
        },
        {
          "name": "augmentations_by_pattern.items",
          "line": 421
        },
        {
          "name": "logger.info",
          "line": 456
        },
        {
          "name": "AugmentedDataset",
          "line": 457
        },
        {
          "name": "ValueError",
          "line": 462
        },
        {
          "name": "calculate_adaptive_comfort_factor",
          "line": 470
        },
        {
          "name": "int",
          "line": 487
        },
        {
          "name": "logger.info",
          "line": 489
        },
        {
          "name": "range",
          "line": 530
        },
        {
          "name": "logger.info",
          "line": 295
        },
        {
          "name": "local_bias_scores.items",
          "line": 296
        },
        {
          "name": "logger.info",
          "line": 315
        },
        {
          "name": "....items",
          "line": 348
        },
        {
          "name": "calculate_adaptive_comfort_factor",
          "line": 362
        },
        {
          "name": "int",
          "line": 386
        },
        {
          "name": "logger.info",
          "line": 388
        },
        {
          "name": "range",
          "line": 434
        },
        {
          "name": "len",
          "line": 465
        },
        {
          "name": "logger.info",
          "line": 472
        },
        {
          "name": "logger.info",
          "line": 473
        },
        {
          "name": "logger.info",
          "line": 481
        },
        {
          "name": "isinstance",
          "line": 495
        },
        {
          "name": "isinstance",
          "line": 498
        },
        {
          "name": "logger.warning",
          "line": 505
        },
        {
          "name": "range",
          "line": 507
        },
        {
          "name": "logger.warning",
          "line": 522
        },
        {
          "name": "random.choice",
          "line": 532
        },
        {
          "name": "pattern_indices.append",
          "line": 533
        },
        {
          "name": "random.randint",
          "line": 539
        },
        {
          "name": "apply_pattern_augmentation",
          "line": 540
        },
        {
          "name": "augmented_samples.append",
          "line": 544
        },
        {
          "name": "torch.cuda.is_available",
          "line": 286
        },
        {
          "name": "logger.info",
          "line": 297
        },
        {
          "name": "logger.info",
          "line": 301
        },
        {
          "name": "logger.info",
          "line": 309
        },
        {
          "name": "logger.info",
          "line": 364
        },
        {
          "name": "logger.info",
          "line": 365
        },
        {
          "name": "logger.info",
          "line": 373
        },
        {
          "name": "len",
          "line": 379
        },
        {
          "name": "....items",
          "line": 396
        },
        {
          "name": "isinstance",
          "line": 403
        },
        {
          "name": "logger.warning",
          "line": 410
        },
        {
          "name": "list",
          "line": 412
        },
        {
          "name": "range",
          "line": 413
        },
        {
          "name": "logger.warning",
          "line": 426
        },
        {
          "name": "random.choice",
          "line": 436
        },
        {
          "name": "pattern_indices.append",
          "line": 437
        },
        {
          "name": "random.randint",
          "line": 443
        },
        {
          "name": "apply_pattern_augmentation",
          "line": 444
        },
        {
          "name": "augmented_samples.append",
          "line": 448
        },
        {
          "name": "PatternRecognitionService",
          "line": 501
        },
        {
          "name": "range",
          "line": 502
        },
        {
          "name": "len",
          "line": 507
        },
        {
          "name": "random.choice",
          "line": 508
        },
        {
          "name": "sample_patterns.items",
          "line": 519
        },
        {
          "name": "PatternRecognitionService",
          "line": 406
        },
        {
          "name": "range",
          "line": 407
        },
        {
          "name": "local_bias_scores.keys",
          "line": 412
        },
        {
          "name": "len",
          "line": 413
        },
        {
          "name": "random.choice",
          "line": 414
        },
        {
          "name": "sample_patterns.items",
          "line": 423
        },
        {
          "name": "len",
          "line": 456
        },
        {
          "name": "len",
          "line": 502
        },
        {
          "name": "pattern_service.get_pattern_type",
          "line": 503
        },
        {
          "name": "list",
          "line": 508
        },
        {
          "name": "len",
          "line": 308
        },
        {
          "name": "int",
          "line": 398
        },
        {
          "name": "len",
          "line": 407
        },
        {
          "name": "pattern_service.get_pattern_type",
          "line": 408
        },
        {
          "name": "str",
          "line": 505
        },
        {
          "name": "bias_scores.keys",
          "line": 508
        },
        {
          "name": "str",
          "line": 410
        }
      ],
      "docstring": "\n    Create pre-augmented dataset based on pattern bias scores or pre-existing augmentation data.\n    \n    Args:\n        dataset: Original dataset\n        pattern_map: Pattern mapping for the dataset\n        bias_scores: Normalized bias scores by pattern type (optional if augmentation_data is provided)\n        augmentation_ratio: Ratio of augmented samples to original samples (default: 0.3)\n        device: Device to use for processing\n        comfort_factor: Factor to emphasize the model's natural bias (0.0-0.5 recommended)\n                       where 0.0 means no comfort weighting (direct mapping)\n        train_acc: Optional training accuracy for adaptive comfort factor\n        val_acc: Optional validation accuracy for adaptive comfort factor\n        augmentation_data: Pre-existing augmentation data to use instead of creating new augmentations\n        \n    Returns:\n        AugmentedDataset with pre-computed augmentations\n    ",
      "code_snippet": "\n\ndef create_pattern_biased_augmentations(dataset, pattern_map, bias_scores=None, \n                                      augmentation_ratio=0.3, device=None,\n                                      comfort_factor=0.2, train_acc=None, val_acc=None,\n                                      augmentation_data=None):\n    \"\"\"\n    Create pre-augmented dataset based on pattern bias scores or pre-existing augmentation data.\n    \n    Args:\n        dataset: Original dataset\n        pattern_map: Pattern mapping for the dataset\n        bias_scores: Normalized bias scores by pattern type (optional if augmentation_data is provided)\n        augmentation_ratio: Ratio of augmented samples to original samples (default: 0.3)\n        device: Device to use for processing\n        comfort_factor: Factor to emphasize the model's natural bias (0.0-0.5 recommended)\n                       where 0.0 means no comfort weighting (direct mapping)\n        train_acc: Optional training accuracy for adaptive comfort factor\n        val_acc: Optional validation accuracy for adaptive comfort factor\n        augmentation_data: Pre-existing augmentation data to use instead of creating new augmentations\n        \n    Returns:\n        AugmentedDataset with pre-computed augmentations\n    \"\"\"\n    # Auto-detect device if not provided\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n    # Check if we're using pre-existing augmentation data\n    if augmentation_data is not None:\n        logger.info(\"Using augmentation data for creating augmented samples\")\n        \n        # Extract bias scores if they exist in the augmentation data\n        if 'bias_scores' in augmentation_data:\n            local_bias_scores = augmentation_data['bias_scores']\n            logger.info(\"Using bias scores from augmentation data\")\n            for pattern_type, score in local_bias_scores.items():\n                logger.info(f\"  {pattern_type}: {score:.4f}\")\n        elif bias_scores is not None:\n            # Use provided bias scores\n            local_bias_scores = bias_scores\n            logger.info(\"Using provided bias scores\")\n        else:\n            # Create uniform bias scores\n            if pattern_map and 'pattern_types' in pattern_map:\n                pattern_types = pattern_map['pattern_types']\n            else:\n                pattern_types = ['structural', 'statistical', 'temporal']\n            local_bias_scores = {pt: 1.0 / len(pattern_types) for pt in pattern_types}\n            logger.info(\"Using uniform bias scores (no scores provided)\")\n        \n        # Extract augmentation ratio from metadata or use default\n        augmentation_ratio = 0.33  # Default to 33%\n        if 'metadata' in augmentation_data and 'augmentation_ratio' in augmentation_data['metadata']:\n            augmentation_ratio = augmentation_data['metadata']['augmentation_ratio']\n            logger.info(f\"Using augmentation ratio from metadata: {augmentation_ratio:.2f}\")\n        logger.info(f\"Creating augmented samples with ratio {augmentation_ratio:.2f} (33%)\")\n        \n        # The rest of the function should continue with actual augmentation creation\n        # using the local_bias_scores and augmentation_ratio\n        \n        # Create the augmentation metadata to preserve important fields\n        augmentation_metadata = {}\n        \n        # Transfer critical components from augmentation_data\n        critical_components = [\n            'pattern_distribution', 'pattern_complexities', 'pattern_types',\n            'sample_to_pattern', 'patterns_by_complexity', 'pattern_map_path'\n        ]\n        \n        # First copy from metadata if it exists\n        if 'metadata' in augmentation_data:\n            metadata = augmentation_data['metadata']\n            for component in critical_components:\n                if component in metadata:\n                    augmentation_metadata[component] = metadata[component]\n        \n        # Then check top level keys\n        for component in critical_components:\n            if component in augmentation_data and component not in augmentation_metadata:\n                augmentation_metadata[component] = augmentation_data[component]\n        \n        # If complete pattern map is available, store it\n        if 'complete_pattern_map' in augmentation_data:\n            augmentation_metadata['complete_pattern_map'] = augmentation_data['complete_pattern_map']\n            \n        # Preserve other metadata fields\n        if 'metadata' in augmentation_data:\n            for key, value in augmentation_data['metadata'].items():\n                if key not in augmentation_metadata:\n                    augmentation_metadata[key] = value\n        \n        # Ensure weighted scores are included\n        if 'weighted_scores' in augmentation_data:\n            augmentation_metadata['weighted_scores'] = augmentation_data['weighted_scores']\n        elif 'weighted_scores' in augmentation_metadata:\n            pass  # Already transferred from metadata\n        elif 'weighted_distribution' in pattern_map:\n            augmentation_metadata['weighted_scores'] = pattern_map['weighted_distribution']\n        \n        # Determine comfort factor - use adaptive if training metrics provided\n        if train_acc is not None and val_acc is not None:\n            adaptive_comfort = calculate_adaptive_comfort_factor(train_acc, val_acc, base_comfort=comfort_factor)\n            if adaptive_comfort != comfort_factor:\n                logger.info(f\"Using adaptive comfort factor: {adaptive_comfort:.3f} (from base: {comfort_factor:.3f})\")\n                logger.info(f\"  Based on train_acc: {train_acc:.2f}%, val_acc: {val_acc:.2f}%\")\n                comfort_factor = adaptive_comfort\n        \n        # Apply comfort zone weighting\n        weighted_scores = calculate_three_pattern_weighting(local_bias_scores, comfort_factor)\n        logger.info(f\"Applied comfort zone weighting (factor={comfort_factor:.3f})\")\n        for pattern_type in local_bias_scores:\n            if pattern_type in weighted_scores:\n                logger.info(f\"  {pattern_type}: {local_bias_scores[pattern_type]:.4f} -> {weighted_scores[pattern_type]:.4f}\")\n        \n        # Store weighted scores in metadata for later use by optimizer\n        augmentation_metadata['weighted_scores'] = weighted_scores\n        \n        # Determine number of augmentations to create based on the ratio\n        total_augmentations = int(len(dataset) * augmentation_ratio)\n        logger.info(f\"Creating {total_augmentations} augmented samples based on weighted bias scores\")\n        \n        # Allocate augmentations by pattern type based on weighted scores\n        augmentations_by_pattern = {}\n        for pattern_type, score in weighted_scores.items():\n            # Higher scores get more augmentations\n            count = int(total_augmentations * score)\n            augmentations_by_pattern[pattern_type] = count\n            logger.info(f\"  {pattern_type}: {count} augmentations ({score:.2f} weighted score)\")\n        \n        # Get pattern assignments for each sample from pattern map\n        sample_patterns = {}\n        \n        try:\n            if 'sample_to_pattern' in pattern_map:\n                # Direct mapping from sample index to pattern type\n                for idx_str, pattern_type in pattern_map['sample_to_pattern'].items():\n                    try:\n                        idx = int(idx_str)\n                        sample_patterns[idx] = pattern_type\n                    except ValueError:\n                        # Skip non-integer indices\n                        pass\n            elif isinstance(pattern_map, dict):\n                # Need to query through a service\n                from isekaizen.pattern.detection import PatternRecognitionService\n                pattern_service = PatternRecognitionService(pattern_map)\n                for i in range(len(dataset)):\n                    sample_patterns[i] = pattern_service.get_pattern_type(i)\n        except Exception as e:\n            logger.warning(f\"Error mapping patterns: {str(e)}. Using random assignment.\")\n            # Fallback: random assignment for testing\n            pattern_types = list(local_bias_scores.keys())\n            for i in range(len(dataset)):\n                sample_patterns[i] = random.choice(pattern_types)\n        \n        # Create augmentations\n        augmented_samples = []\n        augmentation_metadata['count_by_pattern'] = {}\n        augmentation_metadata['indices_by_pattern'] = {}\n        \n        for pattern_type, count in augmentations_by_pattern.items():\n            # Get indices of samples with this pattern type\n            indices = [i for i, p in sample_patterns.items() if p == pattern_type]\n            \n            if not indices:\n                logger.warning(f\"No samples found for pattern type: {pattern_type}\")\n                continue\n            \n            # Track augmented samples for this pattern\n            pattern_augmentations = 0\n            pattern_indices = []\n            \n            # Create augmentations for this pattern type\n            for _ in range(count):\n                # Select a random sample of this pattern type\n                idx = random.choice(indices)\n                pattern_indices.append(idx)\n                \n                # Get original sample\n                original_input, target = dataset[idx]\n                \n                # Apply pattern-specific augmentation\n                aug_level = random.randint(1, 3)  # Random augmentation level\n                augmented_input = apply_pattern_augmentation(\n                    original_input, pattern_type, aug_level)\n                \n                # Add to augmented samples\n                augmented_samples.append((augmented_input, target))\n                pattern_augmentations += 1\n            \n            # Update metadata\n            augmentation_metadata['count_by_pattern'][pattern_type] = pattern_augmentations\n            augmentation_metadata['indices_by_pattern'][pattern_type] = pattern_indices\n        \n        # Create the actual augmented dataset with real samples\n        logger.info(f\"Created {len(augmented_samples)} real augmented samples\")\n        return AugmentedDataset(dataset, augmented_samples, augmentation_metadata)\n    \n    # If no augmentation_data was provided, continue with normal augmentation process\n    # Ensure bias_scores is provided when no augmentation_data is given\n    if bias_scores is None:\n        raise ValueError(\"Either bias_scores or augmentation_data must be provided\")\n    \n    # Determine number of augmentations to create\n    total_augmentations = int(len(dataset) * augmentation_ratio)\n    logger.info(f\"Creating {total_augmentations} augmented samples based on bias scores\")\n    \n    # Determine comfort factor - use adaptive if training metrics provided\n    if train_acc is not None and val_acc is not None:\n        adaptive_comfort = calculate_adaptive_comfort_factor(train_acc, val_acc, base_comfort=comfort_factor)\n        if adaptive_comfort != comfort_factor:\n            logger.info(f\"Using adaptive comfort factor: {adaptive_comfort:.3f} (from base: {comfort_factor:.3f})\")\n            logger.info(f\"  Based on train_acc: {train_acc:.2f}%, val_acc: {val_acc:.2f}%\")\n            comfort_factor = adaptive_comfort\n    \n    # Apply comfort zone weighting\n    weighted_scores = calculate_three_pattern_weighting(bias_scores, comfort_factor)\n    logger.info(f\"Applied comfort zone weighting (factor={comfort_factor:.3f})\")\n    for pattern_type in bias_scores:\n        if pattern_type in weighted_scores:\n            logger.info(f\"  {pattern_type}: {bias_scores[pattern_type]:.4f} -> {weighted_scores[pattern_type]:.4f}\")\n    \n    # Allocate augmentations by pattern type based on weighted scores\n    augmentations_by_pattern = {}\n    for pattern_type, score in weighted_scores.items():\n        # Higher scores get more augmentations\n        count = int(total_augmentations * score)\n        augmentations_by_pattern[pattern_type] = count\n        logger.info(f\"  {pattern_type}: {count} augmentations ({score:.2f} weighted score)\")\n    \n    # Get pattern assignments for each sample\n    sample_patterns = {}\n    \n    try:\n        if isinstance(pattern_map, dict) and 'sample_patterns' in pattern_map:\n            # Direct mapping from sample index to pattern type\n            sample_patterns = pattern_map['sample_patterns']\n        elif isinstance(pattern_map, dict):\n            # Need to query through a service\n            from isekaizen.pattern.detection import PatternRecognitionService\n            pattern_service = PatternRecognitionService(pattern_map)\n            for i in range(len(dataset)):\n                sample_patterns[i] = pattern_service.get_pattern_type(i)\n    except Exception as e:\n        logger.warning(f\"Error mapping patterns: {str(e)}. Using random assignment.\")\n        # Fallback: random assignment for testing\n        for i in range(len(dataset)):\n            sample_patterns[i] = random.choice(list(bias_scores.keys()))\n    \n    # Create augmentations\n    augmented_samples = []\n    augmentation_metadata = {\n        'count_by_pattern': {},\n        'indices_by_pattern': {}\n    }\n    \n    for pattern_type, count in augmentations_by_pattern.items():\n        # Get indices of samples with this pattern type\n        indices = [i for i, p in sample_patterns.items() if p == pattern_type]\n        \n        if not indices:\n            logger.warning(f\"No samples found for pattern type: {pattern_type}\")\n            continue\n        \n        # Track augmented samples for this pattern\n        pattern_augmentations = 0\n        pattern_indices = []\n        \n        # Create augmentations for this pattern type\n        for _ in range(count):\n            # Select a random sample of this pattern type\n            idx = random.choice(indices)\n            pattern_indices.append(idx)\n            \n            # Get original sample\n            original_input, target = dataset[idx]\n            \n            # Apply pattern-specific augmentation\n            aug_level = random.randint(1, 3)  # Random augmentation level\n            augmented_input = apply_pattern_augmentation(\n                original_input, pattern_type, aug_level)\n            \n            # Add to augmented samples\n            augmented_samples.append((augmented_input, target))\n            pattern_augmentations += 1\n        \n        # Update metadata\n        augmentation_metadata['count_by_pattern'][pattern_type] = pattern_augmentations\n        augmentation_metadata['indices_by_pattern'][pattern_type] = pattern_indices\n    \n    # Store weighted scores in metadata for later use by optimizer\n    augmentation_metadata['weighted_scores'] = weighted_scores\n    \n    # Create augmented dataset\n    return AugmentedDataset(dataset, augmented_samples, augmentation_metadata)\n\ndef save_augmentation_data(augmented_dataset, path, pattern_map=None):\n    \"\"\"\n    Save augmented dataset information for reproducibility."
    },
    "save_augmentation_data": {
      "start_line": 557,
      "end_line": 635,
      "parameters": [
        {
          "name": "augmented_dataset"
        },
        {
          "name": "path"
        },
        {
          "name": "pattern_map"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "augmented_dataset.get_augmentation_info",
          "line": 570
        },
        {
          "name": "....keys",
          "line": 614
        },
        {
          "name": "logger.info",
          "line": 631
        },
        {
          "name": "isinstance",
          "line": 615
        },
        {
          "name": "open",
          "line": 628
        },
        {
          "name": "json.dump",
          "line": 629
        },
        {
          "name": "logger.info",
          "line": 633
        },
        {
          "name": "....keys",
          "line": 616
        },
        {
          "name": "isinstance",
          "line": 617
        },
        {
          "name": "list",
          "line": 596
        },
        {
          "name": "....keys",
          "line": 596
        },
        {
          "name": "list",
          "line": 598
        },
        {
          "name": "all",
          "line": 620
        },
        {
          "name": "....keys",
          "line": 598
        },
        {
          "name": "len",
          "line": 622
        },
        {
          "name": "min",
          "line": 623
        },
        {
          "name": "max",
          "line": 624
        },
        {
          "name": "isinstance",
          "line": 620
        }
      ],
      "docstring": "\n    Save augmented dataset information for reproducibility.\n    \n    This doesn't save the actual dataset, just metadata about the augmentations.\n    \n    Args:\n        augmented_dataset: The augmented dataset\n        path: Path to save metadata\n        pattern_map: Optional pattern map to include in the saved data\n    ",
      "code_snippet": "    return AugmentedDataset(dataset, augmented_samples, augmentation_metadata)\n\ndef save_augmentation_data(augmented_dataset, path, pattern_map=None):\n    \"\"\"\n    Save augmented dataset information for reproducibility.\n    \n    This doesn't save the actual dataset, just metadata about the augmentations.\n    \n    Args:\n        augmented_dataset: The augmented dataset\n        path: Path to save metadata\n        pattern_map: Optional pattern map to include in the saved data\n    \"\"\"\n    import json\n    \n    metadata = augmented_dataset.get_augmentation_info()\n    \n    # Include complete pattern map if provided\n    if pattern_map:\n        # Store critical pattern map components separately for immediate access\n        # 1. Pattern Distribution Data\n        if 'weighted_distribution' not in metadata['metadata'] and 'weighted_scores' in metadata['metadata']:\n            metadata['weighted_scores'] = metadata['metadata']['weighted_scores']\n        elif 'weighted_distribution' in pattern_map:\n            metadata['weighted_scores'] = pattern_map['weighted_distribution']\n        \n        # 2. Pattern Complexities\n        if 'pattern_complexities' in pattern_map:\n            metadata['pattern_complexities'] = pattern_map['pattern_complexities']\n        \n        # 3. Pattern Distribution\n        if 'pattern_distribution' in pattern_map:\n            metadata['pattern_distribution'] = pattern_map['pattern_distribution']\n        \n        # 4. Pattern Types\n        if 'pattern_types' in pattern_map:\n            metadata['pattern_types'] = pattern_map['pattern_types']\n        elif 'pattern_types' not in metadata:\n            # Fallback: Get pattern types from other data\n            pattern_types = []\n            if 'pattern_distribution' in pattern_map:\n                pattern_types = list(pattern_map['pattern_distribution'].keys())\n            elif 'pattern_complexities' in pattern_map:\n                pattern_types = list(pattern_map['pattern_complexities'].keys())\n            metadata['pattern_types'] = pattern_types\n        \n        # 5. Sample to Pattern Mapping\n        if 'sample_to_pattern' in pattern_map:\n            # This is critical for pattern recognition to work\n            metadata['sample_to_pattern'] = pattern_map['sample_to_pattern']\n        \n        # Store the pattern map path for reference\n        if 'path' in pattern_map:\n            metadata['pattern_map_path'] = pattern_map['path']\n        \n        # Store the complete pattern map as well\n        metadata['complete_pattern_map'] = pattern_map\n    \n    # Convert any non-serializable types\n    for key in metadata['metadata'].keys():\n        if isinstance(metadata['metadata'][key], dict):\n            for subkey in metadata['metadata'][key].keys():\n                if isinstance(metadata['metadata'][key][subkey], list):\n                    # Convert lists of indices to count and range info\n                    indices = metadata['metadata'][key][subkey]\n                    if indices and all(isinstance(x, int) for x in indices):\n                        metadata['metadata'][key][subkey] = {\n                            'count': len(indices),\n                            'min_idx': min(indices),\n                            'max_idx': max(indices)\n                        }\n    \n    # Save to file\n    with open(path, 'w') as f:\n        json.dump(metadata, f, indent=2)\n    \n    logger.info(f\"Saved augmentation metadata to {path}\")\n    if pattern_map:\n        logger.info(f\"Included complete pattern map data for seamless loading\")"
    }
  },
  "constants": {}
}