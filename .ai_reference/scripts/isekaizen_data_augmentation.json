{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\isekaizen\\data\\augmentation.py",
  "imports": [
    {
      "name": "logging",
      "line": 11
    },
    {
      "name": "random",
      "line": 12
    },
    {
      "name": "typing.Dict",
      "line": 13
    },
    {
      "name": "typing.Any",
      "line": 13
    },
    {
      "name": "typing.Optional",
      "line": 13
    },
    {
      "name": "typing.List",
      "line": 13
    },
    {
      "name": "typing.Callable",
      "line": 13
    },
    {
      "name": "typing.Tuple",
      "line": 13
    },
    {
      "name": "typing.Union",
      "line": 13
    },
    {
      "name": "numpy",
      "line": 15
    },
    {
      "name": "torch",
      "line": 16
    },
    {
      "name": "torchvision.transforms",
      "line": 17
    },
    {
      "name": "torch.utils.data.Dataset",
      "line": 18
    },
    {
      "name": "torch.utils.data.ConcatDataset",
      "line": 18
    },
    {
      "name": "torch.utils.data.Subset",
      "line": 18
    },
    {
      "name": "isekaizen.utils.generate_timestamp",
      "line": 20
    },
    {
      "name": "isekaizen.data.augmented_dataset.TransformedSubset",
      "line": 477
    },
    {
      "name": "isekaizen.data.augmented_dataset.AugmentedDataset",
      "line": 159
    },
    {
      "name": "random",
      "line": 375
    },
    {
      "name": "re",
      "line": 376
    }
  ],
  "classes": {
    "AugmentationManager": {
      "start_line": 25,
      "end_line": 315,
      "methods": {
        "__init__": {
          "start_line": 38,
          "end_line": 60,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logger.info",
              "line": 57
            },
            {
              "name": "....join",
              "line": 58
            },
            {
              "name": "self.augmentation_levels.items",
              "line": 58
            }
          ],
          "docstring": "Initialize the augmentation manager.",
          "code_snippet": "    \"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize the augmentation manager.\"\"\"\n        # Define augmentation levels\n        self.augmentation_levels = {\n            'none': 0.0,   # No augmentation\n            'light': 0.1,  # Add 10% more examples\n            'medium': 0.25, # Add 25% more examples\n            'heavy': 0.5,  # Add 50% more examples\n        }\n        \n        # Start with no augmentation\n        self.current_augmentation_level = 'none'\n        \n        # Track previously augmented datasets\n        self.augmented_datasets = {}\n        \n        # Record augmentation history\n        self.augmentation_history = []\n        \n        logger.info(\"AugmentationManager initialized with levels: \" + \n                   \", \".join([f\"{k}: {v:.2f}\" for k, v in self.augmentation_levels.items()]))\n    \n    def preload_augmentation_levels(self, custom_levels: Optional[Dict[str, float]] = None) -> None:\n        \"\"\"\n        Preload custom augmentation levels."
        },
        "preload_augmentation_levels": {
          "start_line": 60,
          "end_line": 72,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "custom_levels"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.augmentation_levels.update",
              "line": 68
            },
            {
              "name": "logger.info",
              "line": 69
            },
            {
              "name": "....join",
              "line": 70
            },
            {
              "name": "self.augmentation_levels.items",
              "line": 70
            }
          ],
          "docstring": "\n        Preload custom augmentation levels.\n        \n        Args:\n            custom_levels: Dictionary mapping level names to augmentation factors\n        ",
          "code_snippet": "                   \", \".join([f\"{k}: {v:.2f}\" for k, v in self.augmentation_levels.items()]))\n    \n    def preload_augmentation_levels(self, custom_levels: Optional[Dict[str, float]] = None) -> None:\n        \"\"\"\n        Preload custom augmentation levels.\n        \n        Args:\n            custom_levels: Dictionary mapping level names to augmentation factors\n        \"\"\"\n        if custom_levels:\n            self.augmentation_levels.update(custom_levels)\n            logger.info(\"Augmentation levels updated to: \" + \n                       \", \".join([f\"{k}: {v:.2f}\" for k, v in self.augmentation_levels.items()]))\n    \n    def get_image_augmentation_transform(self, intensity: str = 'medium') -> transforms.Compose:\n        \"\"\"\n        Get an image augmentation transform composition based on intensity level."
        },
        "get_image_augmentation_transform": {
          "start_line": 72,
          "end_line": 119,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "intensity",
              "type": "str"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "transforms.Compose",
              "line": 117
            },
            {
              "name": "transforms.ToTensor",
              "line": 84
            },
            {
              "name": "transforms.Normalize",
              "line": 85
            },
            {
              "name": "transforms.RandomHorizontalFlip",
              "line": 91
            },
            {
              "name": "transforms.RandomRotation",
              "line": 92
            },
            {
              "name": "transforms.ColorJitter",
              "line": 93
            },
            {
              "name": "transforms.RandomHorizontalFlip",
              "line": 98
            },
            {
              "name": "transforms.RandomRotation",
              "line": 99
            },
            {
              "name": "transforms.ColorJitter",
              "line": 100
            },
            {
              "name": "transforms.RandomAffine",
              "line": 101
            },
            {
              "name": "transforms.Compose",
              "line": 114
            },
            {
              "name": "transforms.RandomHorizontalFlip",
              "line": 106
            },
            {
              "name": "transforms.RandomVerticalFlip",
              "line": 107
            },
            {
              "name": "transforms.RandomRotation",
              "line": 108
            },
            {
              "name": "transforms.ColorJitter",
              "line": 109
            },
            {
              "name": "transforms.RandomAffine",
              "line": 110
            },
            {
              "name": "transforms.RandomPerspective",
              "line": 111
            }
          ],
          "docstring": "\n        Get an image augmentation transform composition based on intensity level.\n        \n        Args:\n            intensity: Augmentation intensity level ('light', 'medium', 'heavy')\n            \n        Returns:\n            torchvision.transforms.Compose: Composition of image augmentation transforms\n        ",
          "code_snippet": "                       \", \".join([f\"{k}: {v:.2f}\" for k, v in self.augmentation_levels.items()]))\n    \n    def get_image_augmentation_transform(self, intensity: str = 'medium') -> transforms.Compose:\n        \"\"\"\n        Get an image augmentation transform composition based on intensity level.\n        \n        Args:\n            intensity: Augmentation intensity level ('light', 'medium', 'heavy')\n            \n        Returns:\n            torchvision.transforms.Compose: Composition of image augmentation transforms\n        \"\"\"\n        # Base transformations\n        base_transforms = [\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ]\n        \n        # Light augmentation\n        if intensity == 'light':\n            augment_transforms = [\n                transforms.RandomHorizontalFlip(),\n                transforms.RandomRotation(10),\n                transforms.ColorJitter(brightness=0.1, contrast=0.1),\n            ]\n        # Medium augmentation\n        elif intensity == 'medium':\n            augment_transforms = [\n                transforms.RandomHorizontalFlip(),\n                transforms.RandomRotation(15),\n                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),\n                transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n            ]\n        # Heavy augmentation\n        elif intensity == 'heavy':\n            augment_transforms = [\n                transforms.RandomHorizontalFlip(),\n                transforms.RandomVerticalFlip(),\n                transforms.RandomRotation(20),\n                transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.1),\n                transforms.RandomAffine(degrees=10, translate=(0.15, 0.15), scale=(0.9, 1.1)),\n                transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n            ]\n        else:  # No augmentation\n            return transforms.Compose(base_transforms)\n        \n        # Combine transforms\n        return transforms.Compose(augment_transforms + base_transforms)\n    \n    def apply_adaptive_augmentation(self, dataset: Dataset, \n                                   val_loss: Optional[float] = None,\n                                   val_acc: Optional[float] = None,"
        },
        "apply_adaptive_augmentation": {
          "start_line": 119,
          "end_line": 189,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "dataset",
              "type": "Dataset"
            },
            {
              "name": "val_loss"
            },
            {
              "name": "val_acc"
            },
            {
              "name": "optimizer"
            },
            {
              "name": "current_epoch",
              "type": "int"
            }
          ],
          "return_type": "Dataset",
          "calls": [
            {
              "name": "self._determine_augmentation_level",
              "line": 138
            },
            {
              "name": "self.get_image_augmentation_transform",
              "line": 155
            },
            {
              "name": "logger.info",
              "line": 143
            },
            {
              "name": "logger.info",
              "line": 148
            },
            {
              "name": "AugmentedDataset",
              "line": 160
            },
            {
              "name": "self.augmentation_history.append",
              "line": 171
            },
            {
              "name": "logger.info",
              "line": 180
            },
            {
              "name": "logger.info",
              "line": 181
            },
            {
              "name": "logger.error",
              "line": 186
            },
            {
              "name": "self._get_focused_patterns",
              "line": 164
            },
            {
              "name": "len",
              "line": 177
            },
            {
              "name": "len",
              "line": 181
            },
            {
              "name": "len",
              "line": 181
            },
            {
              "name": "str",
              "line": 186
            }
          ],
          "docstring": "\n        Apply adaptive augmentation based on validation metrics and pattern performance.\n        \n        Args:\n            dataset: Original dataset to augment\n            val_loss: Current validation loss\n            val_acc: Current validation accuracy\n            optimizer: Optimizer with pattern metrics\n            current_epoch: Current training epoch\n            \n        Returns:\n            Dataset: Original or augmented dataset\n        ",
          "code_snippet": "        return transforms.Compose(augment_transforms + base_transforms)\n    \n    def apply_adaptive_augmentation(self, dataset: Dataset, \n                                   val_loss: Optional[float] = None,\n                                   val_acc: Optional[float] = None,\n                                   optimizer: Optional[Any] = None,\n                                   current_epoch: int = 0) -> Dataset:\n        \"\"\"\n        Apply adaptive augmentation based on validation metrics and pattern performance.\n        \n        Args:\n            dataset: Original dataset to augment\n            val_loss: Current validation loss\n            val_acc: Current validation accuracy\n            optimizer: Optimizer with pattern metrics\n            current_epoch: Current training epoch\n            \n        Returns:\n            Dataset: Original or augmented dataset\n        \"\"\"\n        # Determine augmentation level based on metrics\n        aug_level = self._determine_augmentation_level(val_loss, val_acc, optimizer, current_epoch)\n        self.current_augmentation_level = aug_level\n        \n        # If no augmentation needed, return original dataset\n        if aug_level == 'none' or self.augmentation_levels[aug_level] <= 0:\n            logger.info(f\"No augmentation applied (level: {aug_level})\")\n            return dataset\n        \n        # Check if we already have this augmentation level\n        if aug_level in self.augmented_datasets:\n            logger.info(f\"Using cached augmentation (level: {aug_level})\")\n            return self.augmented_datasets[aug_level]\n        \n        # Create augmented dataset\n        augmentation_factor = self.augmentation_levels[aug_level]\n        \n        # Create appropriate transform\n        transform = self.get_image_augmentation_transform(aug_level)\n        \n        # Create augmented dataset (with extra examples)\n        try:\n            from isekaizen.data.augmented_dataset import AugmentedDataset\n            augmented = AugmentedDataset(\n                base_dataset=dataset,\n                augmentation_factor=augmentation_factor,\n                transform=transform,\n                focused_patterns=self._get_focused_patterns(optimizer)\n            )\n            \n            # Cache for reuse\n            self.augmented_datasets[aug_level] = augmented\n            \n            # Record in history\n            self.augmentation_history.append({\n                'epoch': current_epoch,\n                'level': aug_level,\n                'factor': augmentation_factor,\n                'val_loss': val_loss,\n                'val_acc': val_acc,\n                'dataset_size': len(augmented)\n            })\n            \n            logger.info(f\"Applied {aug_level} augmentation (factor: {augmentation_factor:.2f})\")\n            logger.info(f\"Augmented dataset size: {len(augmented)} (original: {len(dataset)})\")\n            \n            return augmented\n            \n        except Exception as e:\n            logger.error(f\"Failed to create augmented dataset: {str(e)}\")\n            return dataset\n    \n    def _determine_augmentation_level(self, val_loss: Optional[float], \n                                     val_acc: Optional[float],\n                                     optimizer: Optional[Any],"
        },
        "_determine_augmentation_level": {
          "start_line": 189,
          "end_line": 271,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "val_loss"
            },
            {
              "name": "val_acc"
            },
            {
              "name": "optimizer"
            },
            {
              "name": "current_epoch",
              "type": "int"
            }
          ],
          "return_type": "str",
          "calls": [
            {
              "name": "hasattr",
              "line": 235
            },
            {
              "name": "len",
              "line": 225
            },
            {
              "name": "....get",
              "line": 226
            },
            {
              "name": "optimizer.get_pattern_metrics",
              "line": 237
            },
            {
              "name": "list",
              "line": 229
            },
            {
              "name": "levels.index",
              "line": 230
            },
            {
              "name": "len",
              "line": 242
            },
            {
              "name": "logger.warning",
              "line": 267
            },
            {
              "name": "self.augmentation_levels.keys",
              "line": 229
            },
            {
              "name": "len",
              "line": 231
            },
            {
              "name": "len",
              "line": 254
            },
            {
              "name": "sum",
              "line": 256
            },
            {
              "name": "len",
              "line": 256
            },
            {
              "name": "ratios.values",
              "line": 256
            },
            {
              "name": "str",
              "line": 267
            }
          ],
          "docstring": "\n        Determine the appropriate augmentation level based on metrics.\n        \n        Args:\n            val_loss: Current validation loss\n            val_acc: Current validation accuracy\n            optimizer: Optimizer with pattern metrics\n            current_epoch: Current training epoch\n            \n        Returns:\n            str: Augmentation level name\n        ",
          "code_snippet": "            return dataset\n    \n    def _determine_augmentation_level(self, val_loss: Optional[float], \n                                     val_acc: Optional[float],\n                                     optimizer: Optional[Any],\n                                     current_epoch: int) -> str:\n        \"\"\"\n        Determine the appropriate augmentation level based on metrics.\n        \n        Args:\n            val_loss: Current validation loss\n            val_acc: Current validation accuracy\n            optimizer: Optimizer with pattern metrics\n            current_epoch: Current training epoch\n            \n        Returns:\n            str: Augmentation level name\n        \"\"\"\n        # Default to no augmentation\n        aug_level = 'none'\n        \n        # Early epochs - use minimal augmentation\n        if current_epoch < 5:\n            return 'light' if current_epoch > 2 else 'none'\n        \n        # If no metrics, can't make an informed decision\n        if val_loss is None and val_acc is None:\n            return 'light'  # Default to light augmentation\n        \n        # Check validation metrics\n        if val_loss is not None:\n            # High or increasing validation loss indicates need for more data\n            if val_loss > 1.0:\n                aug_level = 'medium'\n            if val_loss > 2.0:\n                aug_level = 'heavy'\n            \n            # Check if val_loss is increasing (need for more data)\n            if len(self.augmentation_history) > 0:\n                last_val_loss = self.augmentation_history[-1].get('val_loss')\n                if last_val_loss and val_loss > last_val_loss * 1.05:  # 5% increase\n                    # Increase augmentation level\n                    levels = list(self.augmentation_levels.keys())\n                    current_idx = levels.index(self.current_augmentation_level)\n                    if current_idx < len(levels) - 1:\n                        aug_level = levels[current_idx + 1]\n        \n        # Check pattern metrics from optimizer if available\n        if optimizer and hasattr(optimizer, 'get_pattern_metrics'):\n            try:\n                pattern_metrics = optimizer.get_pattern_metrics()\n                \n                # Check for patterns below minimum bound\n                if 'patterns_below_min' in pattern_metrics and pattern_metrics['patterns_below_min']:\n                    # More patterns below min = more augmentation needed\n                    below_min_count = len(pattern_metrics['patterns_below_min'])\n                    \n                    if below_min_count > 3:\n                        aug_level = 'heavy'\n                    elif below_min_count > 1:\n                        aug_level = 'medium'\n                    else:\n                        aug_level = 'light'\n                \n                # Check risk/accuracy ratios\n                if 'risk_accuracy_ratios' in pattern_metrics:\n                    ratios = pattern_metrics['risk_accuracy_ratios']\n                    if ratios and len(ratios) > 0:\n                        # Calculate average ratio\n                        avg_ratio = sum(ratios.values()) / len(ratios)\n                        \n                        # High ratio = high risk relative to accuracy = more augmentation\n                        if avg_ratio > 1.5:\n                            aug_level = 'heavy'\n                        elif avg_ratio > 1.2:\n                            aug_level = 'medium'\n                        elif avg_ratio > 1.0:\n                            aug_level = 'light'\n            \n            except Exception as e:\n                logger.warning(f\"Error getting pattern metrics for augmentation: {str(e)}\")\n        \n        return aug_level\n    \n    def _get_focused_patterns(self, optimizer: Optional[Any]) -> List[str]:\n        \"\"\"\n        Get patterns to focus on during augmentation based on optimizer metrics."
        },
        "_get_focused_patterns": {
          "start_line": 271,
          "end_line": 306,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "optimizer"
            }
          ],
          "return_type": "complex_type",
          "calls": [
            {
              "name": "hasattr",
              "line": 283
            },
            {
              "name": "optimizer.get_pattern_metrics",
              "line": 285
            },
            {
              "name": "list",
              "line": 299
            },
            {
              "name": "focused_patterns.extend",
              "line": 289
            },
            {
              "name": "focused_patterns.extend",
              "line": 296
            },
            {
              "name": "dict.fromkeys",
              "line": 299
            },
            {
              "name": "logger.warning",
              "line": 302
            },
            {
              "name": "ratios.items",
              "line": 295
            },
            {
              "name": "str",
              "line": 302
            }
          ],
          "docstring": "\n        Get patterns to focus on during augmentation based on optimizer metrics.\n        \n        Args:\n            optimizer: Optimizer with pattern metrics\n            \n        Returns:\n            List of pattern types to focus on\n        ",
          "code_snippet": "        return aug_level\n    \n    def _get_focused_patterns(self, optimizer: Optional[Any]) -> List[str]:\n        \"\"\"\n        Get patterns to focus on during augmentation based on optimizer metrics.\n        \n        Args:\n            optimizer: Optimizer with pattern metrics\n            \n        Returns:\n            List of pattern types to focus on\n        \"\"\"\n        focused_patterns = []\n        \n        if optimizer and hasattr(optimizer, 'get_pattern_metrics'):\n            try:\n                pattern_metrics = optimizer.get_pattern_metrics()\n                \n                # Focus on patterns below minimum bound\n                if 'patterns_below_min' in pattern_metrics:\n                    focused_patterns.extend(pattern_metrics['patterns_below_min'])\n                \n                # Focus on patterns with high risk/accuracy ratio\n                if 'risk_accuracy_ratios' in pattern_metrics:\n                    ratios = pattern_metrics['risk_accuracy_ratios']\n                    # Add patterns with ratios > 1.2\n                    high_risk_patterns = [p for p, r in ratios.items() if r > 1.2]\n                    focused_patterns.extend(high_risk_patterns)\n                \n                # Remove duplicates while preserving order\n                focused_patterns = list(dict.fromkeys(focused_patterns))\n            \n            except Exception as e:\n                logger.warning(f\"Error getting focused patterns for augmentation: {str(e)}\")\n        \n        return focused_patterns\n    \n    def get_augmentation_history(self) -> List[Dict[str, Any]]:\n        \"\"\"\n        Get the history of augmentation decisions for analysis."
        },
        "get_augmentation_history": {
          "start_line": 306,
          "end_line": 315,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": "complex_type",
          "calls": [],
          "docstring": "\n        Get the history of augmentation decisions for analysis.\n        \n        Returns:\n            List of dictionaries with augmentation history details\n        ",
          "code_snippet": "        return focused_patterns\n    \n    def get_augmentation_history(self) -> List[Dict[str, Any]]:\n        \"\"\"\n        Get the history of augmentation decisions for analysis.\n        \n        Returns:\n            List of dictionaries with augmentation history details\n        \"\"\"\n        return self.augmentation_history\n\n\ndef create_transform_pipeline(augmentation_level: str = 'medium', \n                            dataset_type: str = 'image') -> Callable:"
        }
      },
      "class_variables": [],
      "bases": [],
      "docstring": "\n    Manages augmentation for training datasets based on pattern performance metrics.\n    \n    This class provides utilities for adaptively augmenting datasets during training,\n    focusing on patterns that need more examples based on their risk/accuracy ratios.\n    \n    Attributes:\n        augmentation_levels: Predefined augmentation levels and their intensities\n        current_augmentation_level: Current level of augmentation being applied\n        augmented_datasets: Dictionary of previously augmented datasets\n    "
    }
  },
  "functions": {
    "create_transform_pipeline": {
      "start_line": 316,
      "end_line": 421,
      "parameters": [
        {
          "name": "augmentation_level",
          "type": "str"
        },
        {
          "name": "dataset_type",
          "type": "str"
        }
      ],
      "return_type": "Callable",
      "calls": [
        {
          "name": "transforms.ToTensor",
          "line": 332
        },
        {
          "name": "transforms.Normalize",
          "line": 333
        },
        {
          "name": "transforms.Compose",
          "line": 338
        },
        {
          "name": "transforms.Compose",
          "line": 342
        },
        {
          "name": "text.split",
          "line": 383
        },
        {
          "name": "words.copy",
          "line": 384
        },
        {
          "name": "range",
          "line": 398
        },
        {
          "name": "....join",
          "line": 414
        },
        {
          "name": "transforms.Compose",
          "line": 351
        },
        {
          "name": "max",
          "line": 388
        },
        {
          "name": "random.randint",
          "line": 402
        },
        {
          "name": "transforms.RandomHorizontalFlip",
          "line": 343
        },
        {
          "name": "transforms.RandomRotation",
          "line": 344
        },
        {
          "name": "transforms.ColorJitter",
          "line": 345
        },
        {
          "name": "transforms.Compose",
          "line": 361
        },
        {
          "name": "int",
          "line": 388
        },
        {
          "name": "max",
          "line": 391
        },
        {
          "name": "max",
          "line": 394
        },
        {
          "name": "result.pop",
          "line": 406
        },
        {
          "name": "transforms.RandomHorizontalFlip",
          "line": 352
        },
        {
          "name": "transforms.RandomRotation",
          "line": 353
        },
        {
          "name": "transforms.ColorJitter",
          "line": 354
        },
        {
          "name": "transforms.RandomAffine",
          "line": 355
        },
        {
          "name": "int",
          "line": 391
        },
        {
          "name": "int",
          "line": 394
        },
        {
          "name": "len",
          "line": 402
        },
        {
          "name": "random.random",
          "line": 405
        },
        {
          "name": "random.randint",
          "line": 409
        },
        {
          "name": "transforms.RandomHorizontalFlip",
          "line": 362
        },
        {
          "name": "transforms.RandomVerticalFlip",
          "line": 363
        },
        {
          "name": "transforms.RandomRotation",
          "line": 364
        },
        {
          "name": "transforms.ColorJitter",
          "line": 365
        },
        {
          "name": "transforms.RandomAffine",
          "line": 366
        },
        {
          "name": "transforms.RandomPerspective",
          "line": 367
        },
        {
          "name": "len",
          "line": 388
        },
        {
          "name": "random.random",
          "line": 408
        },
        {
          "name": "len",
          "line": 408
        },
        {
          "name": "random.randint",
          "line": 411
        },
        {
          "name": "len",
          "line": 391
        },
        {
          "name": "len",
          "line": 394
        },
        {
          "name": "len",
          "line": 409
        },
        {
          "name": "len",
          "line": 411
        }
      ],
      "docstring": "\n    Create a transform pipeline based on augmentation level and dataset type.\n    \n    Args:\n        augmentation_level: Augmentation intensity ('none', 'light', 'medium', 'heavy')\n        dataset_type: Type of dataset ('image', 'text', 'tabular')\n        \n    Returns:\n        Callable: Transform function or pipeline\n    ",
      "code_snippet": "\n\ndef create_transform_pipeline(augmentation_level: str = 'medium', \n                            dataset_type: str = 'image') -> Callable:\n    \"\"\"\n    Create a transform pipeline based on augmentation level and dataset type.\n    \n    Args:\n        augmentation_level: Augmentation intensity ('none', 'light', 'medium', 'heavy')\n        dataset_type: Type of dataset ('image', 'text', 'tabular')\n        \n    Returns:\n        Callable: Transform function or pipeline\n    \"\"\"\n    # Image dataset transformation\n    if dataset_type == 'image':\n        # Base normalization\n        base_transforms = [\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ]\n        \n        # No augmentation\n        if augmentation_level == 'none':\n            return transforms.Compose(base_transforms)\n        \n        # Light augmentation\n        elif augmentation_level == 'light':\n            return transforms.Compose([\n                transforms.RandomHorizontalFlip(),\n                transforms.RandomRotation(10),\n                transforms.ColorJitter(brightness=0.1, contrast=0.1),\n                *base_transforms\n            ])\n        \n        # Medium augmentation\n        elif augmentation_level == 'medium':\n            return transforms.Compose([\n                transforms.RandomHorizontalFlip(),\n                transforms.RandomRotation(15),\n                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),\n                transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n                *base_transforms\n            ])\n        \n        # Heavy augmentation\n        elif augmentation_level == 'heavy':\n            return transforms.Compose([\n                transforms.RandomHorizontalFlip(),\n                transforms.RandomVerticalFlip(),\n                transforms.RandomRotation(20),\n                transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.1),\n                transforms.RandomAffine(degrees=10, translate=(0.15, 0.15), scale=(0.9, 1.1)),\n                transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n                *base_transforms\n            ])\n    \n    # Text dataset transformation\n    elif dataset_type == 'text':\n        # For text data, return a function that applies text augmentation\n        def text_transform(text):\n            import random\n            import re\n            \n            # No augmentation\n            if augmentation_level == 'none':\n                return text\n            \n            # Apply text augmentation based on level\n            words = text.split()\n            result = words.copy()\n            \n            # Light augmentation - synonym replacement for a small number of words\n            if augmentation_level == 'light':\n                num_to_replace = max(1, int(0.05 * len(words)))\n            # Medium augmentation - more synonym replacements + random word deletion\n            elif augmentation_level == 'medium':\n                num_to_replace = max(1, int(0.1 * len(words)))\n            # Heavy augmentation - more aggressive replacements, deletion, and swapping\n            else:  # heavy\n                num_to_replace = max(1, int(0.15 * len(words)))\n            \n            # Apply transformations\n            # (simplified version - in practice would use NLP libraries for synonyms)\n            for _ in range(num_to_replace):\n                if not words:\n                    break\n                    \n                idx = random.randint(0, len(words) - 1)\n                \n                # Random deletion\n                if random.random() < 0.3 and augmentation_level != 'light':\n                    result.pop(idx)\n                # Word swapping (for heavy only)\n                elif random.random() < 0.3 and augmentation_level == 'heavy' and len(words) > 2:\n                    swap_idx = random.randint(0, len(words) - 1)\n                    while swap_idx == idx:\n                        swap_idx = random.randint(0, len(words) - 1)\n                    result[idx], result[swap_idx] = result[swap_idx], result[idx]\n            \n            return \" \".join(result)\n        \n        return text_transform\n    \n    # Default for other data types - no transformation\n    return lambda x: x\n\n\ndef apply_augmentation_to_dataset(dataset: Dataset, \n                               augmentation_factor: float = 0.2,"
    },
    "apply_augmentation_to_dataset": {
      "start_line": 422,
      "end_line": 487,
      "parameters": [
        {
          "name": "dataset",
          "type": "Dataset"
        },
        {
          "name": "augmentation_factor",
          "type": "float"
        },
        {
          "name": "augmentation_level",
          "type": "str"
        },
        {
          "name": "focused_indices"
        }
      ],
      "return_type": "Dataset",
      "calls": [
        {
          "name": "len",
          "line": 442
        },
        {
          "name": "int",
          "line": 443
        },
        {
          "name": "Subset",
          "line": 470
        },
        {
          "name": "create_transform_pipeline",
          "line": 473
        },
        {
          "name": "random.sample",
          "line": 467
        },
        {
          "name": "TransformedSubset",
          "line": 478
        },
        {
          "name": "ConcatDataset",
          "line": 481
        },
        {
          "name": "len",
          "line": 449
        },
        {
          "name": "len",
          "line": 451
        },
        {
          "name": "random.sample",
          "line": 452
        },
        {
          "name": "focused_indices.copy",
          "line": 455
        },
        {
          "name": "range",
          "line": 467
        },
        {
          "name": "logger.error",
          "line": 484
        },
        {
          "name": "len",
          "line": 456
        },
        {
          "name": "set",
          "line": 459
        },
        {
          "name": "list",
          "line": 460
        },
        {
          "name": "range",
          "line": 459
        },
        {
          "name": "random.sample",
          "line": 463
        },
        {
          "name": "indices_to_augment.extend",
          "line": 464
        },
        {
          "name": "set",
          "line": 460
        },
        {
          "name": "min",
          "line": 463
        },
        {
          "name": "str",
          "line": 484
        },
        {
          "name": "len",
          "line": 463
        }
      ],
      "docstring": "\n    Apply augmentation to a dataset, creating a new dataset with additional examples.\n    \n    Args:\n        dataset: Original dataset to augment\n        augmentation_factor: Fraction of original dataset to add as augmented samples\n        augmentation_level: Augmentation intensity level\n        focused_indices: Optional list of indices to focus augmentation on\n        \n    Returns:\n        Dataset: Combined dataset with original and augmented examples\n    ",
      "code_snippet": "\n\ndef apply_augmentation_to_dataset(dataset: Dataset, \n                               augmentation_factor: float = 0.2,\n                               augmentation_level: str = 'medium',\n                               focused_indices: Optional[List[int]] = None) -> Dataset:\n    \"\"\"\n    Apply augmentation to a dataset, creating a new dataset with additional examples.\n    \n    Args:\n        dataset: Original dataset to augment\n        augmentation_factor: Fraction of original dataset to add as augmented samples\n        augmentation_level: Augmentation intensity level\n        focused_indices: Optional list of indices to focus augmentation on\n        \n    Returns:\n        Dataset: Combined dataset with original and augmented examples\n    \"\"\"\n    if augmentation_factor <= 0:\n        return dataset\n    \n    # Determine number of examples to augment\n    orig_size = len(dataset)\n    num_to_augment = int(orig_size * augmentation_factor)\n    \n    if num_to_augment <= 0:\n        return dataset\n    \n    # If focused indices provided, use them, otherwise sample randomly\n    if focused_indices and len(focused_indices) > 0:\n        # If more focused indices than needed, sample from them\n        if len(focused_indices) > num_to_augment:\n            indices_to_augment = random.sample(focused_indices, num_to_augment)\n        # Otherwise use all focused indices and sample additional if needed\n        else:\n            indices_to_augment = focused_indices.copy()\n            additional_needed = num_to_augment - len(focused_indices)\n            if additional_needed > 0:\n                # Create list of all non-focused indices\n                all_indices = set(range(orig_size))\n                non_focused = list(all_indices - set(focused_indices))\n                # Sample from non-focused indices\n                if non_focused:\n                    additional_indices = random.sample(non_focused, min(additional_needed, len(non_focused)))\n                    indices_to_augment.extend(additional_indices)\n    else:\n        # Random sampling\n        indices_to_augment = random.sample(range(orig_size), num_to_augment)\n    \n    # Create a dataset for selected indices\n    augment_subset = Subset(dataset, indices_to_augment)\n    \n    # Create transform for augmentation\n    augment_transform = create_transform_pipeline(augmentation_level)\n    \n    # Create new dataset with transformed data\n    try:\n        from isekaizen.data.augmented_dataset import TransformedSubset\n        augmented_subset = TransformedSubset(augment_subset, transform=augment_transform)\n        \n        # Return combined dataset (original + augmented)\n        return ConcatDataset([dataset, augmented_subset])\n    \n    except Exception as e:\n        logger.error(f\"Error applying augmentation: {str(e)}\")\n        return dataset"
    }
  },
  "constants": {}
}