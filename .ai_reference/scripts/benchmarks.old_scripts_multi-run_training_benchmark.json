{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\benchmarks.old\\scripts\\multi-run_training_benchmark.py",
  "imports": [
    {
      "name": "torch",
      "line": 2
    },
    {
      "name": "torch.nn",
      "line": 3
    },
    {
      "name": "torch.optim",
      "line": 4
    },
    {
      "name": "time",
      "line": 5
    },
    {
      "name": "numpy",
      "line": 6
    },
    {
      "name": "pandas",
      "line": 7
    },
    {
      "name": "datetime.datetime",
      "line": 8
    },
    {
      "name": "argparse",
      "line": 165
    }
  ],
  "classes": {
    "SimpleModel": {
      "start_line": 10,
      "end_line": 26,
      "methods": {
        "__init__": {
          "start_line": 11,
          "end_line": 22,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "....__init__",
              "line": 12
            },
            {
              "name": "nn.Sequential",
              "line": 13
            },
            {
              "name": "nn.Linear",
              "line": 14
            },
            {
              "name": "nn.ReLU",
              "line": 15
            },
            {
              "name": "nn.Linear",
              "line": 16
            },
            {
              "name": "nn.ReLU",
              "line": 17
            },
            {
              "name": "nn.Linear",
              "line": 18
            },
            {
              "name": "nn.ReLU",
              "line": 19
            },
            {
              "name": "nn.Linear",
              "line": 20
            },
            {
              "name": "super",
              "line": 12
            }
          ],
          "code_snippet": "\nclass SimpleModel(nn.Module):\n    def __init__(self):\n        super(SimpleModel, self).__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(784, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 10)\n        )\n    \n    def forward(self, x):\n        return self.layers(x)"
        },
        "forward": {
          "start_line": 23,
          "end_line": 26,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "x"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.layers",
              "line": 24
            }
          ],
          "code_snippet": "        )\n    \n    def forward(self, x):\n        return self.layers(x)\n\ndef run_single_benchmark(batch_sizes, device, iterations_per_batch, run_index):\n    \"\"\"Run a single full benchmark across all batch sizes\"\"\"\n    run_results = []"
        }
      },
      "class_variables": [],
      "bases": [
        "..."
      ]
    }
  },
  "functions": {
    "run_single_benchmark": {
      "start_line": 26,
      "end_line": 95,
      "parameters": [
        {
          "name": "batch_sizes"
        },
        {
          "name": "device"
        },
        {
          "name": "iterations_per_batch"
        },
        {
          "name": "run_index"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "....to",
          "line": 32
        },
        {
          "name": "nn.CrossEntropyLoss",
          "line": 33
        },
        {
          "name": "optim.Adam",
          "line": 34
        },
        {
          "name": "torch.randn",
          "line": 38
        },
        {
          "name": "torch.randint",
          "line": 39
        },
        {
          "name": "range",
          "line": 42
        },
        {
          "name": "time.time",
          "line": 53
        },
        {
          "name": "range",
          "line": 55
        },
        {
          "name": "time.time",
          "line": 68
        },
        {
          "name": "run_results.append",
          "line": 80
        },
        {
          "name": "model.parameters",
          "line": 34
        },
        {
          "name": "optimizer.zero_grad",
          "line": 43
        },
        {
          "name": "model",
          "line": 44
        },
        {
          "name": "criterion",
          "line": 45
        },
        {
          "name": "loss.backward",
          "line": 46
        },
        {
          "name": "optimizer.step",
          "line": 47
        },
        {
          "name": "torch.cuda.synchronize",
          "line": 50
        },
        {
          "name": "optimizer.zero_grad",
          "line": 57
        },
        {
          "name": "model",
          "line": 58
        },
        {
          "name": "criterion",
          "line": 59
        },
        {
          "name": "loss.backward",
          "line": 62
        },
        {
          "name": "optimizer.step",
          "line": 63
        },
        {
          "name": "torch.cuda.empty_cache",
          "line": 91
        },
        {
          "name": "SimpleModel",
          "line": 32
        },
        {
          "name": "torch.cuda.synchronize",
          "line": 66
        },
        {
          "name": "torch.cuda.memory_allocated",
          "line": 76
        }
      ],
      "docstring": "Run a single full benchmark across all batch sizes",
      "code_snippet": "        return self.layers(x)\n\ndef run_single_benchmark(batch_sizes, device, iterations_per_batch, run_index):\n    \"\"\"Run a single full benchmark across all batch sizes\"\"\"\n    run_results = []\n    \n    for batch_size in batch_sizes:\n        # Create a fresh model and optimizer for each batch size test\n        model = SimpleModel().to(device)\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.Adam(model.parameters(), lr=0.001)\n        \n        # Create synthetic training data\n        input_dim = 784\n        inputs = torch.randn(batch_size, input_dim, device=device)\n        labels = torch.randint(0, 10, (batch_size,), device=device)\n        \n        # Warmup\n        for _ in range(5):\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n        if device == \"cuda\":\n            torch.cuda.synchronize()\n        \n        # Run the timed test\n        start_time = time.time()\n        \n        for _ in range(iterations_per_batch):\n            # Forward pass\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            \n            # Backward pass and optimize\n            loss.backward()\n            optimizer.step()\n            \n            if device == \"cuda\":\n                torch.cuda.synchronize()\n        \n        end_time = time.time()\n        \n        # Calculate metrics\n        total_time = end_time - start_time\n        time_per_batch = total_time / iterations_per_batch\n        samples_per_second = batch_size / time_per_batch\n        \n        if device == \"cuda\":\n            memory_allocated = torch.cuda.memory_allocated() / 1e6  # MB\n        else:\n            memory_allocated = 0\n        \n        run_results.append({\n            'batch_size': batch_size,\n            'time_per_batch': time_per_batch,\n            'samples_per_second': samples_per_second,\n            'memory_allocated': memory_allocated,\n            'run_index': run_index\n        })\n        \n        # Clean up to prevent memory accumulation\n        del model, optimizer, inputs, labels\n        if device == \"cuda\":\n            torch.cuda.empty_cache()\n    \n    return run_results\n\ndef run_training_benchmark(num_runs=5):\n    \"\"\"Run the full training benchmark multiple times and average the results\"\"\"\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
    },
    "run_training_benchmark": {
      "start_line": 95,
      "end_line": 164,
      "parameters": [
        {
          "name": "num_runs"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "print",
          "line": 98
        },
        {
          "name": "print",
          "line": 99
        },
        {
          "name": "range",
          "line": 107
        },
        {
          "name": "pd.DataFrame",
          "line": 117
        },
        {
          "name": "....agg",
          "line": 120
        },
        {
          "name": "print",
          "line": 127
        },
        {
          "name": "print",
          "line": 128
        },
        {
          "name": "print",
          "line": 130
        },
        {
          "name": "print",
          "line": 131
        },
        {
          "name": "print",
          "line": 132
        },
        {
          "name": "print",
          "line": 137
        },
        {
          "name": "print",
          "line": 138
        },
        {
          "name": "print",
          "line": 139
        },
        {
          "name": "print",
          "line": 144
        },
        {
          "name": "print",
          "line": 145
        },
        {
          "name": "print",
          "line": 146
        },
        {
          "name": "....idxmax",
          "line": 152
        },
        {
          "name": "print",
          "line": 153
        },
        {
          "name": "print",
          "line": 154
        },
        {
          "name": "....strftime",
          "line": 157
        },
        {
          "name": "df.to_csv",
          "line": 161
        },
        {
          "name": "print",
          "line": 162
        },
        {
          "name": "torch.cuda.is_available",
          "line": 97
        },
        {
          "name": "print",
          "line": 108
        },
        {
          "name": "run_single_benchmark",
          "line": 111
        },
        {
          "name": "all_results.extend",
          "line": 112
        },
        {
          "name": "print",
          "line": 114
        },
        {
          "name": "print",
          "line": 135
        },
        {
          "name": "print",
          "line": 142
        },
        {
          "name": "print",
          "line": 149
        },
        {
          "name": "df.groupby",
          "line": 120
        },
        {
          "name": "datetime.now",
          "line": 157
        }
      ],
      "docstring": "Run the full training benchmark multiple times and average the results",
      "code_snippet": "    return run_results\n\ndef run_training_benchmark(num_runs=5):\n    \"\"\"Run the full training benchmark multiple times and average the results\"\"\"\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Running training benchmark on: {device}\")\n    print(f\"Will run the complete benchmark {num_runs} times for more reliable results\")\n    \n    batch_sizes = [32, 64, 128, 256, 512, 1024]\n    iterations_per_batch = 50\n    \n    all_results = []\n    \n    # Run the complete benchmark multiple times\n    for run in range(num_runs):\n        print(f\"\\n--- Starting benchmark run {run+1}/{num_runs} ---\")\n        \n        # Run a complete benchmark\n        run_results = run_single_benchmark(batch_sizes, device, iterations_per_batch, run)\n        all_results.extend(run_results)\n        \n        print(f\"--- Completed benchmark run {run+1}/{num_runs} ---\")\n    \n    # Convert to DataFrame for easier analysis\n    df = pd.DataFrame(all_results)\n    \n    # Calculate aggregate statistics across runs\n    aggregated = df.groupby('batch_size').agg({\n        'time_per_batch': ['mean', 'std', 'min', 'max'],\n        'samples_per_second': ['mean', 'std', 'min', 'max'], \n        'memory_allocated': 'mean'\n    })\n    \n    # Print a detailed summary\n    print(\"\\n===== TRAINING PERFORMANCE SUMMARY (AVERAGED OVER MULTIPLE RUNS) =====\")\n    print(f\"Number of complete benchmark runs: {num_runs}\")\n    \n    print(\"\\n--- Time per Batch (seconds) ---\")\n    print(\"Batch Size | Mean   | Std Dev | Min    | Max\")\n    print(\"-\" * 50)\n    for batch_size in batch_sizes:\n        stats = aggregated.loc[batch_size]['time_per_batch']\n        print(f\"{batch_size:^10} | {stats['mean']:.5f} | {stats['std']:.5f} | {stats['min']:.5f} | {stats['max']:.5f}\")\n    \n    print(\"\\n--- Training Throughput (samples/second) ---\")\n    print(\"Batch Size | Mean     | Std Dev  | Min      | Max\")\n    print(\"-\" * 55)\n    for batch_size in batch_sizes:\n        stats = aggregated.loc[batch_size]['samples_per_second']\n        print(f\"{batch_size:^10} | {stats['mean']:8.1f} | {stats['std']:8.1f} | {stats['min']:8.1f} | {stats['max']:8.1f}\")\n    \n    print(\"\\n--- GPU Memory Usage (MB) ---\")\n    print(\"Batch Size | Memory\")\n    print(\"-\" * 25)\n    for batch_size in batch_sizes:\n        memory = aggregated.loc[batch_size]['memory_allocated']['mean']\n        print(f\"{batch_size:^10} | {memory:.1f}\")\n    \n    # Find optimal batch size based on throughput\n    max_throughput_idx = aggregated['samples_per_second']['mean'].idxmax()\n    print(f\"\\nOptimal batch size for throughput: {max_throughput_idx}\")\n    print(f\"Maximum throughput: {aggregated.loc[max_throughput_idx]['samples_per_second']['mean']:.1f} samples/second\")\n    \n    # Generate a timestamp for the results file\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    \n    # Save the detailed results to CSV for further analysis\n    csv_filename = f\"training_results_{timestamp}.csv\"\n    df.to_csv(csv_filename, index=False)\n    print(f\"\\nDetailed results saved to {csv_filename}\")\n\nif __name__ == \"__main__\":\n    import argparse\n    "
    }
  },
  "constants": {}
}