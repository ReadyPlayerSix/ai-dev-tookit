{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\benchmarks.old\\models\\src\\utils\\analysist_tools\\log_analyzer.py",
  "imports": [
    {
      "name": "numpy",
      "line": 1
    },
    {
      "name": "pathlib.Path",
      "line": 2
    },
    {
      "name": "re",
      "line": 3
    },
    {
      "name": "datetime.datetime",
      "line": 4
    },
    {
      "name": "matplotlib.pyplot",
      "line": 5
    },
    {
      "name": "typing.Dict",
      "line": 6
    },
    {
      "name": "typing.List",
      "line": 6
    },
    {
      "name": "typing.Tuple",
      "line": 6
    },
    {
      "name": "typing.Any",
      "line": 6
    },
    {
      "name": "typing.Optional",
      "line": 6
    },
    {
      "name": "json",
      "line": 7
    },
    {
      "name": "logging",
      "line": 8
    },
    {
      "name": "tqdm.tqdm",
      "line": 9
    },
    {
      "name": "collections.defaultdict",
      "line": 10
    },
    {
      "name": "argparse",
      "line": 318
    }
  ],
  "classes": {
    "TrainingLogAnalyzer": {
      "start_line": 12,
      "end_line": 317,
      "methods": {
        "__init__": {
          "start_line": 15,
          "end_line": 40,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "log_path",
              "type": "str"
            },
            {
              "name": "chunk_size",
              "type": "int"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "Path",
              "line": 16
            },
            {
              "name": "set",
              "line": 21
            },
            {
              "name": "logging.basicConfig",
              "line": 34
            },
            {
              "name": "logging.getLogger",
              "line": 35
            },
            {
              "name": "float",
              "line": 28
            }
          ],
          "code_snippet": "    \"\"\"Analyzes large training log files from the Neural Network Cortex System\"\"\"\n    \n    def __init__(self, log_path: str, chunk_size: int = 1024 * 1024):\n        self.log_path = Path(log_path)\n        self.chunk_size = chunk_size\n        \n        # Use synchronized storage for metrics\n        self.metrics_entries = []\n        self.seen_batches = set()\n        \n        # Performance tracking\n        self.performance = {\n            \"total_batches\": 0,\n            \"avg_processing_time\": 0.0,\n            \"peak_processing_time\": 0.0,\n            \"min_processing_time\": float('inf'),\n            \"acceleration_factor\": 0.0,\n            \"anomalies_detected\": 0\n        }\n        \n        # Setup logging\n        logging.basicConfig(level=logging.INFO)\n        self.logger = logging.getLogger(__name__)\n        \n        self.last_valid_timestamp = None\n        self.high_speed_mode = False\n    \n    def _parse_timestamp(self, line: str) -> Optional[datetime]:\n        \"\"\"Extract and validate timestamp from log line\"\"\"\n        try:"
        },
        "_parse_timestamp": {
          "start_line": 40,
          "end_line": 59,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "line",
              "type": "str"
            }
          ],
          "return_type": "complex_type",
          "calls": [
            {
              "name": "datetime.strptime",
              "line": 44
            },
            {
              "name": "line.split",
              "line": 43
            },
            {
              "name": "time_diff.total_seconds",
              "line": 49
            },
            {
              "name": "time_diff.total_seconds",
              "line": 51
            }
          ],
          "docstring": "Extract and validate timestamp from log line",
          "code_snippet": "        self.high_speed_mode = False\n    \n    def _parse_timestamp(self, line: str) -> Optional[datetime]:\n        \"\"\"Extract and validate timestamp from log line\"\"\"\n        try:\n            timestamp_str = line.split(' - ')[0]\n            timestamp = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S,%f')\n            \n            # Validate timestamp is reasonable\n            if self.last_valid_timestamp:\n                time_diff = timestamp - self.last_valid_timestamp\n                if time_diff.total_seconds() < -1:  # Allow 1 second backwards for minor log issues\n                    return None\n                elif time_diff.total_seconds() == 0:\n                    self.high_speed_mode = True\n            \n            self.last_valid_timestamp = timestamp\n            return timestamp\n        except:\n            return None\n    \n    def _extract_batch_info(self, line: str) -> Dict[str, Any]:\n        \"\"\"Extract and validate batch processing information\"\"\"\n        info = {}"
        },
        "_extract_batch_info": {
          "start_line": 59,
          "end_line": 94,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "line",
              "type": "str"
            }
          ],
          "return_type": "complex_type",
          "calls": [
            {
              "name": "re.search",
              "line": 64
            },
            {
              "name": "re.search",
              "line": 72
            },
            {
              "name": "re.search",
              "line": 79
            },
            {
              "name": "re.search",
              "line": 86
            },
            {
              "name": "line.lower",
              "line": 64
            },
            {
              "name": "int",
              "line": 66
            },
            {
              "name": "float",
              "line": 74
            },
            {
              "name": "float",
              "line": 81
            },
            {
              "name": "float",
              "line": 88
            },
            {
              "name": "batch_match.group",
              "line": 66
            },
            {
              "name": "self.seen_batches.add",
              "line": 69
            },
            {
              "name": "complexity_match.group",
              "line": 74
            },
            {
              "name": "lr_match.group",
              "line": 81
            },
            {
              "name": "memory_match.group",
              "line": 88
            }
          ],
          "docstring": "Extract and validate batch processing information",
          "code_snippet": "            return None\n    \n    def _extract_batch_info(self, line: str) -> Dict[str, Any]:\n        \"\"\"Extract and validate batch processing information\"\"\"\n        info = {}\n        \n        # Extract batch number\n        batch_match = re.search(r'batch[^\\d]*(\\d+)', line.lower())\n        if batch_match:\n            batch_num = int(batch_match.group(1))\n            if batch_num not in self.seen_batches:\n                info['batch_number'] = batch_num\n                self.seen_batches.add(batch_num)\n        \n        # Extract complexity\n        complexity_match = re.search(r'complexity[^\\d]*(\\d+\\.\\d+)', line)\n        if complexity_match:\n            complexity = float(complexity_match.group(1))\n            if 0 <= complexity <= 100:\n                info['complexity'] = complexity\n        \n        # Extract learning rate\n        lr_match = re.search(r'learning rate:[^\\d]*(\\d+\\.\\d+)', line)\n        if lr_match:\n            lr = float(lr_match.group(1))\n            if 0 <= lr <= 1:\n                info['learning_rate'] = lr\n        \n        # Extract memory usage\n        memory_match = re.search(r'memory:?[^\\d]*(\\d+\\.?\\d*)\\s*MB', line)\n        if memory_match:\n            memory = float(memory_match.group(1))\n            if 0 <= memory <= 12288:\n                info['memory_mb'] = memory\n        \n        return info\n    \n    def _calculate_processing_time(self, current_time: datetime, last_time: datetime) -> Optional[float]:\n        \"\"\"Calculate and validate processing time between log entries\"\"\"\n        if not (last_time and current_time):"
        },
        "_calculate_processing_time": {
          "start_line": 94,
          "end_line": 112,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "current_time",
              "type": "datetime"
            },
            {
              "name": "last_time",
              "type": "datetime"
            }
          ],
          "return_type": "complex_type",
          "calls": [
            {
              "name": "....total_seconds",
              "line": 99
            }
          ],
          "docstring": "Calculate and validate processing time between log entries",
          "code_snippet": "        return info\n    \n    def _calculate_processing_time(self, current_time: datetime, last_time: datetime) -> Optional[float]:\n        \"\"\"Calculate and validate processing time between log entries\"\"\"\n        if not (last_time and current_time):\n            return None\n            \n        time_diff = (current_time - last_time).total_seconds() * 1000\n        \n        # Handle high-speed processing\n        if self.high_speed_mode and time_diff == 0:\n            return 0.001  # 1 microsecond minimum\n        \n        # Validate reasonable timing\n        if 0 <= time_diff <= 1000:\n            return time_diff\n        else:\n            self.performance[\"anomalies_detected\"] += 1\n            return None\n    \n    def analyze_log(self) -> Dict[str, Any]:\n        \"\"\"Analyze the training log file in chunks with improved validation\"\"\"\n        self.logger.info(f\"Analyzing log file: {self.log_path}\")"
        },
        "analyze_log": {
          "start_line": 112,
          "end_line": 163,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": "complex_type",
          "calls": [
            {
              "name": "self.logger.info",
              "line": 114
            },
            {
              "name": "self._generate_analysis_report",
              "line": 161
            },
            {
              "name": "self.log_path.stat",
              "line": 115
            },
            {
              "name": "tqdm",
              "line": 121
            },
            {
              "name": "open",
              "line": 122
            },
            {
              "name": "f.read",
              "line": 124
            },
            {
              "name": "chunk.split",
              "line": 128
            },
            {
              "name": "pbar.update",
              "line": 159
            },
            {
              "name": "self._parse_timestamp",
              "line": 134
            },
            {
              "name": "self._extract_batch_info",
              "line": 147
            },
            {
              "name": "current_entry.update",
              "line": 148
            },
            {
              "name": "len",
              "line": 159
            },
            {
              "name": "self.metrics_entries.append",
              "line": 156
            },
            {
              "name": "chunk.encode",
              "line": 159
            },
            {
              "name": "self._calculate_processing_time",
              "line": 137
            },
            {
              "name": "current_entry.copy",
              "line": 156
            }
          ],
          "docstring": "Analyze the training log file in chunks with improved validation",
          "code_snippet": "            return None\n    \n    def analyze_log(self) -> Dict[str, Any]:\n        \"\"\"Analyze the training log file in chunks with improved validation\"\"\"\n        self.logger.info(f\"Analyzing log file: {self.log_path}\")\n        file_size = self.log_path.stat().st_size\n        \n        last_timestamp = None\n        current_entry = {}\n        \n        # Process file in chunks\n        with tqdm(total=file_size, unit='B', unit_scale=True, desc=\"Processing log\") as pbar:\n            with open(self.log_path, 'r', encoding='utf-8') as f:\n                while True:\n                    chunk = f.read(self.chunk_size)\n                    if not chunk:\n                        break\n                    \n                    lines = chunk.split('\\n')\n                    for line in lines:\n                        if not line:\n                            continue\n                        \n                        # Process timestamp\n                        current_timestamp = self._parse_timestamp(line)\n                        if current_timestamp:\n                            if last_timestamp:\n                                processing_time = self._calculate_processing_time(\n                                    current_timestamp, \n                                    last_timestamp\n                                )\n                                if processing_time is not None:\n                                    current_entry['timestamp'] = current_timestamp\n                                    current_entry['processing_time'] = processing_time\n                            last_timestamp = current_timestamp\n                        \n                        # Process batch information\n                        batch_info = self._extract_batch_info(line)\n                        current_entry.update(batch_info)\n                        \n                        # If we have a complete entry, store it\n                        if 'timestamp' in current_entry and (\n                            'batch_number' in current_entry or\n                            'complexity' in current_entry or\n                            'learning_rate' in current_entry\n                        ):\n                            self.metrics_entries.append(current_entry.copy())\n                            current_entry = {}\n                    \n                    pbar.update(len(chunk.encode('utf-8')))\n        \n        return self._generate_analysis_report()\n    \n    def _calculate_acceleration(self) -> float:\n        \"\"\"Calculate acceleration factor with improved accuracy\"\"\"\n        if not self.metrics_entries:"
        },
        "_calculate_acceleration": {
          "start_line": 163,
          "end_line": 190,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": "float",
          "calls": [
            {
              "name": "min",
              "line": 176
            },
            {
              "name": "min",
              "line": 177
            },
            {
              "name": "len",
              "line": 172
            },
            {
              "name": "np.mean",
              "line": 182
            },
            {
              "name": "np.mean",
              "line": 183
            },
            {
              "name": "len",
              "line": 176
            }
          ],
          "docstring": "Calculate acceleration factor with improved accuracy",
          "code_snippet": "        return self._generate_analysis_report()\n    \n    def _calculate_acceleration(self) -> float:\n        \"\"\"Calculate acceleration factor with improved accuracy\"\"\"\n        if not self.metrics_entries:\n            return 1.0\n        \n        # Get processing times\n        times = [entry['processing_time'] for entry in self.metrics_entries \n                if 'processing_time' in entry]\n        \n        if len(times) < 2:\n            return 1.0\n        \n        # Compare start and end periods\n        n_samples = min(100, len(times) // 10)\n        warmup_period = min(50, n_samples)\n        \n        initial_times = times[warmup_period:warmup_period + n_samples]\n        final_times = times[-n_samples:]\n        \n        initial_avg = np.mean(initial_times) if initial_times else 1.0\n        final_avg = np.mean(final_times) if final_times else 1.0\n        \n        if final_avg <= 0 or initial_avg <= 0:\n            return 1.0\n            \n        return initial_avg / final_avg\n    \n    def _generate_analysis_report(self) -> Dict[str, Any]:\n        \"\"\"Generate comprehensive analysis report with validated metrics\"\"\"\n        if not self.metrics_entries:"
        },
        "_generate_analysis_report": {
          "start_line": 190,
          "end_line": 253,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": "complex_type",
          "calls": [
            {
              "name": "self.performance.update",
              "line": 205
            },
            {
              "name": "self.logger.warning",
              "line": 193
            },
            {
              "name": "self.logger.warning",
              "line": 201
            },
            {
              "name": "len",
              "line": 206
            },
            {
              "name": "float",
              "line": 207
            },
            {
              "name": "float",
              "line": 208
            },
            {
              "name": "float",
              "line": 209
            },
            {
              "name": "self._calculate_acceleration",
              "line": 210
            },
            {
              "name": "float",
              "line": 240
            },
            {
              "name": "float",
              "line": 241
            },
            {
              "name": "float",
              "line": 242
            },
            {
              "name": "len",
              "line": 247
            },
            {
              "name": "np.mean",
              "line": 207
            },
            {
              "name": "np.max",
              "line": 208
            },
            {
              "name": "np.min",
              "line": 209
            },
            {
              "name": "max",
              "line": 236
            },
            {
              "name": "float",
              "line": 237
            },
            {
              "name": "np.min",
              "line": 240
            },
            {
              "name": "np.max",
              "line": 241
            },
            {
              "name": "np.mean",
              "line": 242
            },
            {
              "name": "np.mean",
              "line": 237
            }
          ],
          "docstring": "Generate comprehensive analysis report with validated metrics",
          "code_snippet": "        return initial_avg / final_avg\n    \n    def _generate_analysis_report(self) -> Dict[str, Any]:\n        \"\"\"Generate comprehensive analysis report with validated metrics\"\"\"\n        if not self.metrics_entries:\n            self.logger.warning(\"No valid metrics found!\")\n            return {\"error\": \"No valid metrics found\"}\n        \n        # Extract valid processing times\n        processing_times = [entry['processing_time'] for entry in self.metrics_entries \n                          if 'processing_time' in entry and entry['processing_time'] > 0]\n        \n        if not processing_times:\n            self.logger.warning(\"No valid processing times found!\")\n            return {\"error\": \"No valid processing times found\"}\n        \n        # Update performance metrics\n        self.performance.update({\n            \"total_batches\": len(self.seen_batches),\n            \"avg_processing_time\": float(np.mean(processing_times)),\n            \"peak_processing_time\": float(np.max(processing_times)),\n            \"min_processing_time\": float(np.min(processing_times)),\n            \"acceleration_factor\": self._calculate_acceleration(),\n            \"high_speed_mode_detected\": self.high_speed_mode\n        })\n        \n        # Calculate complexity growth\n        complexities = [entry['complexity'] for entry in self.metrics_entries \n                       if 'complexity' in entry]\n        complexity_growth = (complexities[-1] / complexities[0]) if complexities else None\n        \n        # Calculate learning rates\n        learning_rates = [entry['learning_rate'] for entry in self.metrics_entries \n                         if 'learning_rate' in entry]\n        \n        # Calculate memory usage\n        memory_usage = [entry['memory_mb'] for entry in self.metrics_entries \n                       if 'memory_mb' in entry]\n        \n        # Generate report\n        report = {\n            \"performance_metrics\": self.performance,\n            \"learning_progression\": {\n                \"initial_learning_rate\": learning_rates[0] if learning_rates else None,\n                \"final_learning_rate\": learning_rates[-1] if learning_rates else None,\n                \"complexity_growth\": complexity_growth\n            },\n            \"resource_usage\": {\n                \"peak_memory\": max(memory_usage) if memory_usage else None,\n                \"avg_memory\": float(np.mean(memory_usage)) if memory_usage else None\n            },\n            \"processing_speed\": {\n                \"fastest_batch\": float(np.min(processing_times)),\n                \"slowest_batch\": float(np.max(processing_times)),\n                \"average_batch\": float(np.mean(processing_times))\n            },\n            \"validation_info\": {\n                \"anomalies_detected\": self.performance[\"anomalies_detected\"],\n                \"high_speed_mode\": self.high_speed_mode,\n                \"total_valid_entries\": len(processing_times)\n            }\n        }\n        \n        return report\n    \n    def generate_visualizations(self, output_dir: str = \"analysis_results\"):\n        \"\"\"Generate visualization plots with improved data validation\"\"\"\n        if not self.metrics_entries:"
        },
        "generate_visualizations": {
          "start_line": 253,
          "end_line": 317,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "output_dir",
              "type": "str"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "Path",
              "line": 259
            },
            {
              "name": "output_path.mkdir",
              "line": 260
            },
            {
              "name": "self.logger.warning",
              "line": 256
            },
            {
              "name": "plt.figure",
              "line": 274
            },
            {
              "name": "plt.plot",
              "line": 275
            },
            {
              "name": "plt.title",
              "line": 276
            },
            {
              "name": "plt.xlabel",
              "line": 277
            },
            {
              "name": "plt.ylabel",
              "line": 278
            },
            {
              "name": "plt.yscale",
              "line": 279
            },
            {
              "name": "plt.grid",
              "line": 280
            },
            {
              "name": "plt.savefig",
              "line": 281
            },
            {
              "name": "plt.close",
              "line": 282
            },
            {
              "name": "plt.figure",
              "line": 286
            },
            {
              "name": "plt.plot",
              "line": 287
            },
            {
              "name": "plt.title",
              "line": 288
            },
            {
              "name": "plt.xlabel",
              "line": 289
            },
            {
              "name": "plt.ylabel",
              "line": 290
            },
            {
              "name": "plt.grid",
              "line": 291
            },
            {
              "name": "plt.savefig",
              "line": 292
            },
            {
              "name": "plt.close",
              "line": 293
            },
            {
              "name": "plt.figure",
              "line": 297
            },
            {
              "name": "plt.plot",
              "line": 298
            },
            {
              "name": "plt.title",
              "line": 299
            },
            {
              "name": "plt.xlabel",
              "line": 300
            },
            {
              "name": "plt.ylabel",
              "line": 301
            },
            {
              "name": "plt.grid",
              "line": 302
            },
            {
              "name": "plt.savefig",
              "line": 303
            },
            {
              "name": "plt.close",
              "line": 304
            },
            {
              "name": "plt.figure",
              "line": 308
            },
            {
              "name": "plt.plot",
              "line": 309
            },
            {
              "name": "plt.title",
              "line": 310
            },
            {
              "name": "plt.xlabel",
              "line": 311
            },
            {
              "name": "plt.ylabel",
              "line": 312
            },
            {
              "name": "plt.grid",
              "line": 313
            },
            {
              "name": "plt.savefig",
              "line": 314
            },
            {
              "name": "plt.close",
              "line": 315
            }
          ],
          "docstring": "Generate visualization plots with improved data validation",
          "code_snippet": "        return report\n    \n    def generate_visualizations(self, output_dir: str = \"analysis_results\"):\n        \"\"\"Generate visualization plots with improved data validation\"\"\"\n        if not self.metrics_entries:\n            self.logger.warning(\"No data available for visualization\")\n            return\n            \n        output_path = Path(output_dir)\n        output_path.mkdir(parents=True, exist_ok=True)\n        \n        # Extract time series data\n        times = [entry['processing_time'] for entry in self.metrics_entries \n                if 'processing_time' in entry]\n        complexities = [entry['complexity'] for entry in self.metrics_entries \n                       if 'complexity' in entry]\n        learning_rates = [entry['learning_rate'] for entry in self.metrics_entries \n                         if 'learning_rate' in entry]\n        memory_usage = [entry['memory_mb'] for entry in self.metrics_entries \n                       if 'memory_mb' in entry]\n        \n        # Processing time progression\n        if times:\n            plt.figure(figsize=(12, 6))\n            plt.plot(times)\n            plt.title(\"Processing Time Progression\")\n            plt.xlabel(\"Batch Number\")\n            plt.ylabel(\"Processing Time (ms)\")\n            plt.yscale('log')\n            plt.grid(True)\n            plt.savefig(output_path / \"processing_time.png\")\n            plt.close()\n        \n        # Learning rate progression\n        if learning_rates:\n            plt.figure(figsize=(12, 6))\n            plt.plot(learning_rates)\n            plt.title(\"Learning Rate Progression\")\n            plt.xlabel(\"Checkpoint\")\n            plt.ylabel(\"Learning Rate\")\n            plt.grid(True)\n            plt.savefig(output_path / \"learning_rate.png\")\n            plt.close()\n        \n        # Complexity growth\n        if complexities:\n            plt.figure(figsize=(12, 6))\n            plt.plot(complexities)\n            plt.title(\"Pattern Complexity Growth\")\n            plt.xlabel(\"Batch Number\")\n            plt.ylabel(\"Complexity\")\n            plt.grid(True)\n            plt.savefig(output_path / \"complexity.png\")\n            plt.close()\n        \n        # Memory usage\n        if memory_usage:\n            plt.figure(figsize=(12, 6))\n            plt.plot(memory_usage)\n            plt.title(\"Memory Usage Over Time\")\n            plt.xlabel(\"Checkpoint\")\n            plt.ylabel(\"Memory Usage (MB)\")\n            plt.grid(True)\n            plt.savefig(output_path / \"memory_usage.png\")\n            plt.close()\n\ndef main():\n    import argparse\n    "
        }
      },
      "class_variables": [],
      "bases": [],
      "docstring": "Analyzes large training log files from the Neural Network Cortex System"
    }
  },
  "functions": {
    "main": {
      "start_line": 317,
      "end_line": 350,
      "parameters": [],
      "return_type": null,
      "calls": [
        {
          "name": "argparse.ArgumentParser",
          "line": 320
        },
        {
          "name": "parser.add_argument",
          "line": 321
        },
        {
          "name": "parser.add_argument",
          "line": 322
        },
        {
          "name": "parser.parse_args",
          "line": 324
        },
        {
          "name": "TrainingLogAnalyzer",
          "line": 327
        },
        {
          "name": "analyzer.analyze_log",
          "line": 328
        },
        {
          "name": "analyzer.generate_visualizations",
          "line": 331
        },
        {
          "name": "....absolute",
          "line": 334
        },
        {
          "name": "output_path.mkdir",
          "line": 335
        },
        {
          "name": "print",
          "line": 340
        },
        {
          "name": "print",
          "line": 341
        },
        {
          "name": "....get",
          "line": 342
        },
        {
          "name": "print",
          "line": 344
        },
        {
          "name": "print",
          "line": 345
        },
        {
          "name": "print",
          "line": 348
        },
        {
          "name": "open",
          "line": 336
        },
        {
          "name": "json.dump",
          "line": 337
        },
        {
          "name": "print",
          "line": 343
        },
        {
          "name": "print",
          "line": 347
        },
        {
          "name": "Path",
          "line": 334
        }
      ],
      "code_snippet": "            plt.close()\n\ndef main():\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='Analyze Neural Network Cortex System training logs')\n    parser.add_argument('log_file', help='Path to the log file')\n    parser.add_argument('--output', default='analysis_results', help='Output directory for results')\n    \n    args = parser.parse_args()\n    \n    # Create analyzer and process log\n    analyzer = TrainingLogAnalyzer(args.log_file)\n    report = analyzer.analyze_log()\n    \n    # Generate visualizations\n    analyzer.generate_visualizations(args.output)\n    \n    # Save report\n    output_path = Path(args.output).absolute()\n    output_path.mkdir(parents=True, exist_ok=True)\n    with open(output_path / 'analysis_report.json', 'w') as f:\n        json.dump(report, f, indent=2)\n    \n    # Print summary\n    print(\"\\nAnalysis Complete!\")\n    print(f\"Total batches processed: {report['performance_metrics']['total_batches']:,}\")\n    if report['performance_metrics'].get('high_speed_mode_detected'):\n        print(\"High-speed processing mode detected!\")\n    print(f\"Acceleration factor: {report['performance_metrics']['acceleration_factor']:.2f}x\")\n    print(f\"Final processing speed: {report['processing_speed']['fastest_batch']:.3f}ms\")\n    if report['validation_info']['anomalies_detected']:\n        print(f\"\\nNote: {report['validation_info']['anomalies_detected']} timing anomalies detected and corrected\")\n    print(f\"\\nFull results saved to: {output_path}\")\n\nif __name__ == \"__main__\":\n    main()"
    }
  },
  "constants": {}
}