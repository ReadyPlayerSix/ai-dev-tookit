{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\isekaizen\\core\\training\\callbacks.py",
  "imports": [
    {
      "name": "logging",
      "line": 10
    },
    {
      "name": "os",
      "line": 11
    },
    {
      "name": "torch",
      "line": 12
    },
    {
      "name": "typing.Dict",
      "line": 13
    },
    {
      "name": "typing.Any",
      "line": 13
    },
    {
      "name": "typing.Optional",
      "line": 13
    }
  ],
  "classes": {
    "EarlyStoppingCallback": {
      "start_line": 17,
      "end_line": 101,
      "methods": {
        "__init__": {
          "start_line": 33,
          "end_line": 56,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "patience",
              "type": "int"
            },
            {
              "name": "monitor",
              "type": "str"
            },
            {
              "name": "min_delta",
              "type": "float"
            },
            {
              "name": "mode",
              "type": "str"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logger.info",
              "line": 53
            },
            {
              "name": "float",
              "line": 51
            },
            {
              "name": "float",
              "line": 51
            }
          ],
          "docstring": "\n        Initialize the early stopping callback.\n        \n        Args:\n            patience: Number of epochs to wait for improvement before stopping\n            monitor: Metric to monitor ('val_loss' or 'val_acc')\n            min_delta: Minimum change to qualify as an improvement\n            mode: 'min' for loss, 'max' for accuracy\n        ",
          "code_snippet": "    \"\"\"\n    \n    def __init__(self, patience: int = 5, monitor: str = 'val_loss', \n                min_delta: float = 0.001, mode: str = 'min'):\n        \"\"\"\n        Initialize the early stopping callback.\n        \n        Args:\n            patience: Number of epochs to wait for improvement before stopping\n            monitor: Metric to monitor ('val_loss' or 'val_acc')\n            min_delta: Minimum change to qualify as an improvement\n            mode: 'min' for loss, 'max' for accuracy\n        \"\"\"\n        self.patience = patience\n        self.monitor = monitor\n        self.min_delta = min_delta\n        self.mode = mode\n        \n        # Initialize tracking variables\n        self.wait_count = 0\n        self.best_value = float('inf') if mode == 'min' else float('-inf')\n        \n        logger.info(f\"EarlyStoppingCallback initialized: monitoring '{monitor}', \"\n                  f\"patience={patience}, mode='{mode}'\")\n    \n    def __call__(self, epoch: int, history: Dict[str, Any], model: Any, optimizer: Any) -> bool:\n        \"\"\"\n        Check if training should be stopped."
        },
        "__call__": {
          "start_line": 56,
          "end_line": 101,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "epoch",
              "type": "int"
            },
            {
              "name": "history"
            },
            {
              "name": "model",
              "type": "Any"
            },
            {
              "name": "optimizer",
              "type": "Any"
            }
          ],
          "return_type": "bool",
          "calls": [
            {
              "name": "logger.warning",
              "line": 71
            },
            {
              "name": "logger.info",
              "line": 86
            },
            {
              "name": "logger.info",
              "line": 91
            },
            {
              "name": "logger.info",
              "line": 96
            }
          ],
          "docstring": "\n        Check if training should be stopped.\n        \n        Args:\n            epoch: Current epoch number\n            history: Training history dictionary\n            model: Current model\n            optimizer: Current optimizer\n            \n        Returns:\n            bool: True if training should stop, False otherwise\n        ",
          "code_snippet": "                  f\"patience={patience}, mode='{mode}'\")\n    \n    def __call__(self, epoch: int, history: Dict[str, Any], model: Any, optimizer: Any) -> bool:\n        \"\"\"\n        Check if training should be stopped.\n        \n        Args:\n            epoch: Current epoch number\n            history: Training history dictionary\n            model: Current model\n            optimizer: Current optimizer\n            \n        Returns:\n            bool: True if training should stop, False otherwise\n        \"\"\"\n        # Get current value\n        if self.monitor not in history or not history[self.monitor]:\n            logger.warning(f\"Metric '{self.monitor}' not found in history\")\n            return False\n        \n        current_value = history[self.monitor][-1]\n        \n        # Check if improvement\n        if self.mode == 'min':\n            is_improved = current_value < (self.best_value - self.min_delta)\n        else:\n            is_improved = current_value > (self.best_value + self.min_delta)\n        \n        if is_improved:\n            # Reset counter and update best value\n            self.wait_count = 0\n            self.best_value = current_value\n            logger.info(f\"Epoch {epoch+1}: {self.monitor} improved to {current_value:.4f}\")\n            return False\n        else:\n            # Increment counter\n            self.wait_count += 1\n            logger.info(f\"Epoch {epoch+1}: {self.monitor}={current_value:.4f}, \"\n                      f\"no improvement for {self.wait_count}/{self.patience} epochs\")\n            \n            # Check if patience exceeded\n            if self.wait_count >= self.patience:\n                logger.info(f\"Early stopping triggered after {epoch+1} epochs\")\n                return True\n        \n        return False\n\n\nclass ModelCheckpointCallback:\n    \"\"\""
        }
      },
      "class_variables": [],
      "bases": [],
      "docstring": "\n    Early stopping callback that halts training when validation metrics stop improving.\n    \n    This callback tracks validation loss or accuracy and stops training if no improvement\n    is seen for a specified number of epochs.\n    \n    Attributes:\n        patience: Number of epochs to wait for improvement before stopping\n        monitor: Metric to monitor ('val_loss' or 'val_acc')\n        min_delta: Minimum change to qualify as an improvement\n        mode: 'min' for loss, 'max' for accuracy\n        best_value: Best value observed so far\n        wait_count: Number of epochs without improvement\n    "
    },
    "ModelCheckpointCallback": {
      "start_line": 102,
      "end_line": 209,
      "methods": {
        "__init__": {
          "start_line": 118,
          "end_line": 146,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "filepath",
              "type": "str"
            },
            {
              "name": "monitor",
              "type": "str"
            },
            {
              "name": "save_best_only",
              "type": "bool"
            },
            {
              "name": "save_last",
              "type": "bool"
            },
            {
              "name": "mode",
              "type": "str"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "os.makedirs",
              "line": 141
            },
            {
              "name": "logger.info",
              "line": 143
            },
            {
              "name": "float",
              "line": 138
            },
            {
              "name": "float",
              "line": 138
            },
            {
              "name": "os.path.dirname",
              "line": 141
            },
            {
              "name": "os.path.abspath",
              "line": 141
            }
          ],
          "docstring": "\n        Initialize the model checkpoint callback.\n        \n        Args:\n            filepath: Directory or path template for saving checkpoints\n            monitor: Metric to monitor ('val_loss' or 'val_acc')\n            save_best_only: Whether to save only models with improved metrics\n            save_last: Whether to save the last model regardless of metrics\n            mode: 'min' for loss, 'max' for accuracy\n        ",
          "code_snippet": "    \"\"\"\n    \n    def __init__(self, filepath: str, monitor: str = 'val_loss', \n                save_best_only: bool = True, save_last: bool = True,\n                mode: str = 'min'):\n        \"\"\"\n        Initialize the model checkpoint callback.\n        \n        Args:\n            filepath: Directory or path template for saving checkpoints\n            monitor: Metric to monitor ('val_loss' or 'val_acc')\n            save_best_only: Whether to save only models with improved metrics\n            save_last: Whether to save the last model regardless of metrics\n            mode: 'min' for loss, 'max' for accuracy\n        \"\"\"\n        self.filepath = filepath\n        self.monitor = monitor\n        self.save_best_only = save_best_only\n        self.save_last = save_last\n        self.mode = mode\n        \n        # Initialize tracking variables\n        self.best_value = float('inf') if mode == 'min' else float('-inf')\n        \n        # Create directory if it doesn't exist\n        os.makedirs(os.path.dirname(os.path.abspath(filepath)), exist_ok=True)\n        \n        logger.info(f\"ModelCheckpointCallback initialized: monitoring '{monitor}', \"\n                  f\"filepath='{filepath}', mode='{mode}'\")\n    \n    def __call__(self, epoch: int, history: Dict[str, Any], model: Any, optimizer: Any) -> bool:\n        \"\"\"\n        Save model checkpoint if needed."
        },
        "__call__": {
          "start_line": 146,
          "end_line": 209,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "epoch",
              "type": "int"
            },
            {
              "name": "history"
            },
            {
              "name": "model",
              "type": "Any"
            },
            {
              "name": "optimizer",
              "type": "Any"
            }
          ],
          "return_type": "bool",
          "calls": [
            {
              "name": "logger.warning",
              "line": 160
            },
            {
              "name": "....replace",
              "line": 174
            },
            {
              "name": "....replace",
              "line": 192
            },
            {
              "name": "torch.save",
              "line": 185
            },
            {
              "name": "logger.info",
              "line": 186
            },
            {
              "name": "torch.save",
              "line": 202
            },
            {
              "name": "logger.debug",
              "line": 203
            },
            {
              "name": "self.filepath.replace",
              "line": 174
            },
            {
              "name": "model.state_dict",
              "line": 180
            },
            {
              "name": "optimizer.state_dict",
              "line": 181
            },
            {
              "name": "logger.error",
              "line": 188
            },
            {
              "name": "self.filepath.replace",
              "line": 192
            },
            {
              "name": "model.state_dict",
              "line": 197
            },
            {
              "name": "optimizer.state_dict",
              "line": 198
            },
            {
              "name": "logger.error",
              "line": 205
            },
            {
              "name": "str",
              "line": 188
            },
            {
              "name": "str",
              "line": 205
            }
          ],
          "docstring": "\n        Save model checkpoint if needed.\n        \n        Args:\n            epoch: Current epoch number\n            history: Training history dictionary\n            model: Current model\n            optimizer: Current optimizer\n            \n        Returns:\n            bool: Always False (never stops training)\n        ",
          "code_snippet": "                  f\"filepath='{filepath}', mode='{mode}'\")\n    \n    def __call__(self, epoch: int, history: Dict[str, Any], model: Any, optimizer: Any) -> bool:\n        \"\"\"\n        Save model checkpoint if needed.\n        \n        Args:\n            epoch: Current epoch number\n            history: Training history dictionary\n            model: Current model\n            optimizer: Current optimizer\n            \n        Returns:\n            bool: Always False (never stops training)\n        \"\"\"\n        if not history or self.monitor not in history or not history[self.monitor]:\n            logger.warning(f\"Metric '{self.monitor}' not found in history\")\n            return False\n        \n        current_value = history[self.monitor][-1]\n        \n        # Determine if this is the best model\n        if self.mode == 'min':\n            is_improved = current_value < self.best_value\n        else:\n            is_improved = current_value > self.best_value\n        \n        # Save best model\n        if self.save_best_only and is_improved:\n            self.best_value = current_value\n            best_path = self.filepath.replace('{epoch}', f\"{epoch+1}\").replace('{val}', f\"{current_value:.4f}\")\n            \n            # Save the model\n            try:\n                checkpoint = {\n                    'epoch': epoch,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'history': history,\n                    'best_value': self.best_value\n                }\n                torch.save(checkpoint, best_path)\n                logger.info(f\"Epoch {epoch+1}: Saved best model to {best_path}\")\n            except Exception as e:\n                logger.error(f\"Error saving best model checkpoint: {str(e)}\")\n        \n        # Save last model\n        if self.save_last:\n            last_path = self.filepath.replace('{epoch}', 'last').replace('{val}', 'last')\n            \n            try:\n                checkpoint = {\n                    'epoch': epoch,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'history': history,\n                    'current_value': current_value\n                }\n                torch.save(checkpoint, last_path)\n                logger.debug(f\"Epoch {epoch+1}: Saved last model to {last_path}\")\n            except Exception as e:\n                logger.error(f\"Error saving last model checkpoint: {str(e)}\")\n        \n        return False  # Never stop training"
        }
      },
      "class_variables": [],
      "bases": [],
      "docstring": "\n    Callback to save model checkpoints during training.\n    \n    This callback saves model checkpoints based on validation metrics,\n    with options to save the best model, last model, or both.\n    \n    Attributes:\n        filepath: Directory or path template for saving checkpoints\n        monitor: Metric to monitor ('val_loss' or 'val_acc')\n        save_best_only: Whether to save only models with improved metrics\n        save_last: Whether to save the last model regardless of metrics\n        mode: 'min' for loss, 'max' for accuracy\n        best_value: Best value observed so far\n    "
    }
  },
  "functions": {},
  "constants": {}
}