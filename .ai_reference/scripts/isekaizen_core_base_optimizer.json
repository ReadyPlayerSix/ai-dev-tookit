{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\isekaizen\\core\\base_optimizer.py",
  "imports": [
    {
      "name": "torch",
      "line": 5
    },
    {
      "name": "logging",
      "line": 6
    },
    {
      "name": "numpy",
      "line": 7
    },
    {
      "name": "abc.ABC",
      "line": 8
    },
    {
      "name": "abc.abstractmethod",
      "line": 8
    },
    {
      "name": "typing.Dict",
      "line": 9
    },
    {
      "name": "typing.Any",
      "line": 9
    },
    {
      "name": "typing.Optional",
      "line": 9
    },
    {
      "name": "typing.Union",
      "line": 9
    },
    {
      "name": "typing.Tuple",
      "line": 9
    },
    {
      "name": "isekaizen.hardware.analyzer.HardwareAnalyzer",
      "line": 51
    },
    {
      "name": "isekaizen.hardware.memory.ModelMemoryAnalyzer",
      "line": 52
    },
    {
      "name": "isekaizen.cognitive.efficiency.CognitiveEfficiencyCalculator",
      "line": 53
    },
    {
      "name": "isekaizen.utils.input_shapes.infer_input_shape",
      "line": 102
    }
  ],
  "classes": {
    "BaseOptimizer": {
      "start_line": 13,
      "end_line": 177,
      "methods": {
        "__init__": {
          "start_line": 21,
          "end_line": 82,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "model"
            },
            {
              "name": "device"
            },
            {
              "name": "run_diagnostics",
              "type": "bool"
            },
            {
              "name": "total_epochs",
              "type": "int"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logger.info",
              "line": 48
            },
            {
              "name": "HardwareAnalyzer",
              "line": 56
            },
            {
              "name": "ModelMemoryAnalyzer",
              "line": 57
            },
            {
              "name": "CognitiveEfficiencyCalculator",
              "line": 58
            },
            {
              "name": "logger.info",
              "line": 80
            },
            {
              "name": "sum",
              "line": 47
            },
            {
              "name": "self._run_diagnostics",
              "line": 62
            },
            {
              "name": "kwargs.get",
              "line": 65
            },
            {
              "name": "kwargs.get",
              "line": 66
            },
            {
              "name": "logger.info",
              "line": 67
            },
            {
              "name": "logger.info",
              "line": 74
            },
            {
              "name": "ValueError",
              "line": 78
            },
            {
              "name": "torch.cuda.is_available",
              "line": 42
            },
            {
              "name": "torch.device",
              "line": 42
            },
            {
              "name": "torch.device",
              "line": 42
            },
            {
              "name": "kwargs.get",
              "line": 62
            },
            {
              "name": "min",
              "line": 72
            },
            {
              "name": "p.numel",
              "line": 47
            },
            {
              "name": "max",
              "line": 72
            },
            {
              "name": "model.parameters",
              "line": 47
            }
          ],
          "docstring": "\n        Initialize the base optimizer with automatic batch boundary detection.\n        \n        Args:\n            model: PyTorch model to optimize\n            device: Device to use (default: None, auto-detect)\n            run_diagnostics: Whether to run automatic diagnostics to determine batch boundaries\n            total_epochs: Total number of epochs for the training\n            **kwargs: Additional parameters, including:\n                - override_batch_min: Override minimum batch size (for testing)\n                - override_batch_max: Override maximum batch size (for testing)\n        ",
          "code_snippet": "    \"\"\"\n    \n    def __init__(\n        self, \n        model: torch.nn.Module, \n        device: Optional[torch.device] = None, \n        run_diagnostics: bool = True,\n        total_epochs: int = 50,\n        **kwargs\n    ):\n        \"\"\"\n        Initialize the base optimizer with automatic batch boundary detection.\n        \n        Args:\n            model: PyTorch model to optimize\n            device: Device to use (default: None, auto-detect)\n            run_diagnostics: Whether to run automatic diagnostics to determine batch boundaries\n            total_epochs: Total number of epochs for the training\n            **kwargs: Additional parameters, including:\n                - override_batch_min: Override minimum batch size (for testing)\n                - override_batch_max: Override maximum batch size (for testing)\n        \"\"\"\n        self.model = model\n        self.device = device or (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n        self.total_epochs = total_epochs\n        self.current_epoch = 0\n        \n        # Calculate model complexity (parameter count in millions)\n        self.model_complexity = sum(p.numel() for p in model.parameters()) / 1_000_000\n        logger.info(f\"Model complexity: {self.model_complexity:.2f}M parameters\")\n        \n        # Import utility classes\n        from isekaizen.hardware.analyzer import HardwareAnalyzer\n        from isekaizen.hardware.memory import ModelMemoryAnalyzer\n        from isekaizen.cognitive.efficiency import CognitiveEfficiencyCalculator\n        \n        # Initialize hardware and memory analyzers\n        self.hardware_analyzer = HardwareAnalyzer(self.device)\n        self.memory_analyzer = ModelMemoryAnalyzer(model, self.device)\n        self.efficiency_calculator = CognitiveEfficiencyCalculator()\n        \n        # Run diagnostics to determine batch boundaries or use overrides\n        if run_diagnostics:\n            self._run_diagnostics(kwargs.get('diagnostic_input_shape', None))\n        else:\n            # Use provided overrides or conservative defaults\n            self.min_batch = kwargs.get('override_batch_min', 4)\n            self.max_batch = kwargs.get('override_batch_max', 128)\n            logger.info(f\"Diagnostics skipped. Using boundaries: [{self.min_batch}, {self.max_batch}]\")\n            \n            # Initialize resonance zone conservatively\n            self.resonance_zone = (\n                self.min_batch, \n                min(self.max_batch, max(self.min_batch * 2, self.min_batch + 32))\n            )\n            logger.info(f\"Using default resonance zone: {self.resonance_zone}\")\n        \n        # Validate parameters\n        if self.min_batch > self.max_batch:\n            raise ValueError(f\"min_batch ({self.min_batch}) cannot be greater than max_batch ({self.max_batch})\")\n        \n        logger.info(f\"Initialized {self.__class__.__name__} with batch range: [{self.min_batch}, {self.max_batch}]\")\n    \n    def _run_diagnostics(self, input_shape: Optional[Tuple[int, ...]] = None) -> None:\n        \"\"\"\n        Run diagnostics to determine optimal batch boundaries."
        },
        "_run_diagnostics": {
          "start_line": 82,
          "end_line": 121,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "input_shape"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logger.info",
              "line": 92
            },
            {
              "name": "self.hardware_analyzer.analyze_memory",
              "line": 95
            },
            {
              "name": "logger.info",
              "line": 97
            },
            {
              "name": "self.memory_analyzer.calculate_max_batch_size",
              "line": 107
            },
            {
              "name": "logger.info",
              "line": 108
            },
            {
              "name": "self.efficiency_calculator.calculate_optimal_batch_boundaries",
              "line": 111
            },
            {
              "name": "self.efficiency_calculator.calculate_resonance_zone",
              "line": 115
            },
            {
              "name": "logger.info",
              "line": 118
            },
            {
              "name": "logger.info",
              "line": 119
            },
            {
              "name": "self.hardware_analyzer.get_memory_safety_factor",
              "line": 96
            },
            {
              "name": "infer_input_shape",
              "line": 103
            },
            {
              "name": "logger.info",
              "line": 104
            }
          ],
          "docstring": "\n        Run diagnostics to determine optimal batch boundaries.\n        \n        This default implementation analyzes hardware memory constraints and model complexity\n        to determine appropriate batch size boundaries.\n        \n        Args:\n            input_shape: Optional input shape to use for diagnostics\n        ",
          "code_snippet": "        logger.info(f\"Initialized {self.__class__.__name__} with batch range: [{self.min_batch}, {self.max_batch}]\")\n    \n    def _run_diagnostics(self, input_shape: Optional[Tuple[int, ...]] = None) -> None:\n        \"\"\"\n        Run diagnostics to determine optimal batch boundaries.\n        \n        This default implementation analyzes hardware memory constraints and model complexity\n        to determine appropriate batch size boundaries.\n        \n        Args:\n            input_shape: Optional input shape to use for diagnostics\n        \"\"\"\n        logger.info(\"Running batch size diagnostics...\")\n        \n        # Analyze hardware memory\n        memory_info = self.hardware_analyzer.analyze_memory()\n        available_memory = memory_info['free_memory'] * self.hardware_analyzer.get_memory_safety_factor()\n        logger.info(f\"Available memory for training: {available_memory / (1024**3):.2f} GB\")\n        \n        # Determine input shape if not provided\n        if input_shape is None:\n            # Use the utility function to infer input shape\n            from isekaizen.utils.input_shapes import infer_input_shape\n            input_shape = infer_input_shape(self.model)\n            logger.info(f\"Inferred input shape: {input_shape}\")\n        \n        # Calculate maximum batch size based on hardware constraints\n        max_hardware_batch = self.memory_analyzer.calculate_max_batch_size(input_shape, available_memory)\n        logger.info(f\"Maximum hardware batch size: {max_hardware_batch}\")\n        \n        # Calculate optimal batch boundaries based on cognitive efficiency\n        self.min_batch, self.max_batch = self.efficiency_calculator.calculate_optimal_batch_boundaries(\n            max_hardware_batch, self.model_complexity)\n        \n        # Calculate resonance zone\n        self.resonance_zone = self.efficiency_calculator.calculate_resonance_zone(\n            self.min_batch, self.max_batch, self.model_complexity)\n        \n        logger.info(f\"Diagnostics complete: Batch size boundaries set to [{self.min_batch}, {self.max_batch}]\")\n        logger.info(f\"Resonance zone: {self.resonance_zone}\")\n    \n    @abstractmethod\n    def get_optimal_batch_size(self) -> int:\n        \"\"\""
        },
        "get_optimal_batch_size": {
          "start_line": 122,
          "end_line": 131,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": "int",
          "calls": [],
          "docstring": "\n        Get optimal batch size for current training state.\n        \n        Returns:\n            Optimal batch size (int)\n        ",
          "code_snippet": "    \n    @abstractmethod\n    def get_optimal_batch_size(self) -> int:\n        \"\"\"\n        Get optimal batch size for current training state.\n        \n        Returns:\n            Optimal batch size (int)\n        \"\"\"\n        pass\n    \n    def update_with_metrics(self, metrics: Dict[str, Any]) -> None:\n        \"\"\"\n        Update optimizer state with training metrics."
        },
        "update_with_metrics": {
          "start_line": 131,
          "end_line": 140,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "metrics"
            }
          ],
          "return_type": null,
          "calls": [],
          "docstring": "\n        Update optimizer state with training metrics.\n        \n        Args:\n            metrics: Dictionary of training metrics (loss, accuracy, etc.)\n        ",
          "code_snippet": "        pass\n    \n    def update_with_metrics(self, metrics: Dict[str, Any]) -> None:\n        \"\"\"\n        Update optimizer state with training metrics.\n        \n        Args:\n            metrics: Dictionary of training metrics (loss, accuracy, etc.)\n        \"\"\"\n        pass\n    \n    def update_with_loss_info(self, per_example_losses: list, batch_indices: list) -> None:\n        \"\"\"\n        Update optimizer with per-example loss information."
        },
        "update_with_loss_info": {
          "start_line": 140,
          "end_line": 153,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "per_example_losses",
              "type": "list"
            },
            {
              "name": "batch_indices",
              "type": "list"
            }
          ],
          "return_type": null,
          "calls": [],
          "docstring": "\n        Update optimizer with per-example loss information.\n        \n        This can be used by subclasses to implement more sophisticated batch size strategies\n        that take into account loss distributions.\n        \n        Args:\n            per_example_losses: List of loss values for each example\n            batch_indices: List of indices corresponding to each example in the dataset\n        ",
          "code_snippet": "        pass\n    \n    def update_with_loss_info(self, per_example_losses: list, batch_indices: list) -> None:\n        \"\"\"\n        Update optimizer with per-example loss information.\n        \n        This can be used by subclasses to implement more sophisticated batch size strategies\n        that take into account loss distributions.\n        \n        Args:\n            per_example_losses: List of loss values for each example\n            batch_indices: List of indices corresponding to each example in the dataset\n        \"\"\"\n        pass\n    \n    def increment_epoch(self) -> None:\n        \"\"\"Increment the epoch counter.\"\"\"\n        self.current_epoch += 1"
        },
        "increment_epoch": {
          "start_line": 153,
          "end_line": 157,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [],
          "docstring": "Increment the epoch counter.",
          "code_snippet": "        pass\n    \n    def increment_epoch(self) -> None:\n        \"\"\"Increment the epoch counter.\"\"\"\n        self.current_epoch += 1\n        \n    def get_progress(self) -> float:\n        \"\"\"\n        Get current training progress as a fraction."
        },
        "get_progress": {
          "start_line": 157,
          "end_line": 168,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": "float",
          "calls": [
            {
              "name": "min",
              "line": 166
            }
          ],
          "docstring": "\n        Get current training progress as a fraction.\n        \n        Returns:\n            Progress value between 0.0 and 1.0\n        ",
          "code_snippet": "        self.current_epoch += 1\n        \n    def get_progress(self) -> float:\n        \"\"\"\n        Get current training progress as a fraction.\n        \n        Returns:\n            Progress value between 0.0 and 1.0\n        \"\"\"\n        if self.total_epochs <= 0:\n            return 0.0\n        return min(1.0, self.current_epoch / self.total_epochs)\n    \n    def get_batch_range(self) -> Tuple[int, int]:\n        \"\"\"\n        Get the current batch size range."
        },
        "get_batch_range": {
          "start_line": 168,
          "end_line": 177,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": "complex_type",
          "calls": [],
          "docstring": "\n        Get the current batch size range.\n        \n        Returns:\n            Tuple of (min_batch, max_batch)\n        ",
          "code_snippet": "        return min(1.0, self.current_epoch / self.total_epochs)\n    \n    def get_batch_range(self) -> Tuple[int, int]:\n        \"\"\"\n        Get the current batch size range.\n        \n        Returns:\n            Tuple of (min_batch, max_batch)\n        \"\"\"\n        return self.min_batch, self.max_batch"
        }
      },
      "class_variables": [],
      "bases": [
        "ABC"
      ],
      "docstring": "\n    Abstract base class for all batch size optimizers.\n    \n    This base class implements automatic batch boundary detection based on hardware\n    capabilities, model complexity, and cognitive efficiency principles.\n    "
    }
  },
  "functions": {},
  "constants": {}
}