{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\benchmarks.old\\examples\\refactored\\run_refactored_optimizer.py",
  "imports": [
    {
      "name": "os",
      "line": 11
    },
    {
      "name": "sys",
      "line": 12
    },
    {
      "name": "time",
      "line": 13
    },
    {
      "name": "logging",
      "line": 14
    },
    {
      "name": "argparse",
      "line": 15
    },
    {
      "name": "torch",
      "line": 16
    },
    {
      "name": "torch.nn",
      "line": 17
    },
    {
      "name": "torch.optim",
      "line": 18
    },
    {
      "name": "torchvision",
      "line": 19
    },
    {
      "name": "torchvision.transforms",
      "line": 20
    },
    {
      "name": "torchvision.models",
      "line": 21
    },
    {
      "name": "matplotlib.pyplot",
      "line": 22
    },
    {
      "name": "numpy",
      "line": 23
    },
    {
      "name": "datetime.datetime",
      "line": 24
    },
    {
      "name": "isekaizen.trainer.ModelTrainer",
      "line": 30
    },
    {
      "name": "isekaizen.core.RiskAwarePatternIsekaiZen",
      "line": 31
    },
    {
      "name": "isekaizen.core.cortex_components_available",
      "line": 31
    },
    {
      "name": "isekaizen.pattern.data_loading.load_latest_pattern_map",
      "line": 32
    },
    {
      "name": "json",
      "line": 157
    },
    {
      "name": "isekaizen.utils.input_shapes.infer_input_shape",
      "line": 158
    },
    {
      "name": "traceback",
      "line": 345
    }
  ],
  "classes": {},
  "functions": {
    "load_cifar10_data": {
      "start_line": 38,
      "end_line": 67,
      "parameters": [],
      "return_type": null,
      "calls": [
        {
          "name": "transforms.Compose",
          "line": 46
        },
        {
          "name": "transforms.Compose",
          "line": 53
        },
        {
          "name": "torchvision.datasets.CIFAR10",
          "line": 59
        },
        {
          "name": "torchvision.datasets.CIFAR10",
          "line": 62
        },
        {
          "name": "transforms.RandomCrop",
          "line": 47
        },
        {
          "name": "transforms.RandomHorizontalFlip",
          "line": 48
        },
        {
          "name": "transforms.ToTensor",
          "line": 49
        },
        {
          "name": "transforms.Normalize",
          "line": 50
        },
        {
          "name": "transforms.ToTensor",
          "line": 54
        },
        {
          "name": "transforms.Normalize",
          "line": 55
        }
      ],
      "docstring": "\n    Load and prepare CIFAR-10 dataset.\n    \n    Returns:\n        Train dataset, test dataset\n    ",
      "code_snippet": "logger = logging.getLogger(__name__)\n\ndef load_cifar10_data():\n    \"\"\"\n    Load and prepare CIFAR-10 dataset.\n    \n    Returns:\n        Train dataset, test dataset\n    \"\"\"\n    # Data transforms\n    transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n\n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n\n    # Load datasets\n    trainset = torchvision.datasets.CIFAR10(\n        root='./data', train=True, download=True, transform=transform_train)\n\n    testset = torchvision.datasets.CIFAR10(\n        root='./data', train=False, download=True, transform=transform_test)\n\n    return trainset, testset\n\ndef create_model():\n    \"\"\"\n    Create a model for CIFAR-10."
    },
    "create_model": {
      "start_line": 67,
      "end_line": 86,
      "parameters": [],
      "return_type": null,
      "calls": [
        {
          "name": "models.resnet18",
          "line": 75
        },
        {
          "name": "nn.Conv2d",
          "line": 78
        },
        {
          "name": "nn.Identity",
          "line": 79
        },
        {
          "name": "nn.Linear",
          "line": 82
        }
      ],
      "docstring": "\n    Create a model for CIFAR-10.\n    \n    Returns:\n        Model instance\n    ",
      "code_snippet": "    return trainset, testset\n\ndef create_model():\n    \"\"\"\n    Create a model for CIFAR-10.\n    \n    Returns:\n        Model instance\n    \"\"\"\n    # Use ResNet-18 with appropriate output layer for CIFAR-10\n    model = models.resnet18(pretrained=False)\n\n    # Modify first conv layer to handle CIFAR-10's 32x32 images\n    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n    model.maxpool = nn.Identity()  # Remove maxpool as it's too aggressive for 32x32\n\n    # Modify the final fully connected layer for 10 classes\n    model.fc = nn.Linear(model.fc.in_features, 10)\n\n    return model\n\ndef visualize_training_results(history, output_path=None):\n    \"\"\"\n    Visualize training results."
    },
    "visualize_training_results": {
      "start_line": 86,
      "end_line": 147,
      "parameters": [
        {
          "name": "history"
        },
        {
          "name": "output_path"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "plt.figure",
          "line": 95
        },
        {
          "name": "range",
          "line": 98
        },
        {
          "name": "plt.subplot",
          "line": 101
        },
        {
          "name": "plt.plot",
          "line": 102
        },
        {
          "name": "plt.title",
          "line": 105
        },
        {
          "name": "plt.xlabel",
          "line": 106
        },
        {
          "name": "plt.ylabel",
          "line": 107
        },
        {
          "name": "plt.legend",
          "line": 108
        },
        {
          "name": "plt.grid",
          "line": 109
        },
        {
          "name": "plt.subplot",
          "line": 112
        },
        {
          "name": "plt.plot",
          "line": 113
        },
        {
          "name": "plt.title",
          "line": 116
        },
        {
          "name": "plt.xlabel",
          "line": 117
        },
        {
          "name": "plt.ylabel",
          "line": 118
        },
        {
          "name": "plt.legend",
          "line": 119
        },
        {
          "name": "plt.grid",
          "line": 120
        },
        {
          "name": "plt.subplot",
          "line": 123
        },
        {
          "name": "plt.plot",
          "line": 124
        },
        {
          "name": "plt.title",
          "line": 125
        },
        {
          "name": "plt.xlabel",
          "line": 126
        },
        {
          "name": "plt.ylabel",
          "line": 127
        },
        {
          "name": "plt.grid",
          "line": 128
        },
        {
          "name": "plt.subplot",
          "line": 131
        },
        {
          "name": "plt.plot",
          "line": 132
        },
        {
          "name": "plt.title",
          "line": 133
        },
        {
          "name": "plt.xlabel",
          "line": 134
        },
        {
          "name": "plt.ylabel",
          "line": 135
        },
        {
          "name": "plt.grid",
          "line": 136
        },
        {
          "name": "plt.tight_layout",
          "line": 138
        },
        {
          "name": "plt.show",
          "line": 145
        },
        {
          "name": "plt.plot",
          "line": 104
        },
        {
          "name": "plt.plot",
          "line": 115
        },
        {
          "name": "plt.savefig",
          "line": 142
        },
        {
          "name": "logger.info",
          "line": 143
        },
        {
          "name": "len",
          "line": 98
        }
      ],
      "docstring": "\n    Visualize training results.\n    \n    Args:\n        history: Training history dictionary\n        output_path: Optional output file path\n    ",
      "code_snippet": "    return model\n\ndef visualize_training_results(history, output_path=None):\n    \"\"\"\n    Visualize training results.\n    \n    Args:\n        history: Training history dictionary\n        output_path: Optional output file path\n    \"\"\"\n    # Create figure\n    plt.figure(figsize=(15, 10))\n    \n    # Create epochs range\n    epochs = range(1, len(history['train_loss']) + 1)\n    \n    # 1. Training metrics: Loss\n    plt.subplot(2, 2, 1)\n    plt.plot(epochs, history['train_loss'], 'b-', label='Training Loss')\n    if 'val_loss' in history:\n        plt.plot(epochs, history['val_loss'], 'r-', label='Validation Loss')\n    plt.title('Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n    \n    # 2. Training metrics: Accuracy\n    plt.subplot(2, 2, 2)\n    plt.plot(epochs, history['train_acc'], 'b-', label='Training Accuracy')\n    if 'val_acc' in history:\n        plt.plot(epochs, history['val_acc'], 'r-', label='Validation Accuracy')\n    plt.title('Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy (%)')\n    plt.legend()\n    plt.grid(True)\n    \n    # 3. Batch sizes\n    plt.subplot(2, 2, 3)\n    plt.plot(epochs, history['batch_sizes'], 'g-o', label='Batch Size')\n    plt.title('Batch Size')\n    plt.xlabel('Epoch')\n    plt.ylabel('Batch Size')\n    plt.grid(True)\n    \n    # 4. Epoch times\n    plt.subplot(2, 2, 4)\n    plt.plot(epochs, history['epoch_times'], 'm-o', label='Epoch Time')\n    plt.title('Training Time')\n    plt.xlabel('Epoch')\n    plt.ylabel('Time (s)')\n    plt.grid(True)\n    \n    plt.tight_layout()\n    \n    # Save figure if output path provided\n    if output_path:\n        plt.savefig(output_path)\n        logger.info(f\"Training visualization saved to: {output_path}\")\n    \n    plt.show()\n\ndef save_batch_diagnostics(model, device, optimizer):\n    \"\"\"\n    Save detailed batch calculation diagnostics to a JSON file."
    },
    "save_batch_diagnostics": {
      "start_line": 147,
      "end_line": 206,
      "parameters": [
        {
          "name": "model"
        },
        {
          "name": "device"
        },
        {
          "name": "optimizer"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "sum",
          "line": 161
        },
        {
          "name": "infer_input_shape",
          "line": 166
        },
        {
          "name": "os.path.join",
          "line": 193
        },
        {
          "name": "os.makedirs",
          "line": 194
        },
        {
          "name": "os.path.join",
          "line": 195
        },
        {
          "name": "logger.info",
          "line": 200
        },
        {
          "name": "sum",
          "line": 162
        },
        {
          "name": "hasattr",
          "line": 169
        },
        {
          "name": "hasattr",
          "line": 170
        },
        {
          "name": "....strftime",
          "line": 174
        },
        {
          "name": "open",
          "line": 197
        },
        {
          "name": "json.dump",
          "line": 198
        },
        {
          "name": "logger.error",
          "line": 203
        },
        {
          "name": "p.numel",
          "line": 161
        },
        {
          "name": "int",
          "line": 176
        },
        {
          "name": "float",
          "line": 177
        },
        {
          "name": "float",
          "line": 178
        },
        {
          "name": "int",
          "line": 187
        },
        {
          "name": "int",
          "line": 188
        },
        {
          "name": "model.parameters",
          "line": 161
        },
        {
          "name": "datetime.now",
          "line": 174
        },
        {
          "name": "int",
          "line": 179
        },
        {
          "name": "torch.cuda.get_device_name",
          "line": 183
        },
        {
          "name": "float",
          "line": 184
        },
        {
          "name": "....strftime",
          "line": 195
        },
        {
          "name": "p.numel",
          "line": 162
        },
        {
          "name": "p.element_size",
          "line": 162
        },
        {
          "name": "model.parameters",
          "line": 162
        },
        {
          "name": "str",
          "line": 203
        },
        {
          "name": "datetime.now",
          "line": 195
        },
        {
          "name": "torch.cuda.get_device_properties",
          "line": 184
        }
      ],
      "docstring": "\n    Save detailed batch calculation diagnostics to a JSON file.\n    \n    Args:\n        model: The model being optimized\n        device: The compute device being used\n        optimizer: The optimizer instance\n    ",
      "code_snippet": "    plt.show()\n\ndef save_batch_diagnostics(model, device, optimizer):\n    \"\"\"\n    Save detailed batch calculation diagnostics to a JSON file.\n    \n    Args:\n        model: The model being optimized\n        device: The compute device being used\n        optimizer: The optimizer instance\n    \"\"\"\n    try:\n        import json\n        from isekaizen.utils.input_shapes import infer_input_shape\n        \n        # Get basic model info\n        model_params = sum(p.numel() for p in model.parameters())\n        model_size_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024**2)\n        model_complexity = model_params / 1_000_000\n        \n        # Get input shape\n        input_shape = infer_input_shape(model)\n        \n        # Get batch range\n        min_batch = optimizer.min_batch if hasattr(optimizer, 'min_batch') else 0\n        max_batch = optimizer.max_batch if hasattr(optimizer, 'max_batch') else 0\n        \n        # Create diagnostics data\n        diagnostics = {\n            'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n            'model_info': {\n                'parameters': int(model_params),\n                'size_mb': float(model_size_mb),\n                'complexity_millions': float(model_complexity),\n                'input_shape': [int(dim) for dim in input_shape]\n            },\n            'device_info': {\n                'type': device.type,\n                'name': torch.cuda.get_device_name(device) if device.type == 'cuda' else 'CPU',\n                'memory_gb': float(torch.cuda.get_device_properties(device).total_memory / (1024**3)) if device.type == 'cuda' else 0\n            },\n            'batch_info': {\n                'min_batch': int(min_batch),\n                'max_batch': int(max_batch)\n            }\n        }\n        \n        # Save to file\n        output_dir = os.path.join(\"examples\", \"refactored\", \"output\")\n        os.makedirs(output_dir, exist_ok=True)\n        filepath = os.path.join(output_dir, f\"batch_diagnostics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n        \n        with open(filepath, 'w') as f:\n            json.dump(diagnostics, f, indent=2)\n            \n        logger.info(f\"Diagnostics saved to: {filepath}\")\n        return filepath\n    except Exception as e:\n        logger.error(f\"Error saving diagnostics: {str(e)}\")\n        return None\n\ndef main():\n    \"\"\"\n    Run the example using the refactored IsekaiZen optimizer."
    },
    "main": {
      "start_line": 206,
      "end_line": 338,
      "parameters": [],
      "return_type": null,
      "calls": [
        {
          "name": "argparse.ArgumentParser",
          "line": 211
        },
        {
          "name": "parser.add_argument",
          "line": 212
        },
        {
          "name": "parser.add_argument",
          "line": 213
        },
        {
          "name": "parser.add_argument",
          "line": 214
        },
        {
          "name": "parser.add_argument",
          "line": 215
        },
        {
          "name": "parser.add_argument",
          "line": 216
        },
        {
          "name": "parser.add_argument",
          "line": 217
        },
        {
          "name": "parser.add_argument",
          "line": 218
        },
        {
          "name": "parser.add_argument",
          "line": 219
        },
        {
          "name": "parser.add_argument",
          "line": 220
        },
        {
          "name": "parser.add_argument",
          "line": 221
        },
        {
          "name": "parser.parse_args",
          "line": 223
        },
        {
          "name": "logger.info",
          "line": 226
        },
        {
          "name": "logger.info",
          "line": 227
        },
        {
          "name": "logger.info",
          "line": 228
        },
        {
          "name": "logger.info",
          "line": 229
        },
        {
          "name": "logger.info",
          "line": 230
        },
        {
          "name": "logger.info",
          "line": 231
        },
        {
          "name": "logger.info",
          "line": 232
        },
        {
          "name": "logger.info",
          "line": 237
        },
        {
          "name": "torch.device",
          "line": 240
        },
        {
          "name": "logger.info",
          "line": 241
        },
        {
          "name": "logger.info",
          "line": 256
        },
        {
          "name": "load_cifar10_data",
          "line": 257
        },
        {
          "name": "logger.info",
          "line": 258
        },
        {
          "name": "logger.info",
          "line": 261
        },
        {
          "name": "create_model",
          "line": 262
        },
        {
          "name": "ModelTrainer",
          "line": 285
        },
        {
          "name": "logger.info",
          "line": 310
        },
        {
          "name": "trainer.train",
          "line": 311
        },
        {
          "name": "os.path.join",
          "line": 320
        },
        {
          "name": "os.makedirs",
          "line": 321
        },
        {
          "name": "os.path.join",
          "line": 322
        },
        {
          "name": "visualize_training_results",
          "line": 323
        },
        {
          "name": "logger.info",
          "line": 326
        },
        {
          "name": "trainer.evaluate",
          "line": 327
        },
        {
          "name": "logger.info",
          "line": 328
        },
        {
          "name": "logger.info",
          "line": 329
        },
        {
          "name": "os.path.join",
          "line": 332
        },
        {
          "name": "trainer.save_model",
          "line": 333
        },
        {
          "name": "logger.info",
          "line": 334
        },
        {
          "name": "logger.info",
          "line": 336
        },
        {
          "name": "logger.info",
          "line": 234
        },
        {
          "name": "logger.info",
          "line": 266
        },
        {
          "name": "RiskAwarePatternIsekaiZen",
          "line": 267
        },
        {
          "name": "save_batch_diagnostics",
          "line": 280
        },
        {
          "name": "logger.info",
          "line": 281
        },
        {
          "name": "torch.cuda.is_available",
          "line": 240
        },
        {
          "name": "load_latest_pattern_map",
          "line": 247
        },
        {
          "name": "nn.CrossEntropyLoss",
          "line": 287
        },
        {
          "name": "logger.info",
          "line": 249
        },
        {
          "name": "logger.warning",
          "line": 251
        },
        {
          "name": "logger.error",
          "line": 253
        },
        {
          "name": "len",
          "line": 258
        },
        {
          "name": "len",
          "line": 258
        },
        {
          "name": "....strftime",
          "line": 322
        },
        {
          "name": "datetime.now",
          "line": 322
        },
        {
          "name": "str",
          "line": 253
        }
      ],
      "docstring": "\n    Run the example using the refactored IsekaiZen optimizer.\n    ",
      "code_snippet": "        return None\n\ndef main():\n    \"\"\"\n    Run the example using the refactored IsekaiZen optimizer.\n    \"\"\"\n    # Parse command line arguments\n    parser = argparse.ArgumentParser(description=\"Run training with refactored IsekaiZen optimizer\")\n    parser.add_argument(\"--epochs\", type=int, default=10, help=\"Number of epochs for training (default: 10)\")\n    parser.add_argument(\"--skip-diagnostics\", action=\"store_true\", help=\"Skip batch boundary diagnostics\")\n    parser.add_argument(\"--override-batch-min\", type=int, default=None, help=\"Override minimum batch size\")\n    parser.add_argument(\"--override-batch-max\", type=int, default=None, help=\"Override maximum batch size\")\n    parser.add_argument(\"--risk-aversion\", type=float, default=0.5, help=\"Risk aversion factor (0.0-1.0)\")\n    parser.add_argument(\"--exploration-rate\", type=float, default=0.1, help=\"Exploration rate\")\n    parser.add_argument(\"--use-pattern-map\", action=\"store_true\", help=\"Load and use pattern map\")\n    parser.add_argument(\"--early-stopping\", action=\"store_true\", help=\"Enable early stopping\")\n    parser.add_argument(\"--patience\", type=int, default=3, help=\"Early stopping patience\")\n    parser.add_argument(\"--diagnostics-only\", action=\"store_true\", help=\"Run only diagnostics and exit\")\n    \n    args = parser.parse_args()\n    \n    # Show configuration\n    logger.info(\"=== Refactored IsekaiZen Optimization Example ===\")\n    logger.info(f\"Epochs: {args.epochs}\")\n    logger.info(f\"Skip diagnostics: {args.skip_diagnostics}\")\n    logger.info(f\"Risk aversion: {args.risk_aversion}\")\n    logger.info(f\"Exploration rate: {args.exploration_rate}\")\n    logger.info(f\"Use pattern map: {args.use_pattern_map}\")\n    logger.info(f\"Early stopping: {args.early_stopping} (patience: {args.patience})\")\n    if args.diagnostics_only:\n        logger.info(\"Running in diagnostics-only mode\")\n    \n    # Check Cortex components availability\n    logger.info(f\"Cortex components available: {cortex_components_available}\")\n    \n    # Set device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    logger.info(f\"Using device: {device}\")\n    \n    # Load pattern map if requested\n    pattern_map = None\n    if args.use_pattern_map:\n        try:\n            pattern_map = load_latest_pattern_map()\n            if pattern_map:\n                logger.info(\"Pattern map loaded successfully\")\n            else:\n                logger.warning(\"No pattern map found\")\n        except Exception as e:\n            logger.error(f\"Error loading pattern map: {str(e)}\")\n    \n    # Load dataset\n    logger.info(\"Loading CIFAR-10 dataset...\")\n    trainset, testset = load_cifar10_data()\n    logger.info(f\"Dataset loaded: {len(trainset)} training samples, {len(testset)} test samples\")\n    \n    # Create model\n    logger.info(\"Creating model...\")\n    model = create_model()\n    \n    # Create RiskAwarePatternIsekaiZen optimizer directly for diagnostics\n    if args.diagnostics_only:\n        logger.info(\"Running diagnostics only...\")\n        optimizer = RiskAwarePatternIsekaiZen(\n            model=model,\n            device=device,\n            pattern_map=pattern_map,\n            run_diagnostics=True,\n            total_epochs=args.epochs,\n            risk_aversion=args.risk_aversion,\n            exploration_rate=args.exploration_rate,\n            override_batch_min=args.override_batch_min,\n            override_batch_max=args.override_batch_max\n        )\n        \n        # Save diagnostics to file\n        save_batch_diagnostics(model, device, optimizer)\n        logger.info(\"Diagnostics completed. Exiting as requested.\")\n        return\n    \n    # Create trainer with risk-aware optimizer\n    trainer = ModelTrainer(\n        model=model,\n        criterion=nn.CrossEntropyLoss(),\n        optimizer_class=optim.SGD,\n        optimizer_kwargs={\n            \"lr\": 0.01,\n            \"momentum\": 0.9,\n            \"weight_decay\": 5e-4\n        },\n        scheduler_class=optim.lr_scheduler.CosineAnnealingLR,\n        scheduler_kwargs={\n            \"T_max\": args.epochs\n        },\n        device=device,\n        use_risk_aware=True,  # Use the risk-aware optimizer\n        pattern_map=pattern_map,\n        run_diagnostics=not args.skip_diagnostics,\n        total_epochs=args.epochs,\n        risk_aversion=args.risk_aversion,\n        exploration_rate=args.exploration_rate,\n        override_batch_min=args.override_batch_min,\n        override_batch_max=args.override_batch_max\n    )\n    \n    # Train the model\n    logger.info(f\"Starting training for {args.epochs} epochs...\")\n    history = trainer.train(\n        train_dataset=trainset,\n        val_dataset=testset,\n        epochs=args.epochs,\n        early_stopping=args.early_stopping,\n        patience=args.patience\n    )\n    \n    # Visualize results\n    output_dir = os.path.join(\"examples\", \"refactored\", \"output\")\n    os.makedirs(output_dir, exist_ok=True)\n    output_path = os.path.join(output_dir, f\"training_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\")\n    visualize_training_results(history, output_path)\n    \n    # Final evaluation\n    logger.info(\"Performing final evaluation...\")\n    final_eval = trainer.evaluate(testset)\n    logger.info(f\"Final test accuracy: {final_eval['accuracy']:.2f}%\")\n    logger.info(f\"Final test loss: {final_eval['loss']:.4f}\")\n    \n    # Save model\n    model_path = os.path.join(output_dir, \"model.pth\")\n    trainer.save_model(model_path)\n    logger.info(f\"Model saved to {model_path}\")\n    \n    logger.info(\"Example completed successfully\")\n\nif __name__ == \"__main__\":\n    try:\n        main()"
    }
  },
  "constants": {}
}