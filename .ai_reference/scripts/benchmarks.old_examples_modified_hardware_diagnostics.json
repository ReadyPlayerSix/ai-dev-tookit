{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\benchmarks.old\\examples\\modified\\hardware_diagnostics.py",
  "imports": [
    {
      "name": "os",
      "line": 12
    },
    {
      "name": "sys",
      "line": 13
    },
    {
      "name": "json",
      "line": 14
    },
    {
      "name": "time",
      "line": 15
    },
    {
      "name": "logging",
      "line": 16
    },
    {
      "name": "torch",
      "line": 17
    },
    {
      "name": "torch.nn",
      "line": 18
    },
    {
      "name": "psutil",
      "line": 19
    },
    {
      "name": "platform",
      "line": 20
    },
    {
      "name": "multiprocessing",
      "line": 21
    },
    {
      "name": "typing.Dict",
      "line": 22
    },
    {
      "name": "typing.List",
      "line": 22
    },
    {
      "name": "typing.Any",
      "line": 22
    },
    {
      "name": "typing.Optional",
      "line": 22
    },
    {
      "name": "typing.Tuple",
      "line": 22
    },
    {
      "name": "datetime.datetime",
      "line": 23
    },
    {
      "name": "isekaizen.hardware.analyzer.HardwareAnalyzer",
      "line": 34
    },
    {
      "name": "re",
      "line": 109
    }
  ],
  "classes": {
    "HardwareDiagnostics": {
      "start_line": 41,
      "end_line": 578,
      "methods": {
        "__init__": {
          "start_line": 44,
          "end_line": 63,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "datetime.now",
              "line": 46
            },
            {
              "name": "self.diagnostics_time.isoformat",
              "line": 48
            },
            {
              "name": "HardwareAnalyzer",
              "line": 61
            }
          ],
          "docstring": "Initialize the hardware diagnostics",
          "code_snippet": "    \"\"\"Hardware diagnostics and benchmarking for IsekaiZen\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize the hardware diagnostics\"\"\"\n        self.diagnostics_time = datetime.now()\n        self.results = {\n            \"timestamp\": self.diagnostics_time.isoformat(),\n            \"platform\": {},\n            \"cpu\": {},\n            \"memory\": {},\n            \"gpu\": {},\n            \"storage\": {},\n            \"compatibility\": {},\n            \"benchmark_results\": {}\n        }\n        \n        # Initialize IsekaiZen hardware analyzer if available\n        self.analyzer = None\n        if USING_ISEKAIZEN_ANALYZER:\n            self.analyzer = HardwareAnalyzer()\n    \n    def run_full_diagnostics(self) -> Dict[str, Any]:\n        \"\"\"Run all diagnostics and return results\"\"\"\n        logger.info(\"Starting comprehensive hardware diagnostics...\")"
        },
        "run_full_diagnostics": {
          "start_line": 63,
          "end_line": 81,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": "complex_type",
          "calls": [
            {
              "name": "logger.info",
              "line": 65
            },
            {
              "name": "self._check_platform",
              "line": 67
            },
            {
              "name": "self._check_cpu",
              "line": 68
            },
            {
              "name": "self._check_memory",
              "line": 69
            },
            {
              "name": "self._check_gpu",
              "line": 70
            },
            {
              "name": "self._check_storage",
              "line": 71
            },
            {
              "name": "self._check_compatibility",
              "line": 72
            },
            {
              "name": "torch.cuda.is_available",
              "line": 75
            },
            {
              "name": "logger.info",
              "line": 78
            },
            {
              "name": "self._run_quick_gpu_benchmark",
              "line": 76
            }
          ],
          "docstring": "Run all diagnostics and return results",
          "code_snippet": "            self.analyzer = HardwareAnalyzer()\n    \n    def run_full_diagnostics(self) -> Dict[str, Any]:\n        \"\"\"Run all diagnostics and return results\"\"\"\n        logger.info(\"Starting comprehensive hardware diagnostics...\")\n        \n        self._check_platform()\n        self._check_cpu()\n        self._check_memory()\n        self._check_gpu()\n        self._check_storage()\n        self._check_compatibility()\n        \n        # Run basic benchmark if we have a GPU\n        if torch.cuda.is_available():\n            self._run_quick_gpu_benchmark()\n        \n        logger.info(\"Hardware diagnostics complete\")\n        return self.results\n    \n    def _check_platform(self):\n        \"\"\"Check platform information\"\"\"\n        logger.info(\"Checking platform information...\")"
        },
        "_check_platform": {
          "start_line": 81,
          "end_line": 93,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logger.info",
              "line": 83
            },
            {
              "name": "platform.system",
              "line": 86
            },
            {
              "name": "platform.version",
              "line": 87
            },
            {
              "name": "platform.platform",
              "line": 88
            },
            {
              "name": "platform.machine",
              "line": 89
            },
            {
              "name": "platform.python_version",
              "line": 90
            },
            {
              "name": "platform.python_implementation",
              "line": 91
            }
          ],
          "docstring": "Check platform information",
          "code_snippet": "        return self.results\n    \n    def _check_platform(self):\n        \"\"\"Check platform information\"\"\"\n        logger.info(\"Checking platform information...\")\n        \n        self.results[\"platform\"] = {\n            \"system\": platform.system(),\n            \"version\": platform.version(),\n            \"platform\": platform.platform(),\n            \"machine\": platform.machine(),\n            \"python_version\": platform.python_version(),\n            \"python_implementation\": platform.python_implementation()\n        }\n    \n    def _check_cpu(self):\n        \"\"\"Check CPU information\"\"\""
        },
        "_check_cpu": {
          "start_line": 94,
          "end_line": 127,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logger.info",
              "line": 96
            },
            {
              "name": "platform.processor",
              "line": 99
            },
            {
              "name": "psutil.cpu_count",
              "line": 100
            },
            {
              "name": "psutil.cpu_count",
              "line": 101
            },
            {
              "name": "psutil.cpu_freq",
              "line": 102
            },
            {
              "name": "psutil.cpu_percent",
              "line": 103
            },
            {
              "name": "os.path.exists",
              "line": 107
            },
            {
              "name": "platform.system",
              "line": 107
            },
            {
              "name": "re.search",
              "line": 114
            },
            {
              "name": "re.search",
              "line": 119
            },
            {
              "name": "open",
              "line": 110
            },
            {
              "name": "f.read",
              "line": 111
            },
            {
              "name": "model_match.group",
              "line": 116
            },
            {
              "name": "cache_match.group",
              "line": 121
            },
            {
              "name": "logger.warning",
              "line": 123
            }
          ],
          "docstring": "Check CPU information",
          "code_snippet": "        }\n    \n    def _check_cpu(self):\n        \"\"\"Check CPU information\"\"\"\n        logger.info(\"Checking CPU information...\")\n        \n        cpu_info = {\n            \"processor\": platform.processor(),\n            \"physical_cores\": psutil.cpu_count(logical=False),\n            \"logical_cores\": psutil.cpu_count(logical=True),\n            \"current_frequency\": psutil.cpu_freq(),\n            \"current_usage_percent\": psutil.cpu_percent(interval=1),\n        }\n        \n        # Check if we can get more detailed CPU info on Linux\n        if platform.system() == \"Linux\" and os.path.exists(\"/proc/cpuinfo\"):\n            try:\n                import re\n                with open(\"/proc/cpuinfo\", \"r\") as f:\n                    cpuinfo = f.read()\n                \n                # Extract model name\n                model_match = re.search(r\"model name\\s+:\\s+(.*)\", cpuinfo)\n                if model_match:\n                    cpu_info[\"model_name\"] = model_match.group(1)\n                \n                # Extract cache information\n                cache_match = re.search(r\"cache size\\s+:\\s+(.*)\", cpuinfo)\n                if cache_match:\n                    cpu_info[\"cache_size\"] = cache_match.group(1)\n            except Exception as e:\n                logger.warning(f\"Error reading CPU info: {e}\")\n        \n        self.results[\"cpu\"] = cpu_info\n    \n    def _check_memory(self):\n        \"\"\"Check memory information\"\"\"\n        logger.info(\"Checking memory information...\")"
        },
        "_check_memory": {
          "start_line": 127,
          "end_line": 144,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logger.info",
              "line": 129
            },
            {
              "name": "psutil.virtual_memory",
              "line": 131
            },
            {
              "name": "psutil.swap_memory",
              "line": 132
            }
          ],
          "docstring": "Check memory information",
          "code_snippet": "        self.results[\"cpu\"] = cpu_info\n    \n    def _check_memory(self):\n        \"\"\"Check memory information\"\"\"\n        logger.info(\"Checking memory information...\")\n        \n        virtual_memory = psutil.virtual_memory()\n        swap_memory = psutil.swap_memory()\n        \n        self.results[\"memory\"] = {\n            \"total\": virtual_memory.total,\n            \"available\": virtual_memory.available,\n            \"used\": virtual_memory.used,\n            \"free\": virtual_memory.free,\n            \"percent_used\": virtual_memory.percent,\n            \"swap_total\": swap_memory.total,\n            \"swap_used\": swap_memory.used,\n            \"swap_percent_used\": swap_memory.percent\n        }\n    \n    def _check_gpu(self):\n        \"\"\"Check GPU information\"\"\""
        },
        "_check_gpu": {
          "start_line": 145,
          "end_line": 185,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logger.info",
              "line": 147
            },
            {
              "name": "torch.cuda.is_available",
              "line": 150
            },
            {
              "name": "range",
              "line": 157
            },
            {
              "name": "logger.info",
              "line": 181
            },
            {
              "name": "torch.cuda.is_available",
              "line": 151
            },
            {
              "name": "torch.cuda.is_available",
              "line": 152
            },
            {
              "name": "torch.cuda.device_count",
              "line": 152
            },
            {
              "name": "torch.cuda.get_device_properties",
              "line": 158
            },
            {
              "name": "torch.cuda.set_device",
              "line": 161
            },
            {
              "name": "torch.cuda.empty_cache",
              "line": 162
            },
            {
              "name": "torch.cuda.memory_reserved",
              "line": 163
            },
            {
              "name": "torch.cuda.memory_allocated",
              "line": 164
            },
            {
              "name": "....append",
              "line": 176
            },
            {
              "name": "logger.info",
              "line": 178
            }
          ],
          "docstring": "Check GPU information",
          "code_snippet": "        }\n    \n    def _check_gpu(self):\n        \"\"\"Check GPU information\"\"\"\n        logger.info(\"Checking GPU information...\")\n        \n        gpu_info = {\n            \"is_available\": torch.cuda.is_available(),\n            \"driver_version\": torch.version.cuda if torch.cuda.is_available() else None,\n            \"device_count\": torch.cuda.device_count() if torch.cuda.is_available() else 0,\n            \"devices\": []\n        }\n        \n        if gpu_info[\"is_available\"]:\n            for i in range(gpu_info[\"device_count\"]):\n                device_properties = torch.cuda.get_device_properties(i)\n                \n                # Get memory info through built-in PyTorch methods\n                torch.cuda.set_device(i)\n                torch.cuda.empty_cache()\n                reserved_memory = torch.cuda.memory_reserved(i)\n                allocated_memory = torch.cuda.memory_allocated(i)\n                \n                device_info = {\n                    \"index\": i,\n                    \"name\": device_properties.name,\n                    \"total_memory\": device_properties.total_memory,\n                    \"reserved_memory\": reserved_memory,\n                    \"allocated_memory\": allocated_memory,\n                    \"multi_processor_count\": device_properties.multi_processor_count,\n                    \"compute_capability\": f\"{device_properties.major}.{device_properties.minor}\",\n                }\n                \n                gpu_info[\"devices\"].append(device_info)\n                \n                logger.info(f\"Found GPU {i}: {device_properties.name} \"\n                           f\"({device_properties.total_memory / 1e9:.2f} GB)\")\n        else:\n            logger.info(\"No CUDA-compatible GPU found\")\n        \n        self.results[\"gpu\"] = gpu_info\n    \n    def _check_storage(self):\n        \"\"\"Check storage information\"\"\"\n        logger.info(\"Checking storage information...\")"
        },
        "_check_storage": {
          "start_line": 185,
          "end_line": 226,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logger.info",
              "line": 187
            },
            {
              "name": "psutil.disk_partitions",
              "line": 193
            },
            {
              "name": "os.getcwd",
              "line": 212
            },
            {
              "name": "psutil.disk_usage",
              "line": 213
            },
            {
              "name": "psutil.disk_usage",
              "line": 195
            },
            {
              "name": "....append",
              "line": 205
            },
            {
              "name": "logger.warning",
              "line": 222
            }
          ],
          "docstring": "Check storage information",
          "code_snippet": "        self.results[\"gpu\"] = gpu_info\n    \n    def _check_storage(self):\n        \"\"\"Check storage information\"\"\"\n        logger.info(\"Checking storage information...\")\n        \n        storage_info = {\n            \"partitions\": []\n        }\n        \n        for partition in psutil.disk_partitions():\n            try:\n                usage = psutil.disk_usage(partition.mountpoint)\n                partition_info = {\n                    \"device\": partition.device,\n                    \"mountpoint\": partition.mountpoint,\n                    \"fstype\": partition.fstype,\n                    \"total\": usage.total,\n                    \"used\": usage.used,\n                    \"free\": usage.free,\n                    \"percent_used\": usage.percent\n                }\n                storage_info[\"partitions\"].append(partition_info)\n            except PermissionError:\n                # Skip partitions that can't be accessed\n                continue\n        \n        # Get current working directory disk usage\n        try:\n            cwd = os.getcwd()\n            cwd_usage = psutil.disk_usage(cwd)\n            storage_info[\"working_directory\"] = {\n                \"path\": cwd,\n                \"total\": cwd_usage.total,\n                \"used\": cwd_usage.used,\n                \"free\": cwd_usage.free,\n                \"percent_used\": cwd_usage.percent\n            }\n        except Exception as e:\n            logger.warning(f\"Error checking working directory storage: {e}\")\n        \n        self.results[\"storage\"] = storage_info\n    \n    def _check_compatibility(self):\n        \"\"\"Check system compatibility for ML training\"\"\"\n        logger.info(\"Checking system compatibility for ML training...\")"
        },
        "_check_compatibility": {
          "start_line": 226,
          "end_line": 275,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logger.info",
              "line": 228
            },
            {
              "name": "torch.cuda.is_available",
              "line": 238
            },
            {
              "name": "....append",
              "line": 239
            },
            {
              "name": "....append",
              "line": 240
            },
            {
              "name": "any",
              "line": 244
            },
            {
              "name": "....append",
              "line": 251
            },
            {
              "name": "....append",
              "line": 255
            },
            {
              "name": "....append",
              "line": 256
            },
            {
              "name": "....append",
              "line": 245
            },
            {
              "name": "....append",
              "line": 246
            },
            {
              "name": "....append",
              "line": 262
            },
            {
              "name": "....append",
              "line": 263
            }
          ],
          "docstring": "Check system compatibility for ML training",
          "code_snippet": "        self.results[\"storage\"] = storage_info\n    \n    def _check_compatibility(self):\n        \"\"\"Check system compatibility for ML training\"\"\"\n        logger.info(\"Checking system compatibility for ML training...\")\n        \n        compatibility_info = {\n            \"recommended_for_training\": False,\n            \"issues\": [],\n            \"warnings\": [],\n            \"recommendations\": []\n        }\n        \n        # Check for GPU\n        if not torch.cuda.is_available():\n            compatibility_info[\"issues\"].append(\"No CUDA-compatible GPU detected\")\n            compatibility_info[\"recommendations\"].append(\n                \"Training will be very slow without a GPU. Consider using a CUDA-compatible NVIDIA GPU.\")\n        \n        # Check GPU memory for ML training\n        elif any(device[\"total_memory\"] < 4 * 1e9 for device in self.results[\"gpu\"][\"devices\"]):\n            compatibility_info[\"warnings\"].append(\"GPU(s) with less than 4GB memory may struggle with larger models\")\n            compatibility_info[\"recommendations\"].append(\n                \"Consider using smaller batch sizes and model architectures to fit in GPU memory\")\n        \n        # Check CPU cores\n        if self.results[\"cpu\"][\"logical_cores\"] < 4:\n            compatibility_info[\"warnings\"].append(\"System has fewer than 4 CPU cores, which may slow data preprocessing\")\n        \n        # Check RAM\n        if self.results[\"memory\"][\"total\"] < 8 * 1e9:  # Less than 8GB\n            compatibility_info[\"warnings\"].append(\"System has less than 8GB RAM, which may limit data loading capabilities\")\n            compatibility_info[\"recommendations\"].append(\n                \"Consider using smaller datasets or increasing virtual memory/swap space\")\n        \n        # Check storage space in working directory\n        if \"working_directory\" in self.results[\"storage\"]:\n            if self.results[\"storage\"][\"working_directory\"][\"free\"] < 10 * 1e9:  # Less than 10GB\n                compatibility_info[\"warnings\"].append(\"Less than 10GB free space in working directory\")\n                compatibility_info[\"recommendations\"].append(\n                    \"Free up disk space or change to a directory with more space for storing models and datasets\")\n        \n        # Set overall recommendation\n        if not compatibility_info[\"issues\"]:\n            if not compatibility_info[\"warnings\"]:\n                compatibility_info[\"recommended_for_training\"] = True\n            else:\n                compatibility_info[\"recommended_for_training\"] = \"with_limitations\"\n        \n        self.results[\"compatibility\"] = compatibility_info\n    \n    def _run_quick_gpu_benchmark(self):\n        \"\"\"Run a quick GPU benchmark to test performance\"\"\"\n        logger.info(\"Running quick GPU benchmark...\")"
        },
        "_run_quick_gpu_benchmark": {
          "start_line": 275,
          "end_line": 454,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "logger.info",
              "line": 277
            },
            {
              "name": "range",
              "line": 287
            },
            {
              "name": "torch.cuda.is_available",
              "line": 282
            },
            {
              "name": "logger.info",
              "line": 283
            },
            {
              "name": "torch.cuda.device_count",
              "line": 287
            },
            {
              "name": "torch.device",
              "line": 288
            },
            {
              "name": "logger.info",
              "line": 291
            },
            {
              "name": "torch.cuda.empty_cache",
              "line": 294
            },
            {
              "name": "torch.cuda.memory_allocated",
              "line": 297
            },
            {
              "name": "torch.cuda.empty_cache",
              "line": 449
            },
            {
              "name": "torch.cuda.get_device_properties",
              "line": 289
            },
            {
              "name": "logger.info",
              "line": 309
            },
            {
              "name": "torch.randn",
              "line": 312
            },
            {
              "name": "torch.randn",
              "line": 313
            },
            {
              "name": "torch.matmul",
              "line": 316
            },
            {
              "name": "torch.cuda.synchronize",
              "line": 317
            },
            {
              "name": "time.time",
              "line": 320
            },
            {
              "name": "torch.matmul",
              "line": 321
            },
            {
              "name": "torch.cuda.synchronize",
              "line": 322
            },
            {
              "name": "logger.info",
              "line": 325
            },
            {
              "name": "torch.cuda.empty_cache",
              "line": 345
            },
            {
              "name": "torch.cuda.get_device_properties",
              "line": 302
            },
            {
              "name": "time.time",
              "line": 323
            },
            {
              "name": "torch.cuda.max_memory_allocated",
              "line": 328
            },
            {
              "name": "logger.info",
              "line": 350
            },
            {
              "name": "....to",
              "line": 353
            },
            {
              "name": "torch.randn",
              "line": 368
            },
            {
              "name": "torch.randint",
              "line": 369
            },
            {
              "name": "nn.CrossEntropyLoss",
              "line": 370
            },
            {
              "name": "model",
              "line": 373
            },
            {
              "name": "time.time",
              "line": 376
            },
            {
              "name": "model",
              "line": 377
            },
            {
              "name": "torch.cuda.synchronize",
              "line": 378
            },
            {
              "name": "criterion",
              "line": 382
            },
            {
              "name": "time.time",
              "line": 383
            },
            {
              "name": "loss.backward",
              "line": 384
            },
            {
              "name": "torch.cuda.synchronize",
              "line": 385
            },
            {
              "name": "logger.info",
              "line": 388
            },
            {
              "name": "logger.info",
              "line": 389
            },
            {
              "name": "device_results.update",
              "line": 392
            },
            {
              "name": "torch.cuda.empty_cache",
              "line": 402
            },
            {
              "name": "torch.cuda.empty_cache",
              "line": 412
            },
            {
              "name": "logger.warning",
              "line": 445
            },
            {
              "name": "time.time",
              "line": 379
            },
            {
              "name": "time.time",
              "line": 386
            },
            {
              "name": "logger.warning",
              "line": 405
            },
            {
              "name": "logger.warning",
              "line": 414
            },
            {
              "name": "torch.randn",
              "line": 417
            },
            {
              "name": "torch.randn",
              "line": 418
            },
            {
              "name": "torch.cuda.synchronize",
              "line": 420
            },
            {
              "name": "time.time",
              "line": 421
            },
            {
              "name": "torch.matmul",
              "line": 422
            },
            {
              "name": "torch.cuda.synchronize",
              "line": 423
            },
            {
              "name": "str",
              "line": 446
            },
            {
              "name": "nn.Sequential",
              "line": 353
            },
            {
              "name": "time.time",
              "line": 424
            },
            {
              "name": "torch.cuda.max_memory_allocated",
              "line": 426
            },
            {
              "name": "logger.warning",
              "line": 441
            },
            {
              "name": "nn.Conv2d",
              "line": 354
            },
            {
              "name": "nn.ReLU",
              "line": 355
            },
            {
              "name": "nn.MaxPool2d",
              "line": 356
            },
            {
              "name": "nn.Conv2d",
              "line": 357
            },
            {
              "name": "nn.ReLU",
              "line": 358
            },
            {
              "name": "nn.MaxPool2d",
              "line": 359
            },
            {
              "name": "nn.Conv2d",
              "line": 360
            },
            {
              "name": "nn.ReLU",
              "line": 361
            },
            {
              "name": "nn.AdaptiveAvgPool2d",
              "line": 362
            },
            {
              "name": "nn.Flatten",
              "line": 363
            },
            {
              "name": "nn.Linear",
              "line": 364
            },
            {
              "name": "str",
              "line": 442
            }
          ],
          "docstring": "Run a quick GPU benchmark to test performance",
          "code_snippet": "        self.results[\"compatibility\"] = compatibility_info\n    \n    def _run_quick_gpu_benchmark(self):\n        \"\"\"Run a quick GPU benchmark to test performance\"\"\"\n        logger.info(\"Running quick GPU benchmark...\")\n        \n        benchmark_results = {}\n        \n        # Skip benchmark if no GPU\n        if not torch.cuda.is_available():\n            logger.info(\"Skipping GPU benchmark (no GPU available)\")\n            return\n        \n        # Custom benchmark for each GPU\n        for i in range(torch.cuda.device_count()):\n            device = torch.device(f\"cuda:{i}\")\n            device_name = torch.cuda.get_device_properties(i).name\n            \n            logger.info(f\"Benchmarking GPU {i}: {device_name}\")\n            \n            # Clear memory first\n            torch.cuda.empty_cache()\n            \n            # Record initial memory use\n            initial_memory = torch.cuda.memory_allocated(i)\n            \n            # Create a model for testing memory capacity and compute\n            try:\n                # Adjust matrix size based on available GPU memory\n                device_memory = torch.cuda.get_device_properties(i).total_memory\n                if device_memory < 4 * 1e9:  # Less than 4GB\n                    matrix_size = 2000\n                elif device_memory < 8 * 1e9:  # Less than 8GB\n                    matrix_size = 3000\n                else:\n                    matrix_size = 5000\n                logger.info(f\"Running {matrix_size}x{matrix_size} matrix multiplication test...\")\n                \n                # First create matrices with compatible dimensions for multiplication\n                matrix_a = torch.randn(matrix_size, matrix_size, device=device)\n                matrix_b = torch.randn(matrix_size, matrix_size, device=device)\n                \n                # Warmup - ensure slicing creates compatible shapes\n                warm_result = torch.matmul(matrix_a[:100, :100], matrix_b[:100, :100])\n                torch.cuda.synchronize()\n                \n                # Benchmark\n                start_time = time.time()\n                result = torch.matmul(matrix_a, matrix_b)\n                torch.cuda.synchronize()\n                matmul_time = time.time() - start_time\n                \n                logger.info(f\"Matrix multiplication time: {matmul_time:.4f} seconds\")\n                \n                # Memory test\n                peak_memory = torch.cuda.max_memory_allocated(i) - initial_memory\n                \n                # Performance metrics\n                matmul_perf = (2 * (matrix_size ** 3)) / matmul_time / 1e9  # Approx. FLOPs in G\n                \n                # Record results\n                device_results = {\n                    \"device_name\": device_name,\n                    \"matrix_size\": matrix_size,\n                    \"matmul_time_seconds\": matmul_time,\n                    \"matmul_performance_gflops\": matmul_perf,\n                    \"peak_memory_used\": peak_memory,\n                    \"memory_efficiency\": peak_memory / self.results[\"gpu\"][\"devices\"][i][\"total_memory\"]\n                }\n                \n                # Reset memory before next test\n                del matrix_a, matrix_b, result\n                torch.cuda.empty_cache()\n                \n                # Simple CNN test\n                try:\n                    batch_size = 64\n                    logger.info(f\"Running CNN forward/backward pass test with batch size {batch_size}...\")\n                    \n                    # Create a simple CNN\n                    model = nn.Sequential(\n                        nn.Conv2d(3, 64, kernel_size=3, padding=1),\n                        nn.ReLU(),\n                        nn.MaxPool2d(2),\n                        nn.Conv2d(64, 128, kernel_size=3, padding=1),\n                        nn.ReLU(),\n                        nn.MaxPool2d(2),\n                        nn.Conv2d(128, 256, kernel_size=3, padding=1),\n                        nn.ReLU(),\n                        nn.AdaptiveAvgPool2d((1, 1)),\n                        nn.Flatten(),\n                        nn.Linear(256, 10)\n                    ).to(device)\n                    \n                    # Create random input and target\n                    input_data = torch.randn(batch_size, 3, 32, 32, device=device)\n                    target = torch.randint(0, 10, (batch_size,), device=device)\n                    criterion = nn.CrossEntropyLoss()\n                    \n                    # Warmup\n                    _ = model(input_data[:4])\n                    \n                    # Forward pass timing\n                    start_time = time.time()\n                    output = model(input_data)\n                    torch.cuda.synchronize()\n                    forward_time = time.time() - start_time\n                    \n                    # Backward pass timing\n                    loss = criterion(output, target)\n                    start_time = time.time()\n                    loss.backward()\n                    torch.cuda.synchronize()\n                    backward_time = time.time() - start_time\n                    \n                    logger.info(f\"CNN forward time: {forward_time:.4f} seconds\")\n                    logger.info(f\"CNN backward time: {backward_time:.4f} seconds\")\n                    \n                    # Add CNN results\n                    device_results.update({\n                        \"cnn_forward_time\": forward_time,\n                        \"cnn_backward_time\": backward_time,\n                        \"cnn_total_time\": forward_time + backward_time,\n                        \"cnn_batch_size\": batch_size,\n                        \"cnn_samples_per_second\": batch_size / (forward_time + backward_time)\n                    })\n                    \n                    # Reset CNN memory\n                    del model, input_data, target, output, loss\n                    torch.cuda.empty_cache()\n                    \n                except Exception as e:\n                    logger.warning(f\"Error during CNN benchmark on device {i}: {e}\")\n                \n                # Add device results to overall benchmark results\n                benchmark_results[f\"gpu_{i}\"] = device_results\n                \n            except torch.cuda.OutOfMemoryError:\n                # If we get OOM, retry with smaller matrix\n                torch.cuda.empty_cache()\n                try:\n                    logger.warning(f\"OOM error on device {i}. Retrying with smaller matrix...\")\n                    matrix_size = 1000  # Much smaller matrix\n                    \n                    matrix_a = torch.randn(matrix_size, matrix_size, device=device)\n                    matrix_b = torch.randn(matrix_size, matrix_size, device=device)\n                    \n                    torch.cuda.synchronize()\n                    start_time = time.time()\n                    result = torch.matmul(matrix_a, matrix_b)\n                    torch.cuda.synchronize()\n                    matmul_time = time.time() - start_time\n                    \n                    peak_memory = torch.cuda.max_memory_allocated(i) - initial_memory\n                    matmul_perf = (2 * (matrix_size ** 3)) / matmul_time / 1e9\n                    \n                    device_results = {\n                        \"device_name\": device_name,\n                        \"matrix_size\": matrix_size,\n                        \"matmul_time_seconds\": matmul_time,\n                        \"matmul_performance_gflops\": matmul_perf,\n                        \"peak_memory_used\": peak_memory,\n                        \"memory_efficiency\": peak_memory / self.results[\"gpu\"][\"devices\"][i][\"total_memory\"],\n                        \"fallback_used\": True\n                    }\n                    benchmark_results[f\"gpu_{i}\"] = device_results\n                    \n                except Exception as e:\n                    logger.warning(f\"Error during fallback benchmark on device {i}: {e}\")\n                    benchmark_results[f\"gpu_{i}\"] = {\"error\": str(e), \"fallback_failed\": True}\n            \n            except Exception as e:\n                logger.warning(f\"Error during benchmark on device {i}: {e}\")\n                benchmark_results[f\"gpu_{i}\"] = {\"error\": str(e)}\n            \n            # Final cleanup\n            torch.cuda.empty_cache()\n        \n        # Store benchmark results\n        self.results[\"benchmark_results\"] = benchmark_results\n    \n    def get_summary(self) -> Dict[str, Any]:\n        \"\"\"Get a user-friendly summary of the diagnostics results\"\"\"\n        summary = {"
        },
        "get_summary": {
          "start_line": 454,
          "end_line": 510,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": "complex_type",
          "calls": [
            {
              "name": "self.results.get",
              "line": 500
            },
            {
              "name": "self.diagnostics_time.strftime",
              "line": 457
            },
            {
              "name": "....get",
              "line": 481
            },
            {
              "name": "compatibility.get",
              "line": 502
            },
            {
              "name": "compatibility.get",
              "line": 503
            },
            {
              "name": "compatibility.get",
              "line": 504
            },
            {
              "name": "compatibility.get",
              "line": 505
            },
            {
              "name": "....get",
              "line": 460
            },
            {
              "name": "round",
              "line": 464
            },
            {
              "name": "round",
              "line": 465
            },
            {
              "name": "....get",
              "line": 466
            },
            {
              "name": "round",
              "line": 469
            },
            {
              "name": "....get",
              "line": 472
            },
            {
              "name": "....get",
              "line": 473
            },
            {
              "name": "....append",
              "line": 482
            },
            {
              "name": "....items",
              "line": 491
            },
            {
              "name": "....get",
              "line": 458
            },
            {
              "name": "....get",
              "line": 458
            },
            {
              "name": "....get",
              "line": 461
            },
            {
              "name": "....get",
              "line": 461
            },
            {
              "name": "....get",
              "line": 464
            },
            {
              "name": "....get",
              "line": 465
            },
            {
              "name": "....get",
              "line": 469
            },
            {
              "name": "device.get",
              "line": 483
            },
            {
              "name": "round",
              "line": 484
            },
            {
              "name": "device.get",
              "line": 485
            },
            {
              "name": "round",
              "line": 494
            },
            {
              "name": "round",
              "line": 495
            },
            {
              "name": "....get",
              "line": 469
            },
            {
              "name": "device.get",
              "line": 484
            },
            {
              "name": "results.get",
              "line": 494
            },
            {
              "name": "results.get",
              "line": 495
            },
            {
              "name": "self.results.get",
              "line": 469
            }
          ],
          "docstring": "Get a user-friendly summary of the diagnostics results",
          "code_snippet": "        self.results[\"benchmark_results\"] = benchmark_results\n    \n    def get_summary(self) -> Dict[str, Any]:\n        \"\"\"Get a user-friendly summary of the diagnostics results\"\"\"\n        summary = {\n            \"timestamp\": self.diagnostics_time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n            \"system\": f\"{self.results['platform'].get('system', 'Unknown')} {self.results['platform'].get('version', '')}\",\n            \"cpu\": {\n                \"model\": self.results[\"cpu\"].get(\"processor\", \"Unknown\"),\n                \"cores\": f\"{self.results['cpu'].get('physical_cores', 0)} physical, {self.results['cpu'].get('logical_cores', 0)} logical\"\n            },\n            \"memory\": {\n                \"total_gb\": round(self.results[\"memory\"].get(\"total\", 0) / 1e9, 1),\n                \"available_gb\": round(self.results[\"memory\"].get(\"available\", 0) / 1e9, 1),\n                \"percent_used\": self.results[\"memory\"].get(\"percent_used\", 0)\n            },\n            \"storage\": {\n                \"working_dir_free_gb\": round(self.results.get(\"storage\", {}).get(\"working_directory\", {}).get(\"free\", 0) / 1e9, 1)\n            },\n            \"gpu\": {\n                \"available\": self.results[\"gpu\"].get(\"is_available\", False),\n                \"count\": self.results[\"gpu\"].get(\"device_count\", 0),\n                \"devices\": []\n            },\n            \"training_readiness\": {}\n        }\n        \n        # Add GPU details if available\n        if summary[\"gpu\"][\"available\"]:\n            for device in self.results[\"gpu\"].get(\"devices\", []):\n                summary[\"gpu\"][\"devices\"].append({\n                    \"name\": device.get(\"name\", \"Unknown\"),\n                    \"memory_gb\": round(device.get(\"total_memory\", 0) / 1e9, 1),\n                    \"compute_capability\": device.get(\"compute_capability\", \"Unknown\")\n                })\n            \n            # Add benchmark results if available\n            if \"benchmark_results\" in self.results and self.results[\"benchmark_results\"]:\n                benchmark_summary = {}\n                for gpu_id, results in self.results[\"benchmark_results\"].items():\n                    if \"error\" not in results:\n                        benchmark_summary[gpu_id] = {\n                            \"matmul_gflops\": round(results.get(\"matmul_performance_gflops\", 0), 1),\n                            \"cnn_samples_per_sec\": round(results.get(\"cnn_samples_per_second\", 0), 1)\n                        }\n                summary[\"benchmark\"] = benchmark_summary\n        \n        # Add training readiness assessment\n        compatibility = self.results.get(\"compatibility\", {})\n        summary[\"training_readiness\"] = {\n            \"ready\": compatibility.get(\"recommended_for_training\", False),\n            \"issues\": compatibility.get(\"issues\", []),\n            \"warnings\": compatibility.get(\"warnings\", []),\n            \"recommendations\": compatibility.get(\"recommendations\", [])\n        }\n        \n        return summary\n    \n    def save_results(self, filepath=None) -> str:\n        \"\"\"Save diagnostics results to a file\"\"\"\n        if filepath is None:"
        },
        "save_results": {
          "start_line": 510,
          "end_line": 525,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "filepath"
            }
          ],
          "return_type": "str",
          "calls": [
            {
              "name": "os.makedirs",
              "line": 517
            },
            {
              "name": "logger.info",
              "line": 522
            },
            {
              "name": "self.diagnostics_time.strftime",
              "line": 513
            },
            {
              "name": "os.path.join",
              "line": 514
            },
            {
              "name": "os.path.dirname",
              "line": 517
            },
            {
              "name": "open",
              "line": 519
            },
            {
              "name": "json.dump",
              "line": 520
            },
            {
              "name": "os.getcwd",
              "line": 514
            },
            {
              "name": "os.path.abspath",
              "line": 517
            }
          ],
          "docstring": "Save diagnostics results to a file",
          "code_snippet": "        return summary\n    \n    def save_results(self, filepath=None) -> str:\n        \"\"\"Save diagnostics results to a file\"\"\"\n        if filepath is None:\n            timestamp = self.diagnostics_time.strftime(\"%Y%m%d_%H%M%S\")\n            filepath = os.path.join(os.getcwd(), f\"hardware_diagnostics_{timestamp}.json\")\n        \n        # Ensure directory exists\n        os.makedirs(os.path.dirname(os.path.abspath(filepath)), exist_ok=True)\n        \n        with open(filepath, 'w') as f:\n            json.dump(self.results, f, indent=2, default=str)\n        \n        logger.info(f\"Diagnostics results saved to {filepath}\")\n        return filepath\n    \n    def print_summary(self):\n        \"\"\"Print a summary of the diagnostics results to the console\"\"\"\n        summary = self.get_summary()"
        },
        "print_summary": {
          "start_line": 525,
          "end_line": 578,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "self.get_summary",
              "line": 527
            },
            {
              "name": "print",
              "line": 529
            },
            {
              "name": "print",
              "line": 530
            },
            {
              "name": "print",
              "line": 531
            },
            {
              "name": "print",
              "line": 533
            },
            {
              "name": "print",
              "line": 534
            },
            {
              "name": "print",
              "line": 535
            },
            {
              "name": "print",
              "line": 536
            },
            {
              "name": "print",
              "line": 559
            },
            {
              "name": "print",
              "line": 576
            },
            {
              "name": "....center",
              "line": 530
            },
            {
              "name": "print",
              "line": 539
            },
            {
              "name": "enumerate",
              "line": 540
            },
            {
              "name": "print",
              "line": 548
            },
            {
              "name": "print",
              "line": 562
            },
            {
              "name": "print",
              "line": 567
            },
            {
              "name": "print",
              "line": 572
            },
            {
              "name": "print",
              "line": 541
            },
            {
              "name": "print",
              "line": 564
            },
            {
              "name": "print",
              "line": 569
            },
            {
              "name": "print",
              "line": 574
            },
            {
              "name": "print",
              "line": 546
            }
          ],
          "docstring": "Print a summary of the diagnostics results to the console",
          "code_snippet": "        return filepath\n    \n    def print_summary(self):\n        \"\"\"Print a summary of the diagnostics results to the console\"\"\"\n        summary = self.get_summary()\n        \n        print(\"\\n\" + \"=\" * 80)\n        print(\"IsekaiZen Hardware Diagnostics Summary\".center(80))\n        print(\"=\" * 80)\n        \n        print(f\"\\nSystem: {summary['system']}\")\n        print(f\"CPU: {summary['cpu']['model']} ({summary['cpu']['cores']})\")\n        print(f\"Memory: {summary['memory']['total_gb']} GB total, {summary['memory']['available_gb']} GB available\")\n        print(f\"Working Directory Free Space: {summary['storage']['working_dir_free_gb']} GB\")\n        \n        if summary['gpu']['available']:\n            print(f\"\\nGPU: {summary['gpu']['count']} device(s) found\")\n            for i, device in enumerate(summary['gpu']['devices']):\n                print(f\"  GPU {i}: {device['name']} ({device['memory_gb']} GB)\")\n                \n                # Add benchmark results if available\n                if 'benchmark' in summary and f'gpu_{i}' in summary['benchmark']:\n                    bench = summary['benchmark'][f'gpu_{i}']\n                    print(f\"    Performance: {bench['matmul_gflops']} GFLOPs, {bench['cnn_samples_per_sec']} samples/sec\")\n        else:\n            print(\"\\nGPU: None detected (CPU-only mode)\")\n        \n        # Training readiness\n        ready_status = summary['training_readiness']['ready']\n        if ready_status is True:\n            status_msg = \"Ready for optimal training\"\n        elif ready_status == \"with_limitations\":\n            status_msg = \"Ready with some limitations\"\n        else:\n            status_msg = \"Not recommended for training\"\n        \n        print(f\"\\nTraining Readiness: {status_msg}\")\n        \n        if summary['training_readiness']['issues']:\n            print(\"\\nIssues:\")\n            for issue in summary['training_readiness']['issues']:\n                print(f\"  - {issue}\")\n        \n        if summary['training_readiness']['warnings']:\n            print(\"\\nWarnings:\")\n            for warning in summary['training_readiness']['warnings']:\n                print(f\"  - {warning}\")\n        \n        if summary['training_readiness']['recommendations']:\n            print(\"\\nRecommendations:\")\n            for rec in summary['training_readiness']['recommendations']:\n                print(f\"  - {rec}\")\n        \n        print(\"\\n\" + \"=\" * 80)\n\n\ndef main():\n    \"\"\"Run hardware diagnostics as a standalone script\"\"\""
        }
      },
      "class_variables": [],
      "bases": [],
      "docstring": "Hardware diagnostics and benchmarking for IsekaiZen"
    }
  },
  "functions": {
    "main": {
      "start_line": 579,
      "end_line": 591,
      "parameters": [],
      "return_type": null,
      "calls": [
        {
          "name": "print",
          "line": 581
        },
        {
          "name": "HardwareDiagnostics",
          "line": 583
        },
        {
          "name": "diagnostics.run_full_diagnostics",
          "line": 584
        },
        {
          "name": "diagnostics.print_summary",
          "line": 585
        },
        {
          "name": "diagnostics.save_results",
          "line": 588
        },
        {
          "name": "print",
          "line": 589
        }
      ],
      "docstring": "Run hardware diagnostics as a standalone script",
      "code_snippet": "\n\ndef main():\n    \"\"\"Run hardware diagnostics as a standalone script\"\"\"\n    print(\"Running IsekaiZen Hardware Diagnostics...\")\n    \n    diagnostics = HardwareDiagnostics()\n    diagnostics.run_full_diagnostics()\n    diagnostics.print_summary()\n    \n    # Save results to file\n    filepath = diagnostics.save_results()\n    print(f\"\\nFull diagnostics results saved to: {filepath}\")\n\n\nif __name__ == \"__main__\":\n    main()"
    }
  },
  "constants": {}
}