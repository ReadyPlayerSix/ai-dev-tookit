{
  "path": "D:\\Projects\\isekaiZen\\machine-learning-optimizer\\benchmarks.old\\examples\\run_streamlined_pattern_mapping.py",
  "imports": [
    {
      "name": "os",
      "line": 15
    },
    {
      "name": "timeit.default_timer",
      "line": 16
    },
    {
      "name": "sys",
      "line": 17
    },
    {
      "name": "time",
      "line": 18
    },
    {
      "name": "json",
      "line": 19
    },
    {
      "name": "logging",
      "line": 20
    },
    {
      "name": "argparse",
      "line": 21
    },
    {
      "name": "random",
      "line": 22
    },
    {
      "name": "torch",
      "line": 23
    },
    {
      "name": "torch.nn",
      "line": 24
    },
    {
      "name": "torchvision",
      "line": 25
    },
    {
      "name": "torchvision.transforms",
      "line": 26
    },
    {
      "name": "torchvision.models",
      "line": 27
    },
    {
      "name": "matplotlib.pyplot",
      "line": 28
    },
    {
      "name": "numpy",
      "line": 29
    },
    {
      "name": "cv2",
      "line": 30
    },
    {
      "name": "datetime.datetime",
      "line": 31
    },
    {
      "name": "enum.Enum",
      "line": 32
    },
    {
      "name": "dataclasses.dataclass",
      "line": 33
    },
    {
      "name": "typing.Dict",
      "line": 34
    },
    {
      "name": "typing.List",
      "line": 34
    },
    {
      "name": "typing.Any",
      "line": 34
    },
    {
      "name": "typing.Optional",
      "line": 34
    },
    {
      "name": "typing.Set",
      "line": 34
    },
    {
      "name": "typing.Union",
      "line": 34
    },
    {
      "name": "isekaizen.semantic.mapper_math.SemanticTopographicalMapper",
      "line": 40
    },
    {
      "name": "isekaizen.semantic.enhanced_pattern_mapper.EnhancedPatternMapper",
      "line": 42
    },
    {
      "name": "cortex.semantic_core.SemanticType",
      "line": 47
    },
    {
      "name": "cortex.semantic_core.SemanticPattern",
      "line": 47
    },
    {
      "name": "cortex.semantic_core.SemanticPatternRegistry",
      "line": 47
    },
    {
      "name": "cortex.semantic_core.DomainPatternExtractor",
      "line": 47
    }
  ],
  "classes": {
    "NumpySafeEncoder": {
      "start_line": 113,
      "end_line": 129,
      "methods": {
        "default": {
          "start_line": 114,
          "end_line": 129,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "obj"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "isinstance",
              "line": 115
            },
            {
              "name": "....default",
              "line": 127
            },
            {
              "name": "int",
              "line": 116
            },
            {
              "name": "isinstance",
              "line": 117
            },
            {
              "name": "float",
              "line": 118
            },
            {
              "name": "isinstance",
              "line": 119
            },
            {
              "name": "super",
              "line": 127
            },
            {
              "name": "obj.tolist",
              "line": 120
            },
            {
              "name": "hasattr",
              "line": 121
            },
            {
              "name": "np.issubdtype",
              "line": 121
            },
            {
              "name": "np.issubdtype",
              "line": 122
            },
            {
              "name": "float",
              "line": 122
            },
            {
              "name": "int",
              "line": 122
            },
            {
              "name": "hasattr",
              "line": 123
            },
            {
              "name": "hasattr",
              "line": 125
            },
            {
              "name": "obj.__class__.__name__.endswith",
              "line": 123
            },
            {
              "name": "hasattr",
              "line": 124
            },
            {
              "name": "str",
              "line": 124
            }
          ],
          "code_snippet": "    \"\"\"\n    class NumpySafeEncoder(json.JSONEncoder):\n        def default(self, obj):\n            if isinstance(obj, np.integer):\n                return int(obj)\n            elif isinstance(obj, np.floating):\n                return float(obj)\n            elif isinstance(obj, np.ndarray):\n                return obj.tolist()\n            elif hasattr(obj, 'dtype') and np.issubdtype(obj.dtype, np.number):\n                return float(obj) if np.issubdtype(obj.dtype, np.floating) else int(obj)\n            elif hasattr(obj, '__module__') and ('enum' in obj.__module__ or obj.__class__.__name__.endswith('Enum')):\n                return obj.name if hasattr(obj, 'name') else str(obj)\n            elif hasattr(obj, 'value'):\n                return obj.value\n            return super().default(obj)\n\n    json.dump(obj, file, cls=NumpySafeEncoder, **kwargs)\n\ndef load_cifar10_data(augment=True):"
        }
      },
      "class_variables": [],
      "bases": [
        "..."
      ]
    },
    "SemanticType": {
      "start_line": 52,
      "end_line": 60,
      "methods": {},
      "class_variables": [
        {
          "name": "STRUCTURE",
          "line": 54
        },
        {
          "name": "RELATIONSHIP",
          "line": 55
        },
        {
          "name": "INTENSITY",
          "line": 56
        },
        {
          "name": "DOMINANCE",
          "line": 57
        },
        {
          "name": "TEMPORAL",
          "line": 58
        }
      ],
      "bases": [
        "Enum"
      ],
      "docstring": "Semantic pattern types"
    },
    "SemanticPattern": {
      "start_line": 61,
      "end_line": 70,
      "methods": {},
      "class_variables": [],
      "bases": [],
      "docstring": "A semantic pattern with type and features"
    },
    "SemanticPatternRegistry": {
      "start_line": 70,
      "end_line": 91,
      "methods": {
        "__init__": {
          "start_line": 72,
          "end_line": 76,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [],
          "code_snippet": "    class SemanticPatternRegistry:\n        \"\"\"Registry for semantic patterns\"\"\"\n        def __init__(self):\n            self.patterns = {pattern_type: [] for pattern_type in SemanticType}\n            self.metrics = {\"patterns_recognized\": 0, \"pattern_confidences\": []}\n\n        def recognize_pattern(self, pattern):\n            \"\"\"Process and store a pattern\"\"\"\n            self.patterns[pattern.pattern_type].append(pattern)"
        },
        "recognize_pattern": {
          "start_line": 76,
          "end_line": 83,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "pattern"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "....append",
              "line": 78
            },
            {
              "name": "....append",
              "line": 80
            }
          ],
          "docstring": "Process and store a pattern",
          "code_snippet": "            self.metrics = {\"patterns_recognized\": 0, \"pattern_confidences\": []}\n\n        def recognize_pattern(self, pattern):\n            \"\"\"Process and store a pattern\"\"\"\n            self.patterns[pattern.pattern_type].append(pattern)\n            self.metrics[\"patterns_recognized\"] += 1\n            self.metrics[\"pattern_confidences\"].append(pattern.confidence)\n            return {\"stored\": True, \"pattern_type\": pattern.pattern_type.value}\n\n        def get_semantic_stats(self):\n            \"\"\"Get statistics about recognized patterns\"\"\"\n            return {"
        },
        "get_semantic_stats": {
          "start_line": 83,
          "end_line": 91,
          "parameters": [
            {
              "name": "self"
            }
          ],
          "return_type": null,
          "calls": [
            {
              "name": "len",
              "line": 88
            },
            {
              "name": "self.patterns.items",
              "line": 89
            }
          ],
          "docstring": "Get statistics about recognized patterns",
          "code_snippet": "            return {\"stored\": True, \"pattern_type\": pattern.pattern_type.value}\n\n        def get_semantic_stats(self):\n            \"\"\"Get statistics about recognized patterns\"\"\"\n            return {\n                \"total_patterns\": self.metrics[\"patterns_recognized\"],\n                \"patterns_by_type\": {\n                    pattern_type.value: len(patterns)\n                    for pattern_type, patterns in self.patterns.items()\n                }\n            }\n\n    class DomainPatternExtractor:"
        }
      },
      "class_variables": [],
      "bases": [],
      "docstring": "Registry for semantic patterns"
    },
    "DomainPatternExtractor": {
      "start_line": 93,
      "end_line": 99,
      "methods": {
        "extract_visual_semantics": {
          "start_line": 95,
          "end_line": 99,
          "parameters": [
            {
              "name": "self"
            },
            {
              "name": "image_data"
            }
          ],
          "return_type": null,
          "calls": [],
          "docstring": "Extract patterns from image data",
          "code_snippet": "    class DomainPatternExtractor:\n        \"\"\"Extracts patterns from different domains\"\"\"\n        def extract_visual_semantics(self, image_data):\n            \"\"\"Extract patterns from image data\"\"\"\n            pass\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)"
        }
      },
      "class_variables": [],
      "bases": [],
      "docstring": "Extracts patterns from different domains"
    }
  },
  "functions": {
    "numpy_safe_json_dump": {
      "start_line": 104,
      "end_line": 131,
      "parameters": [
        {
          "name": "obj"
        },
        {
          "name": "file"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "json.dump",
          "line": 129
        },
        {
          "name": "isinstance",
          "line": 115
        },
        {
          "name": "....default",
          "line": 127
        },
        {
          "name": "int",
          "line": 116
        },
        {
          "name": "isinstance",
          "line": 117
        },
        {
          "name": "float",
          "line": 118
        },
        {
          "name": "isinstance",
          "line": 119
        },
        {
          "name": "super",
          "line": 127
        },
        {
          "name": "obj.tolist",
          "line": 120
        },
        {
          "name": "hasattr",
          "line": 121
        },
        {
          "name": "np.issubdtype",
          "line": 121
        },
        {
          "name": "np.issubdtype",
          "line": 122
        },
        {
          "name": "float",
          "line": 122
        },
        {
          "name": "int",
          "line": 122
        },
        {
          "name": "hasattr",
          "line": 123
        },
        {
          "name": "hasattr",
          "line": 125
        },
        {
          "name": "obj.__class__.__name__.endswith",
          "line": 123
        },
        {
          "name": "hasattr",
          "line": 124
        },
        {
          "name": "str",
          "line": 124
        }
      ],
      "docstring": "\n    Safely dump Python objects with NumPy types to JSON.\n    \n    Args:\n        obj: Object to serialize\n        file: File-like object to write to\n        **kwargs: Additional arguments for json.dump\n    ",
      "code_snippet": "\n\ndef numpy_safe_json_dump(obj, file, **kwargs):\n    \"\"\"\n    Safely dump Python objects with NumPy types to JSON.\n    \n    Args:\n        obj: Object to serialize\n        file: File-like object to write to\n        **kwargs: Additional arguments for json.dump\n    \"\"\"\n    class NumpySafeEncoder(json.JSONEncoder):\n        def default(self, obj):\n            if isinstance(obj, np.integer):\n                return int(obj)\n            elif isinstance(obj, np.floating):\n                return float(obj)\n            elif isinstance(obj, np.ndarray):\n                return obj.tolist()\n            elif hasattr(obj, 'dtype') and np.issubdtype(obj.dtype, np.number):\n                return float(obj) if np.issubdtype(obj.dtype, np.floating) else int(obj)\n            elif hasattr(obj, '__module__') and ('enum' in obj.__module__ or obj.__class__.__name__.endswith('Enum')):\n                return obj.name if hasattr(obj, 'name') else str(obj)\n            elif hasattr(obj, 'value'):\n                return obj.value\n            return super().default(obj)\n\n    json.dump(obj, file, cls=NumpySafeEncoder, **kwargs)\n\ndef load_cifar10_data(augment=True):\n    \"\"\"\n    Load and prepare CIFAR-10 dataset."
    },
    "load_cifar10_data": {
      "start_line": 131,
      "end_line": 170,
      "parameters": [
        {
          "name": "augment"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "transforms.Compose",
          "line": 156
        },
        {
          "name": "torchvision.datasets.CIFAR10",
          "line": 162
        },
        {
          "name": "torchvision.datasets.CIFAR10",
          "line": 165
        },
        {
          "name": "transforms.Compose",
          "line": 143
        },
        {
          "name": "transforms.Compose",
          "line": 151
        },
        {
          "name": "transforms.ToTensor",
          "line": 157
        },
        {
          "name": "transforms.Normalize",
          "line": 158
        },
        {
          "name": "transforms.RandomCrop",
          "line": 144
        },
        {
          "name": "transforms.RandomHorizontalFlip",
          "line": 145
        },
        {
          "name": "transforms.ToTensor",
          "line": 146
        },
        {
          "name": "transforms.Normalize",
          "line": 147
        },
        {
          "name": "transforms.ToTensor",
          "line": 152
        },
        {
          "name": "transforms.Normalize",
          "line": 153
        }
      ],
      "docstring": "\n    Load and prepare CIFAR-10 dataset.\n    \n    Args:\n        augment: Whether to use data augmentation for training set\n        \n    Returns:\n        Train dataset, test dataset\n    ",
      "code_snippet": "    json.dump(obj, file, cls=NumpySafeEncoder, **kwargs)\n\ndef load_cifar10_data(augment=True):\n    \"\"\"\n    Load and prepare CIFAR-10 dataset.\n    \n    Args:\n        augment: Whether to use data augmentation for training set\n        \n    Returns:\n        Train dataset, test dataset\n    \"\"\"\n    # Define transforms\n    if augment:\n        transform_train = transforms.Compose([\n            transforms.RandomCrop(32, padding=4),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n        ])\n    else:\n        # For mapping, we might want to avoid augmentation\n        transform_train = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n        ])\n\n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n\n    # Load datasets\n    trainset = torchvision.datasets.CIFAR10(\n        root='./data', train=True, download=True, transform=transform_train)\n\n    testset = torchvision.datasets.CIFAR10(\n        root='./data', train=False, download=True, transform=transform_test)\n    \n    return trainset, testset\n\ndef calculate_pattern_complexity(pattern_features):\n    \"\"\"\n    Calculate complexity score from pattern features."
    },
    "calculate_pattern_complexity": {
      "start_line": 170,
      "end_line": 229,
      "parameters": [
        {
          "name": "pattern_features"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "max",
          "line": 182
        },
        {
          "name": "max",
          "line": 183
        },
        {
          "name": "max",
          "line": 184
        },
        {
          "name": "max",
          "line": 185
        },
        {
          "name": "min",
          "line": 227
        },
        {
          "name": "pattern_features.get",
          "line": 182
        },
        {
          "name": "pattern_features.get",
          "line": 183
        },
        {
          "name": "pattern_features.get",
          "line": 184
        },
        {
          "name": "pattern_features.get",
          "line": 185
        },
        {
          "name": "max",
          "line": 188
        },
        {
          "name": "len",
          "line": 192
        },
        {
          "name": "len",
          "line": 211
        },
        {
          "name": "np.mean",
          "line": 212
        },
        {
          "name": "np.mean",
          "line": 213
        },
        {
          "name": "min",
          "line": 214
        },
        {
          "name": "max",
          "line": 227
        },
        {
          "name": "pattern_features.items",
          "line": 188
        },
        {
          "name": "min",
          "line": 194
        },
        {
          "name": "max",
          "line": 194
        },
        {
          "name": "np.histogram",
          "line": 199
        },
        {
          "name": "isinstance",
          "line": 189
        },
        {
          "name": "len",
          "line": 201
        },
        {
          "name": "np.log2",
          "line": 202
        },
        {
          "name": "np.sum",
          "line": 202
        },
        {
          "name": "len",
          "line": 202
        },
        {
          "name": "np.log2",
          "line": 202
        }
      ],
      "docstring": "\n    Calculate complexity score from pattern features.\n    This is a domain-agnostic approach that works for any feature set.\n    \n    Args:\n        pattern_features: Dictionary of features from a pattern\n        \n    Returns:\n        Complexity score from 0.1 to 4.9\n    ",
      "code_snippet": "    return trainset, testset\n\ndef calculate_pattern_complexity(pattern_features):\n    \"\"\"\n    Calculate complexity score from pattern features.\n    This is a domain-agnostic approach that works for any feature set.\n    \n    Args:\n        pattern_features: Dictionary of features from a pattern\n        \n    Returns:\n        Complexity score from 0.1 to 4.9\n    \"\"\"\n    # Extract complexity-related features with safe defaults\n    edge_density = max(pattern_features.get('edge_density', 0.0), 1e-10)\n    texture_complexity = max(pattern_features.get('texture_complexity', 0.0), 1e-10)\n    contrast = max(pattern_features.get('contrast', 0.0), 1e-10)\n    directionality = max(pattern_features.get('directionality', 0.0), 1e-10)\n\n    # Get all numeric features for entropy calculation\n    numeric_features = [max(v, 1e-10) for k, v in pattern_features.items()\n                     if isinstance(v, (int, float)) and k != 'confidence']\n\n    # Calculate entropy if we have enough features\n    if len(numeric_features) >= 3:\n        # Normalize values to [0,1] range safely\n        min_val, max_val = min(numeric_features), max(numeric_features)\n        if max_val - min_val > 1e-10:\n            normalized = [(v - min_val) / (max_val - min_val) for v in numeric_features]\n\n            # Calculate entropy using histogram\n            hist, _ = np.histogram(normalized, bins=10, range=(0, 1), density=True)\n            hist = hist[hist > 0]  # Remove zeros\n            if len(hist) > 0:\n                entropy = -np.sum(hist * np.log2(hist + 1e-10)) / np.log2(len(hist) + 1e-10)\n            else:\n                entropy = 0.5\n        else:\n            entropy = 0.2  # Low entropy for constant values\n    else:\n        entropy = 0.5  # Default value\n\n    # Calculate feature variance safely\n    if len(numeric_features) > 1:\n        mean = np.mean(numeric_features)\n        variance = np.mean([(x - mean) ** 2 for x in numeric_features])\n        normalized_variance = min(1.0, variance * 10.0)\n    else:\n        normalized_variance = 0.5\n\n    # Combine metrics with appropriate weights\n    complexity = (\n        entropy * 0.4 +  # Information content\n        normalized_variance * 0.3 +  # Feature diversity\n        (edge_density + texture_complexity) * 0.15 +  # Structure\n        (contrast + directionality) * 0.15  # Visual properties\n    ) * 5.0  # Scale to 0-5 range\n\n    # Ensure range is between 0.1 and 4.9\n    return min(4.9, max(0.1, complexity))\n\ndef extract_visual_patterns_from_cifar(image, idx, device):\n    \"\"\"Extract semantic patterns from a CIFAR-10 image.\"\"\"\n    try:"
    },
    "extract_visual_patterns_from_cifar": {
      "start_line": 229,
      "end_line": 351,
      "parameters": [
        {
          "name": "image"
        },
        {
          "name": "idx"
        },
        {
          "name": "device"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "isinstance",
          "line": 233
        },
        {
          "name": "float",
          "line": 249
        },
        {
          "name": "float",
          "line": 250
        },
        {
          "name": "float",
          "line": 251
        },
        {
          "name": "float",
          "line": 252
        },
        {
          "name": "float",
          "line": 253
        },
        {
          "name": "float",
          "line": 254
        },
        {
          "name": "float",
          "line": 256
        },
        {
          "name": "max",
          "line": 257
        },
        {
          "name": "float",
          "line": 259
        },
        {
          "name": "max",
          "line": 260
        },
        {
          "name": "cv2.cvtColor",
          "line": 263
        },
        {
          "name": "cv2.Sobel",
          "line": 264
        },
        {
          "name": "cv2.Sobel",
          "line": 265
        },
        {
          "name": "cv2.cartToPolar",
          "line": 266
        },
        {
          "name": "max",
          "line": 268
        },
        {
          "name": "float",
          "line": 269
        },
        {
          "name": "np.sum",
          "line": 273
        },
        {
          "name": "max",
          "line": 280
        },
        {
          "name": "float",
          "line": 281
        },
        {
          "name": "cv2.Canny",
          "line": 284
        },
        {
          "name": "max",
          "line": 285
        },
        {
          "name": "float",
          "line": 286
        },
        {
          "name": "np.indices",
          "line": 289
        },
        {
          "name": "float",
          "line": 290
        },
        {
          "name": "float",
          "line": 299
        },
        {
          "name": "float",
          "line": 300
        },
        {
          "name": "max",
          "line": 303
        },
        {
          "name": "max",
          "line": 304
        },
        {
          "name": "max",
          "line": 305
        },
        {
          "name": "np.sqrt",
          "line": 308
        },
        {
          "name": "max",
          "line": 309
        },
        {
          "name": "sum",
          "line": 323
        },
        {
          "name": "SemanticPattern",
          "line": 330
        },
        {
          "name": "torch.clamp",
          "line": 237
        },
        {
          "name": "....numpy",
          "line": 238
        },
        {
          "name": "np.array",
          "line": 240
        },
        {
          "name": "np.mean",
          "line": 246
        },
        {
          "name": "np.std",
          "line": 247
        },
        {
          "name": "np.mean",
          "line": 256
        },
        {
          "name": "np.mean",
          "line": 259
        },
        {
          "name": "np.uint8",
          "line": 263
        },
        {
          "name": "np.histogram",
          "line": 272
        },
        {
          "name": "np.std",
          "line": 280
        },
        {
          "name": "np.sum",
          "line": 290
        },
        {
          "name": "max",
          "line": 321
        },
        {
          "name": "pattern_scores.values",
          "line": 323
        },
        {
          "name": "min",
          "line": 328
        },
        {
          "name": "logger.error",
          "line": 342
        },
        {
          "name": "SemanticPattern",
          "line": 343
        },
        {
          "name": "torch.device",
          "line": 234
        },
        {
          "name": "image.cpu",
          "line": 235
        },
        {
          "name": "....view",
          "line": 236
        },
        {
          "name": "np.mean",
          "line": 268
        },
        {
          "name": "np.ones_like",
          "line": 278
        },
        {
          "name": "len",
          "line": 278
        },
        {
          "name": "np.sum",
          "line": 285
        },
        {
          "name": "np.sum",
          "line": 293
        },
        {
          "name": "np.sum",
          "line": 294
        },
        {
          "name": "pattern_scores.items",
          "line": 321
        },
        {
          "name": "str",
          "line": 336
        },
        {
          "name": "....view",
          "line": 236
        },
        {
          "name": "image.permute",
          "line": 238
        },
        {
          "name": "str",
          "line": 349
        },
        {
          "name": "torch.tensor",
          "line": 236
        },
        {
          "name": "str",
          "line": 342
        },
        {
          "name": "str",
          "line": 348
        },
        {
          "name": "torch.tensor",
          "line": 236
        }
      ],
      "docstring": "Extract semantic patterns from a CIFAR-10 image.",
      "code_snippet": "    return min(4.9, max(0.1, complexity))\n\ndef extract_visual_patterns_from_cifar(image, idx, device):\n    \"\"\"Extract semantic patterns from a CIFAR-10 image.\"\"\"\n    try:\n        # Convert image to numpy for processing\n        if isinstance(image, torch.Tensor):\n            if image.device != torch.device('cpu'):\n                image = image.cpu()\n            image = image * torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1) + torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n            image = torch.clamp(image, 0, 1)\n            image_np = image.permute(1, 2, 0).numpy()\n        else:\n            image_np = np.array(image)\n\n        # Extract basic features with safety checks\n        features = {}\n\n        # Color features (safe division)\n        mean_color = np.mean(image_np, axis=(0, 1)) + 1e-10\n        std_color = np.std(image_np, axis=(0, 1)) + 1e-10\n\n        features['color_mean_r'] = float(mean_color[0])\n        features['color_mean_g'] = float(mean_color[1])\n        features['color_mean_b'] = float(mean_color[2])\n        features['color_std_r'] = float(std_color[0])\n        features['color_std_g'] = float(std_color[1])\n        features['color_std_b'] = float(std_color[2])\n\n        intensity = float(np.mean(mean_color))\n        features['intensity'] = max(intensity, 1e-10)\n\n        contrast = float(np.mean(std_color))\n        features['contrast'] = max(contrast, 1e-10)\n\n        # Texture features (safe)\n        gray = cv2.cvtColor(np.uint8(image_np * 255), cv2.COLOR_RGB2GRAY)\n        gx = cv2.Sobel(gray, cv2.CV_32F, 1, 0, ksize=3)\n        gy = cv2.Sobel(gray, cv2.CV_32F, 0, 1, ksize=3)\n        mag, angle = cv2.cartToPolar(gx, gy)\n\n        texture_complexity = max(np.mean(mag) / 255.0, 1e-10)\n        features['texture_complexity'] = float(texture_complexity)\n\n        # Safe directional histogram calculation\n        direction_hist = np.histogram(angle, bins=8, range=(0, 2*np.pi))[0]\n        sum_hist = np.sum(direction_hist)\n        if sum_hist > 1e-10:  # Safe minimum threshold\n            direction_hist = direction_hist / sum_hist\n        else:\n            # If sum is too small, use uniform distribution\n            direction_hist = np.ones_like(direction_hist) / len(direction_hist)\n\n        directionality = max(np.std(direction_hist), 1e-10)  # Ensure non-zero directionality\n        features['directionality'] = float(directionality)\n\n        # Structure features (safe edge density)\n        edges = cv2.Canny(gray, 100, 200)\n        edge_density = max(np.sum(edges > 0) / (32.0 * 32.0), 1e-10)\n        features['edge_density'] = float(edge_density)\n\n        # Safe center of mass calculation\n        y_indices, x_indices = np.indices(gray.shape)\n        gray_sum = float(np.sum(gray))\n\n        if gray_sum > 1e-10:  # Safe minimum threshold\n            center_x = np.sum(x_indices * gray) / (gray_sum * gray.shape[1])\n            center_y = np.sum(y_indices * gray) / (gray_sum * gray.shape[0])\n        else:\n            # Default to center if sum is too small\n            center_x, center_y = 0.5, 0.5\n\n        features['center_of_mass_x'] = float(center_x)\n        features['center_of_mass_y'] = float(center_y)\n\n        # Calculate pattern scores (safe)\n        structure_score = max(edge_density * 2.0, 1e-10)\n        relationship_score = max(texture_complexity * 3.0, 1e-10)\n        intensity_score = max(contrast * 2.0, 1e-10)\n\n        # Calculate centrality score (safe)\n        dist = np.sqrt((center_x - 0.5)**2 + (center_y - 0.5)**2)\n        centrality = max(1.0 - (dist / 0.7071), 1e-10)\n        dominance_score = centrality * 1.5\n        temporal_score = 0.1\n\n        pattern_scores = {\n            SemanticType.STRUCTURE: structure_score,\n            SemanticType.RELATIONSHIP: relationship_score,\n            SemanticType.INTENSITY: intensity_score,\n            SemanticType.DOMINANCE: dominance_score,\n            SemanticType.TEMPORAL: temporal_score\n        }\n\n        pattern_type = max(pattern_scores.items(), key=lambda x: x[1])[0]\n        winning_score = pattern_scores[pattern_type]\n        total_score = sum(pattern_scores.values())\n\n        # Calculate confidence (safe division)\n        confidence = 0.7  # default confidence\n        if total_score > 1e-10:  # Safe minimum threshold\n            confidence = min(0.95, winning_score / (total_score * 0.5))\n\n        pattern = SemanticPattern(\n            pattern_type=pattern_type,\n            features=features,\n            confidence=confidence,\n            source_domain=\"visual\",\n            context={\"image_idx\": idx},\n            cortex_flow_id=str(idx)\n        )\n\n        return pattern\n\n    except Exception as e:\n        logger.error(f\"Error processing image {idx}: {str(e)}\")\n        return SemanticPattern(\n            pattern_type=SemanticType.STRUCTURE,\n            features={'error': 1.0},\n            confidence=0.1,\n            source_domain=\"visual\",\n            context={\"image_idx\": idx, \"error\": str(e)},\n            cortex_flow_id=str(idx)\n        )\n\n\ndef create_pattern_map(dataset, device, sample_limit=None):"
    },
    "create_pattern_map": {
      "start_line": 353,
      "end_line": 466,
      "parameters": [
        {
          "name": "dataset"
        },
        {
          "name": "device"
        },
        {
          "name": "sample_limit"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "logger.info",
          "line": 365
        },
        {
          "name": "SemanticPatternRegistry",
          "line": 368
        },
        {
          "name": "logger.info",
          "line": 386
        },
        {
          "name": "time.time",
          "line": 392
        },
        {
          "name": "range",
          "line": 396
        },
        {
          "name": "logger.info",
          "line": 436
        },
        {
          "name": "calculate_pattern_complexities",
          "line": 439
        },
        {
          "name": "order_patterns_by_complexity",
          "line": 440
        },
        {
          "name": "logger.info",
          "line": 443
        },
        {
          "name": "pattern_counts.items",
          "line": 444
        },
        {
          "name": "logger.info",
          "line": 449
        },
        {
          "name": "len",
          "line": 379
        },
        {
          "name": "random.sample",
          "line": 381
        },
        {
          "name": "list",
          "line": 383
        },
        {
          "name": "len",
          "line": 396
        },
        {
          "name": "logger.info",
          "line": 433
        },
        {
          "name": "time.time",
          "line": 435
        },
        {
          "name": "logger.info",
          "line": 446
        },
        {
          "name": "len",
          "line": 372
        },
        {
          "name": "logger.info",
          "line": 374
        },
        {
          "name": "len",
          "line": 376
        },
        {
          "name": "range",
          "line": 381
        },
        {
          "name": "range",
          "line": 383
        },
        {
          "name": "extract_visual_patterns_from_cifar",
          "line": 404
        },
        {
          "name": "pattern_registry.recognize_pattern",
          "line": 407
        },
        {
          "name": "calculate_pattern_complexity",
          "line": 411
        },
        {
          "name": "....append",
          "line": 425
        },
        {
          "name": "....strftime",
          "line": 457
        },
        {
          "name": "len",
          "line": 458
        },
        {
          "name": "len",
          "line": 381
        },
        {
          "name": "len",
          "line": 383
        },
        {
          "name": "len",
          "line": 386
        },
        {
          "name": "str",
          "line": 414
        },
        {
          "name": "len",
          "line": 445
        },
        {
          "name": "min",
          "line": 433
        },
        {
          "name": "len",
          "line": 433
        },
        {
          "name": "datetime.now",
          "line": 457
        },
        {
          "name": "len",
          "line": 433
        }
      ],
      "docstring": "\n    Create a pattern map from a dataset.\n    \n    Args:\n        dataset: PyTorch dataset\n        device: Computation device\n        sample_limit: Maximum number of samples to analyze\n        \n    Returns:\n        Pattern map\n    ",
      "code_snippet": "\n\ndef create_pattern_map(dataset, device, sample_limit=None):\n    \"\"\"\n    Create a pattern map from a dataset.\n    \n    Args:\n        dataset: PyTorch dataset\n        device: Computation device\n        sample_limit: Maximum number of samples to analyze\n        \n    Returns:\n        Pattern map\n    \"\"\"\n    logger.info(\"Starting semantic pattern analysis...\")\n    \n    # Initialize pattern registry\n    pattern_registry = SemanticPatternRegistry()\n    \n    # Determine number of examples to process    \n    if sample_limit is None:\n        if len(dataset) > 10000:\n            sample_limit = 5000\n            logger.info(f\"Large dataset detected. Using sample limit of {sample_limit} for pattern analysis.\")\n        else:\n            sample_limit = len(dataset)\n    \n    # Get sample indices\n    if sample_limit < len(dataset):\n        # Random sampling to get representative examples\n        samples = random.sample(range(len(dataset)), sample_limit)\n    else:\n        samples = list(range(len(dataset)))\n    \n    # Process samples\n    logger.info(f\"Analyzing patterns in {len(samples)} examples...\")\n    \n    pattern_map = {}\n    patterns_by_type = {}\n    pattern_counts = {pattern_type.value: 0 for pattern_type in SemanticType}\n    \n    start_time = time.time()\n    \n    # Process in smaller batches to show progress\n    batch_size = 500\n    for i in range(0, len(samples), batch_size):\n        batch_indices = samples[i:i+batch_size]\n        \n        for idx in batch_indices:\n            # Get the image\n            image, _ = dataset[idx]\n            \n            # Extract pattern\n            pattern = extract_visual_patterns_from_cifar(image, idx, device)\n            \n            # Store pattern\n            pattern_registry.recognize_pattern(pattern)\n            pattern_counts[pattern.pattern_type.value] += 1\n            \n            # Calculate complexity\n            complexity = calculate_pattern_complexity(pattern.features)\n            \n            # Store in pattern map\n            pattern_map[str(idx)] = {\n                'pattern_type': pattern.pattern_type.value,\n                'features': pattern.features,\n                'confidence': pattern.confidence,\n                'complexity': complexity\n            }\n            \n            # Group by pattern type\n            if pattern.pattern_type.value not in patterns_by_type:\n                patterns_by_type[pattern.pattern_type.value] = []\n                \n            patterns_by_type[pattern.pattern_type.value].append({\n                'idx': idx,\n                'features': pattern.features,\n                'confidence': pattern.confidence,\n                'complexity': complexity\n            })\n        \n        # Log progress\n        logger.info(f\"Processed {min(i+batch_size, len(samples))}/{len(samples)} examples...\")\n    \n    elapsed_time = time.time() - start_time\n    logger.info(f\"Pattern analysis completed in {elapsed_time:.2f} seconds\")\n    \n    # Calculate pattern complexities\n    pattern_complexities = calculate_pattern_complexities(patterns_by_type)\n    patterns_by_complexity = order_patterns_by_complexity(pattern_complexities)\n    \n    # Calculate pattern distribution\n    logger.info(\"Pattern distribution:\")\n    for pattern_type, count in pattern_counts.items():\n        percentage = (count / len(samples)) * 100\n        logger.info(f\"  {pattern_type}: {count} examples ({percentage:.1f}%)\")\n    \n    # Compile results\n    logger.info(\"Compiling pattern map...\")\n    result = {\n        'pattern_map': pattern_map,\n        'patterns_by_type': patterns_by_type,\n        'pattern_distribution': pattern_counts,\n        'pattern_complexities': pattern_complexities,\n        'patterns_by_complexity': patterns_by_complexity,\n        'metadata': {\n            'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n            'samples_analyzed': len(samples),\n            'analysis_time': elapsed_time,\n            'version': '2.0-streamlined'\n        }\n    }\n    \n    return result\n\ndef calculate_pattern_complexities(patterns_by_type):\n    \"\"\"Calculate complexity metrics for each pattern type.\"\"\"\n    logger.info(\"Calculating pattern complexities...\")"
    },
    "calculate_pattern_complexities": {
      "start_line": 466,
      "end_line": 531,
      "parameters": [
        {
          "name": "patterns_by_type"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "logger.info",
          "line": 468
        },
        {
          "name": "patterns_by_type.items",
          "line": 472
        },
        {
          "name": "complexity_values.append",
          "line": 482
        },
        {
          "name": "features.items",
          "line": 484
        },
        {
          "name": "len",
          "line": 501
        },
        {
          "name": "min",
          "line": 503
        },
        {
          "name": "max",
          "line": 504
        },
        {
          "name": "len",
          "line": 515
        },
        {
          "name": "logger.error",
          "line": 519
        },
        {
          "name": "sum",
          "line": 502
        },
        {
          "name": "len",
          "line": 502
        },
        {
          "name": "str",
          "line": 519
        }
      ],
      "docstring": "Calculate complexity metrics for each pattern type.",
      "code_snippet": "    return result\n\ndef calculate_pattern_complexities(patterns_by_type):\n    \"\"\"Calculate complexity metrics for each pattern type.\"\"\"\n    logger.info(\"Calculating pattern complexities...\")\n    \n    complexities = {}\n    \n    for pattern_type, patterns in patterns_by_type.items():\n        try:\n            # Calculate average feature values (safe)\n            feature_sums = {}\n            feature_counts = {}\n            complexity_values = []\n            \n            for pattern in patterns:\n                features = pattern['features']\n                complexity = pattern['complexity']\n                complexity_values.append(complexity)\n                \n                for feature, value in features.items():\n                    if feature not in feature_sums:\n                        feature_sums[feature] = 0.0\n                        feature_counts[feature] = 0\n                    \n                    feature_sums[feature] += value\n                    feature_counts[feature] += 1\n            \n            # Calculate averages (safe)\n            feature_avgs = {}\n            for feature in feature_sums:\n                if feature_counts[feature] > 0:\n                    feature_avgs[feature] = feature_sums[feature] / feature_counts[feature]\n                else:\n                    feature_avgs[feature] = 0.1  # Default for empty features\n            \n            # Calculate complexity statistics (safe)\n            if len(complexity_values) > 0:\n                avg_complexity = sum(complexity_values) / len(complexity_values)\n                min_complexity = min(complexity_values)\n                max_complexity = max(complexity_values)\n            else:\n                avg_complexity = 2.5\n                min_complexity = 0.1\n                max_complexity = 4.9\n            \n            complexities[pattern_type] = {\n                'avg_features': feature_avgs,\n                'avg_complexity': avg_complexity,\n                'min_complexity': min_complexity,\n                'max_complexity': max_complexity,\n                'pattern_count': len(patterns)\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error calculating complexity for pattern type {pattern_type}: {str(e)}\")\n            # Provide safe default values\n            complexities[pattern_type] = {\n                'avg_features': {},\n                'avg_complexity': 2.5,\n                'min_complexity': 0.1,\n                'max_complexity': 4.9,\n                'pattern_count': 0\n            }\n    \n    return complexities\n\ndef order_patterns_by_complexity(pattern_complexities):\n    \"\"\"Order patterns by their complexity for risk-accuracy training.\"\"\"\n    # Sort patterns by average complexity"
    },
    "order_patterns_by_complexity": {
      "start_line": 531,
      "end_line": 553,
      "parameters": [
        {
          "name": "pattern_complexities"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "sorted",
          "line": 534
        },
        {
          "name": "max",
          "line": 541
        },
        {
          "name": "len",
          "line": 541
        },
        {
          "name": "pattern_complexities.items",
          "line": 535
        }
      ],
      "docstring": "Order patterns by their complexity for risk-accuracy training.",
      "code_snippet": "    return complexities\n\ndef order_patterns_by_complexity(pattern_complexities):\n    \"\"\"Order patterns by their complexity for risk-accuracy training.\"\"\"\n    # Sort patterns by average complexity\n    sorted_patterns = sorted(\n        [(p, data['avg_complexity']) for p, data in pattern_complexities.items()],\n        key=lambda x: x[1]\n    )\n    \n    # Group patterns into low, medium, and high complexity\n    pattern_types = [p for p, _ in sorted_patterns]\n    third = max(1, len(pattern_types) // 3)\n    \n    low_complexity = pattern_types[:third]\n    medium_complexity = pattern_types[third:2*third]\n    high_complexity = pattern_types[2*third:]\n    \n    return {\n        'ordered_by_complexity': [p for p, _ in sorted_patterns],\n        'low_complexity': low_complexity,\n        'medium_complexity': medium_complexity,\n        'high_complexity': high_complexity\n    }\n\ndef visualize_pattern_map(pattern_map, output_path):\n    \"\"\"Create visualizations of the pattern map.\"\"\""
    },
    "visualize_pattern_map": {
      "start_line": 554,
      "end_line": 680,
      "parameters": [
        {
          "name": "pattern_map"
        },
        {
          "name": "output_path"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "os.makedirs",
          "line": 556
        },
        {
          "name": "max",
          "line": 560
        },
        {
          "name": "plt.figure",
          "line": 563
        },
        {
          "name": "sorted",
          "line": 564
        },
        {
          "name": "plt.bar",
          "line": 567
        },
        {
          "name": "plt.xlabel",
          "line": 574
        },
        {
          "name": "plt.ylabel",
          "line": 575
        },
        {
          "name": "plt.title",
          "line": 576
        },
        {
          "name": "plt.grid",
          "line": 577
        },
        {
          "name": "plt.xticks",
          "line": 578
        },
        {
          "name": "os.path.join",
          "line": 580
        },
        {
          "name": "plt.savefig",
          "line": 581
        },
        {
          "name": "plt.close",
          "line": 582
        },
        {
          "name": "logger.info",
          "line": 583
        },
        {
          "name": "os.path.join",
          "line": 637
        },
        {
          "name": "logger.info",
          "line": 678
        },
        {
          "name": "sum",
          "line": 560
        },
        {
          "name": "pattern_distribution.keys",
          "line": 564
        },
        {
          "name": "bar.get_height",
          "line": 570
        },
        {
          "name": "plt.text",
          "line": 571
        },
        {
          "name": "plt.figure",
          "line": 587
        },
        {
          "name": "sorted",
          "line": 589
        },
        {
          "name": "plt.figure",
          "line": 612
        },
        {
          "name": "sorted",
          "line": 613
        },
        {
          "name": "open",
          "line": 638
        },
        {
          "name": "f.write",
          "line": 639
        },
        {
          "name": "f.write",
          "line": 640
        },
        {
          "name": "f.write",
          "line": 642
        },
        {
          "name": "f.write",
          "line": 643
        },
        {
          "name": "f.write",
          "line": 648
        },
        {
          "name": "f.write",
          "line": 650
        },
        {
          "name": "f.write",
          "line": 651
        },
        {
          "name": "pattern_distribution.items",
          "line": 652
        },
        {
          "name": "f.write",
          "line": 655
        },
        {
          "name": "pattern_distribution.values",
          "line": 560
        },
        {
          "name": "pattern_complexities.keys",
          "line": 589
        },
        {
          "name": "plt.barh",
          "line": 593
        },
        {
          "name": "plt.xlabel",
          "line": 600
        },
        {
          "name": "plt.ylabel",
          "line": 601
        },
        {
          "name": "plt.title",
          "line": 602
        },
        {
          "name": "plt.grid",
          "line": 603
        },
        {
          "name": "os.path.join",
          "line": 605
        },
        {
          "name": "plt.savefig",
          "line": 606
        },
        {
          "name": "plt.close",
          "line": 607
        },
        {
          "name": "logger.info",
          "line": 608
        },
        {
          "name": "pattern_complexities.keys",
          "line": 613
        },
        {
          "name": "plt.barh",
          "line": 620
        },
        {
          "name": "enumerate",
          "line": 623
        },
        {
          "name": "plt.xlabel",
          "line": 626
        },
        {
          "name": "plt.ylabel",
          "line": 627
        },
        {
          "name": "plt.title",
          "line": 628
        },
        {
          "name": "plt.grid",
          "line": 629
        },
        {
          "name": "os.path.join",
          "line": 631
        },
        {
          "name": "plt.savefig",
          "line": 632
        },
        {
          "name": "plt.close",
          "line": 633
        },
        {
          "name": "logger.info",
          "line": 634
        },
        {
          "name": "metadata.items",
          "line": 646
        },
        {
          "name": "f.write",
          "line": 654
        },
        {
          "name": "f.write",
          "line": 658
        },
        {
          "name": "f.write",
          "line": 659
        },
        {
          "name": "....items",
          "line": 660
        },
        {
          "name": "f.write",
          "line": 668
        },
        {
          "name": "f.write",
          "line": 669
        },
        {
          "name": "f.write",
          "line": 672
        },
        {
          "name": "f.write",
          "line": 673
        },
        {
          "name": "f.write",
          "line": 674
        },
        {
          "name": "f.write",
          "line": 675
        },
        {
          "name": "f.write",
          "line": 676
        },
        {
          "name": "bar.get_x",
          "line": 571
        },
        {
          "name": "bar.get_width",
          "line": 596
        },
        {
          "name": "plt.text",
          "line": 597
        },
        {
          "name": "plt.plot",
          "line": 624
        },
        {
          "name": "f.write",
          "line": 647
        },
        {
          "name": "f.write",
          "line": 661
        },
        {
          "name": "f.write",
          "line": 662
        },
        {
          "name": "f.write",
          "line": 663
        },
        {
          "name": "f.write",
          "line": 664
        },
        {
          "name": "f.write",
          "line": 665
        },
        {
          "name": "bar.get_width",
          "line": 571
        },
        {
          "name": "int",
          "line": 572
        },
        {
          "name": "bar.get_y",
          "line": 597
        },
        {
          "name": "range",
          "line": 620
        },
        {
          "name": "....join",
          "line": 672
        },
        {
          "name": "....join",
          "line": 673
        },
        {
          "name": "....join",
          "line": 674
        },
        {
          "name": "....join",
          "line": 676
        },
        {
          "name": "bar.get_height",
          "line": 597
        },
        {
          "name": "len",
          "line": 620
        }
      ],
      "docstring": "Create visualizations of the pattern map.",
      "code_snippet": "    }\n\ndef visualize_pattern_map(pattern_map, output_path):\n    \"\"\"Create visualizations of the pattern map.\"\"\"\n    os.makedirs(output_path, exist_ok=True)\n    \n    # Extract pattern distribution data\n    pattern_distribution = pattern_map['pattern_distribution']\n    total_patterns = max(sum(pattern_distribution.values()), 1)  # Safe division\n    \n    # Create pattern distribution visualization\n    plt.figure(figsize=(12, 6))\n    pattern_types = sorted(pattern_distribution.keys())\n    counts = [pattern_distribution[pt] for pt in pattern_types]\n    \n    bars = plt.bar(pattern_types, counts, color='lightgreen')\n    \n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n                f'{int(height)}', ha='center', va='bottom')\n    \n    plt.xlabel('Pattern Type')\n    plt.ylabel('Number of Examples')\n    plt.title('Pattern Distribution in Dataset')\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.xticks(rotation=30, ha='right')\n    \n    distribution_path = os.path.join(output_path, 'pattern_distribution.png')\n    plt.savefig(distribution_path)\n    plt.close()\n    logger.info(f\"Saved pattern distribution visualization to {distribution_path}\")\n    \n    # Create pattern complexity visualization\n    if 'pattern_complexities' in pattern_map:\n        plt.figure(figsize=(12, 6))\n        pattern_complexities = pattern_map['pattern_complexities']\n        pattern_types = sorted(pattern_complexities.keys())\n        \n        if pattern_types:\n            avg_complexities = [pattern_complexities[pt]['avg_complexity'] for pt in pattern_types]\n            bars = plt.barh(pattern_types, avg_complexities, color='salmon')\n            \n            for bar in bars:\n                width = bar.get_width()\n                plt.text(width + 0.1, bar.get_y() + bar.get_height()/2.,\n                        f'{width:.2f}', va='center')\n            \n            plt.xlabel('Average Complexity')\n            plt.ylabel('Pattern Type')\n            plt.title('Average Complexity by Pattern Type')\n            plt.grid(axis='x', linestyle='--', alpha=0.7)\n            \n            complexity_path = os.path.join(output_path, 'pattern_complexity.png')\n            plt.savefig(complexity_path)\n            plt.close()\n            logger.info(f\"Saved pattern complexity visualization to {complexity_path}\")\n    \n    # Create complexity range visualization\n    if 'pattern_complexities' in pattern_map:\n        plt.figure(figsize=(12, 6))\n        pattern_types = sorted(pattern_complexities.keys())\n        \n        if pattern_types:\n            mins = [pattern_complexities[pt]['min_complexity'] for pt in pattern_types]\n            maxs = [pattern_complexities[pt]['max_complexity'] for pt in pattern_types]\n            avgs = [pattern_complexities[pt]['avg_complexity'] for pt in pattern_types]\n            \n            plt.barh(pattern_types, [maxs[i] - mins[i] for i in range(len(pattern_types))], \n                    left=mins, alpha=0.3, color='lightblue')\n            \n            for i, pt in enumerate(pattern_types):\n                plt.plot([avgs[i], avgs[i]], [i - 0.25, i + 0.25], 'ro-', linewidth=2)\n            \n            plt.xlabel('Complexity Range')\n            plt.ylabel('Pattern Type')\n            plt.title('Complexity Ranges by Pattern Type')\n            plt.grid(axis='x', linestyle='--', alpha=0.7)\n            \n            range_path = os.path.join(output_path, 'pattern_complexity_range.png')\n            plt.savefig(range_path)\n            plt.close()\n            logger.info(f\"Saved pattern complexity range visualization to {range_path}\")\n    \n    # Create summary text file\n    summary_path = os.path.join(output_path, 'pattern_map_summary.txt')\n    with open(summary_path, 'w') as f:\n        f.write(\"SEMANTIC PATTERN MAP SUMMARY\\n\")\n        f.write(\"===========================\\n\\n\")\n        \n        f.write(\"METADATA\\n\")\n        f.write(\"--------\\n\")\n        if 'metadata' in pattern_map:\n            metadata = pattern_map['metadata']\n            for key, value in metadata.items():\n                f.write(f\"{key}: {value}\\n\")\n        f.write(\"\\n\")\n        \n        f.write(\"PATTERN DISTRIBUTION\\n\")\n        f.write(\"-------------------\\n\")\n        for pattern_type, count in pattern_distribution.items():\n            percentage = (count / total_patterns) * 100\n            f.write(f\"{pattern_type}: {count} examples ({percentage:.1f}%)\\n\")\n        f.write(\"\\n\")\n        \n        if 'pattern_complexities' in pattern_map:\n            f.write(\"PATTERN COMPLEXITY STATISTICS\\n\")\n            f.write(\"----------------------------\\n\")\n            for pattern_type, stats in pattern_map['pattern_complexities'].items():\n                f.write(f\"{pattern_type}:\\n\")\n                f.write(f\"  Count: {stats['pattern_count']}\\n\")\n                f.write(f\"  Average complexity: {stats['avg_complexity']:.2f}\\n\")\n                f.write(f\"  Complexity range: {stats['min_complexity']:.2f} - {stats['max_complexity']:.2f}\\n\")\n                f.write(\"\\n\")\n        \n        if 'patterns_by_complexity' in pattern_map:\n            f.write(\"PATTERN COMPLEXITY GROUPING\\n\")\n            f.write(\"--------------------------\\n\")\n            complexity_groups = pattern_map['patterns_by_complexity']\n            \n            f.write(f\"Low complexity patterns: {', '.join(complexity_groups['low_complexity'])}\\n\")\n            f.write(f\"Medium complexity patterns: {', '.join(complexity_groups['medium_complexity'])}\\n\")\n            f.write(f\"High complexity patterns: {', '.join(complexity_groups['high_complexity'])}\\n\")\n            f.write(\"\\n\")\n            f.write(f\"Ordered by complexity (low to high): {', '.join(complexity_groups['ordered_by_complexity'])}\\n\")\n    \n    logger.info(f\"Saved pattern map summary to {summary_path}\")\n\ndef save_pattern_map(pattern_map, filepath):\n    \"\"\"Save pattern map to file.\"\"\"\n    with open(filepath, 'w') as f:"
    },
    "save_pattern_map": {
      "start_line": 680,
      "end_line": 688,
      "parameters": [
        {
          "name": "pattern_map"
        },
        {
          "name": "filepath"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "logger.info",
          "line": 685
        },
        {
          "name": "open",
          "line": 682
        },
        {
          "name": "numpy_safe_json_dump",
          "line": 683
        }
      ],
      "docstring": "Save pattern map to file.",
      "code_snippet": "    logger.info(f\"Saved pattern map summary to {summary_path}\")\n\ndef save_pattern_map(pattern_map, filepath):\n    \"\"\"Save pattern map to file.\"\"\"\n    with open(filepath, 'w') as f:\n        numpy_safe_json_dump(pattern_map, f, indent=2)\n    \n    logger.info(f\"Pattern map saved to {filepath}\")\n    return pattern_map\n\ndef create_streamlined_pattern_map(dataset, device, sample_limit=None, use_enhanced_mapper=False):\n    \"\"\"Create a streamlined pattern map using either the base or enhanced mapper.\"\"\"\n    if use_enhanced_mapper:"
    },
    "create_streamlined_pattern_map": {
      "start_line": 688,
      "end_line": 749,
      "parameters": [
        {
          "name": "dataset"
        },
        {
          "name": "device"
        },
        {
          "name": "sample_limit"
        },
        {
          "name": "use_enhanced_mapper"
        }
      ],
      "return_type": null,
      "calls": [
        {
          "name": "logger.info",
          "line": 691
        },
        {
          "name": "EnhancedPatternMapper",
          "line": 693
        },
        {
          "name": "mapper.extract_pattern_information",
          "line": 700
        },
        {
          "name": "....items",
          "line": 717
        },
        {
          "name": "calculate_pattern_complexities",
          "line": 742
        },
        {
          "name": "order_patterns_by_complexity",
          "line": 743
        },
        {
          "name": "create_pattern_map",
          "line": 747
        },
        {
          "name": "pattern_info.get",
          "line": 705
        },
        {
          "name": "pattern_data.get",
          "line": 718
        },
        {
          "name": "pattern_data.get",
          "line": 719
        },
        {
          "name": "pattern_data.get",
          "line": 720
        },
        {
          "name": "calculate_pattern_complexity",
          "line": 722
        },
        {
          "name": "....append",
          "line": 734
        },
        {
          "name": "....strftime",
          "line": 708
        },
        {
          "name": "len",
          "line": 709
        },
        {
          "name": "pattern_info.get",
          "line": 710
        },
        {
          "name": "pattern_info.get",
          "line": 717
        },
        {
          "name": "str",
          "line": 724
        },
        {
          "name": "pattern_info.get",
          "line": 709
        },
        {
          "name": "datetime.now",
          "line": 708
        }
      ],
      "docstring": "Create a streamlined pattern map using either the base or enhanced mapper.",
      "code_snippet": "    return pattern_map\n\ndef create_streamlined_pattern_map(dataset, device, sample_limit=None, use_enhanced_mapper=False):\n    \"\"\"Create a streamlined pattern map using either the base or enhanced mapper.\"\"\"\n    if use_enhanced_mapper:\n        logger.info(\"Using enhanced pattern mapper with specialist integration\")\n        \n        mapper = EnhancedPatternMapper(\n            proxy_model_types=['forest', 'logistic', 'svm'],\n            ensemble_size=None,\n            batch_size=None,\n            device=device\n        )\n        \n        pattern_info = mapper.extract_pattern_information(dataset, sample_limit)\n        \n        pattern_map = {\n            'pattern_map': {},\n            'patterns_by_type': {},\n            'pattern_distribution': pattern_info.get('pattern_distribution', {}),\n            'pattern_complexities': {},\n            'metadata': {\n                'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n                'samples_analyzed': len(pattern_info.get('analyzed_samples', [])),\n                'analysis_time': pattern_info.get('analysis_time', 0),\n                'version': '2.0-streamlined-enhanced'\n            }\n        }\n        \n        patterns_by_type = {}\n        \n        for idx, pattern_data in pattern_info.get('pattern_details', {}).items():\n            pattern_type = pattern_data.get('pattern_type')\n            features = pattern_data.get('features', {})\n            confidence = pattern_data.get('confidence', 0.8)\n            \n            complexity = calculate_pattern_complexity(features)\n            \n            pattern_map['pattern_map'][str(idx)] = {\n                'pattern_type': pattern_type,\n                'features': features,\n                'confidence': confidence,\n                'complexity': complexity\n            }\n            \n            if pattern_type not in patterns_by_type:\n                patterns_by_type[pattern_type] = []\n                \n            patterns_by_type[pattern_type].append({\n                'idx': idx,\n                'features': features,\n                'confidence': confidence,\n                'complexity': complexity\n            })\n        \n        pattern_map['patterns_by_type'] = patterns_by_type\n        pattern_map['pattern_complexities'] = calculate_pattern_complexities(patterns_by_type)\n        pattern_map['patterns_by_complexity'] = order_patterns_by_complexity(pattern_map['pattern_complexities'])\n        \n        return pattern_map\n    else:\n        return create_pattern_map(dataset, device, sample_limit=sample_limit)\n\ndef main():\n    \"\"\"Run the streamlined pattern mapping.\"\"\"\n    overall_start_time = timer()"
    },
    "main": {
      "start_line": 749,
      "end_line": 815,
      "parameters": [],
      "return_type": null,
      "calls": [
        {
          "name": "timer",
          "line": 751
        },
        {
          "name": "argparse.ArgumentParser",
          "line": 753
        },
        {
          "name": "parser.add_argument",
          "line": 754
        },
        {
          "name": "parser.add_argument",
          "line": 756
        },
        {
          "name": "parser.add_argument",
          "line": 758
        },
        {
          "name": "parser.add_argument",
          "line": 760
        },
        {
          "name": "parser.parse_args",
          "line": 762
        },
        {
          "name": "torch.device",
          "line": 767
        },
        {
          "name": "logger.info",
          "line": 768
        },
        {
          "name": "....strftime",
          "line": 770
        },
        {
          "name": "os.path.join",
          "line": 773
        },
        {
          "name": "os.makedirs",
          "line": 774
        },
        {
          "name": "logger.info",
          "line": 776
        },
        {
          "name": "load_cifar10_data",
          "line": 777
        },
        {
          "name": "logger.info",
          "line": 778
        },
        {
          "name": "logger.info",
          "line": 780
        },
        {
          "name": "time.time",
          "line": 781
        },
        {
          "name": "create_streamlined_pattern_map",
          "line": 783
        },
        {
          "name": "logger.info",
          "line": 791
        },
        {
          "name": "logger.info",
          "line": 793
        },
        {
          "name": "visualize_pattern_map",
          "line": 794
        },
        {
          "name": "os.path.join",
          "line": 796
        },
        {
          "name": "save_pattern_map",
          "line": 797
        },
        {
          "name": "os.path.join",
          "line": 799
        },
        {
          "name": "logger.info",
          "line": 802
        },
        {
          "name": "timer",
          "line": 804
        },
        {
          "name": "divmod",
          "line": 806
        },
        {
          "name": "divmod",
          "line": 807
        },
        {
          "name": "logger.info",
          "line": 809
        },
        {
          "name": "logger.info",
          "line": 810
        },
        {
          "name": "logger.info",
          "line": 811
        },
        {
          "name": "logger.info",
          "line": 812
        },
        {
          "name": "logger.info",
          "line": 813
        },
        {
          "name": "logger.warning",
          "line": 765
        },
        {
          "name": "time.time",
          "line": 790
        },
        {
          "name": "open",
          "line": 800
        },
        {
          "name": "f.write",
          "line": 801
        },
        {
          "name": "torch.cuda.is_available",
          "line": 767
        },
        {
          "name": "datetime.now",
          "line": 770
        },
        {
          "name": "len",
          "line": 778
        },
        {
          "name": "len",
          "line": 778
        },
        {
          "name": "int",
          "line": 810
        },
        {
          "name": "int",
          "line": 810
        }
      ],
      "docstring": "Run the streamlined pattern mapping.",
      "code_snippet": "        return create_pattern_map(dataset, device, sample_limit=sample_limit)\n\ndef main():\n    \"\"\"Run the streamlined pattern mapping.\"\"\"\n    overall_start_time = timer()\n    \n    parser = argparse.ArgumentParser(description=\"Create streamlined pattern map for dataset\")\n    parser.add_argument(\"--use-enhanced-mapper\", action=\"store_true\",\n                        help=\"Use enhanced pattern mapper with specialist integration\")\n    parser.add_argument(\"--sample-limit\", type=int, default=None,\n                        help=\"Maximum number of samples to analyze\")\n    parser.add_argument(\"--output-dir\", type=str, default=\"benchmarks/semantic_maps\",\n                      help=\"Directory to save pattern map and visualizations\")\n    parser.add_argument(\"--disable-parallel\", action=\"store_true\",\n                      help=\"Disable parallel processing for pattern extraction\")\n    args = parser.parse_args()\n    \n    if not isekAI_components_available:\n        logger.warning(\"isekAI pattern recognition components not found. Using fallback implementation.\")\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    logger.info(f\"Using device: {device}\")\n    \n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    \n    output_dir = args.output_dir\n    pattern_vis_dir = os.path.join(output_dir, f\"pattern_vis_{timestamp}\")\n    os.makedirs(output_dir, exist_ok=True)\n    \n    logger.info(\"Loading CIFAR-10 dataset...\")\n    trainset, testset = load_cifar10_data(augment=False)\n    logger.info(f\"Dataset loaded: {len(trainset)} training samples, {len(testset)} test samples\")\n    \n    logger.info(\"Starting streamlined pattern mapping...\")\n    pattern_start_time = time.time()\n    \n    pattern_map = create_streamlined_pattern_map(\n        trainset, \n        device,\n        sample_limit=args.sample_limit,\n        use_enhanced_mapper=args.use_enhanced_mapper\n    )\n    \n    pattern_elapsed = time.time() - pattern_start_time\n    logger.info(f\"Pattern mapping completed in {pattern_elapsed:.2f} seconds\")\n    \n    logger.info(\"Creating pattern map visualizations...\")\n    visualize_pattern_map(pattern_map, pattern_vis_dir)\n    \n    pattern_map_path = os.path.join(output_dir, f\"cifar10_streamlined_pattern_map_{timestamp}.json\")\n    save_pattern_map(pattern_map, pattern_map_path)\n    \n    latest_path = os.path.join(output_dir, \"latest_pattern_map_path.txt\")\n    with open(latest_path, 'w') as f:\n        f.write(pattern_map_path)\n    logger.info(f\"Latest pattern map path saved to {latest_path}\")\n    \n    overall_end_time = timer()\n    total_execution_time = overall_end_time - overall_start_time\n    hours, remainder = divmod(total_execution_time, 3600)\n    minutes, seconds = divmod(remainder, 60)\n    \n    logger.info(f\"\\n========== TOTAL EXECUTION TIME ===========\")\n    logger.info(f\"Total time: {int(hours):02d}:{int(minutes):02d}:{seconds:.2f}\")\n    logger.info(f\"Total seconds: {total_execution_time:.2f}\")\n    logger.info(f\"Pattern map saved to: {pattern_map_path}\")\n    logger.info(f\"Visualizations saved to: {pattern_vis_dir}\")\n\nif __name__ == \"__main__\":\n    main()"
    }
  },
  "constants": {}
}